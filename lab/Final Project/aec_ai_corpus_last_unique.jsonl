{"url":"https:\/\/geospatialworld.net\/news\/spatialinfo-selected-by-bsi-for-mapvantage-network-mapping-and-monitoring-solution\/","title":"SPATIALinfo Selected by BSI for MapVantage Network Mapping and Monitoring Solution","date":1023235200000,"text":"SPATIALinfo, a supplier of network and spatial information solutions, has announced an agreement with Broadband Services, Inc. (BSI), the leading outsource provider of integrated supply chain management, network design and fulfillment services, under which SPATIALinfo\u2019s products will be incorporated into BSI\u2019s MapVantage\u2122 offering to the Cable and Broadband industry.\nUnder the agreement, which was announced in a press conference at the Society of Cable Telecommunications Engineers Cable-Tec Expo here, BSI will utilize SPATIALinfo software that enables MapVantage to translate information from cable system databases to graphical representations of system architectures.\nMapVantage is the first cost-effective intelligent mapping, network asset management and data storage solution for the communications industry. MapVantage has been created to enable HFC network providers to seamlessly aggregate, correlate and analyze physical network assets residing on both legacy and next-generation networks in a cost saving centralized manner.\n\u201cOne of the defining features of MapVantage is the ability of every department at the cable system and corporate levels to view and manage system and subscriber data in real time,\u201d said Andy Paff, CTO, Broadband Services, Inc. \u201cUsing SPATIALinfo\u2019s software, we\u2019ve provided our customers with the tools they need to maximize the value of the information they have generated.\u201d","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/chaos-group-video-showcases-latest-developments-in-real-time-ray-tracing\/","title":"Chaos Group video showcases latest developments in real-time ray tracing","date":1565654400000,"text":"Project Lavina now includes animation support, Nvidia RTX GPU scaling, a new noise algorithm and more\nChaos Group has previewed major features coming to its Project Lavina real time ray tracing technology in a new video it showed at Siggraph earlier this month.\nThe video includes a one-billion polygon KitBash3d city designed by Blizzard Entertainment\u2019s Evan Butler and a fully interactive walkthrough of Kevin Margo\u2019s CONSTRUCT construction site, including looping fight animations that were displayed at a minimum of 24 frames per second, using an RTX Studio laptop. With the ability to support multiple RTX GPUs, Chaos Group projects VR frame rates in the near future.\n\u201cWe\u2019ve come a long way in five years,\u201d said Lon Grohs, Global Head of Creative at Chaos Group. \u201cWhen CONSTRUCT started, Kevin was visualizing characters at 24 frames a second with over $250,000 in hardware behind him. Today, he could just use Lavina, achieving full ray tracing with even higher fidelity on a $1,000 RTX card. Artists have never seen anything like this. Now that it\u2019s within reach, it\u2019s going to change everything.\u201d\nChaos Group\u2019s real-time ray tracing technology is designed to fully ray trace massive 3D scenes without workarounds or raster graphics.\nMeanwhile, to see how Kohn Pedersen Fox (KPF) is using Project Lavina and other real time visualisation technologies, check out Cobus Bothma\u2019s presentation at NXT BLD\nTo express interest in the Project Lavina beta, sign up here.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/new-kid-on-the-block\/","title":"New kid on the block","date":1406678400000,"text":"There is a new player in the Building Information Modelling world. French CAD giant, Dassault Syst\u00e8mes, is tailoring its powerful Catia modelling solution to address all aspects of BIM\nIn 2011 Bernard Charles, CEO of the French high-end CAD developer Dassault Syst\u00e8mes (DS) told AEC Magazine that it was going to get into the AEC market with a product called SolidWorks Live Buildings.\nUp until that point, DS was mainly a provider of engineering design and management solutions, especially for automotive, aerospace and product design firms. Its flagship product, Catia, is the Ferrari of the CAD world and comes with an equivalent price tag.\nThe decision to brand its AEC solution under the SolidWorks brand hopefully pointed to a more mid-priced solution.\nThe dream here is that DS may actually produce an AEC design system that can handle huge models and data complexity in the same way its customers, such as Boeing, use Catia to work on the 777 and the Dreamliner.\nToo many BIM solutions lack scalability and require models to be cut up to be managed in system memory. In the past, DS partnered with Gehry Technologies, the consulting and technology arm of Frank Gehry\u2019s practice, by part funding and licensing Catia to enable Gehry to create an AEC flavour, which was eventually called Digital Project (DP).\nDigital Project had limited success in signature architectural and structural firms, most notably used by Gehry\u2019s own firm and Zaha Hadid. Eventually DP became more a tool for Gehry Technologies to use in its own consulting business. In the subsequent three years of development there have been some changes in thought and direction of approaching the AEC market at DS.\nNow coming to market, the AEC product is no longer branded under the \u2018SolidWorks\u2019 mark or called Live Buildings. DS has also rebranded Catia V6 as \u20183DEXPERIENCE Platform\u2019 and this year has started to announce AEC applications that run on this 3DEXPERIENCE architecture, namely \u2018Building Space and Planning\u2019, and now \u2018Fa\u00e7ade Design for Fabrication\u2019. Cloud Management\nTo be honest the current BIM developers have only just started to address the collaboration issues that have maligned group work. Any demonstration of the DS AEC portfolio starts with logging into its management portal, powered by its Enovia engine, which is a mature management tool, originally developed for the company\u2019s manufacturing clients.\nWith the V6 generation of products, DS has embraced the cloud and this Enovia management tool is, by default, hosted on the Internet or can be run behind your own firewall, as a project Intranet.\nDS realised that AEC projects run differently to those of manufacturing firms, so offers a different interface more suited to project work than a pure file-based system. Enovia also integrates community capabilities, powerful tag-based project search, user-specific pages, project management (gant charts), workflow and \u2018widgets\u2019 for quick access to functionality.\nUsing the Enovia platform project teams can access project models at a granular level, locking out components in work packages. This way it would be possible to work on parts of a design collaboratively, although this requires an active connection.\nIt is also possible to work offline by downloading the model locally for resubmission when a connection is available. There are also viewing tools, such as 3DPlay, which uses DS\u2019 lightweight 3Dxml format.\nThis enables other project members to view and interrogate project data either on a workstation or mobile device. The DS vision of the cloud is one which blurs local desktop with functionality on the Internet. Its EXPERIENCE Platform is architected to use Enovia on the cloud, so all the data is centrally located.\nThe Catia portion is a desktop downloadable, while some applications and features may be hosted online or locally. To the end user this appears seamless, unless heavy network traffic causes transaction delays. Massing All this sounds like where the next generation of BIM tools should start, however, those looking for a \u2018Revit killer\u2019, will have a long wait.\nAs the development stands, there are two main AEC applications: Buildings Space Planning and Fa\u00e7ade Design for Fabrication \u2014 neither of which will replace any of the mainstream BIM tools. With Catia as the geometry creation platform, all of DS\u2019 AEC tools work in a common environment. Buildings Space Planning (BSP) is more akin to SketchUp than it is Revit or ArchiCAD, however it is considerably more capable and potentially points the way to eventually becoming a full BIM solution.\nFor now BSP is a mass modeller, where layout sketches can be made, floors automatically added and quickly turned into masses. Using Catia\u2019s dynamic constraints management system, parametrics can be associatively applied to drive the geometry. It\u2019s all easy to use and made incredibly simple with a push \/ pull interface. Holes can be punched in walls and simple geometry created. Advanced functionality includes complex roofing, spline-drive geometry and curtain walls.\nThe model can also be driven from space dimensions in a spreadsheet, which import as boxes conforming to the required area, which can be moved, placed and edited in the mass model. One assumes that DS is working on the next step but for now, the company is cleverly opting to find holes in the current BIM market offerings, to potentially offer something relevant, than truly competitive.\nFa\u00e7ade Design Catia is an industry proven design to fabrication tool and comes from a strong manufacturing heritage, so complex fa\u00e7ade design, the cutting of sheet metal, steel, and glass assemblies is an obvious application area in which DS could pick a BIM fight. Fa\u00e7ade Design for Fabrication enables the modelling of all aspects of a fa\u00e7ade, from pannelisation concept, array through to details such as fixings and sheet metal parts. Ensuring constructability is a direct benefit of this system.\nFa\u00e7ades can be calculated to follow complex paths using DS\u2019 Knowledge Pattern Editor which uses a powerful scripting language to compute potential fa\u00e7ade solutions. These scripts can be developed and saved as templates for future use. In some way this is similar to Grasshoper for Rhino, Bentley\u2019s Generative Components or Autodesk\u2019s Dynamo.\nI imagine an expert user developing templates for design teams to use to populate in ongoing projects. Industry Foundation Classes (IFC) DS has adopted IFC as a core format for exchange and collaboration. There are a wide range of import, export and filter options in the base package, which at least allows the current applications to play friendly with the existing tools.\nWhile DS has yet to produce a proper BIM modeler, it can import BIM models from Revit, ArchiCAD, Tekla Structures and other popular products. DS is taking an active interest in the development of IFC 4.0 and is keen to support industry common standards. Conclusion On a first viewing, given the wide gamut of the industry these are very disparate and focused solutions \u2013 concept \/ mass modelling and fa\u00e7ade design.\nBut I can see the Catia and Enovia backbone just waiting for additional DS AEC applications to take this to the next level. A single platform solution with cloud management and collaborative capabilities would have great potential but DS is jumping into a market with competitors who have about 10 years development on them. However, we can\u2019t rule out DS to become a serious player in this industry, and DS was an original investor in the firm that developed Revit and financially benefitted from selling it to Autodesk.\nDS knows the strengths and weaknesses of Autodesk\u2019s flagship BIM tool. The key elements of the DS offering are: outstanding geometry capability, cloud collaboration built-in, industry proven project and document management tools and the ability to handle complex assemblies and display big models. Could DS eventually target advanced Revit customers who have run out of leg room? If so, there seems to be a long way to go with huge gaps to fill in.\nTo enter the AEC market DS also has a considerable disadvantage in that it doesn\u2019t have a network of knowledgeable AEC resellers around the globe. It does have what it calls the \u2018Professional Channel\u2019, which sells SolidWorks mainly to manufacturers and the \u2018Value Solutions Channel\u2019 of 3DEXPERIENCE (Catia) resellers.\nFor now it seems the Value Channel will be the main route to market in AEC for DS, despite the lack of experience in selling to the AEC market. There will be a learning curve here and will probably lead to the DS channel focusing on a small number of large AEC practices. When it comes down to price of anything I\u2019ve seen, this is a complete unknown and is certainly my biggest bugbear with the DS AEC approach to the market.\nDS doesn\u2019t have list prices for its products. Prices are only revealed to prospects and, anecdotally, vary considerably. In the UK there\u2019s an old adage, \u2018If you have to ask how much something is then you can\u2019t afford it\u2019 and the AEC market is exceptionally price sensitive. Functionally DS\u2019 Buildings Space Planning tool compares to SketchUp, which is free and McNeel Rhino, which is \u00a3995.\nGiven that DS applications are built on top of Catia, as a prerequisite, which can costs thousands depending on the applications, I can\u2019t help but see this as a disincentive for potential users, fearing a hefty price tag. DS needs to at least recognize that concept \/ mass modeling and detail design are catered for by commodity products and should be priced as such.\nAlthough the idea of using Catia for mass modeling is like using a Formula 1 car to pop to the shops. The Fa\u00e7ade design may prove more popular as it\u2019s an area in search of a good solution. DS has stated that it intends to address all the design disciplines in the AEC space but it\u2019s going to take a few years more development to complete that goal and currently either needs to find some customers with a long view to adopt the current offerings or a firm that has a specific need for a fa\u00e7ade system development.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-hedwig-heinsman-dus-architects-aectual\/","title":"Video: NXT BLD 2018 London conference - Hedwig Heinsman, DUS architects \/ Aectual","date":1530144000000,"text":"Aectual construction \u2013 sustainable, customizable, 3D printed. \u2013 NXT BLD London, June 2018\nThe morning keynote came from Netherlands-based, Hedwig Heinsmann of DUS Architects and Aectual. In her mind-blowing talk, she took us through her practice\u2019s experimentation with designing and producing 3D printed buildings, together with exploring new environmentally friendly materials with which to print them.\nHeinsmann explored a number of her projects, including the 3D Print Canal House, and the developments that her team had made in creating a reliable 3D printer which could produce building parts at scale. DUS has a factory which produces 3D components for other architectural firms.\nThe second part of her talk covered a new start-up called Aectual, which utilises industrial robots in construction. At the moment, Aectual has deployed robots in fa\u00e7ade design, the production of moulds and creating complex and beautiful 3D printed floors.\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-autodesk-r2-update\/","title":"NEWS: Autodesk releases Revit 2015 R2 featuring Site Designer Extension","date":1410912000000,"text":"We recently announced the beta version of Autodesk\u2019s \u2018Sundial\u2019, its mid-life update to Revit. We are just in time to announce the launch of Revit 2015 R2\nWith Autodesk now striving for a pure Subscription-based customer base, the old days of the yearly upgrade clearly will not cut it anymore. When you pay a subscription fee you expect more than just the next release at the end of the subscription period. With adding value in mind, Autodesk\u2019s AEC team had already planned a mid-life \u2018beta\u2019 refresh of Revit, with plenty of new and expanded features to please its installed base.\nThe preview of Sundial showed off some other technology that Autodesk has in development: namely the ability to run Revit as a hosted application. This means that Revit is not installed on a local hard drive, but hosted on Autodesk\u2019s cloud servers. Revit has all the same functionality but is run remotely with the graphics appearing real time in a browser. The Sundial version of Revit can be run without any large downloads or installs and could even be used on a system with an existing Revit deployment.\nThis is a very clever move on Autodesk\u2019s behalf, allowing users to experience running Revit on a hosted service. It does not mean that Autodesk is going to be removing the desktop version, but you can expect that running Revit over the web will be a valid option soon and, depending on your Internet connection, may prove to be one of the fastest ways to run the software, especially if your local machine currently struggles with large models.\nSundial R2 features\nAutodesk Revit 2015 R2 includes over 30 user-requested features. Autodesk breaks down the R2 advantages into Power, Performance and Productivity.\nIn the power category, R2 has seen enhancements to Industry Foundation Classes (IFC) model referencing and can now link existing geometry in an IFC file as reference for dimension, alignment, snapping and hosting of some families in a Revit model.\nThere is a new \u2018Reveal constraints mode\u2019 to assist in the troubleshooting of \u2018why did it do that?\u2019 moments. In the new mode, Revit will make all dimension constraints and alignment constraints visible in the current view.\nAutodesk\u2019s open source computational design tool Dynamo is now automatically installed with Revit. Using visual programming and popular scripting, it is possible to drive Revit geometry by programmatic means, super charging the design process. If you have not tried Dynamo we highly recommend it, for all those designers into McNeel Grasshopper and Bentley Generative Components (GC).\nAutodesk also announced Project Solon with R2, which provides web-calculated dashboards to give accurate energy analysis results from Green Building Studio, accessible through the Result and Compare dialog.\nPerformance is one of the all important updates for advanced Revit users. Autodesk says R2 is faster. Complex topology generation and building pads gets a boost. The energy analysis engine is faster and uses less memory on large models.\nRendering is faster and updates views with multiple instances of part families quicker. Links in views are also quicker.\nIn terms of productivity, R2 brings the capability to work in perspective views, which means you can make quick adjustments without having to change the style of view. There is a single click group option for quick editing of \u2018wall join\u2019 behaviour. The Type Selector gets search capabilities and PDFs can be navigated with hyperlinked views.\nSite Designer\nThe biggest surprise in R2, which will get the warmest welcome, is that Autodesk is finally delivering a Revit extension that provides access to site planning tools. Called Site Designer Extension for Revit 2015, it is available for select Subscription customers and is based on an acquisition of site planning software formerly known as SiteWorks from Eagle Point Software Corporation.\nSite Designer enables customers to shape the terrain for building sites. It runs natively within the Revit environment with the site design becoming part of the overall building model.\nUsers can model alternatives for mass grading, building pads, streets, pavements\/sidewalks, parking spaces and retaining walls within the familiar Revit environment.\nSite Designer automatically updates the underlying toposurface and can then be shared between Revit and Autodesk AutoCAD Civil 3D 2015 software through Land XML files, improving collaboration between architects and engineers.\nEnhancements in Revit R2 specifically optimise the performance of Site Designer by speeding the edit and regeneration of complex site design elements.\nConclusion\nAutodesk Revit R2 delivers a number of great new tools; and the site design element has been a long time in coming. IFC referencing improvements will be well received in the UK with the looming 2016 deadline for mandatory Building Information Modelling (BIM) usage on public sector projects and a corresponding increasing use of IFC files in federated projects.\nThere has been no mention with this release of Autodesk\u2019s stated intent to try and retrospectively add some of these new features into previous releases of Revit, thus adding value to users stuck on old releases due to project constraints. Maybe there will be more on this later.\nWe are also waiting to hear on work to the Revit graphics pipeline that will enable it to get acceleration from GPU-enabled graphics cards.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/csi-wireless-awarded-patent-for-dgps-signal-filter\/","title":"CSI wireless awarded patent for DGPS signal filter","date":1057536000000,"text":"CSI Wireless Inc., a designer and manufacturer of advanced wireless and GPS products, announced that it has been awarded a patent on its new ceramic frequency filter for differential GPS (DGPS) correction signals. The filter uses a ceramic material, which permits increased sensitivity, better signal acquisition, and less interference than traditional filters using metal inductors. The ceramic filter is utilized in all of CSI\u2019s beacon receivers, providing high performance across the entire line of receivers. It comes encased in a single unit for plug-and-play integration \u2013 providing a quick, easy, low-cost solution without any tuning requirements. The patented filter is the key element in CSI Wireless\u2019 MBX3 differential GPS receiver. The United States Department of Agriculture (USDA) has taken delivery of more than 5000 MBX3 units in the last 12 months through distribution partner 3D Marketing LLC, as announced in CSI Wireless\u2019 press release of April 16, 2003. Some of CSI\u2019s largest marine customers also purchase CSI\u2019s combination GPS and beacon receivers and antennas, featuring the ceramic filter, which provide the best location accuracy in a highly integrated product. \u201cIntegrating the ceramic filter into our GPS products gives the products important price and performance advantages over the competition and enables to offer the customers lower prices while also improving the margins. Typical frequency filters are constructed using metal inductors and must be individually tuned and tested for frequency accuracy. These inductor-based filters can fall out of tune over time. Our ceramic filter is shipped completely tuned \u2014 and stays fully tuned. Frequency filters lay between the antenna and the receiver. They are essential components for any type of radio receiver because they filter out unwanted frequencies so the receiver accepts only the \u201cchannels\u201d that are required for the application in question. CSI\u2019s ceramic filter for DGPS rejects all unwanted frequencies and precisely isolates the differential correction signals broadcast from beacon reference stations around the world. The signals are then cleanly transmitted to CSI\u2019s beacon receiver engine.Ceramic material has a very high quality for precise tuning to a particular\nfrequency with minimal susceptibility to out-of-band interference, a very common problem in low-frequency radio spectrum where differential signals reside. Ceramic is also very stable over temperature variations. The most significant advantage of the ceramic filter architecture is the increased out-of-band rejection. This permits the receiver to digitize the entire beacon band so search-and-tracking algorithms can be implemented in software rather than in hardware. The sampling of the entire band permits multiple channels to be tracked with software. The high rejection of the ceramic filter also permits the digitizing rates to be lower, which reduces power consumption and processor loading. As a result, CSI\u2019s simplified solution decreases overall manufacturing costs.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/nxt-bld\/nxt-bld-goes-virtual-8-14-october\/","title":"NXT BLD Virtual: 8-15 October 2020","date":1600473600000,"text":"From 8-15 October 2020, we\u2019ll be hosting our first virtual conference, taking place entirely online in daily 90 minute sessions. Our line up of speakers is unparalleled, tickets completely free, and the event fully interactive with a live Q&A\nAs 2020 continues to do its worst, NXT BLD (Next Build), AEC Magazine\u2019s and Lenovo\u2019s conference on future AEC technology will not be suppressed any longer. While the physical event has been put back to 16 June 2021, for obvious reasons, we will put on our first virtual variant starting 8 October 2020, running 9, 12, 13, 14, 15 October.\nNXT BLD Virtual will be broken up into 90 minute instalments to run in the afternoon (UK BST) over the five days.\nEach session will start with three 20 minute presentations followed by 30 minutes of live Q&A with the speakers. It\u2019s completely free to register and you\u2019ll also get to view the content online afterwards.\nFor those unfamiliar with NXT BLD we bring together a mix of researchers, practitioners and technologists to give a series of talks on future technologies, innovative projects and updates on the state of research from around the globe. There are presentations about: robots, AI, blockchain, digital fabrication, modular, digital twins, realtime rendering, VR\/AR, knitted buildings, 3D printing, laser scanning, computational and generative design and lots more. Past talks can be seen here.\nConference lineup\nThis year we have an incredible lineup of speakers from across the globe.\nThe Open Letter to Autodesk group\nWe\u2019re very excited to welcome the Open Letter to Autodesk group who will be discussing the future of Digital Design workflows. The vocal group of architectural practices, who wrote that infamous letter to Autodesk\u2019s CEO, will be discussing what they\u2019d like to see \u2014 and what the industry needs \u2014 from future design technologies.\n\u2022 The future of Digital Design workflows\nMollie Claypool \u2013 Automated Architecture\nMollie Claypool is an architecture theorist, critic and educator at the Bartlett. Her recently released book, Robotic Building: Architecture in the Age of Automation caught our attention and we are thrilled to have her speak.\n\u2022 Robots in construction\nNate Miller \u2013 Proving Ground\nNate Miller is founder of Proving Ground, assisting some of the biggest firms with their tricksiest data and computation problems. We asked Nate to look at the future of conceptual design tools.\n\u2022 The future of conceptual tools\nAlex Coulombe \u2013 Agile Lens\nAlex Coulombe of Agile Lens will be looking at the development of VR and its use in architectural design, through a real world project over a number of years, as the technology matured and the quality improved dramatically.\n\u2022 Advanced VR for architectural design\nBruce Bell \u2013 Facit Homes\nBruce Bell of Facit Homes, the first bespoke design to digital fabrication residential practice, has spent the last year looking at what new technologies are coming to building fabrication.\n\u2022 The future of digital fabrication\nElif Erdine \u2013 Architectural Association\nElif Erdine, director of Emergent Technologies at the AA, has done some fascinating research into computational design, robotic assisted fabrication and construction, all of which made her a natural choice for NXT BLD.\n\u2022 Parametrics and materials\nMark Taylor \u2013 Royal BAM Group\nMark Taylor, senior digital construction manager at Royal BAM Group has been involved in developing new technologies for on and offsite construction. Mark will look at how technologies like 3D printed concrete are expanding Royal BAM\u2019s capabilities.\n\u2022 3D printed concrete \/ digital fab\nJulie Dorsey \u2013 Mental Canvas\nJulie Dorsey is the Founder and Chief Scientist of Mental Canvas, a New York-based software company whose mission is to reinvent visual communication by transforming the medium of drawing \u2013 its conception, creation and consumption. Julie will show you conceptual design like you\u2019ve never seen before.\n\u2022 Conceptual design \u2013 the future of drawing\nTal Friedman \u2013 Tal Friedman Architecture & Design\nTal Friedman is an architect and construction-tech entrepreneur who is active in automated algorithm-based design-to-fabrication, will explore the transition from parametric architecture to digital construction.\n\u2022 Digital Construction\nElena Casini \u2013 Most Architecture \/ Fernando Garcia Blanco \u2013 Foster + Partners\nFernando Garcia Blanco (Foster + Partners) will join Elena Casini (Most Architecture) to give a fascinating insight into how humans and robots will interact in buildings of the future \u2014 a future where robots build, maintain and learn, and coexist with humans in a natural environment.\n\u2022 Human-robot interaction\nAlexander T\u00fcrk \u2013 Aeditive\nAlexander T\u00fcrk of Aeditive will share new developments in construction 3D printing.\n\u2022 Construction 3D printing\nKen Pimentel \u2013 Epic Games\nKen Pimentel is AEC Industry Manager at Epic Games. Through Unreal Engine the company is pioneering high poly, real-time rendering for architectural visualisation and Ken will look at what\u2019s coming next.\n\u2022 Real-time visualisation\nChris Ruffo \u2013 Lenovo\nScott Ruppert, Chris Ruffo (pictured) and Mike Leach will discuss survey results and feedback collected by Lenovo about how Covid has changed they way people work.\n\u2022 How Covid has changed the way AEC firms work\nAndrew Rink \u2013 Nvidia\nAndrew Rink of Nvidia will share the latest on Omniverse, an exciting new platform technology that brings together architects and other stakeholders in a visually rich, real-time collaborative environment.\n\u2022 Collaborative workflows with visual computing\nMichael Katz \u2013 AMD\nAMD\u2019s Michael Katz will show how Threadripper Pro, a beast of a CPU with up to 64-cores that makes the new Lenovo ThinkStation P620 tick, is transforming AEC workflows.\n\u2022 High performance workstations\nHilmar Gunnarsson \u2013 Arkio\nJohan Hanegraaf and Hilmar Gunnarsson will give their presentation from within VR, showing the latest developments in their groundbreaking collaborative conceptual design tool Arkio.\n\u2022 VR for conceptual design\nCheck out the NXTBLD website for more details and the full line up of speakers.\nVirtual conference details\n8, 9, 12, 13, 14, 15 October 2020\n90 minute afternoon sessions (starting at 4.00pm BST)\nClaim your FREE conference pass at nxtbld.com\nDesign Computation I\/O\nThis year we will also co-host Design Computation I\/O, a start-up research conference on computational design methods, which has also gone virtual.\nIf you know anyone in computational design, it\u2019s definitely worth alerting them to this in-depth event designcomputation.org\/dcio\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/the-new-world-of-rapid-site-design\/","title":"The new world of rapid site design","date":1459382400000,"text":"From laser scanners and drones to the cloud and \u2018optioneering\u2019 it has never been easier to capture exisiting site conditions and explore new designs. By Randall S. Newton\nIf it has been a while since you thought about site work, and what it takes to get the information you need from a site to plan a building or infrastructure project, I hope you are sober. Because there is a dizzying array of new and improved technologies, as well as things you probably never thought would be part of construction site design. If you had a three-martini lunch today, you better read this some other time.\nFirst of all, scanning to create terrain models for design is getting big. Prices are coming down rapidly, and quality is continuing to rise. It is no longer necessary to spend \u00a3100,000 for a terrestrial scanner (but you can if you really need the top end).\nTotal stations, those ubiquitous threelegged devices surveyors take everywhere, can communicate with iPads. All the major CAD design applications can work with point clouds, the data gathered by scanners; processing those millions of tiny three-dimensional points of light (called \u201cvoxels\u201d) is no longer the computational nightmare it was a few years ago. Drones are a thing for gathering site data. And so are smartphone cameras.\nAll vendors involved in this new world of rapid site design agree there is value galore in the new ways of data gathering.\n\u201cBeing able to create accurate \u2018as-is\u2019 infrastructure models using point clouds makes it easier to analyse different roadway construction staging scenarios,\u201d says Karen Weiss, P.E., senior industry strategy manager for infrastructure owners at Autodesk. \u201cPrecise co-ordination of the existing environment and the new design has become vital in crafting staging that minimises disruption of service.\u201d\nWhat follows is a roundup of various new and\/or improved ways to gather site data and turn it into design data.\nPoint clouds arise\nLiDAR is the most common term today for the various remote-sensing technologies that illuminate a target (usually with a laser) and analyse the reflected light. The points of light are collectively known as point clouds. LiDAR data collection includes scanners in aircraft, satellites, railcars, and vehicles as well as tripods and other stationary installations. A common approach to LiDAR use today is to capture everything in sight and then assemble the data into a point cloud. A few years ago that would have been crazy, but today\u2019s faster computers and new software algorithms make it feasible.\nAs LiDAR technology continues to evolve, the focus has shifted from gathering data to efficiently managing, analysing and utilising the large 3D datasets collected by airborne and terrestrial scanners. Technicians use processing software such as Trimble RealWorks to combine data from multiple scans into a cohesive data set, removing noise, superfluous points and outliers as part of the quality control process. We are not at the point of tossing flying balls into a cave to get a 3D map in real time, like in the 2012 movie Prometheus, but it feels close.\nWho needs this data?\nThere are basically three client groups for this data. The first are those who want visualisation and basic measurements from orthoimages or point clouds, primarily planners and architects or engineers working up project presentations. The second client group are those who want the 3D data for design. A third group consists of industrial construction clients who turn to service providers to provider ready-to-work datasets of the very largest projects, such as process\/power plants and major civil engineering or infrastructure projects. Sometimes this last group works directly in the point cloud data without converting to CAD.\n\u201cIn the past, the bottleneck was always in the back office \u2014 3D modelling was largely a tedious manual process,\u201d says Chris Scotton, president of ClearEdge3D. His company offers products that turn scans into useful models with as little human intervention as possible. \u201cNew technologies such as automated feature extraction, object recognition and pattern recognition are propelling the industry closer to that mythical one-click model. We may never fully get there but we\u2019re certainly leaping forward.\u201d\nThe latest twist on gathering large volumes of terrestrial data is the use of drones (AKA Unmanned Aerial Vehicles or Unmanned Aerial Systems).\nConstruction data specialist Trimble has recently added drones to its product line-up. The Trimble UX5 UAS is commonly used to survey larger areas in applications such as agriculture, open pit mining, landfills and large construction sites. Rotary-wing aircraft including the Trimble ZX5 multirotor UAS are suitable for smaller sites and applications such as fa\u00e7ade measurement or facility inspection where aircraft agility is needed.\nTerry Bennett is a senior civil infrastructure strategist for Autodesk: \u201cThe ability to have personal aerial tools to capture project details is just amazing; it is redefining workflows and how we capture information. I have personally used UAS solutions on a couple of projects where we were able to fly the site in one hour, and then process that into a 3D model using more than 200 aerial photographs,\u201d says Mr Bennett. \u201cAnd the result has the same digital fidelity as if you had surveyed the site by hand.\u201d\nPhotographs join the club\nYes, he said photographs. Not only are LiDAR scans more affordable and more popular than ever, but software breakthroughs have made it possible to extract useful 3D data from standard photographs. Autodesk has several products for turning photographic images into useful 3D data, some of it aimed at product development or unconventional applications such as archaeology. ReCap is an Autodesk product that works with most of its design portfolio, using cloud-based data processing to turn both LiDAR data sets and photographs into accurate 3D scan data.\nBentley Systems is also keen to make the most of this new world of rapid site data gathering. At its recent Year in Infrastructure conference in London, it showed off a new line of ContextCapture technology, which can vacuum up all the site data you can throw at it. One recent project was the creation of a 3D model of Pope Francis\u2019s motorcade route through Philadelphia. Bentley executive vice-president Ray Bentley led the effort to create the visualisations, and said afterward even though he understood the capabilities of the software, it impressed him to see what it could accomplish in such a large-scale task.\nThe software generates a detailed reality mesh incorporating referenced photography. The result, Bentley says, is a navigable 3D model with fine and photorealistic detail, sharp edges, and precise geometric accuracy. These highly detailed models can be of virtually any size or resolution, up to city scale, and he says can be created much more quickly than with other technologies.\nBentley also offers Siteops, cloud-based software to use imagery as source material for exploring countless engineering options. Users can \u201coptioneer\u201d virtually unlimited layout, parking, grading, and drainage options for a site within hours.\nAutodesk\u2019s Mr Bennett puts this kind of visible and functional site information into perspective: \u201cThe ability to scan a site in a matter of hours and then immediately have all the information, and not miss anything the eye can see, streamlines how fast you can get site designs up and running in a 3D modelling application, to start doing alternative design \u201cwhat if \u201d explorations or to check quality of construction during construction.\n\u201cAfter the project is completed, the model and point cloud data can be used for asset maintenance going forward, for example with a civil infrastructure project, to check for any movement, shifts or changes to the site or structures over time.\u201d\nThis article is part of an AEC Magazine Special Report into the Future of Building Design, which takes a holistic view of the technologies and processes, which are set to change and enhance the AEC industry in the coming years \u2014 from concept design all the way to construction.\nClick to read the other articles that make up the report.\n1) Introduction New technologies are empowering architectural firms to improve quality, capabilities and process.\n2) Conceptual design There are a whole host of digital tools for early stage design experimentation.\n3) Rapid site design The rapid capture of site topology is being aided by new technologies.\n4) Benefits of 3D design Evolution, not revolution when making the move to 3D CAD.\n5) Moving to model-based design How to get from 2D to 3D, how to roll out training and how to overcome common issues encountered along the way.\n6) Design viz Advanced new rendering technologies are opening the door to design realism in architectural workflow.\n7) Design, analysis and optimisation Once you have a 3D CAD model, optimse your design for daylighting, energy performance and much more.\n8) Collaboration and model checking How to share models with clients, contractors and construction firms and test the quality of your model.\n9) Workstations What to look out for when choosing a workstation for 3D CAD.\n10) Virtual Reality New technologies are now available to support powerful new design workflows.\n11) 3D printing Architects are 3D printing architectural models with impressive results.\n12) Fabrication As building time gets compressed what will revolutionise fabrication and construction time?\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/preview-bricscad-bim\/","title":"BricsCAD BIM","date":1542931200000,"text":"Considering what a hot topic BIM has become, there have been very few new software firms willing to invest in creating new BIM authoring tools. This month we report on a new product, written on a \u2018familiar\u2019 package and backed by a huge company. Could BricsCAD BIM be the one to take on the might of Revit? By Martyn Day\nIt\u2019s been 18 months since we last saw BricsCAD, from Ghent-based developer, Bricsys. The company started out developing an AutoCAD clone called BricsCAD and did a great job at creating a low-cost alternative to AutoCAD. Since then it has gone beyond AutoCAD and developed all sorts of original functionality to its base platform application. Although the company is dedicated to maintaining compatibility with DWG and AutoCAD, it wants to be perceived as a CAD developer in its own right and from what we saw at the firm\u2019s recent London event, there is good reason.\nBricsys is not new to AEC or BIM. It developed TriForma which Bentley Systems sold as its core BIM application in the 1990s and then its own tool called Architecturals in the 2000s. The firm recently developed a SketchUp work-alike BricsCAD Shape. The company also develops tools for the manufacturing market with powerful parametric solid modelling (based on ACIS) and sheet metal applications.\nAEC Magazine saw the company\u2019s formative new BIM tools 18 months ago. The demonstration showed rectilinear ACIS solids being used to model a building and then the user applying IFC definitions post modelling. This turns products like Revit on their heads, as instead of using a palette of predefined and customisable objects to model with, like Lego, BricsCAD BIM was model first, then define. The benefit of this approach was that the conceptual phase didn\u2019t require the architects to worry about object definitions, just define the forms.\nThis was all happening on top of BricsCAD, which is a DWG drawing and modelling tool which offers LISP, ARX and all the familiar AutoCAD functions you would expect. Bricsys thinks that those who have not yet made the move to Revit, may have more 2D processes and be happier doing their BIM modelling in a familiar environment, with the added advantage of it being cheaper. As things stood 18 months ago, however, the BIM functionality seemed all rather basic and there was evidently a long way to go.\nA new owner\nAt last month\u2019s Bricsys conference in London, the first big news was that the company had been acquired by Hexagon for an undisclosed sum. For those unfamiliar with Hexagon, it is a global giant of a firm, that operates in construction, engineering, mining, automotive and plant and has annual revenues in excess of \u00a31.2 billion. It owns Leica, Intergraph, MSC Software, Z\/I Imaging and many others.\nWhile Bricsys had annual revenues of \u00a313 million, now it is under the Hexagon umbrella it will have all the resources it could ever need, plus access to markets as a mature, trusted CAD platform. Hexagon also has a reputation of leaving the brands it buys alone to carry on developing semi-autonomously.\nHexagon previously ported its AutoCAD-based plant application, CADWorx Plant Design Suite, to Bricsys as an alternative for its customers to paying for Autodesk\u2019s AutoCAD on subscription. Bricsys pricing is based on a perpetual licence and works on Windows, macOS and Linux. The entry-level BricsCAD is $825 forever; AutoCAD is $1,575 per year, LT is $390 per year. Bricsys also offers subscription at $312 per year, if that\u2019s what you would prefer. Bricsys works out a lot cheaper than Autodesk Subscription for AutoCAD. Hexagon was perhaps the biggest thirdparty developer to realise that it could use the familiar APIs that AutoCAD has, to port its existing applications to BricsCAD and offer to save its customers money on their AutoCAD subscriptions.\nBIM and development\nWhat a difference 18 months makes. From seeing the initial BIM development work to what was presented on the stage in London was really quite a marked difference. While the concept is the same, the capability and ease of use really blew us away. At its core, you have a DWG compatible, and very capable, drawing platform. Bricsys has harnessed the ACIS solid modelling kernel to enable designers to create incredibly complex shapes, using booleans, workplanes, chamfers, surfaces, complex wireframe manipulation \u2013 the usual gamut of modelling tools.\nDesigners can create anything they can envision in geometry. So, now comes the real magic. Bricsys has utilised machine learning to start the process of turning these models into BIM. Using the \u2018BIMify\u2019 button, the software will analyse the model and turn the geometry it finds into rooms, floors, slabs, columns, walls, doors, windows \u2013 regular IFC components.\nThe user can then edit or add objects that the automation didn\u2019t classify or omitted. This is seriously impressive and, unlike the first demo we had, it works well on some pretty crazy geometry. Zaha Hadid would have had some fun with this tool. This is a fantastic solution to the problem of linking conceptual modelling to the creation of BIM models, together with the added benefit of the backend processes such as drawing production being all in one package.\nThe curtain wall capability is also powerful; model your spline geometry, select face, create a grid and then dynamically manipulate to make traditional or freeform curtain walls. The software will generate all the frame elements, which of course can also be edited.\nBut as we all know, architecture is really only one component in the mix. Bricsys demonstrated some intelligent modelling of HVAC components within a BIM context. Automatic sizing and connections, together with some elementary auto complete capabilities gave a good indication of the way development was heading. Bricsys isn\u2019t looking to deliver for the front end of the building process, but throughout all professions, including structural and site development (it even handles point clouds).\nBecause BricsCAD BIM is built on a DWG platform, it\u2019s at this point that you really see the benefit in workflow vs something like Revit. We have lost count of the number of firms we have visited who take their Revit drawings into AutoCAD to finish off the documentation. This breaks with the BIM process as, if there are any changes, then the additional hours of drawing in AutoCAD need to be repeated. BricsCAD BIM is already inside its DWG documentation world and this destructive phase of using other tools than Revit can be avoided.\nSpeed was also very impressive. During the day\u2019s talks all product demonstrations were done live from laptops. The underlying BricsCAD platform is a modern CAD product, it is fast, uses multiple cores and benefits from fast GPU graphics card accelerators. While one would assume that modelling architectural elements in full on solids over lightweight geometry would lead to hefty and unwieldy models, that certainly does not seem to be the case.\nParametrics are also built-in systemwide and can be used in a very intuitive fashion when designing. The ability to quickly model floors and partitions was fabulous and at all times the mouse and drawing lines give real-time feedback on relative geometry. Floors can be replicated in just a few clicks. In 2011, Bricsys acquired the intellectual property rights from Russian developer Ledas, which developed very high-end 2D\/3D constraints tools and set up a technology division in Russia under Dmitry Ushakov. This move also enabled considerable advances for BricsCAD Platinum for Mechanical Design, which is aimed at the DS SolidWorks and Autodesk Inventors of this world.\nBricsys demonstrated something it calls A.I propagate, which uses artificial intelligence to replicate component details throughout a model. Simply select a component, plus the elements you want it to propagate over, and the computer does the rest. It also supports the import of RVT components, live building grids and ceilings. This can drastically speed up modelling. The user interface also really gives a clear indication of where the propagation of objects will take place.\nWhen at the documentation stage, BricsCAD has some fabulous capabilities for multi-view layout and for automating call-out details. Again, all in a DWG workflow and so common AutoCAD commands are used for drawing.\nBricsCAD BIM is $2,275 a seat, for a perpetual licence with one year\u2019s maintenance. Alternatively, it can be subscribed to for $910 a year. For comparison, Revit is currently $2,250 a year or $6,075 for a 3-year subscription.\n2D CAD\nBricsCAD started off life as an AutoCAD clone. The company then went on to develop functionality that you won\u2019t find in AutoCAD. As Autodesk went off to develop or acquire other tools (Revit, Inventor, Fusion etc), the mentality that AutoCAD was the hammer that hit all nails disappeared. Bricsys is of the mentality that its DWG CAD platform can be extended to solve BIM, MCAD solid modelling and other verticals. In one respect at least, the persistence of drawings in our new 3D modelled world, would back this view up.\nBricsCAD v19, the new version, will offer a range of powerful tools but we don\u2019t have room to highlight them here. However, one feature that we did see is indicative of the kind of tools under development. We had seen BIMify; now there\u2019s \u2018Blockify\u2019, huge models, 2D or 3D, measuring in the Gigabyte range, automatically turned into blocks, drastically reducing the file size. Again, machine learning at play; the software automatically detects equally shaped solids in a model and replaces them by block references and searches the drawing for an identical set of a 2D entities and replaces them by block references. Just look at AutoCAD 2019\u2019s new feature set and wonder where the new features are, vs enhancements to existing ones.\n24\/7 collaboration\nNot just content with developing products for every vertical market, Bricsys offers a cloud-based collaboration product called 24\/7, which is free to customers on maintenance. It\u2019s a global document management system for 2D drawings and 3D models, which supports multiple roles and access for defined users and has a graphical widget application development ability to automate repetitive tasks. The system has version control, activity logs and search capabilities. For BIM users it has a fast 3D data viewer, model annotation and DWG\/ xref management.\nManagement Q & A\nBricsys CEO, Erik de Keyser and Hexagon\u2019s Rick Allen, PPM executive vice president answered questions on the surprise acquisition (this came as a shock to most Bricsys employers too). de Keyser explained that the driver for the acquisition was that being a great developer was not enough for success; marketing and sales required investment and as things stood, would have diverted R&D funds to enable that. Bricsys had been seeking investment and Hexagon had been interested since it ported its plant products to BricsCAD.\nAllen explained that Hexagon was keen to offer customers a choice and with Autodesk\u2019s pricing changes, subscription- only and potentially web-only products in the future, Bricsys would be there to offer an alternative. In the process of porting CADWorx to BricsCAD, Bricsys was incredibly responsive and over eight million lines of code were ported in nine months and the product is stable.\nAllen, while predominantly dealing with the plant side of Hexagon\u2019s business, sees the Bricsys acquisition as a play in all areas BIM, manufacturing, drafting, as well as plant. Allen identified that Revit, while popular in architecture, isn\u2019t in other professions and they have picked up on a lot of end user dissatisfaction. While Hexagon will certainly focus on AutoCAD seats in its process plant customers, not just CADworx, Hexagon is looking at a much broader play.\nDe Keyser estimated that Revit had only hit 16% of BIM penetration of all the possible users and in certain geographies it was not the No.1 choice, with Vectorworks, Graphisoft and Allplan all occupying decent market share. de Keyser stated that with Hexagon behind them, Bricsys would aim for 20-25% of the market with the sales pitch being \u2018you don\u2019t need to leave DWG or translate to\/from DWG. The workflow stays in one format as you move from application to application \u2013 at a fraction of the price. We will win one customer at a time.\u2019\nLooking forward, de Keyser stated that with IFC 4.0, it will become harder for firms, such as Autodesk, to trap customers in proprietary file formats, as substantially more of the data will be transferable.\nWhile it\u2019s the plant division of Hexagon that led the charge to acquire Bricsys, Allen made it clear that all divisions of Hexagon would have access to the technology and the buy-in was not simply cost justified on the benefit to the plant division. The company is taking aim at Autodesk and all players in BIM, mechanical, plant and drafting. With Autodesk dominating so many areas in the built space and with Autodesk Subscription increasing the cost of ownership, Hexagon is out to offer a lower cost alternative with enhanced functionality.\nConclusion\nMany years ago, we watched Dominic Gallello, then Autodesk VP of Mechanical, launch Autodesk Inventor. This was a big deal for Autodesk as it was the first new code stream the company had delivered since AutoCAD decades before. At the time, Autodesk was trying to get into the 3D modelling market and had established players such as SolidWorks, Dassault Syst\u00e8mes, Siemens, PTC etc. all with mature CAD products.\nGallello\u2019s pitch was don\u2019t judge us by the current feature set, judge us on the product\u2019s \u2018velocity\u2019. Gallello was telling us the development team had new ideas to crack old problems and would be delivering on new functionality in a way the mature competition was not.\nWith BricsCAD BIM, we see that development velocity. In just 18 months the software has gone from being formative building modelling to offering something really different and innovative for BIM workflows. At the same time, it bashes you over the head as it\u2019s all happening in an AutoCAD work-a-like, which is committed to supporting all the 2D processes millions of AutoCAD customers currently use.\nWe are in strange times. Autodesk\u2019s move to Subscription without question increases costs to users in ownership over three years, compared to the traditional historic buy and upgrade cycles of around three years. Subscription does offer suites but that comes at a cost and not many utilise many of the products they get.\nAutodesk\u2019s channel is showing signs of rebellion, after years of deflating margins, with one or two now offering AutoCAD clones as alternatives to AutoCAD LT. This is a clear and present danger, as we estimate 40% of all Autodesk licences are AutoCAD LT. Revit\u2019s development velocity has slowed and many are wondering when it will be re-written as the core is 20 years old. The conceptual side of BIM has never really been solved by anybody who can take that data on to be used throughout the design and documentation workflow.\nBricsys now has a giant backing it up. A giant that sees an opportunity in Autodesk\u2019s base. Bricsys offers low-cost, AutoCAD functionality with perpetual licensing (or subscription), on multiple platforms, with unique BIM workflows, in a single environment, with machine learning cleverness, powerful MCAD part and assembly modelling, Autodesk APIs and even collaborative cloud-based management.\nAt the wrap up from the event, de Keyser took issue with the company being called a \u2018clone developer\u2019 by the press during that day\u2019s tweets. He feels the development teams are now way beyond performing that work and are developing new functionality in all vertical areas. Even though the origins of the company are certainly defined by the work it has done on mimicking DWG functionality, it\u2019s true that Bricsys is now defining a unique path. de Keyser quipped, \u201cWe are not a clone developer, we are a cyclone in the industry!\u201d\nFor now, BricsCAD BIM is certainly something to watch. Revit has decades of development work put into it and I can\u2019t see many switching like for like. However, with the velocity comes a trajectory and backed up with Hexagon, with a low price point, innovative machine learning development, this is a BIM product to watch. It\u2019s the most promising modelling tool we have seen in BIM in the last ten years.\nNext year will certainly be an interesting one in the BIM market. Hexagon has come to the table with Bricsys and a very impressive development team. We hear Dassault Syst\u00e8mes has a construction BIM tool coming out. Siemens and Bentley continue to dance together and are working on many joint developments. Nemetscheck has a new Allplan and Vectorworks and Graphisoft are continuing to execute well. Things are hotting up and new and innovative solutions are still coming to the market. AEC Magazine will be here to keep you up to date.\nDownload a trial version of BricsCAD BIM at \u25a0 bricsys.com\/bim\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/review-grasshopper-archicad-connection\/","title":"Review: Grasshopper ArchiCAD Connection","date":1449532800000,"text":"Graphisoft\u2019s ArchiCAD is catching up with rival\u2019s advanced architectural computational design. By Martyn Day\nComputational design is a niche area and its rise may have passed many in the industry by. It is popular among signature architects, academics and students and essentially drives the creation of complex parametric geometry from crafted scripts, programs, definitions and parametric constrained relationships. So, for instance, if town planners think your complex helical skyscraper with diagrid glazing is 10 metres too tall, you can alter the size in the model and the whole building and diagrid curtain wall regenerates. In the right hands, computational tools take the pain out of modelling complex forms and can be used to create and evaluate new designs, while saving costs on fabrication.\nEarly programs that enabled computational design required programming and scripting knowledge, which limited the appeal and created \u2018toolmakers\u2019 within signature architectural firms. These employees would build project-specific tools to aid architects with complex conceptual geometry.\nThen McNeel & Associates introduced Grasshopper for its Rhino modelling tool and everything became a lot more visual. While scripting in Python or higher languages is still a useful tool for the dedicated computational design fan, Grasshopper introduced a very simple, plug, play and wiring interface, which built the script in the background. Programs are created by dragging components onto a canvas. As a result Grasshopper and Rhino is a popular combination at the likes of Zaha Hadid, Buro Happold, HOK Sport and Foster + Partners.\nArchiCAD is a popular BIM tool developed in Hungary by Graphisoft, which is part of the Nemetschek group. Graphisoft has been lagging its competitors in computational design, despite ArchiCAD\u2019s powerful built-in GDL parametric development language. It seems for quite a while now, Graphisoft has been working with McNeel to build a link between Grasshopper and ArchiCAD. So those architects who have learnt to use Rhino and Grasshopper can now also run their scripts in ArchiCAD and drive a full Building Information Modelling (BIM) model.\nIn a single alliance, Graphisoft has gone from nowhere in generative design to being able to run the most popular, free, computational design package on the planet. Currently available as a public beta, the company expects to support the connection in January 2016.\nThe bad news for ArchiCAD users is that while ArchiCAD runs on Windows and Mac OSX, Grasshopper only works on Windows, for now. Grasshopper is written in .NET, which does not port easily to the Apple environment. McNeel says version 2 is more likely to be multi-platform.\nThere has been support for importing geometry via the Rhino file format in previous versions of ArchiCAD in order to get access to complex geometry created in Rhino \u2014 a one way process. The combination of Rhino, Grasshopper and ArchiCAD is a great mix for to the design process.\nConnectors\nThe ArchiCAD website makes available a pair of export and import add-ons that read and write Rhino\u2019s native file format (3dm). Rhino Export Add-on enables users to pass on the geometry of a model produced in ArchiCAD to Rhino efficiently in a user-definable way. The Rhino Import Add-on enables models created in Rhino to appear in ArchiCAD as GDL objects, maintaining the logical definition from the Rhino model.\nThe new Grasshopper connection provides dynamic links to use the parametric geometry to drive the placement hand configuration of BIM components in ArchiCAD. The system converts Rhino\/Grasshopper geometry directly into GDL BIM elements, while maintaining the computational design control within Grasshopper.\nThrough a live connection, Grasshopper scripts \/ edits can be previewed in a Rhino window and then applied directly to predefined 2D or 3D ArchiCAD elements. By editing the Grasshopper scripts or values, the BIM model and associated components update in ArchiCAD. It is also possible to send the edits of geometry in ArchiCAD back to Rhino\/Grasshopper, which also updates. Should you want to access any of the BIM edit functions in ArchiCAD while using the Grasshopper connection, the geometry needs to be first \u2018detached\u2019. The methodology is somewhat transactional but it works well.\nThis seemingly complex solution overcomes the huge amount of time it would take to develop native support, with Grasshopper directly working within ArchiCAD, but I have been told that deeper integration is more than likely on the cards without the need for Rhino to be present.\nAs Grasshopper scripts are not stored in the ArchiCAD BIM database, there is a need to manage and maintain the various files created by different applications. Rhino is also a powerful manufacturing-grade modelling tool and can produce geometry that is too complex for AEC, so ArchiCAD breaks this down for BIM elements.\nObjectified\nAEC magazine has been exploring the world of manufacturer\u2019s BIM components for a few years now and it is amazing to see the leaps and bounds in the quality and the number of BIM components online.\nWith ArchiCAD being much more popular in mainland Europe than the UK, it is substantially harder for object developers to find good GDL developers to create BIM content. Graphisoft is in the process of developing a handy utility that will help in the creation of simple GDL objects. Simply by drawing the top bottom and side ArchiCAD will create a three-dimensional component based on the actual dimensions. These will not necessarily be parametric parts but could offer a way to batch process to generate 3D parts.\nConclusion\nThe computational design market is now exploding with opportunities. Despite making all the early headway, Bentley is still playing catch-up with Grasshopper, which has the better user interface. ArchiCAD joining forces with Grasshopper will only further drive the popularity of Rhino and Grasshopper.\nWith respect to Autodesk Dynamo, which is also capable of running python scripts, Autodesk plans to support this in all its foundation design tools. Given that GC, Dynamo and Grasshopper are all free to customers, computational design is probably the best value for money in any vertical.\nArchiCAD also offers modellers another benefit: compared to Revit, it is exceptionally economic on file size. Given how well ArchiCAD supports IFC files too, I can see licenses of the software being sold just for early stage rapid BIM modelling.\nThe decision to bypass the GDL skills problem will benefit the commercial BIM object creators, as well as customers. While it is a simple solution it will be beneficial for some customers who wish to rapidly create geometry for families of parts.\n\u25a0 grasshopper3d.com \u25a0 graphisoft.com\/archicad\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/connecting-design-and-construction\/","title":"Connecting design and construction","date":1542931200000,"text":"It\u2019s been two years since Greg Bentley coined the term \u2018constructioneering\u2019, describing a process that aims to solve the fragmented workflow between design and construction. Greg Corke looks at how the latest developments are starting to make this joint vision of Bentley Systems and Topcon a reality\nHaving a vision is one thing; delivering on it is another thing entirely. And for a vision as bold as constructioneering was in 2016, Bentley and Topcon knew the engineering and construction gap would take some time to fill. The creation of a digital twin, a design \/ construction model that responds to the ever changing reality on site, not only requires great technological change but a significant cultural one as well.\nBentley and Topcon laid the first foundations last year by connecting their cloud platforms \u2013 ProjectWise for engineering project collaboration and Magnet Enterprise for construction execution.\nProjects in ProjectWise can now be associated with projects in Magnet Enterprise, making it easier to share data and then push that data out to site, as Jason Hallett, vice president of global software business development at Topcon Positioning Systems explains, \u201cIt allows a cloud-tocloud connection without actually having to move data all the time. You can have access to the data whenever you need it.\u201d\nBentley and Topcon have also added an online cloud conversion service that Hallett says allows firms to take iModels and transform the portions they need straight out to the field for machine control or surveyors or anybody that might need it.\nCapturing reality\nConstructioneering was never designed to be a one-way street and Topcon and Bentley recently announced a new integration that allows survey data to be easily transformed into engineering-ready reality meshes or digital surface models. It involves an integration between Topcon\u2019s Magnet Collage Web platform and Bentley\u2019s ContextCapture reality modelling cloud services.\n\u201cBig data that is collected by Topcon can be pushed through to the cloud service in ContextCapture and returned as a reality model,\u201d explains Hallett. \u201cThe engineer can now take the reality mesh and go do something a lot more meaningful. Point clouds are very cumbersome to work with \u2014 all of us recognise that. The reality mesh is very useful, especially when you\u2019re planning out concept designs.\u201d\nThis is reality modelling as a service, where operators can upload UAV imagery direct-to-web and all of the heavy-duty processing is done in the cloud. It means firms can get results back very quickly and they do not have to invest in expensive local hardware.\nThis helps pave the way for keeping the design and on-site reality in sync, as Ted Lamboo, senior vice president of Bentley\u2019s strategic partnerships, explains \u201cNow, all of Topcon\u2019s positioning technologies can help to continuously survey a site\u2019s digital context, synchronising project digital twins to make constructioneering even more compelling.\u201d\nMagnet Live\nTopcon is currently developing a new service called Magnet Live which, in Hallett\u2019s words, aids communication, data exchange and collaboration around constructible models. Much of this centres on making survey data instantly more usable, without having to change the established workflows of those on site.\n\u201cIf you can go out and generate a BIM model from a survey then you can actually introduce that right into the design tools in a more meaningful way, explains Hallett. \u201cSo, as they [surveyors] are doing their traditional practices, it [Magnet Live] generates a survey BIM model.\u201d\n\u201cThey [surveyors] don\u2019t have time to change their practices and re-train their people, so what we\u2019re trying to do is make that more of an automated thing.\u201d\nTopcon is also looking to use Magnet Live to help contractors get more value out of construction estimation. \u201cWhen you\u2019re doing your estimating, you\u2019re building take off models, so do that into a BIM model as well,\u201d says Hallett, adding that by creating a digital twin at this preliminary stage helps catch potential errors much earlier on.\nReal time flow\nOne of the key visions for constructioneering is for information to flow seamlessly between the construction site and the design office. The quicker an engineer gets eyes on an issue, the quicker it can be resolved and the quicker the new design can be implemented on site. Bentley\u2019s new iModelHub, which tracks and manages change, is set to play an essential role here.\n\u201cOur future with the iModelHub and this new open source, iModel.js, will make it more real-time,\u201d says Hallett. \u201cAs soon as the guy is done with something in the field he can say \u2018here\u2019s my survey for the day, I\u2019ve picked that area you needed\u2019. The engineer gets notified and iModelHub tracks the latest stuff it receives and all of a sudden, they\u2019ve got an iModel sitting out there that represents everything that\u2019s been done.\n\u201cYou\u2019ll get to a point where it will say \u2018hey, there\u2019s a new design\u2019 and it pulls right in and you can see it and you can push that out to the machines.\u201d\nAdding intelligence\nTopcon has been a driving force in construction automation for some time with machine control technology that guides construction machinery. Currently there still needs to be a driver in the cabin with a red button for safety but Hallett says this will evolve. \u201cYou\u2019ll get to a point where you don\u2019t need to have the people because when the machines are aware of the design, the constructible model, the environment around us, then you can have a safe working environment. And that will be where we go with construction and survey.\u201d\nTrue construction automation through the application of Artificial Intelligence may still be some way off, but the impact of AI is already being felt in some areas of construction.\nEarlier this year Topcon acquired ClearEdge, a specialist in computer vision and object recognition whose software compares point clouds of as-built construction against design or fabrication models, flagging out-of-tolerance or poorly installed elements.\n\u201cClearEdge is used by a lot of people to make sure stuff is put into the right place,\u201d says Hallett. \u201cOnce you start pouring concrete it gets pretty tough to change!\u201d\nIt\u2019s a very cool application of the technology he says, but there is also a huge value in applying to other areas. \u201cWe don\u2019t typically acquire tech and leave it as an island,\u201d he says. \u201c[We consider] how can this augment our business and improve our workflows across the board?\u201d\nHallett gives an example of an autonomous car driving down the street which instantly recognises \u2018people\u2019, \u2018stop signs\u2019 etc. \u201cThat\u2019s object recognition from video and it\u2019s not real precise but imagine what you can do if you start integrating some of this stuff.\n\u201cI always love scanning devices. They pick up so much information, but maybe I don\u2019t need all of it. So maybe I can recognise something and only scan that.\u201d\nHallett is being somewhat cryptic, but appears to be describing a world of automatic construction verification, where a drone with a video camera records a site, recognises features, captures specific areas precisely, then highlights any construction errors or future clashes. \u201cImagine what you can catch just by reviewing the iModel at the end of the day,\u201d he says.\nTopcon and Bentley still have a long way to go before constructioneering becomes a seamless workflow. But with links between their cloud platforms and extended services, they have started to lay solid foundations. Automating and regulating the flow of information is the next challenge and iModelHub and iModel.js look set to play an important part here.\nThe ultimate goal for constructioneering is for a continuous representation of reality, where as-built data is captured automatically and fed into a digital twin, identifying and resolving construction issues along the way. This is still some way off, and will require significant change in technology and industry, but it\u2019s a very compelling vision the same.\n\u25a0 topconpositioning.com \u25a0 bentley.com\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/www.wired.com\/story\/uncanny-valley-podcast-amd-ceo-lisa-su-isnt-afraid-of-the-competition\/","title":"AMD CEO Lisa Su Isn\u2019t Afraid of the Competition","date":1765497600000,"text":"Last week, some of the most forward-thinking minds in tech, culture, and politics came together for WIRED\u2019s Big Interview event\u2014a series of live, in-depth conversations with industry leaders. To kick off the event, Lauren Goode sat down with AMD\u2019s CEO Lisa Su. In this episode of Uncanny Valley, hosts Michael Calore and Lauren discuss the key ideas that came up during the interview, as well as the other conversations that caught everyone\u2019s attention during the event.\nArticles mentioned in this episode:\n- AMD CEO Lisa Su Says Concerns About an AI Bubble Are Overblown\n- Can AI Look at Your Retina and Diagnose Alzheimer\u2019s? Eric Topol Hopes So\n- San Francisco Mayor Daniel Lurie: \u2018We Are a City on the Rise\u2019\n- Watch the Highlights From WIRED\u2019s 2025 Big Interview Event Right Here\nYou can follow Michael Calore on Bluesky at @snackfight, Lauren Goode on Bluesky at @laurengoode, and TK on Bluesky at @TK. Write to us at uncannyvalley@wired.com.\nHow to Listen\nYou can always listen to this week's podcast through the audio player on this page, but if you want to subscribe for free to get every episode, here's how:\nIf you're on an iPhone or iPad, open the app called Podcasts, or just tap this link. You can also download an app like Overcast or Pocket Casts and search for \u201cuncanny valley.\u201d We\u2019re on Spotify too.\nTranscript\nNote: This is an automated transcript, which may contain errors.\nMichael Calore: Hey Lauren, how you doing?\nLauren Goode: Mike, I've been pondering the word tape.\nMichael Calore: Because we're taping a podcast?\nLauren Goode: Because we say we're taping a podcast.\nMichael Calore: Sure.\nLauren Goode: I don't think the kids know what that means.\nMichael Calore: I mean, they may know what it means in the abstract, but I don't think they've had the visceral experience of actually recording something onto tape.\nLauren Goode: Onto tape and rewinding the tape. Be kind, rewind, all of that.\nMichael Calore: Yep.\nLauren Goode: No, so I guess we're supposed to say we're recording a podcast now.\nMichael Calore: Recording works.\nLauren Goode: Recording. Yeah.\nMichael Calore: Yeah. It's like when people say, let me film that. You're not actually filming anything. You're shooting a digital video.\nLauren Goode: So then if you have a video podcast, are you shooting the podcast? What do you say? Do you say taping, then?\nMichael Calore: I think you say recording because it just\u2014\nLauren Goode: Recording the pod.\nMichael Calore: Yeah.\nLauren Goode: We're recording the pod.\nMichael Calore: It covers all the bases.\nLauren Goode: We're capturing it.\nMichael Calore: That's what we're doing.\nLauren Goode: We're sublimating it. All right. Well, should we record this pod?\nMichael Calore: I would like to, yes.\nLauren Goode: Let's do it.\nMichael Calore: Honestly, I'm still recovering from last week's Big Interview event. My throat is still feeling a little bit raw, even though it's been like four or five days.\nLauren Goode: You sound delightful to me.\nMichael Calore: Thank you.\nLauren Goode: But that really was an epic event.\nMichael Calore: It was.\nLauren Goode: Yeah.\nMichael Calore: You were on stage.\nLauren Goode: I was. I was first up in the morning. Katie, our boss, gave the intro to the conference and then it was me and Lisa Su, the CEO of AMD. And not only was it a really interesting conversation, but then I was done for the day. I didn't have to do any more interviews after that. And I just got to listen and absorb, and there were some other really great talks.\nMichael Calore: There were, yes. And we're going to talk through some of them. We're also going to listen to your conversation with Lisa Su, and then we'll talk about it, and we'll take listeners behind the scenes of The Big Interview.\nLauren Goode: Let's take them behind the curtain.\nMichael Calore: Let's do it. This is WIRED's Uncanny Valley, a show about the people, power, and influence of Silicon Valley. Today, we're breaking down the key conversations that went down at our Big Interview event in San Francisco last week. Since last year, WIRED has hosted this space where the most forward-thinking innovators, whether they're in tech, politics, science, or culture, have in-depth conversations with us that go beyond the headlines. This year, the lineup included Anthropic's cofounder, Daniela Amodei, scientist and author of Super Agers, Eric Topol, and Wicked's director, Jon M. Chu, among many others. We'll dive into what ideas caught our attention the most, which ones we're skeptical about, and what these conversations tell us about the future of the industries that shape our lives. I'm Michael Calore, director of consumer tech and culture.\nLauren Goode: And I'm Lauren Goode. I'm a senior correspondent.\nMichael Calore: Lauren, before we hear your conversation with AMD's Lisa Su, it is worth saying that this is not your first time speaking with her. Can you share a bit about her background and how your previous reporting informed this conversation that you had with her at Big Interview?\nLauren Goode: I first got to know Lisa Su in person earlier this year. I had the opportunity to go down to Texas where AMD has offices and labs, and that's where Su spends most of her time. And we not only spoke for a big magazine piece that I was working on for WIRED, but she gave me a tour around some of the testing facilities that they have there. So I got to see the chips they make being put through their paces in these labs. Also had this really fun opportunity at one point where we were all traveling in separate cars around, the labs are in the rolling hills of Austin, and so we had to drive between the labs.\nAnd at one point she turned to me and she said, \"Do you want to come in my car?\" And I said, \"Of course I do.\" So it was me, her, her driver, and her bodyguard, and normal day for me. And we just had an in depth conversation about AI and the news and what she's focused on. And we've shared some personal anecdotes about dealing with the healthcare system, but she really is a pretty remarkable CEO. She was born outside of the United States. She was born in Taiwan in 1969, moved to Queens, New York at a very young age, was raised there. Her father worked for this city as a statistician. Her mother was an accountant who later became an entrepreneur. And then Su's background is in electrical engineering. She went to MIT. I tried reading her thesis, her PhD thesis, because she is Dr. Lisa Su, and boy was that just\u2014go straight over my head. I actually sent it to someone who's an electrical engineer and said, \"Could you help me parse this?\"\nMichael Calore: So she's super smart\u2014\nLauren Goode: Super smart.\nMichael Calore: And she runs one of the biggest chip companies in the world.\nLauren Goode: Yeah. Yeah. And then she made her way up through various semiconductor companies. And then she landed at AMD in 2012 and about 18 months later, became CEO.\nMichael Calore: Wow.\nLauren Goode: And she has executed what is widely acknowledged to be a remarkable 10-year turnaround of that company. AMD was on the brink of bankruptcy in the early 2000s. Since 2014, when she took over as CEO, the company's market cap has risen from around $2 billion to nearly $360 billion.\nMichael Calore: It's not bad.\nLauren Goode: It's pretty good. Now, the juxtaposition there, though, is of course Nvidia. People always ask about Nvidia. And Nvidia is a $4 trillion company now and it's a company that's widely recognized as having smartly foreseen the AI revolution and pivoted its business to focus on AI GPUs. And Nvidia still has the overwhelming share of the GPU market. AMD makes both CPUs based on the x86 architecture, but they also have this AI GPU and AI accelerator business, and they have a fast-growing data center business.\nAnd so what we're seeing in the chip market broadly right now is all of these different chip makers, whether it's a company that started out making chips, like an AMD or Nvidia, or whether it's a giant hyperscaler or cloud company, like Google or Amazon, now getting in the game and making chips. Everyone wants a chip. Everyone wants to make a chip, and this is what's really powering the AI revolution. So we had a lot to talk about on stage. I was frankly bummed to be limited to 25 minutes with her. I may have gone over by just a few, and I just always really enjoy talking to Lisa.\nMichael Calore: All right then. We're going to listen to the conversation that you had. It's a slightly condensed version of your conversation, but let's give it a listen.\nLauren Goode: Dr. Lisa Su.\nLisa Su: It's wonderful to be here, Lauren. Thank you for having me.\nLauren Goode: Thank you so much for being here. We're so excited. I should note for everyone that Lisa revealed to be backstage, that she's already been up and boxed this morning at approximately 5:00 AM. So I think you're ready to go.\nLisa Su: I am ready to go.\nLauren Goode: All right.\nLisa Su: Are you guys ready to go?\nLauren Goode: Is everyone ready to go?\nLisa Su: All right.\nLauren Goode: All right. Welcome to San Francisco. You spend a lot of time in Texas. It's where your home base is.\nLisa Su: Yes.\nLauren Goode: So you're traveling around a lot these days. I think it's safe to say that there's a vibe in San Francisco right now, and I think that's due, in large part, to AI. Would you agree with that?\nLisa Su: I would absolutely agree with that. I mean, the talent and the energy and the innovation that has come into San Francisco has definitely just accelerated over the last 12, 18 months, and it's wonderful to see.\nLauren Goode: Which leads me to the question, are we in a bubble?\nLisa Su: Let's start with: Are we in the bubble? And I will say, Lauren, emphatically, from my perspective, no. If we take a step back and you look at where we are in AI today, it sounds like something that people say, but it's something that I truly believe. AI is the most transformative technology of my career, of my lifetime. I mean, you can see the power of the technology, and we're so early in the usage of it. So I find it really interesting when people ask, \"Are we in a bubble?\" I'm like, \"We haven't even gotten started yet.\"\nThe amount of progress that we've made in the last couple of years has been wonderful, but we're still at the very early innings of seeing what AI can do and how it can really enhance productivity, how it really changes businesses, how it really changes the way we think about science and healthcare and all of those reasons. I think we are still so early in the cycle and it's a really, really exciting time for me, personally, because every day you see something new and you learn something new and that's why it's so exciting to be in tech.\nLauren Goode: Do you think that the concerns, then, about it being a potential bubble are totally overstated? I mean, you have to understand, in a sense, why people might be looking at this and looking at the immense amounts of capital that are being invested right now into AI and wonder about this.\nLisa Su: I do think the concerns are somewhat overstated and that's probably because we're not used to bets this big. And it is true, the types of bets that you're seeing in AI for the largest hyperscalers in the world, for the largest technology companies in the world, for companies like ourselves, we are making much bolder and bigger bets because of where we are in the AI cycle and the power that we have. But I think what people are underestimating is the fact that there is so much demand for technology that the underlying health of the ecosystem is really, really strong.\nI mean, there's tremendous free cash flow being generated by the largest hyperscalers. There's a tremendous demand for technology and the macro is actually really quite strong as well. And so when you add those things together, you say, \"Yes, are we investing big?\" Yes, we are, but we are investing big at the right time because of the technology capability. And you have to assume that the people who are running these companies are very rational, smart people.\nLauren Goode: You assume a lot.\nLisa Su: Oh, come on, Lauren, really?\nLauren Goode: Present company excluded, you assume a lot. Let's unpack the demand aspect of this. I had a feeling that you were going to say that. Incredible amount of demand. You've said that before. The leaders of companies like Microsoft, Nvidia, everyone's talking about the immense amount of demand for AI. Unpack that a little bit. What does that actually look like? Who's demanding it? What are they demanding it for? Is this enterprise? Is this customers?\nLisa Su: Yeah, I think you see it in several aspects. When we first started, if you were to go back two years ago or even 12 months ago, I think there was a lot of demand for training models. People were trying to figure out, and even today, we're trying to figure out, hey, who's going to have the best model? We've certainly seen a lot of new innovation in that area. But I think what's really driving demand today is just AI usage. People are asking AI to do more. They're asking more questions. They are putting together more complex tasks. There's more in terms of agents that are doing work.\nAnd with that demand, what we're seeing is that there's just not enough computing power that's installed today. So in a sense, the way I like to tell the analogy is if you equate computing use with using AI to intelligence and you're running a company, why wouldn't you want to have more intelligent capability? And for that reason, the investment is justified, but it takes time. There's a lag between when you want the computing capability and when you actually have it.\nLauren Goode: I guess I wonder if it's more of a top-down effect right now that's happening where tech companies, hyperscalers, chip makers are saying, \"No, there is demand,\" versus bottom-up where it's actually the consumers and the clients who are saying, \"No, we want this power.\" Is it an element of if we build it, they will come?\nLisa Su: No, I think there really is real demand from the standpoint of there are so many more things that we would want to do if there was more computing capability out there. And I think you hear that from just being in the trenches. The way I view it is, I spend a lot of time with our top customers, the top thought leaders in the area. And every conversation is like, yes, it's great where AI is today, but we know it's not yet good enough. We know that there's more innovation that can be done. From a very personal standpoint, when we look at AI within AMD, we've made significant progress using AI within AMD, but I know that it can do more. And the idea of unlocking more capability in terms of building better chips, higher quality, lower cost, really automating that whole process, which is still so early.\nLauren Goode: When you look at the competitive landscape right now, who keeps you up at night?\nLisa Su: When I look at the landscape right now, what keeps me up at night is how do we go even faster in terms of innovation? And I say that because the one thing that we can't ever get back is time. And when you look at how do you make major leaps in technology, it is all about how do you take really good ideas and get them to market faster than your competition. And so from that standpoint, it really is about how do we get our technology out there?\nLauren Goode: So it's about being faster.\nLisa Su: Yes.\nLauren Goode: Faster than who?\nLisa Su: How about fastest? Is that OK? Look, the thing that I'd like to perhaps say about this market that is also important is it is unlike any other market that I've been part of. And when I say it's been unlike any other market, it's because the rate and pace of innovation is faster, the risks that people are willing to take with new technology. And the fact is you see this constant leapfrogging as well, right? So the conversation this month is the great job that Google has done with Gemini three, and you got to give them a lot of credit. It's a great model. But if you think about all of the things that have happened in the last 11, 12 months, we were talking about DeepSeek at the beginning of the year, now we're talking about something completely different. I think that's what's different about AI. It is the rate and pace of change, as such, that there is no concept in my mind of one winner or one loser. I think it's the concept of you're going to constantly see this leapfrogging because there's so much opportunity to take the technology in different directions.\nLauren Goode: I'll bring up some of your competitors. So I'm glad you brought up Google, because Google's been making some really incredible progress with TPU, their own homegrown chip. And there have been reports recently that Google would actually sell that chip outside of its own stack, its own universe. There's obviously Nvidia, which is the world leader in the GPU market. So you both make GPUs, you're both making AI accelerators. You're both trying to appeal to both the training and the inference crowd. From what I hear, everyone wants to make their own custom chip. There's also Amazon. They make Trainium and they work with Anthropic, who we'll hear from shortly. So the competitive landscape is pretty intense where you are. When you look at that cluster, no pun intended, who do you see as the most formidable competitor to you?\nLisa Su: Lauren, we're in this notion of a competition and I'm in this notion of this is a huge market and you're going to need all kinds of chips. And I really believe that. I think if I talk about my vision of where this industry goes, you are going to need CPUs, you're going to need GPUs, you're going to need ASICs or custom chips, you're going to need all kinds of things. And I think the winners are those who can comfortably move across those domains and not feel like, \"Hey, I've lost in that.\"\nSo look, Nvidia is a phenomenal company, really great company. So much respect for Google, so much respect for all of the hyperscalers that you mentioned, but at the end of the day, technology needs choices and technology needs, really, the right chip for the right workload. And I think that's what we do at AMD. I think that our big differentiation in this space is we've been investing in high performance technologies for the last 10 plus years and we have all of the elements that bridge all of these pieces together. So it really is, how do we grow the market capability as quickly as we can? And I am fortunate to be in the company of such great companies.\nLauren Goode: I have to ask you about politics.\nLisa Su: Sure.\nLauren Goode: You've been spending more of your time in Washington, DC, these days. And from what I understand, you interface quite a bit with the commerce secretary, Howard Lutnick, as well. I have a specific question for you, but first let me ask you this. How does your experience with this administration thus far, this presidential administration, compare to your relations with the last?\nLisa Su: Well, one of the things that has become incredibly clear and different is, over the last five years, I think the recognition that semiconductors are so core to national policy has really intensified. It wasn't the same. It wasn't the same in the industry before. And I think that makes sense, right? I think chips are now so powerful and so capable, whether you're talking about national security or the economy or all of the intelligence out there, they're so important. I would say that what's been different about this administration is the speed. I think the administration has been extremely open and wanting to work with industry.\nI very much appreciate Secretary Lutnick, Secretary Wright, David Sacks is amazing. Michael Kratsios, I think the administration truly understands that for this to work, there needs to be a really, really open dialogue between the administration and industry. So that access is really helpful. I think we also have to do our part as a technology industry and be helpful in trying to solve some of the issues that are critical from a national policy standpoint, things like manufacturing more in the United States. That is good for the country, that's good for our industry. That's something that we all have to step up to accelerate. Certainly a lot of conversation about export controls and other things as well.\nLauren Goode: I asked you about export controls when we met in June, and you basically said export controls are a fact of life in your world. There was just some news yesterday that at least one of the bans, I think, that was going to happen on some of your chips being shipped to China has now been lifted. So you're able to apply for the licenses now in order to get your chips to China, as I understand it. Is that correct?\nLisa Su: Well, maybe I'll say it this way. So we actually have gotten some licenses to ship our chips to China. Those happened a few months ago, and there's active conversations about what should happen longer-term with export controls. But I think the ruling yesterday was relative to another aspect of it.\nLauren Goode: So previously AMD and Nvidia had both agreed to pay the government a 15 percent tax or fee on certain chips that would be shipping to China. So what is the status of that now?\nLisa Su: So we have licenses for some of those chips. For us, they're called the MI308 chips. It is a place where we work very closely with our customers and see what their demand is. So we believe we'll be shipping some ships to China over time, but we've not been very specific about how much because we're really wanting to see how the entire dynamic plays out. It's a very dynamic time.\nLauren Goode: But will you be paying the 15 percent tax to the US government on those chips?\nLisa Su: As we ship those chips, we will be.\nLauren Goode: You will be still? OK.\nLisa Su: Yes.\nLauren Goode: So yesterday's ruling didn't change that at all?\nLisa Su: That's correct.\nLauren Goode: OK. Really interesting. OK. What else would you like to see come out of this current administration? It could be related to policies on your chips. It could be related to other policies you feel strongly about as this administration continues.\nLisa Su: Well, what I'm very passionate about is the idea that the US should absolutely lead in AI. Right now, the US leads in AI design and AI technology, AI software, the models, all of that. We should also lead with using AI within the United States. And one of the areas that this administration, President Trump, has been very forward leaning on is his AI action plan that was unveiled during the summer. I think we talk about things and then we actually see things go into action.\nThis administration has been really helpful in bringing some of that to action. So for example, we're talking about much deeper partnerships between the national labs and industry so that we can accelerate the usage of AI so that we can solve some of the important problems around science and research and all those areas. So those are areas that I'm very passionate about. They just announced this massive effort called the Genesis Mission. This is an area where, again, you're going to bring the best minds in the country together, such that we're accelerating our own usage of AI.\nLauren Goode: There is this divergence of opinions right now in the industry, whether or not we should be shipping chips to China, even if it's not the best chips, even if it's the H20 or the MI308, in your case. And some people feel that it's a threat to national security to do that. Some people feel that, by withholding our chips from China, that potentially it would incentivize the country to build their own robust AI chips, which we know that China is doing. Where do you generally stand on that? I mean, I know you have chips to sell, but really, where do you stand on and how much technology the US should be shipping to China?\nLisa Su: Yeah. I think the way I think about this is, first and foremost, US national security is the number one priority. And we are fully supportive of that. It is our duty to make sure that that's the case. And the fact that AI chips can really help from a security standpoint is clearly there. China is an important market. I would say when we look at the long arc, we want to have access to the Chinese market. There are a lot of smart people there.\nThere's a lot of innovation happening there. We would want the entire world to be using the US AI stack because it's not just about, \"Hey, we want to sell chips,\" or Nvidia wants to sell chips. It's really about we want the ecosystem to develop in such a way that we have access to the smartest people using US AI technology because that just helps us ensure that we continue to stay at the bleeding edge. So I do think that the administration is thinking about these things very carefully, the idea that we must protect national security, but we also want US technology to continue to be the best in the world, and we're very much a part of that conversation.\nLauren Goode: One more quick question for you. I asked you a version of this before. I said, \"What impresses you most as a leader and what irritates you most as a leader?\" And now I'm going to ask you a version of that about AI. What impresses you most about AI right now and what irritates you most about AI right now?\nLisa Su: What impresses me the most about it is just how much potential it has and how quickly it gets better. I really\u2014\nLauren Goode: What's an example of that?\nLisa Su: An example of that is when you're using it in your daily life, how often do you use it? I use it 10 times more today than I did even three months ago because it actually is helping me. It's helping me gather information. It's helping me get prepared for things. It's helping me in terms of what information that would be useful. But what irritates me is it's still not right enough of the time. And so it goes back to this notion of we are so early, but the potential is so clear. It is incredibly clear to me that we are going to be super surprised at what we use AI for. Forget about what we use AI for five years from now or 10 years from now. One year from now, we're going to be super surprised at how much AI is a part of all of our daily lives, even though we think we're using it a lot today.\nLauren Goode: Dr. Lisa Su, thank you so much for joining me. I just wanted to say this was a fantastic conversation and we really appreciate you kicking us off. So, thank you.\nLisa Su: Thank you.\nMichael Calore: Let's take a quick break and when we come back, we'll discuss our key takeaways from the Lisa Su interview and other things that happened on The Big Interview stage.\nWelcome back to Uncanny Valley. Lauren, we just listened to your interview with Lisa Su from AMD. It was not surprising to hear her share that she thinks that the fears of an AI bubble are overblown and that the demand for chips will only continue to grow. But it was a bit surprising to see that she didn't really address AMD's competition in a direct way. And considering that AMD is still just, like, a fraction of the size of its biggest competitor, which is Nvidia, did it feel like Nvidia is the elephant in the room and more generally, what do you think the strategy was behind this messaging?\nLauren Goode: I think that's fair to say, yeah, that it is the elephant in the room, but I also think that the competitive landscape is a lot more nuanced than just AMD versus Nvidia. And I think in some way, that's probably what Su and her executive team at AMD are trying to convey. I mean, one reason is that as AI itself evolves and the way that generative AI is being made, changes the needs for compute power change too. So GPUs are really great for training giant AI models like the kind that OpenAI and Anthropic built, but there's also inference. There's a fast-growing market for inference, which requires slightly different combinations of compute power. And so what's your favorite word, Mike? \u201cCompute.\u201d\nMichael Calore: \u201cCompute.\u201d\nLauren Goode: \u201cCompute\u201d as a noun.\nMichael Calore: You can just say \u201ccomputing resources.\u201d It's OK.\nLauren Goode: Computing resources. I like that. Computing resources, let's make that a thing on this podcast. And so I think AMD is trying to position itself as not only a maker of GPUs, but CPUs that can serve different needs. And the other thing is, like I referenced earlier, that it's really not just AMD versus Nvidia because of the fact that so many other giant tech companies are now, they either make chips and they have really strategic partnerships, like Amazon's with Anthropic, or they're definitely looking at making chips, whether it's Meta or OpenAI, which is planning to make a custom chip with Broadcom. So the competition abounds. There's also these tier two level chip makers and cloud companies that are serving different parts of the market too, like Cerebras or Grok or SambaNova or really the list goes on.\nMichael Calore: Yeah. And she mentioned this in your talk, the fact that there are several different chips necessary for each different application of artificial intelligence or machine learning.\nLauren Goode: Right. What stood out to you from the talk?\nMichael Calore: The thing that stood out to me the most is that she was illustrating how quickly the AI market is evolving. And she said at the beginning of the year, we were talking about DeepSeek, which was like the ultra light, ultra inexpensive model that came out of China. And I gasped because I was like, that was less than a year ago\u2014\nLauren Goode: Right.\nMichael Calore: \u2014we were talking about DeepSeek? So I mean, obviously she's going to say that we're not in a bubble, but she's right that the market is just evolving at such a quick pace and the economy is just like going all in on this technology. This is me, this is not Lisa Su talking, but maybe we're not going to see all of the promises that the industry is making, that AI is going to be this big, transformative technology, but it absolutely cannot be ignored. And it absolutely will be sticking around because there's just so much momentum behind it.\nLauren Goode: Totally. And she said earlier this year we were focused on DeepSeek and now we're talking about Gemini 3, which is Google's latest AI model, which uses Google's own chip technology, TPUs, tensor processing unit. And so even though she said specifically Gemini 3, to me, that meant that she was gesturing towards it's Google's chip efforts. It's that full stack integrated effort. And that's something, don't sleep on TPU.\nI mean, I remember even a couple of years ago when I was working on another story for WIRED about Nvidia's Jensen Huang asking someone, an AI CEO in the industry, \"When you're looking at the kinds of computing resources that you need, and it's not just Nvidia, but you're looking at other options who's most interesting to you?\" And they said, \"Well, TPU is interesting.\" And that was then, and Google's been working on this for a long time. So once again, don't sleep on TPU, I think.\nMichael Calore: That's right. I carry around a version of a TPU in my pocket every day on my Pixel phone.\nLauren Goode: And your Pixel phone, is that called the TPU? [inaudible]?\nMichael Calore: Oh yeah, I use it. I run massive amounts of compute on my Pixel phone.\nLauren Goode: We're going to get an email from Google's comms team shortly with a full rundown and specs of the Pixel again\u2014\nMichael Calore: Brilliant.\nLauren Goode: Just in case we forgot.\nMichael Calore: Let's move on to some of the other conversations that happened on stage during The Big Interview event last week. You told me that Eric Topol's interview caught your attention. He is the physician and scientist who also wrote a book called Super Agers about longevity. When I think about longevity in the context of Silicon Valley, I unfortunately immediately think about biohackers and people who have bloodboys and so on, but I hope that Eric Topol's approach was a little bit different.\nLauren Goode: Topol is a real one. Have you listened to him on podcasts before or read any of his work?\nMichael Calore: I saw him on stage.\nLauren Goode: There you go.\nMichael Calore: I have not read his book.\nLauren Goode: There's so many grifters out there, so many snake oil salesmen in health and wellness these days completely fueled by online influencer culture that when you listen to Topol, you realize what he's saying is evidence-based. I love how Sandra asked him to grade the current officials in the health and human services in the United States. And he basically said that RFK Jr. and his ilk are the antithesis of good health. I like how we talked about how screening for certain diseases is age-based now. You get to age 40 and you should go get a mammogram. You get to age 45 or 50, you should get a colonoscopy. And he talked about how maybe that should evolve so that it becomes more data-based, more genetics-based, more risk-based, based on your risk factors, which just seems to make so much sense to me, particularly now when more young people are getting cancers.\nMichael Calore: Yeah. And I'm sure when he said that, all of the folks working in AI in the room sat up because that's a great opportunity for them.\nLauren Goode: Yeah. And he actually had some positive things to say about AI. He thinks it has the potential to spot trends and symptoms to help doctors recognize illness earlier.\nMichael Calore: So you mentioned that he gave some health advice at the end?\nLauren Goode: Yeah. And it was pretty straightforward. It's like have a good diet, exercise often, make sure you get enough sleep. Sleep is hugely important. He talked about building community and not the kind of community that you build through screens, but actually IRL. You and I sitting in this podcast once a week, going for bike rides, gossiping\u2014\nMichael Calore: Gardening.\nLauren Goode: Fuel of life. Yeah. And I'm even going to go back and listen to that session again. That's how much I enjoyed it.\nMichael Calore: We all should.\nLauren Goode: Yeah, we all should. So Mike, before the conference, Katie, our boss, posted something in our WIRED Slack, which was basically, \"Hey, San Francisco folks, I'm interviewing your mayor. What do you want to know?\" And I could see, \"Michael Calore is typing, Michael Calore is typing.\" And I was like, this is his moment. He's been waiting for this.\nMichael Calore: Well, yeah. I mean, our mayor, Daniel Lurie, is relatively new. He's a year, almost, into the job. And there are a lot of things to talk about when you talk about San Francisco's mayor because San Francisco is a city that has been utterly transformed in the last couple of years by the AI industry. It is also a city that has been utterly transformed by the pandemic five years ago because our downtown is hollowed out. We have a seven by seven piece of land that is very difficult to build housing on. So we have a terrible housing crisis. We also, of course, have a terrible street drug problem, just like most big cities right now with the fentanyl crisis and opioid use.\nSo the mayor is somebody who is not an experienced politician. He did not come up through city hall. He has a lot of money and he primarily works in philanthropy and nonprofit sectors. And he came into the office and took a very interesting approach to how he was going to govern San Francisco. He approached it like a manager. And Katie asked him several things about this, particularly when you took office, some of these problems were already solved. Some of them were things that you solved. How much did you inherit your success? How much do you own your success? And he was rather deferential to all of the work that everybody in the city has been doing. But he also talked a lot about the AI industry and how that has transformed housing and how that has transformed the economy. And I thought that all of that was really interesting because he doesn't do a lot of interviews. He doesn't do a lot of press. Daniel Lurie does press conferences\u2014\nLauren Goode: He does bits on social media.\nMichael Calore: He does a lot of bits on social media.\nLauren Goode: Yeah.\nMichael Calore: Yeah.\nLauren Goode: Was there anything particularly illustrative that he said about housing, for example, that stood out to you?\nMichael Calore: Yes. So the big battle in San Francisco right now in housing is the upzoning initiative. Basically, it converts a lot of areas of the city that are high traffic areas near transit hubs, near big streets that it's difficult to build anything that would house a lot of people. So buildings that are more than two or three stories tall, buildings that have high density housing. And the city just passed an ordinance that would change this and it would allow developers to build, build, build in places where they'd previously been locked out. And the process for this was contentious, of course. They just passed this and already people are talking about suing the city, right?\nLauren Goode: Homeowners?\nMichael Calore: Yeah.\nLauren Goode: People who don't want more building.\nMichael Calore: Yeah. People who are against building for a number of reasons. Most of them fall apart when you look at the public good that more housing would do. Some of them are actually valid and there are a lot of small business owners who are worried about losing the building that they're in, things like that, that are real problems. But the mayor was very adamant about the fact that, OK, this went through public approval. The people of San Francisco want this, so we're going to do it and it's going to hurt, but we need to do it.\nAnd this idea that we're going to trust the process and we're going to have public comment, we're going to have public debate, and then we're going to make a vote and whatever the city votes on goes. That has always been a problem in San Francisco because there are so many voices and people feel ignored and people don't feel heard. The other thing, to your question, that he talked about that I thought was really indicative of his managerial style was he talked about how the city government meetings work. There's all of these different agencies within the city government and they only get together so often. Well, he increased the frequency of their face-to-face meetings. He also has installed new people to oversee little pockets of agencies for better coordination, basically just like more management of what's going on within all the agencies of the city. So yeah, he's a suit and he's acting like it.\nLauren Goode: And he wears good socks.\nMichael Calore: Yeah. So the interview started with him talking about how he has a good sock game and then Katie was like, \"Prove it.\" And he took off his shoes and showed the audience his socked feet.\nLauren Goode: Yeah, because it looked like from the top, they were just black socks, but then he was like, \"No, no, no, there's something on the foot.\" So he took off one of his shoes.\nMichael Calore: That's right.\nLauren Goode: Don't think we've had a moment like that at our WIRED conference before.\nMichael Calore: Nope.\nLauren Goode: It sounds like you're pretty positive on Lurie so far.\nMichael Calore: I mean, I'm just like most San Franciscans, I'm never going to be happy with our mayor and I'm never going to be happy with city government because there are still so many problems that need to be solved that I feel like the city is bungling. I was skeptical of him when he came into office. I'm less skeptical of him now that I have seen him work for a year and I've heard him talk about these things on stage at The Big Interview.\nLauren Goode: That's encouraging.\nMichael Calore: I want to talk about one more interview that caught your attention. And you told me it was Michele Jawando, who was the president of the Omidyar Network.\nLauren Goode: Yeah. I know some of the folks at the Omidyar Network and I had never interacted with Michelle before and I still didn't really have the chance to get to know her super well at our event, but she was there and she was a speaker on stage. And also just full disclosure, Omidyar Network happened to be one of the sponsors of our event this year. So OK, we'll get that out of the way. But Michelle, yeah, she's a really interesting person. She's worked as the global public policy lead at Google in the past. She codirected the 2020 global election integrity team there. She's worked in civil rights law. I mean, she's done it all. And her conversation with Katie was very wide-ranging and philosophical. But the thing that stood out to me was that at one point early on in the talk, she made the audience repeat, \"AI is not destiny, it is design.\" And she said it twice.\nAnd she was basically making the argument that we can have agency on how AI is deployed in our lives. She talked a lot about how it's important to have people from various perspectives and industries in the room when decisions are being made about tech design. She said it's a false binary to say that building that way, like in a thoughtful way, means that you have to slow down innovation. And she called for more investment in startups that actually want to push the envelope towards good for society. And because Omidyar basically lobbies for better governance around technology, she was drawing a direct link between this agency or lack of agency we have around technology, power, and then governance. So you need better governance structures, in her mind, I think she made a good case for it, in order to have a more equitable tech deployment.\nMichael Calore: Yeah. And that's been their mantra since they started working with all these organizations, is trying to find solutions that are more equitable for everybody.\nLauren Goode: Yeah.\nMichael Calore: Yeah.\nLauren Goode: And I don't necessarily always believe that governance is the answer to how to build better tech products. But what we've seen is just this complete runaway train of technology over the past 15 years or so. And the moral versus immoral framework that we tend to put on technology companies for how they're building something no longer applies because they've just thrown it out the window. It's build, build, build, scale, scale, scale. And so I think you do need some mitigating forces there in order to make sure that we're not just users being abused by technology.\nMichael Calore: And they can do it without turning off the money faucet.\nLauren Goode: Right.\nMichael Calore: All right. Let's take another break. And then when we come back, we'll do our WIRED and TIRED.\nOK, Lauren, we are winding down to our last segment. It's called WIRED and TIRED, and you know the drill. Whatever is new and cool is WIRED, and whatever is pass\u00e9 is TIRED. So do you want to go first?\nLauren Goode: Sure. This is not new. I'm going to say bubble baths are WIRED. And our producer, Adriana, right now, is going to be like, \"I need a tech recommendation.\" There is a tech part of this.\nMichael Calore: Oh, in bubble baths?\nLauren Goode: In bubble baths.\nMichael Calore: OK.\nLauren Goode: So I'm going to assume that most people who take baths bring their phone into the bath with them, right? We all think we're going to read a book, but come on, phones are waterproof now.\nMichael Calore: I typically just put on music and then leave my phone out of the bath.\nLauren Goode: Some of us may have been in positions before where we had to take a bath because we had to respond to work emails because we were bathing for the day, but I use this bubble bath from Kneipp, a German brand. It's eucalyptus bubble bath. It's great. You just need a small cap full and it just changes the whole vibe. I was reading the bottle the other day and I realized that there's a QR code on the back of the bottle for a Spotify playlist.\nMichael Calore: Oh no.\nLauren Goode: Yeah. It's a Spotify playlist that comes with your bubble bath.\nMichael Calore: So is it like a unique play \u2026 well, it can't be a unique playlist, but I mean, do they change it ever?\nLauren Goode: It was fine. I don't know if they'd change it. I don't think so. It was fine.\nMichael Calore: OK. They went through the effort.\nLauren Goode: You know what? It's a nice idea. There was a time a while ago where I remember getting a pair of running shoes and there was a QR code in them, in the sole, and I thought, this is dumb. Why do I want this QR? But I was like QR code on the bubble bath bottle for a bubble bath playlist. Pretty cool.\nMichael Calore: Yeah. The pandemic changed QR codes forever.\nLauren Goode: It really did. Yeah. And so now they're everywhere. They're in the bath.\nMichael Calore: So how would you rate the bubble bath playlist?\nLauren Goode: Oh, like a solid 4 out of 10.\nMichael Calore: Really?\nLauren Goode: Not great, but you know what? A, for the effort.\nMichael Calore: Did it have the \u201cRubber Ducky\u201d song?\nLauren Goode: No, it was very chill. It's like something you would've heard in a yoga class or something.\nMichael Calore: Oh, OK.\nLauren Goode: Yeah.\nMichael Calore: OK. Appropriate.\nLauren Goode: Appropriate.\nMichael Calore: Nice. So what's your TIRED?\nLauren Goode: My TIRED is Spotify. No, OK. Specific\u2014well, yeah, maybe Spotify. But the Spotify Wrapped this year. Over, done, get it out.\nMichael Calore: Yep.\nLauren Goode: Why are we still doing them? And this year, I'm convinced\u2014I'm going to add Spotify comms to the list of anger emails we're going to get\u2014I'm convinced that the age thing was just so we'd all talk about it.\nMichael Calore: Oh, yeah. What's your musical age?\nLauren Goode: What's your musical age? It was so random. I had friends getting 67. I got 21, and I was like, I don't know why. Either they think I'm listening to '90s music ironically, as a 21-year-old might do right now, or I'm listening to enough vaguely emo stuff that they think ... I was like, I don't get it. What was your age?\nMichael Calore: I'm on Tidal. I'm not on Spotify.\nLauren Goode: Oh.\nMichael Calore: Yeah.\nLauren Goode: Wait, really?\nMichael Calore: So I don't know. My age is my age.\nLauren Goode: But don't I send you Spotify stuff sometimes?\nMichael Calore: Sometimes, yes.\nLauren Goode: Sudden realization that he's never listened to anything I've sent him.\nMichael Calore: No, I have a little service where you plug a link in and it generates a Tidal link. There's a lot of these. It's pretty cool. When people send you links, you can just make them whatever service you listen to, if you're an Apple Music person or a YouTube Music person.\nLauren Goode: I'm guessing your age would've been, if Spotify Wrapped did your age, I think he would've gotten a solid 58 to 62.\nMichael Calore: Oh my God. How dare you?\nLauren Goode: Well, because, yeah. I don't know. Actually, maybe they would've mistaken you for a 37-year-old who goes to lots of shows. Because you do go to watch shows.\nMichael Calore: They don't know that I go to a lot of shows.\nLauren Goode: Approximately 58 would've been your age, I think.\nMichael Calore: Thank you. I'm very\u2014\nLauren Goode: 50, 55, maybe.\nMichael Calore: I very much appreciate that. Yeah.\nLauren Goode: OK. Tell me your WIRED and TIRED. I'm so excited.\nMichael Calore: OK. So for the WIRED, I'm really excited about the new product from Pebble, which, to be clear, I have not tried. But our colleague Julian Chokkattu has tried it, and he's written about it. It's a ring. It's called Index. It's a smart ring and it costs under $100.\nLauren Goode: What? Is this an Oura competitor?\nMichael Calore: No, it's very, very simple. So it's called Index because you wear it on your index finger and it has a little button that sticks out on the side that you press with your thumb. So it has a thumb button on it, and you press that to activate it, and then you can speak your notes and voice notes into it. Those go onto your phone. There's an on-device AI component that does speech-to-text for your notes and gives you notes that are searchable and shareable on the app on your phone. Nothing goes into the cloud. There is no subscription. So it's an AI-powered smart ring that does not have an AI subscription. And I love this. So my actual WIRED is smart technology that you wear on your body that does not come with a subscription.\nLauren Goode: And then you just wait for the ads to appear? Great.\nMichael Calore: Yeah. So it's pretty sweet.\nLauren Goode: OK. It's called the Index.\nMichael Calore: Index. Yeah.\nLauren Goode: Or just Index. OK.\nMichael Calore: Yeah. Index ring.\nLauren Goode: How much does it cost?\nMichael Calore: It's like $99. I think you can buy it for $75 right now. And the weird thing is it has a battery in it that's not rechargeable, because it does so little they can make it without a rechargeable battery. So you just use it until the battery is depleted, which will take about two to three years with regular use. And then you send it back to them for recycling and then you buy a new one. So you're buying a new one every three years, two years, but that feels OK because they're not hitting you for a subscription in between and they're recycling it for you.\nAnd I will say that the idea that you can get a device that is this good and this smart without a subscription these days is great, because obviously the company is leaving money on the table. And I wish more companies would do that, because the subscription thing is just getting so outrageous. Everything is coming with a data subscription now or a service subscription. We've been feeling subscription fatigue for a really long time with streaming, media, and now we're feeling it with our devices and I'm over it and I would just like to buy things that work and I don't need to pay you any more money.\nLauren Goode: Is that your TIRED?\nMichael Calore: That is my TIRED, yes.\nLauren Goode: Beautifully done.\nMichael Calore: Thank you.\nLauren Goode: Yeah. So what's the solution? Are you just going to cancel a bunch of subscriptions in the new year?\nMichael Calore: I cancel subscriptions all the time.\nLauren Goode: I know.\nMichael Calore: And yes, I will be canceling more.\nLauren Goode: You know what's funny, but it's not funny, it's pretty dark. When you are trying to save money, the first thing that well-meaning folks, whether it's a financial adviser or friends or like the FIRE crowd on Reddit will say is like, \"Cancel all your subscriptions.\" When I tally up my subscriptions, it pales in comparison to the cost of food. Then I look at the food column on a budget spreadsheet. I'm like, \"This is absurd.\" I might as well just keep the subscriptions, because it doesn't even matter anymore.\nMichael Calore: And stop eating. Just rice and beans from here on out until 2027, folks.\nLauren Goode: Well, on that note\u2014\nMichael Calore: All right. Well, thanks, Lauren. This was fun. It was good to recap The Big Interview.\nLauren Goode: It really was, and I hope folks take a few minutes to go listen to those interviews.\nMichael Calore: Thanks for listening to Uncanny Valley. If you liked what you heard today, make sure to follow our show and rate it on your podcast app of choice. If you'd like to get in touch with us with any questions, comments, or show suggestions, you can write to us at uncannyvalley@wired.com. Today's show is produced by Adriana Tapia and Mark Leyda. Amar Lal at Macro Sound mixed this episode. Mark Leyda is our San Francisco studio engineer. Kate Osborn is our executive producer, and Katie Drummond is WIRED's global editorial director.","source":"wired.com"}
{"url":"https:\/\/aecmag.com\/workstations\/amd-threadripper-pro\/","title":"Lenovo on board for launch of AMD Threadripper Pro","date":1594684800000,"text":"AMD finally has a CPU focused exclusively on workstations. And with its launch partner Lenovo, a gateway in the highly lucrative workstation market which has been dominated by Intel for so long\nOver the last 12 months AMD has emerged as a serious competitor to Intel. We\u2019ve seen great price \/ performance from the consumer focused 3rd Gen AMD Ryzen, but it\u2019s with 3rd Gen AMD Ryzen Threadripper that AMD has really turned up the pressure.\nWith fast clock speeds, better per-GHz performance, and up to 64-cores in a single socket CPU, Intel simply can\u2019t compete when it comes to highly-threaded applications like ray trace rendering. And this is why Threadripper has been getting so much attention from users of design viz tools like V-Ray and KeyShot.\nDespite this market leading performance, AMD\u2019s impact in the workstation market has been minimal. Smaller workstation manufacturers like Armari, Boxx, Scan and Workstation Specialists have done well with their AMD-based machines, but without the big three on board \u2013 that is Dell, HP and Lenovo \u2013 AMD was never going to take a significant slice of a market that has been dominated by Intel for so long.\nBut this looks set to change. Today AMD launched Threadripper Pro, a CPU designed specifically for enterprise workstations. More importantly, it has partnered with Lenovo to launch the first Threadripper Pro workstation, the ThinkStation P620.\nThis is massive news for the industry. It\u2019s the first time in nearly 15 years that a major OEM has released a workstation with an AMD CPU. When HP launched the AMD Opteron-based HP xw9400 in 2006, the iPhone didn\u2019t exist, skinny jeans were only for goths and Kanye West was expressing his concerns about \u2018gold diggers\u2019.\nWhat is Threadripper Pro?\nIn simple terms, AMD Ryzen Threadripper is to AMD Ryzen Threadripper Pro, as Intel Core is to Intel Xeon.\nBoth AMD CPUs share the same core silicon, but there are several features that set the workstation CPU apart from its consumer or \u2018enthusiast\u2019 focused sibling. These include more memory channels (8 vs 4), higher memory capacity (2TB vs 256GB) and additional PCIe Gen4 lanes (128 vs 64).\nMemory is arguably the biggest differentiator, and this will be especially important in memory intensive applications like Computational Fluid Dynamics (CFD) or Finite Element Analysis (FEA), which are both used heavily in the automotive and aerospace industries. Some of the more complex fluid flow or multi-physics simulations can literally eat up memory and by offering more capacity and feeding data into the CPU much quicker via 8-channels, it should have a big impact on performance.\nThe increase in memory capacity has been enabled through support for RDIMM and LRDIMM modules. Error Correcting Code (ECC) is also supported, which is important for those running simulations over several hours or even days \u2013 and want to minimise the risk of crashes. Consumer Threadripper did support ECC memory, but not on all motherboards.\nThreadripper Pro also covers a wider range of cores and 12, 16, 32 and 64-core models will be available at launch. In comparison, Threadripper comes in 24, 32 or 64-core variants, while consumer CPUs with 16-cores or less come under the Ryzen brand.\nClock speeds for the 32-core Threadripper Pro 3975WX and 64-core 3995WX are slightly lower than consumer Threadrippers with equivalent core counts, both in terms of base and boost frequency. According to AMD, this is because Threadripper Pro offers more functionality within the same power budget \u2013 specifically referring to memory bandwidth, capacity and the number of PCIe Gen4 lanes.\nWhile the slightly lower frequency will have an impact on performance in most applications, the benefits from increased memory bandwidth and capacity in memory intensive applications like CFD could far outweigh the loss of a couple of hundred MHz.\nConsumer doesn\u2019t mean faster frequencies across the board. The 12-core Threadripper Pro 3945WX and 16-core 3955WX actually have higher base clocks than the equivalent Ryzen CPUs, even though the boost speed is lower. This is because these CPUs have much a higher TDP than the equivalent Ryzens, so more power can be pumped in. All Threadripper Pro CPUs are rated at 280W, while the 16-core Ryzen 9 3950X has a default TDP of 105W.\nEnterprise credentials\nFrom a security and manageability perspective, Threadripper Pro comes with several features that will be really important to some enterprise customers. For example, AMD Memory Guard allows the contents of system memory to be fully encrypted, adding an additional layer of security. This is designed to reduce the threat of a physical memory attack, even if a workstation is left in standby mode.\nAs you would expect, there is a small overhead when using encryption but it\u2019s only a few percent says AMD, and for those who need to protect confidential IP, it\u2019s probably a small price to pay.\nThreadripper Pro also features AMD Pro Manageability, which includes a set of features designed to speed and simplify deployment imaging and manageability within an enterprise IT environment, making it easier to support remote workers. AMD Secure Boot offers boot protection to help prevent unauthorised software and malware from taking over critical system functions.\nOpening doors\nThe launch of Threadripper Pro and, more importantly the fact that one of the big three workstation manufacturers has taken it on, will certainly help open doors for AMD.\nEven though 3rd Gen Threadripper was (and still is) an exceptional CPU, some enterprise customers simply wouldn\u2019t touch it as they only buy from major OEMs. Regional manufacturers like Scan, Armari and Workstation Specialists, simply can\u2019t compete on a global stage when it comes to Independent Software Vendor (ISV) certification, support or manageability.\nAll about the software\nIn the run up to the launch of Threadripper Pro, AMD has been working closely with over 60+ different ISVs. This is not only to get software applications certified, which is essential for some enterprise customers, but to get more performance out of the CPU.\nAMD is addressing this on two fronts. First, to help ISVs take better advantage of the vast number of cores available in a Threadripper Pro CPU and second to help AMD processors run faster on software that relies on optimised Intel code.\nSome tools already run extremely well or can be optimised very easily. Ray trace rendering is a prime example and KeyShot, V-Ray, Corona Renderer, Cinema 4D and many others already show excellent performance and scaling as core counts increase. From a development perspective, Chaos Group only had to make a slight tweak to its V-Ray code in order to take full advantage of Threadripper\u2019s 64-cores and 128 threads.\nSimulation software represents a huge opportunity for Threadripper Pro, and Lenovo told us that some firms are now re-evaluating their workflows. Hardcore simulation software often runs on a server or cluster, so studies are typically prepared on the workstation then added to a queue. Having instant access to huge levels of performance on the desktop could have a huge positive impact on design, test, iterate workflows, bringing simulation up front in the design process.\nFor some simulation software, optimisation can be quite involved, but AMD is not doing this from a standing start. It already has good relationships with many of the leading simulation ISVs through its server team, which are responsible for the AMD EPYC CPU.\nIn fact, as the system architectures of 2nd Gen EPYC and Threadripper Pro are very similar and both have 8-channel memory, any benefits seen in EPYC should automatically carry through to the desktop. Users of STAR-CCM+, Abaqus\/Explicit, Openfoam for CFD, LS-DYNA, Ansys Fluent, and Ansys CFX should find some useful performance information in this AMD EPYC community page.\nOf course, even relatively small amounts of development work can still take time. For example, AMD told AEC Magazine that any software that has Intel Math Kernel Library (Intel MKL) integrated into the code needs a small tweak to get the AMD CPU to run math functions at full AVX2 speeds. AMD is working with the ISVs to make this step unnecessary, but it\u2019s still a manual process for most applications.\nSimulation software developers are very familiar with the challenge of squeezing more performance out of their software, but some software is simply easier to optimise than others. In the past we\u2019ve found performance in Ansys Mechanical to peak at 12 or 16 cores on a dual 14-core Intel Xeon workstation, so it will be interesting to see how software like this runs on Threadripper Pro and whether there are benefits to running everything on a single socket or from the increased memory bandwidth. Certainly, firms need to understand exactly how their software works before investing in a 64-core CPU, and not presume that more cores always mean more performance.\nSoftware licensing costs also need to be considered. In the world of simulation, it\u2019s not just about ultimate performance. Some of the licensing models around CFD and FEA software are still quite archaic, with some licensed by socket or number of CPU cores. With this in mind, there\u2019s simply no point in paying for cores that won\u2019t increase performance, or only by a little. It\u2019s also important to consider performance per core, which could make the 12 and 16-core models (which both have significantly higher all core frequencies than the 32 and 64-core models) much more attractive from a combined hardware and software price \/ performance perspective.\nBeyond rendering and simulation there are many other areas of design and engineering that could benefit from Threadripper Pro, including CAM, point cloud processing or generative design. In some of these applications the potential for optimising for more cores is huge. Point cloud processing software Leica Cyclone Register 360, for example, is only optimised to run on 6-threads, as explored in this article, primarily because the software has only ever been tuned for a mainstream desktop workstation. Now, with 64-cores and 2TB of memory to play with, we\u2019re excited to see what Threadripper Pro could bring to the table.\nShake up at the high-end\nIn the past, customers with workflows that could benefit from lots of cores have had to go for a dual socket workstation. At the top end, this meant one with two 28-core Intel Xeon Platinum 8280 processors. With Threadripper Pro, AMD can deliver even more cores in a single socket 64-core CPU.\nAnd Threadripper Pro looks to be winning out on performance. AMD benchmark figures comparing the Threadripper Pro 3995WX to a dual Intel Xeon Platinum 8280 show superior performance in ray trace rendering software \u2013 36% faster in Keyshot and 12% faster in V-Ray CPU. But it\u2019s not just in multi-threaded applications that AMD is shining. AMD reckons Threadripper Pro 3995X is 22% faster in the single threaded Cinema4D benchmark and also delivers faster 3D graphics performance when using the same Nvidia Quadro RTX GPU on both AMD and Intel systems.\nThe fact that AMD Threadripper Pro can pack so many cores in a single socket could also benefit memory hungry applications like CFD and FEA, as AMD\u2019s Chris Hall explains. \u201cThere\u2019s always an overhead associated with a non-uniform memory access in a dual socket system. So, either the software has to copy the data or there is a higher latency time to access data from memory, on the other socket on the motherboard. There may be some niche cases where you could see a reason to split your 64 cores into two 32 [cores] on each socket. But in general, it\u2019s better to have a single socket with the eight channels that we have, and the 64 cores, if you want to go that high.\u201d\nFinally, there\u2019s cost to consider. A pair of Intel Xeon Platinum 8280s will set you back a whopping $20,000. And while AMD has yet to release pricing of Threadripper Pro, we expect it to only come with a relatively small premium over Threadripper, which costs around $4,000 for the 64-core Threadripper 3990X.\nA CPU for everyone?\nWith the spotlight shining on the 64-core Threadripper Pro 3995X, it\u2019s easy to forget that AMD\u2019s new CPU is available in in 12, 16 and 32 core models.\nNone of the Threadripper Pro CPUs are ideally suited to CAD alone, which are generally single threaded applications, but the 12-core Threadripper Pro 3945X and 16-core 3955X should offer an interesting proposition for what Lenovo describes as \u2018CAD plus\u2019.\nThese are architects, engineers and designers who primarily use 3D CAD or BIM software but also rely on a secondary tool like ray trace rendering, CAM, simulation, generative design or point cloud processing, all of which are generally multi-threaded applications.\nHere, we expect the impressive all core base frequencies of the Threadripper Pro 3945X (4.0GHz) and 3955X (3.9GHz) to give AMD a performance advantage in multi-threaded applications over the equivalent 12-core Intel Xeon W-3235 and 16-core Xeon W-3245.\nIn fact, Lenovo has shared results from the multi-threaded Cinebench rendering benchmark, that shows the 16-core Threadripper Pro 3955X actually beating the 18-core Intel Xeon W-2295.\nLenovo ThinkStation P620\nWhen the Lenovo ThinkStation P620 ships this Autumn it will be the first Threadripper Pro workstation. And it will be the only one for some time too, as Lenovo has an exclusive agreement with AMD for six months from today. Even if HP or Dell do decide to take on the processor, we wouldn\u2019t expect to see those machines materialise until mid 2021 when AMD will likely do a Threadripper Pro refresh.\nWhile the ThinkStation P620 is a new product, it has not been designed completely from scratch. It shares the same chassis as the single socket Intel Xeon W-based ThinkStation P520, although Lenovo has enhanced the cooling to accommodate the 280W Threadripper Pro CPU. The main chassis fans remain the same, but the CPU features a more substantial heatsink, custom designed by Lenovo and AMD, with two built-in fans.\nImportantly, Lenovo has not opted for liquid cooling. In the enterprise space, stability and serviceability are of paramount performance. And while custom liquid cooling solutions, such as the one used by Armari in its Magnetar X64T-G3 FWL workstation, allow Threadripper to hit 4.0 GHz on all 64-cores, most enterprise IT departments prefer to keep things simple.\nWe don\u2019t yet know what all-core speeds we can expect to see in the 64-core ThinkStation P620, but it almost certainly will not be 4.0GHz. To hit that frequency, Armari needs between 550W and 800W of power. With Threadripper Pro, the ThinkStation P620 is locked at 280W.\nBut this is a first generation product. While it seems unlikely that Lenovo will turn to liquid cooling in the future, it doesn\u2019t rule it out completely. Lenovo admits it will depend a little on what the competition does.\nThe ThinkStation P620 chassis is a fairly compact 33 litres. It\u2019s perfect for mainstream users but it does mean Lenovo is not able to take full advantage of the Threadripper Pro architecture. With 8 DIMM slots, the machine is limited to 1TB of memory, although the prohibitive cost of 128GB modules means 512GB is a more realistic maximum capacity when it ships this Autumn.\nThe P620 can support two double height GPUs up to the Nvidia Quadro RTX 8000 or four single height GPUs up to the Quadro RTX 4000. Hosting four RTX 4000s is something that can\u2019t be done in an Intel box. It would be an interesting proposition for GPU rendering or even virtualisation with GPU passthrough, as the P620 can be rack mounted in a data centre.\nNvidia RTX GPUs are currently PCIe Gen 3, so you\u2019ll need to wait for the next generation to take full advantage of PCIe Gen 4, although doubling the bandwidth won\u2019t make a difference in all applications.\nTo take full advantage of Threadripper Pro\u2019s 128 PCIe Gen 4 lanes and offer support for four double height GPUs, Lenovo would need a bigger box, but as it stands the ThinkStation P620 should still satisfy the requirements of most users. Also, most firms investing in a workstation with so many CPU cores won\u2019t necessarily have a need for such an array of high-end GPUs.\nThe most immediate benefit of PCIe Gen 4 will come from storage. Compared to Intel-based workstations which remain on PCIe Gen 3, you\u2019ll get significantly faster sequential read\/write performance in the ThinkStation P620. This can give a real benefit when working with large datasets in post-production, simulation or point cloud processing.\nThe motherboard can host two Samsung PM9A1 PCIe Gen 4 M.2 NVMe SSDs, which can be configured in a RAID array for performance or redundancy. There\u2019s an optional add in board which can host four M.2s, but this is PCIe Gen 3 so you miss out on the bandwidth benefits. For those just interested in capacity, the ThinkStation P620 can host up to four 3.5-inch Hard Disk Drives (HDDs).\nThere have been some additional tweaks to the P520 chassis. There\u2019s 10 Gigabit on board rather than the standard 1 Gigabit, which will be useful for shifting large design viz, simulation or point cloud datasets quickly across the network. There are also two USB Type C ports (the P520 only supports USB Type A).\nImportantly, the ThinkStation P620 will support Linux (Ubuntu and Red Hat Enterprise) as well as Windows 10 Pro. While most software used in product development and Architecture, Engineering and Construction (AEC) runs on Windows, Linux is widely used in simulation, so this should help Lenovo break the Intel monopoly in this space. Lenovo only introduced Linux support to all of its ThinkStation and ThinkPad products in June 2020, so the timing of this is perfect.\nConclusion\nThreadripper Pro is an exceedingly important release for AMD. It\u2019s not just a new processor, it\u2019s a gateway in the highly lucrative workstation market which has been dominated by Intel for so long. Bringing a tier one OEM on-board was essential in order for AMD to get its technology into the hands of enterprise customers. After all, an engine without a car can\u2019t go anywhere fast.\nFor Lenovo, it\u2019s a very smart move. Yes, the ThinkStation P620 promises impressive performance, but bringing Threadripper Pro into the fold is also about satisfying the diverse requirements of enterprise customers. For years the tier one manufacturers have held niche machines in their product ranges simply to secure big deals with enterprise customers. The 17-inch mobile workstation is a case in point. It\u2019s never sold in big numbers, but there\u2019s always a handful of users that want one.\nThe 64-core Threadripper Pro will certainly grab all the headlines, and it offers an excellent proposition for users of ray trace rendering or highly threaded simulation software. However, it\u2019s the 12- and 16-core models that will likely get most attention from manufacturing and AEC firms, offering good single threaded performance and excellent multi-threaded performance at what we expect to be an attractive price point.\nWhile this will appeal to so-called \u2018CAD plus\u2019 users, Threadripper Pro doesn\u2019t currently pose a real threat to Intel at the entry-level, where the majority of architects and engineers just need a high frequency CPU with a few cores to run CAD or BIM software. But if AMD can improve its single threaded performance, who knows where Threadripper Pro (or Ryzen Pro) might end up.\nLenovo is certainly in for the long haul. At the moment the ThinkStation P620 covers the middle ground but there\u2019s certainly room for a higher-end machine, with more memory and more GPUs. Lenovo hinted that it will expand its Threadripper portfolio over time, just as it has done with Intel.\nIn bringing AMD on board, Lenovo didn\u2019t just need to think about technology. The move is sure to have had some impact on its relationship with Intel. Now that Lenovo has taken those first big steps, it will be very interesting to see if HP and Dell follow suit.\n2021 is going to be a very important year for AMD in the workstation market. If Threadripper Pro is received well by enterprise customers and taken on by other OEMs, then Intel will surely start to get worried.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/high-roller\/","title":"High roller","date":1412035200000,"text":"Arup leads structural engineering of Las Vegas High Roller by integrating Bentley Systems software with a number of third-party tools including Rhino, Oasys GSA, LS-DYNA, Altair HyperMesh and Navisworks.\nThe 168 metre tall observation wheel, aptly named \u201cHigh Roller\u201d, is the focal point of The LINQ, a new quarter-mile open-air shopping, dining, and entertainment district on the Las Vegas, Nevada, Strip. Featuring 28 spherical cabins supported by a circular wheel rim, the ride creates an immersive audio, video, and lighting experience while providing dizzying views of the city for up to 1,120 people each turn.\nArup served as engineer of record as part of the project team commissioned by Caesars Entertainment Corporation to develop the US$550 million district. The firm performed structural, geotechnical, mechanical, and electrical engineering, as well as acoustics and fire safety consulting for the observation wheel, which sits opposite Caesars Palace.\nAs the primary building information modelling (BIM) software used by the team, Bentley\u2019s Structural Modeler enabled the seamless integration with other applications that was instrumental in 3D co-ordination and delivery of the project.\nMeeting high expectations\nWhen it opened to the public in March 2014, the High Roller fulfilled the client\u2019s goal of becoming the tallest observation wheel in the world, surpassing the London Eye and the Singapore Flyer. The 161-metre-diameter wheel will rotate as many as 650,000 times before it has outlived its 50-year design life.\nThe structure was designed to resist fatigue and withstand concentrated stress on bearings, steelwork, cables and fittings associated with turning 28 cabins, each weighing 44,000 pounds plus occupant load for the duration of the 30-minute ride.\nTo maximise sight lines from within the cabins, the rim element and visible support structure had to be minimised. As a result, the wheel is constructed with tensioned spokes and a unique single rim that is kept in compression. Numerous detailed stress analyses were conducted to optimise attachments to the rim required for power, communication, lighting and safety. All elements and supporting brackets were modelled in Bentley\u2019s Structural Modeler prior to shop drawing production.\nThe narrow footprint of the site presented a number of challenges for the design team. The High Roller is situated over an existing road adjacent to a monorail, so the potential locations for the support legs were limited. After extensive design studies, Arup determined that the best approach was to support the hub of the wheel using four inclined legs each 2.8 metres in diameter, with a single transverse brace founded across the road. The support legs are inclined to provide enough width such that the cables of the wheel serve as an efficient lateral system while minimising the structure\u2019s footprint.\nWind tunnel tests were performed to predict wind-induced motion and damping requirements. Consequently, the support legs contain a total of thirteen tuned mass dampers to arrest vibrations that might negatively impact the ride experience for passengers.\nPassengers experience a floating sensation as the wheel rotates. The spherical cabin shape presented an architecturally arresting design and, at the same time, provided spacious interiors and unobstructed panoramic views. The cabins also presented a climate control challenge to prevent the desert temperatures and solar gain from creating a hot-box environment. Arup\u2019s engineers optimised the design of the cabin air-conditioning and glazing, so the spherical panels are double glazed to limit the air-conditioner power demands.\nSelecting the right tools\nArup overcame these design challenges by using a number of software packages, and selecting each application to fulfill a specific purpose. At the early stages of design, McNeel\u2019s Rhinoceros freeform modelling software was chosen to provide quick and accurate conceptual modelling. GSA Suite, a structural design and analysis package by Oasys (Arup\u2019s software house), was selected for early analysis of simplified beam-element models.\nWhen a highly sophisticated BIM application was required, Arup chose Bentley\u2019s Structural Modeler. Other programs were applied as the project advanced.\nAt the concept stage, as the wheel\u2019s structural geometry became more defined, Arup created a parametric model using GenerativeComponents, Bentley\u2019s associative and parametric modelling system for automating the design processes and accelerating design iterations. The parametric model helped to set all the variables for the wheel geometry and discover which dimensions drove the design. The geometry was then exported to the GSA Suite for structural analysis.\nAnalysis revealed that sections of the wheel would undergo stress fluctuations during normal operation. These fracture-critical elements included the rim, with heavy stress at the six o\u2019clock position and light stress at the 12 o\u2019clock position. Arup created detailed finite element models using LS-DYNA, a general-purpose finite element program by Livermore Software Technology Corporation, to identify the range and location of stress as the wheel completes each rotation.\nThe interoperability of the software applications was crucial to the process of generating accurate fatigue stress calculations. For example, the detailed rim model included every bolt, conduit attachment, lighting fixture, access hatch, etc. This rim model was created in Rhinoceros, transferred to Structural Modeler for design, imported to Altair HyperMesh (a high-performance finite element pre-processor), and finally analysed in LS-DYNA. This revealed hotspots where stresses had to be reduced; then the process was repeated.\nCo-ordinating 3D design\nStructural Modeler was used to combine the outputs from all other third party products into a global co-ordinated model. Even the complex manufacturing models for the drive systems created with Dassault Syst\u00e8mes\u2019 SolidWorks products were imported seamlessly via the Parasolid import functionality. An overall project co-ordination model was exported from Bentley software to Autodesk\u2019s Navisworks project review software.\nBy regularly updating the Navisworks model and making it accessible to all design parties, Arup was able to interface more effectively with teams that were designing other components of the project. The fully integrated, shared geometric model encouraged co-ordination among disciplines and companies. As a result, dimensional clashes were identified early in the design process and addressed prior to fabrication, saving the client time and money.\nWhen field checking steelwork member sizes, engineers eliminated paper drawings by using Navigator Mobile, Bentley\u2019s app for navigating 3D models and documents on an Apple iPad.\nTo document the project, Arup relied upon Bentley\u2019s AECOsim Building Designer, a BIM application that enables multi-discipline teams to design, analyse, construct, document, and visualise buildings of any size, form and complexity. The dynamic view tools were used to create views of every component of the wheel. This enabled a rapid workflow that allowed Arup to meet tight project deadlines for this challenging project.\nEngineered to endure\nAs the primary BIM application used for the High Roller, Structural Modeler provided the precision required to build a complex, atypical structure.\n\u201cThe 3D modelling was vital to bring together all the custom-made elements,\u201d Arup Senior BIM Technician Stephen Corney explained. \u201cThe integration of Bentley products with other software also facilitated collaboration among project team members. The ability to import and export various file formats from other consultants involved in the design process was instrumental to 3D co-ordination and delivery of the project.\u201d\nThe High Roller design is not only structurally efficient but also yields superior fatigue performance for a fairground-style Ferris wheel. Planned to operate 18 hours a day for 50 years, the iconic ride is detailed to withstand fatigue at the point of every weld, penetration, and attachment. It will endure as an engineering feat and world-class tourist attraction on the Las Vegas Strip.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/autodesk-design-computation-symposium\/","title":"Autodesk Design Computation Symposium","date":1240704000000,"text":"At last year\u2019s Autodesk University, Dr Robert Aish hosted a Design Computation Symposium for architects and construction engineers to see how computers can assist in design creation.\nUnder the stewardship of chief executive officer, Carl Bass, Autodesk has accelerated its development of and experimentation with new design technologies. Some of this work has been done in-house, part by acquisition, but there have also been some strategic staff additions, one of which is Dr Robert Aish, former director of research at rival, Bentley Systems.\nDr Aish had developed Generative Components (GC), a parametric modelling system which ran on top of Bentley MicroStation. The software was widely adopted by firms that were at the cutting edge of design modelling. GC provided a tool to enable very complex forms to be generated and manipulated using algorithms. Leading practices such as Foster + Partners, KPF and Arups were all experimenting with the system to create rules-based designs, which could be quickly re-calculated should the underlying geometry change. While still in extended beta, some buildings and bridges were designed and built using GC.\nAt the time Autodesk\u00c6s approach to this need for free form design was to push 3ds Max or Maya but these were not similar parametric solutions. With Autodesk\u00c6s aspirations to move up the food chain and provide tools to signature architects, it recognised that it had to have some new technology tailored to these very specific needs. The net result was Dr Aish leaving Bentley Systems to join Autodesk and to think through the solution from scratch. Dr Aish had the pick of Autodesk\u00c6s vast toolkit of 3D technologies and oddly enough, ended up in the platform development group, working with good old AutoCAD. From this positioning, we can assume that Mr Dr Aish\u00c6s new code will serve as a platform-level technology, available to many industry groups.\nHaving had a little less than a year at the company, Dr Aish\u00c6s first Design Computational Symposium at Autodesk University in Las Vegas, was also his first public outing. With nine industry speakers and Carl Bass on hand, the presentations explored many different types of situations where the computer can take the processing load off the designer: allowing multiple solutions to be devised in minutes; using simple software to create projects of great complexity; enabling models to feed back structural and sustainability performance; allow the design of adaptive systems for buildings; rapidly optimise to best suit design intent; aid digital fabrication and using algorithms to integrate both architectural and urban design briefs, to computationally derive the best fit.\nThe events were very similar to Mr Dr Aish\u00c6s Architectural seminars traditionally held before Bentley\u00c6s yearly gatherings. This was, perhaps, to be expected, with many familiar faces and projects at the event, although this time there was AutoCAD, Ecotect and Inventor. What was more significantly different was the audience, which were, by and large, from practices that were nowhere near investigating GC, let alone dabbling in 3D.\nIt\u00c6s from talking to these attendees that you can, in part, see why Dr Aish has joined Autodesk. With a company the size of Autodesk, and not forgetting its massive resource capability, here Dr Aish\u00c6s vision of \u00e6programmatic design for all\u00c6 could be best popularised.\nWhile the technology is still young, there was a brief demonstration of some new code running on AutoCAD. At the moment, Dr Aish calls it D Sharp (a play on Microsoft\u00c6s C Sharp language) and it\u00c6s a totally brand new programming language that provides a relatively simply way of building relationships between standard AutoCAD geometry.\nThe demonstration showed a 3D tree-like support structure that would snap to the AutoCAD baseline grid but would deform the \u00e6roof\u00c6 surface above it, wherever it was moved within the model. Unfortunately, photographs and videos were not allowed, so for now, I can only describe it. It would seem that \u00e6D Sharp\u00c6 may well extend beyond AutoCAD but for now, that is the target platform. As \u00e6D Sharp\u00c6 is essentially a language based on defining relationships and is not a geometry engine, it could eventually be applied to other Autodesk products, as parametric design is useful across a broad range of application areas.\nBentley Systems is obviously a tad concerned at what Dr Aish is developing at Autodesk. His loss certainly accelerated Bentley System\u00c6s promotion and development of GC within the architectural community. However, looking at McNeel & Associates\u00c6 exciting Grasshopper parametric technology for Rhino, it\u00c6s obvious to see that this programmatic approach to design is really starting to gain momentum.\nConclusion\nIt is always exciting when there is a technology bun fight, an arms race if you will. With the best minds on all sides coming up with better, faster and cheaper ways to wow us with tools that ultimately let amazing forms be realised and built. All this technology means sophisticated results at the same cost for a boring rectangular building.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/cintoo-cloud-makes-light-work-of-gigantic-point-clouds\/","title":"Cintoo Cloud makes light work of gigantic point clouds","date":1541808000000,"text":"New cloud service streams colossal laser scans to any device and supports collaborative BIM processes\nCintoo has launched a new technology designed to handle the gigantic point clouds generated from terrestrial laser scans. Developed with a focus on BIM workflows in the AEC industries, Cintoo Cloud embeds what it describes as a unique, patent pending point cloud-to-surface and surface-to-point cloud technology that enables users to access high precision terrestrial laser scans on demand via the cloud.\nCintoo Cloud stands out for its ability to handle even the highest scan resolution data with over a billion points. Datasets as large as 40GB can be streamed in 3D as a mesh to \u2018any desktop or laptop device\u2019 using a standard web browser. Cintoo says there is no compromise on accuracy.\n\u201cThe data generated by terrestrial laser scanners is so massive that the whole industry is stuck with very desktop-centric workflows. And it\u2019s getting worse. Modern scanners are producing higher quality data all the time, so the size of point clouds is just getting exponentially larger with every new scanner model being released in the market. This is the problem being solved with Cintoo Cloud: we are removing the bottleneck of data size that has limited the adoption of laser scanning for many years,\u201d said Dominique Pouliquen, co-founder and CEO of Cintoo US Inc. in San Francisco.\nWith the point cloud-to-surface and surface-to-point cloud technology, users can convert point cloud data to 3D surfaces within Cintoo Cloud then reconvert it when required.\nThe solution offers a range of visualization features, with display modes in RGB, x-ray, height or 3D surface, that, according to Cintoo, reveals a granular level of scan detail not previously possible in 3D laser scanning.\nScan-to-BIM and Scan-vs-BIM workflows are supported, which Cintoo says allows laser scanning to be fully integrated in collaborative BIM processes. From a workflow perspective, the user can organize a project into work-logic areas called Work Zones (such as rooms, floors, etc.) assigning specific scans to each zone and then exporting only the necessary data for a Scan-to-BIM job. Crops of project sections and elevations can also be easily exported, while new measurements can be performed, and annotations created to inform other project users.\nCintoo Cloud is fully compatible with the Autodesk portfolio thanks to the integration of the ReCap SDK. Cintoo Cloud can read RCP files (Autodesk ReCap format for registered laser scan projects) and can generate RCS files (unified point clouds format for the Autodesk portfolio) of cropped sections or elevations, scans, Work Zones or the entire project. Cintoo Cloud is also connected to BIM 360 Docs via the Autodesk Forge platform, so that users can overlay a 3D BIM model over Reality Data in Cintoo Cloud\u2019s viewer to check for differences. Import and export of point clouds are also available using the e57 exchange format.\nCintoo Cloud can be deployed in the cloud on either Amazon Web Services or Microsoft Azure, and customers have the option to select the specific region where the project data will be hosted. A private cloud deployment is also possible.\nBBA, Canada is one of Cintoo\u2019s North American users. Mario Laflamme, Development Manager, CAD and 3D Scan Technologies explains how Cintoo Cloud\u2122 is changing the way he and his teams work: \u201cCintoo Cloud is a game changer for BBA. It allows our clients and our own teams to collaborate directly on a Reality Capture project regardless of their physical location. BBA has a dozen offices across Canada, so being able to work on the same laser scan data via the cloud saves us an enormous amount of time and resources.\u201d\n\u201cCintoo Cloud 1.0 is just our first step. Once the Reality Data is in the cloud, there are various analytics and measurements that can be performed. Our goal is to enrich the platform in the months to come with the web services that will help our users extract meaningful value from their laser scans: floor flatness, ADA compliance, beam deflection, cloud to cloud or cloud to BIM diff analysis, for example,\u201d said Leonardo Hidd Fonteles, PhD, co-founder and CTO of Cintoo SAS, based in France. \u201cMachine learning running in the cloud will also help to automatically classify the 3D data and allow our users to make the most sense of their assets. Since our Reality Data is mesh-based, we are also direct-VR and simulation ready. In the near future, we expect that remodeling from the point cloud will be less necessary since Cintoo Reality Data is already a 3D model.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/brightearth-global-natural-colour-mosaic-released-by-computamaps\/","title":"BrightEarth-Global natural colour mosaic released by ComputaMaps","date":1171238400000,"text":"South Africa, 9 February 2007 \u2013 ComputaMaps, South Africa-based provider of geospatial data solutions, has announced the release of BrightEarth, a natural colour image mosaic covering 95% of the Earth\u2019s land surface at a resolution of 14.25m.\nThe source for BrightEarth is the orthorectified Landsat 7 imagery acquired between 1999 and 2002 for NASA\u2019s GeoCover programme. Using a proprietary algorithm, ComputaMaps has transformed the original false-color GeoCover imagery mosaics into a single color-balanced global mosaic with bright, vivid natural color that retains the high contrast detail of the original pan-sharpened source imagery.\nBrightEarth imagery has less than 10% cloud cover for 90% of source scenes, and has an absolute horizontal accuracy of \u00b150m RMSE with many areas under 25m. BrightEarth is an ideal global imagery base for Internet map portals, location-based services, mobile content and navigation, 3D visualization and flight simulation, security and relief agency logistics, or any application where a seamless, highly detailed, geographically accurate, and realistic view of the world is required.\nBrightEarth is available in UTM or Geographic projection, WGS84 datum, by individual country or continent, or as a single global coverage in a variety of compressed and uncompressed formats.\n\u2013 About ComputaMaps\nComputaMaps is a provider of geospatial data solutions. It provides essential 3D digital map databases to the wireless telecommunications industry, as well as to flight safety and other industries. ComputaMaps has processed satellite and aerial imagery, and produced terrain models covering over 150 million sq. km of the Earth\u2019s surface. Its catalog of off-the-shelf data exceeds 1200 cities worldwide, ensures quick delivery of the required data. Customized data can also be processed quickly and accurately. For more info visit: www.computamaps.com","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-revit-based-mep-tool-introduces-live-clash-detection\/","title":"NEWS: Revit-based MEP tool introduces live clash detection","date":1530835200000,"text":"MagiCAD 2019 for Revit also features new tools for the ventilation design, sprinkler system design and electrical design\nMagiCAD 2019 for Revit, the latest release of the MEP software from Progman, features a new clash detection tool for live and on-demand detection of hard and soft clashes in Revit projects.\nThe new version also includes new tools for ventilation design that allow air devices to be connected to the network using rectangular ducts. In addition, the length of the final duct segment can now be automatically corrected in BOMs to the nearest longer standard length and equipped with a loose flange.\nMagiCAD 2019 for Revit also offers extended standards support for domestic hot water calculations and enables sprinklers to be connected to the network using flexible pipes. For electrical design, there are also several improvements, including summarisation of power loads from spaces to switchboards and calculation of wire lengths based on the Revit model.\nProgman also offers a version of MagiCAD 2019 for AutoCAD, which provides more advanced automation for Running Index based on user-defined validity settings. At the same time the software introduces an upgraded Change Properties tool which enables improved property search, object filtering and editing of multiple properties simultaneously. There\u2019s also a new option of hiding underlying objects behind dimension texts, which is designed to provide the user with more control over the readability of plan drawings. In electrical design, the number of devices and the longest branch can now be calculated also in low-voltage (LV) and extra-low voltage (ELV) circuits.\nMagiCAD enables MEP design using what Progman claims to be Europe\u2019s largest product model database, featuring over 1,000,000 actual products and product variants from 250 leading manufacturers across the world.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-umbra-streams-visually-rich-bim-models-to-any-device\/","title":"NEWS: Umbra streams visually rich BIM models to any device","date":1511827200000,"text":"Cloud-based 3D solution optimises geometry so complex models can be viewed on tablets and AR\/VR devices\nA technology that has long been used to optimise real time rendering in 3D games, is now being applied to the AEC sector. Umbra Composit is a fully-automated cloud-based 3D solution that applies Umbra\u2019s patented algorithms to intelligently optimise complex models so they can be viewed on virtually any internet-connected device.\nWith Composit, CAD\/BIM models are uploaded to the cloud where they are then automatically optimised through geometry decimation, occlusion culling and texture and light baking.\nComposit also automatically generates Level of Detail (LoD) within the model so only data relevant to the current view is streamed on demand to a WebGL-enabled device, which could be a PC, tablet or AR\/VR platform such as the Microsoft Hololens, Apple ARKit, or Samsung Gear VR. This is different to many AR\/VR workflows which require a large packaged file to be downloaded and then loaded into memory.\nAccording to Umbra, its streamlined workflow makes it possible for large, geographically dispersed teams of varying technical expertise to review renderings in real time, greatly accelerating the design review process. Importantly, the company says that users do not need powerful 3D workstations to view the models.\nUmbra currently offers a \u2018single click\u2019 workflow from Revit and ArchiCAD. Users simply download the relevant Composit plug-in, select the model to optimise and hit \u2018Umbrafy\u2019. Support for Rhino, Navisworks, SketchUp and 3ds Max is coming soon.\nCurrently, when changes are made in the BIM authoring tool, the entire model has to be re-uploaded to the cloud. However, in the future, Umbra will support delta changes. Meta data is not currently included in the model, but this could also change.\n\u201cComposit is addressing multiple pain points in today\u2019s complex AEC design workflow.\u201d says Shawn Adamek, Umbra\u2019s chief marketing officer. \u201cUpstream, our ability to optimise models in minutes enables architects and designers to accelerate revision cycles with colleagues, clients and other stakeholders. Downstream, our platform-agnostic viewer allows anyone, anywhere, on any device to view and interact with 3D models.\u201d\n\u201cUmbra has made a huge difference to our team\u2019s workflow, as now all our Revit users can display models in the HoloLens for clients, which before was impractical and time consuming,\u201d said Dr Maxwell Mallia-Parfitt, Fulcro Applied Technologies.\nComposit is available now, pricing starting at $239 per month. A free 14-day trial of can be downloaded here.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/bluebeam-pdf-revu-90\/","title":"Bluebeam PDF Revu 9.0","date":1306454400000,"text":"Greg Corke reports on a low-cost PDF creation tool that also offers a very capable set of markup, editing and collaboration tools.\nThe PDF (Portable Document Format) is ubiquitous in the AEC sector and files featuring drawings and models can be created from almost any CAD application then viewed by anyone with a free PDF viewer such as Adobe Reader.\nCreating a PDF from a CAD drawing used to require a third party tool such as Adobe\u2019s Acrobat Professional. However, in recent years this has changed and most CAD applications now feature their own PDF publisher.\nFor third party software developers, the commoditisation of PDF creation has meant a change in focus. Bluebeam Software, whose first product Pushbutton PDF was used to publish PDFs from AutoCAD, now focuses on PDF-based design\/ review workflows. Its flagship software, Bluebeam PDF Revu, is in its ninth release and available in three versions.\n\u2018Standard\u2019 is designed for non-CAD users and includes PDF viewing, markup and editing tools, plus one button and batch PDF creation from Microsoft Office and other Windows applications.\n\u2018CAD\u2019 is specifically for users of AutoCAD and Revit and includes all of the functionality of \u2018Standard\u2019 plus one button and batch PDF creation of CAD drawings.\n\u2018eXtreme\u2019 is designed for the so-called power user and includes all of the functionality of \u2018CAD\u2019 plus advanced features such as character recognition, PDF form creation and scripting.\nPDF Revu Standard\nPDF Revu includes a very capable set of markup tools, including text, notes, highlighter, lines, clouds, callouts, stamps and many others. One of the standout features, however, is the ability to customise and re-use markups or comments. This could be a certain shape of revision cloud or a specific symbol. Users can pick mark-ups from a \u2018recent\u2019 list or store them in a library of company standards, which can be shared by multiple users.\nTo help bring order the review process the status of markups can also be changed and tasks assigned to different users. In the design phase this could be a structural or building services engineer while in the construction phase the building or electrical contractor. Assigned responsibilities can be displayed on screen in different colours or sorted in a summary list. Bespoke summary reports can also be created for individuals.\nPDF Revu has a full set of measurement tools, including lengths, areas, angles, and perimeters. There is also a tool to help count objects in a PDF which can be used for quantity take-off.\nNew for version 9 is an area cut out tool, which enables users to remove unwanted portions from measurements. For example, to measure the floor area in a kitchen without counting the kitchen island.\nThere\u2019s also a neat feature for those working on global projects where all markups can be automatically translated into another language using Google Translate.\nWhile PDF Revu has always allowed users to search for text, a new VisualSearch tool allows them to locate symbols on PDFs and apply highlights and hyperlinks to results. For example, if the reviewer decides that all of the sinks in an architect\u2019s plan need to be changed he or she simply draws a rectangle around one sink and PDF Revu will automatically identify those similar and annotate them in one go. This neat feature works with both vector and raster data and can also be used to search on multiple PDFs stored in a specific folder as well as the current file or all open documents.\nThe headline feature is the introduction of 3D PDF viewing and here users can spin, pan, and dynamically zoom around models. To aid communication, specific 3D views can be defined so project participants can be instantly taken to an exact location in the model. To highlight or isolate specific features of a model, parts can be turned on and off, or made transparent.\n3D models can be marked up, but this is not yet done directly in the 3D PDF. Instead, the user takes a snapshot of the model, pastes the 2D image into a blank page in the PDF document and uses the standard 2D markup tools in PDF Revu. Pre-defined 3D views can then be referenced on this 2D page.\nThis 3D\/2D markup process is not perfect, but actually works quite well. We imagine full 3D markup capabilities will be added in a future release, as well as the ability to actually create 3D PDFs.\nPDF Revu CAD\nFor this latest release Bluebeam has made some significant enhancements to its AutoCAD plug-in. Much of this is in the area of batch processing where users can change conversions settings of selected files after they have been placed in the queue. For example, adding a global stamp, changing the page setup options, or automatically adding a prefix or suffix to each converted drawings. AutoCAD sheet sets can also be batch processed now.\nPDF Revu eXtreme\nFor CAD users who create PDFs from scanned drawings, the headline feature in \u2018eXtreme\u2019 is likely to be Optical Character Recognition (OCR), where raster characters are turned into searchable and editable text.\nScripting capabilities are also included in this top\u2013end version so users can automate multi-steps processes such as adding a PDF cover sheet and text stamp, flattening and then emailing a PDF. Scripts can also be shared with other PDF Revu eXtreme users.\nOther PDF Revu eXtreme-only features include PDF form creation and the ability to permanently remove confidential text and images from a PDF.\nConclusion\nPDF Revu CAD and eXtreme will certainly appeal to those looking for more control and automation when creating PDFs from AutoCAD or Revit. But the platform\u2019s real strength lays in its highly-capable markup tools.\nFor many AEC firms, PDFs have become an essential part of the design review process and the software includes a number of excellent tools, some of which are not even available in Adobe\u2019s more costly Acrobat Professional.\nPDF Revu is also likely to find interest from non-CAD users, such as contractors and quantity surveyors on site marking up plans. It already runs on Tablet PC and there have also been rumours of an iPad version.\nOf course one of the real beauties about PDF Revu is the cost. Priced from $179-$299 with volume discounts available for five or more seats, the software is incredibly affordable.\nProduct info\nProduct: PDF Revu\nSupplier: Bluebeam\nPrice: $179-$299\nWebsite: www.bluebeam.com","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/csa-data-compression-exclusive-in-new-geomatica-9\/","title":"CSA data compression exclusive in new Geomatica 9","date":1060214400000,"text":"New technology for hyperspectral data processing and analysis can now be found in Geomatica 9, the latest edition of PCI Geomatics\u2019. Geomatica 9 offers new hyperspectral data tools as well as data compression technology that was developed in cooperation with the Canadian Space Agency (CSA). These latest elements, exclusive to Geomatica 9, make using hyperspectral data \u2013 often considered a cumbersome, memory intensive, and time-consuming data type \u2013 a practical and feasible data option for PCI Geomatics customers.\nHyperspectral image sensing, processing, and analysis allow for easier discrimination of earth surface features that have signal absorption and reflection characteristics over narrow wavelength intervals. These reflection characteristics are normally lost within the relatively large bandwidths of a conventional multispectral scanner. The intricate spectral nature of the data collected, however, typically results in large and difficult to manage data sets. The application of the PCI Geomatics Advanced Hyperspectral Package with the exclusive data compression technology provides users with many productivity advantages. Reduced program execution times have been achieved with impressive accuracy when using compressed hyperspectral data. Processing of large hyperspectral images can be carried out using desktop computers with moderate amounts of RAM and larger hyperspectral images no longer need to be divided into smaller images for processing.\nThe functionality built into the Geomatica 9 Advanced Hyperspectral Package provides viewing and analysis capabilities for effective processing of hyperspectral data (as well as over 100 spatial data formats). Capable of representing the many narrow and practically contiguous bands throughout the visible, near-infrared, and mid-infrared wavelengths, Geomatica 9 allows hyperspectral image sensing, processing, and analysis for easier isolation and accurate differentiation of earth surface features. With advanced GIS features, atmospheric correction algorithms, and now easier hyperspectral processing and data compression capabilities, Geomatica 9 is ready to help people with all of their remote sensing, photogrammetry, spatial analysis, and cartography work. The Advanced Hyperspectral Package is comprised of hyperspectral application programs, visualization programs accessible through the Geomatica 9 viewing environment, and spectral libraries from the United States Geological Survey (USGS).","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/abb-joins-intergraph-smartplant-alliance-program\/","title":"ABB joins Intergraph SmartPlant Alliance Program","date":1173398400000,"text":"Huntsville, USA, 8 March 2007 \u2013 ABB Inc., a leader in power and automation technologies, has joined the Intergraph SmartPlant Enterprise Alliance Program. The alliance extends integration capabilities from initial control system design to plant handover for Engineering, Procurement and Construction and Owner Operators using ABB\u2019s IndustrialIT Extended Automation System 800xA and SmartPlant Enterprise suite, assuring ongoing support for the life cycles of their plants and assets.\nThe enhanced relationship will bring integration benefits for users of the Industrial IT enabled Intergraph SmartPlant Enterprise suite and ABB System 800xA including: initial configuration of the control system; updates to the control system during design changes; documentation of control system configuration; consistent as-built documentation during plant handover; and improved decision making during critical maintenance and operations events.\n\u201cThe increased integration of two of the industry\u2019s most widely used systems resulting from this alliance will provide even more ways to improve overall engineering efficiency and productivity for our customers,\u201d said ABB Group Vice President Mark Taft.\n\u201cThis alliance exemplifies ABB\u2019s ongoing strategy to complement System 800xA through the integration of best-in-class products and capabilities to reduce life cycle costs of our customers\u2019 automation investments and provide them with the tools and technologies they need to get the most value out of that investment.\u201d\n\u201cABB is the third and largest distributed control system supplier to join the SmartPlant Alliance Program,\u201d said Patrick Holcomb, Executive Vice President, Intergraph Process, Power & Marine. \u201cThe addition of ABB to the Alliance Program underscores the widespread acceptance of SmartPlant Enterprise and its open, integrated platform to extend value across the engineering enterprise.\u201d\nABB\u2019s System 800xA extends the scope of traditional control systems to include all automation functions in a single operations and engineering environment so that plants and mills can run smarter and better at substantial cost savings. Its unique engineering environment manages one set of consistent data, for single-point entry, single-point change, and re-use across the plant.\nIntergraph SmartPlant Enterprise is a comprehensive product suite offering increasing value from IT to improve project execution, handover and plant operational efficiency. SmartPlant Enterprise includes the following components: 3D Modeling & Visualization; Information Management, Engineering & Schematics; Materials Management & Project Controls and the SmartPlant Alliance Program. From concept and design through operations, maintenance and decommissioning, Intergraph enables electronic management of all of the plant\u2019s engineering information, integrating information on the physical asset, processes, and regulatory and safety imperatives.\n\u2013 About ABB\nABB (www.abb.com) is a leader in power and automation technologies that enable utility and industry customers to improve their performance while lowering environmental impact. The ABB Group of companies operates in around 100 countries and employs about 108,000 people.\n\u2013 About Intergraph\nIntergraph Corp. is a leading global provider of spatial information management (SIM) software. Security organizations, businesses and governments in more than 60 countries rely on the company\u2019s spatial technology and services to make better and faster operational decisions. Intergraph\u2019s customers organize vast amounts of complex data into understandable visual representations, creating intelligent maps, managing assets, building and operating better plants and ships, and protecting critical infrastructure and millions of people around the world. For more information, visit www.intergraph.com.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/the-age-of-the-reality-mesh\/","title":"The age of the reality mesh","date":1480982400000,"text":"Ever since Bentley Systems acquired Acute3D in February 2015, it has forged ahead with Reality Capture, integrating key technologies into its foundation MicroStation platform and beyond. By Martyn Day and Greg Corke.\nAt the Year In Infrastructure (YII) conference in 2015, Bentley Systems showed exactly how viable reality computing techniques had become, showing off ContextCapture, a new software product based on the Acute3D technology it had acquired earlier that year. Starting with hundreds or thousands, of photos captured by drone or taken by hand, ContextCapture automatically produces a detailed 3D reality mesh. In other words, an engineering ready 3D model complete with textured photos.\nIn just one year, Bentley has come a long way. It has decided that reality meshes are now a new fundamental dataset for MicroStation \u2013 in addition to 2D vector lines, 3D solids, 2D raster and 3D point clouds. It has deeply integrated the reality mesh capabilities into its platform stack and already developed applications for users to integrate mesh work into their design and documentation processes.\nIn the early days of reality capture, it felt that Bentley competitor Autodesk, was pushing the boundaries, having previously licensed a past version of the Acute3D technology, as well as develop ing point cloud applications such as Recap and Memento. However, Autodesk has a disparate array of products \u2013 Revit, AutoCAD, Civil 3D, Inventor, Fusion and Infraworks, which appears to have slowed deployment and direction.\nNow Bentley appears to have taken the initiative. At this year\u2019s YII conference, it demonstrated OpenRoads Designer, an incredibly dynamic environment for civil design which felt as graphically-rich as a game but generated road designs with cut, fill, 2D documentation and even enabled live \u2018optioneering\u2019.\nNow that reality mesh technology is in MicroStation, it seems every Bentley vertical application will get a \u2018Designer\u2019 variant, with versions for the building design application AECOsim soon to be in public beta. This would allow buildings to be designed in context, exterior facades grabbed for remodelling and existing brown or green field sites quickly captured and imported.\nComplementary applications such as SITEOPS can run optimisation algorithms on the mesh, to provide the best orientation and layout of buildings and roads. Cut and fill requirements for grading can also be explored accurately, very early on, helping reduce risk when budgeting for construction.\nBentley Descartes provides a number of tools to get more value from the data. It can produce digital terrain models straight from the mesh, as well automatically extracting features, such as breaklines. Attribute information can be added to specific parts of the mesh using the new reality mesh classifier. This enables users to search and query the mesh based on the associated data, providing a means to visualise geospatial information.\nThe use of reality meshes goes way beyond the design phase. For construction, drones can be set up on flight plans for continuous survey. Daily models of the construction site can be automatically produced, so progress can be monitored, materials located or volumes of material measured. This record of building progress could be stored and accessed later for as-built or facilities management information.\nBentley\u2019s reality mesh technology is not just being used within the Bentley sphere. Topcon, for example, is incorporating ContextCapture image processing for its mass data collection via drones. This is part of a bigger \u2018constructioneering\u2019 collaboration between the two companies where survey, engineering and construction data are more tightly integrated and digital engineering models are used to feed the 3D machine control that guides the construction machinery.\nLaser scanning, the original reality capture technology, isn\u2019t being forgotten. Bentley ContexCapture now supports both point clouds and reality meshes in a single environment. Bentley showed how this could be used to great effect on a rail maintenance project, where a train mounted laser scanner provides a very accurate survey of the track, while photographs taken from a drone provide a broader \u2018reality mesh\u2019 context around the track.\nMesh power\nTo find out more about the integration and delivery of Bentley mesh data for customers, AEC Magazine talked with Santanu Das, senior vice president, design & modelling, Bentley Systems.\n\u201cThe most difficult problem that we had was how to manage such a complex data structure. We call the format 3MX,\u201d he explained. \u201cIt\u2019s one thing to take photographs and create a mesh but it\u2019s another to actually do something with it, such as measure, extract ground levels, clash detections, add models and create a hybrid environment.\n\u201cWe spent a lot of time working out how to integrate meshes into MicroStation to drive workflows with our existing products, as well as our next generation platforms.\u201d\nWith many of its customers working in distributed teams, we asked how Reality Capture meshes would be shared and delivered, Das said, \u201cWe had to work out how to stream the data from our Azure cloud service, as 3MX files are not small, so we compress and expand them on the fly and have to cache areas around the area of interest, ready for scrolling. In some way it\u2019s very similar to what we had to do for point clouds but it\u2019s an order of magnitude in complexity; our meshes have many densely packed triangles, draped textures, snap points, it\u2019s not as straightforward as an array of 3D points. We also add a level of intelligence to understanding the meshed objects making selection and identification easier.\n\u201cThe application area of Reality Capture is just huge, many of today\u2019s projects actually start off as brownfield sites and even if they do have drawings or a CAD model, it\u2019s probably out of date and not \u2018as built\u2019. Construction, civil, industrial, mining, wastewater and even facilities Management. The only area we have not yet found a solution for is interior design as it\u2019s not yet practical to fly drones inside buildings\u2026. But we are working on it,\u201d he smiled.\nConclusion\nBentley\u2019s Reality Capture capabilities are available today and can benefit a range of existing vertical applications. This certainly highlights the benefit of having a single platform through which a vast array of applications can access common datatypes. While many people will utilise this technology without a thought of what\u2019s happening under the hood, it\u2019s actually an incredibly powerful hybrid environment, as models may be derived from data sourced from a wide variety of inputs.\nIn many ways, we are seeing the CAD software development community taming technology which produces ridiculously large quantities of data and making them incredibly digestible in tools we are familiar with. Design tools are finally starting to feel as interactive as games and yet they\u2019re incredibly accurate and useful. The next challenge will be figuring out how to economically store gigabytes of historical project data and serve on demand, I\u2019m sure Projectwise will be able to help you there too.\nReality 1: A9 Extension Project\nAn important part of Bentley\u2019s YII event is the Be inspired awards ceremony, not only for congratulating innovative customers, but offering the opportunity to see detailed presentations of significant projects. Despite reality mesh technology being relatively new to Bentley, there were a number of projects from customers that were actually using it on live projects.\nThe A9 project, presented by joint venture CH2M \/ Fairhurst, is a \u00a33billion extension to a 130km single carriageway that runs between Perth and Inverness. It\u2019s the longest trunk road in Scotland and runs through beautiful and remote landscapes which are heavily protected by environmental designations, including the Cairngorms National Park.\nCH2M \/ Fairhurst is working on a 43km section of the road, taking it from design development through statutory process to supervision and construction of a new dual carriageway road to cut travel times and increase safety.\nCH2M \/ Fairhurst have design teams in multiple locations and use Bentley Projectwise to glue them together and establish string workflows. The project uses the following Bentley applications: Microstation, OpenRoads, OpenBridge, Descartes, Acute3D, LumenRT and MX. The team used a combination of satellite digital terrain maps and Acute3D photogrammetry to mesh captures to build a vast model of the existing road to model in context and to enable various design options to be assessed. This was also used at regular public meetings to engage with communities who would be affected by the building work and benefit from the road upgrade.\nTo double check the quality of the Reality Mesh, CH2M\/Fairhurst sent people to various part of the valley to take photographs, which were compared to views from the captured model. The end results were impressive with very accurate correlation between the real and the modelled topography and textures. With such an accurate and lifelike model, combining CAD data with a reality captured mesh, the team immediately saw the benefit in stakeholder engagement and noted the benefit of letting people \u2018fly around\u2019 the model to see the scale of the work and assess the impact on the countryside.\nReality 2: Helsinki City Model\nAnother Reality Mesh project in contention at Bentley\u2019s YII was the Helsinki 3D+ project, which aims to build a 3D platform over 3 years to aid development, analysis and achieve environmental objectives. This information would be made open to citizens, companies, developers and universities.\nHelsinki 3D+ is not actually the first 3D model of Helsinki; the Finns love 3D and BIM and as far back as 1985 had created a 3D model of the city, albeit limited by the technology available. In 1999, a second model was created using game engine technology and used as a real-time simulator in exhibitions and then again in 2003.\nThis time, the Helsinki team wanted to go beyond the visual aspects of a moRdel and wished to use the model to assist in making Helsinki a \u2018smart\u2019 city. Using CityGML and open standards, the core team of only three people are building an accurate model at LoD2 of over 400km2! To add to the CityGML and GIS imported data, and using Acute3D, they captured 50,000 oblique images which enabled a reality mesh model with 10cm accuracy. IFC BIM models of new buildings or proposed buildings can be added too.\nLaunching in December 2016, the 3D infrastructure model will be provided online. There are plans for 3D virtual parks and even capturing underground and \u2018underwater\u2019 city models too. Serious urban analytics will be applied measuring urban spaces, Co2, GHG emissions and solar potential.\nThe quality of the model is excellent and the ability to fly over the city in real time must be seen to be believed. It\u2019s even more impressive, considering that such a small team managed to pull this off and there is a lot of depth to the many layers of georeferenced information that it contains. See the video here.\nThis article is part of a Bentley Systems Year In Infrastructure special report. Read the other articles below\nOpenRoads Designer: This new breed of civil design tool combines mesh data with detail design in one dynamic environment.\nCloud analytics: Knowledge is power Bentley is helping engineers \u2018design out\u2019 delays before they happen, by giving them access to real-time supply info.\nThe future of AR: The HoloLens and the use of reality meshes have got round tracking issues, paving the way for new AR applications.\nLumenRT embraces VR: LumenRT, an easy-to-use real time viz tool with a powerful \u2018game engine\u2019 environment, now offers VR support.\nBentley Systems futures: Bentley Systems may still be preparing for an IPO, but there has been some important news on its relationship with Siemens that could have big ramifications in the future\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/seeding-the-national-digital-twin\/","title":"Seeding the National Digital Twin","date":1563408000000,"text":"The National Digital Twin is an ambitious project to bring new efficiencies to the UK\u2019s infrastructure. Greg Corke reports on the Cambridge pilot project which aims to lay the foundations for a series of federated twins, connected by securely shared data\nFor infrastructure asset owners, the \u2018digital twin\u2019, a virtual replica of a physical asset that keeps in sync with its real-life counterpart, is a compelling proposition. It promises to optimise operations and maintenance and support better decision making when planning for the future.\nCreating a digital twin of an individual asset is one thing, but the UK National Infrastructure Commission has a much bigger vision. In its 2017 report \u201cData for the Public Good\u201d it recommends the development of a \u2018National Digital Twin\u2019. The idea is that greater sharing of data between infrastructure assets will deliver huge cost and efficiency benefits. It would use technologies like software analytics, machine learning and big data to simulate the entire UK infrastructure system.\nConsidering the scale and diversity of data\/systems, delivering a monolithic twin for the whole of the UK would be impractical and undesirable. \u201cYou don\u2019t want to have a \u2018big brother is watching you\u2019 situation, where somebody owns a 50TB model of the UK and they\u2019ve got all the sewer information, the network communication information, all that stuff,\u201d says Bruce Hutchinson, senior consultant, Bentley Systems.\nThe plan is to create a secure, federated twin, an ecosystem of digital twins owned by different asset owners and connected by securely shared data. These assets\/systems would be represented at different levels of granularity. High quality, standardised data and seamless interoperability is of paramount importance. To help achieve this goal, asset owners, mayors and other leaders in the built environment are being guided by the Gemini Principles (tinyurl.com\/gemini-twin) when developing their own digital twins.\nThe National Digital Twin is all about using it appropriately in different contexts, giving users access to the information that they need, but not too much or too little, says Hutchinson. \u201cThe Mayor of London is not going to care about a specific pump in a plant room, he\u2019s going to be more interested in flood control \u2013 if the Thames goes up what streets will be affected.\u201d\nStarting small, the Centre for Digital Built Britain is supporting this national goal by funding a digital twin pilot project at the University of Cambridge, in partnership with Bentley Systems, Topcon, Geoslam and Internet of Things (IoT) software company RedBite.\nThe objective is to develop a dynamic \u2018living\u2019 twin of the Institute for Manufacturing (IfM) building and the West Cambridge campus, demonstrating its impact on facilities management as well as \u2018wider productivity and well-being\u2019.\n\u201cWe want this twin to provide the foundation for integration with wider city scale digital twins and wider city scale data and see what we can do if we have that integrated picture of digital twins across different types of infrastructure assets such as power, water, transport and so on,\u201d said Dr Ajith Kumar Parlikad, research lead of the project, speaking at Bentley Systems\u2019 Future Infrastructure Symposium in London last April. \u201cHow can we actually have a positive impact through those digital twins on social and economic outcomes. How do we actually develop a twin that is federated, for example, that is open, that is interoperable, but it\u2019s secure as well.\u201d\nThe West Cambridge digital twin is different to many other digital twin pilots in that it focuses on existing buildings and infrastructure rather than on purpose-built assets. This presents a broad set of challenges around the creation of the 3D model.\n\u201cThe building is around ten years old and we never had a good drawing or as-built drawing or model of the building, so one of the things we went out to do was develop a BIM model based on IFCs, \u2013 a standardised BIM model \u2013 that not only represents the structure of the building but also the components within the building as well. For example, the MEP components, the mechanical and electric components and so on,\u201d says Parlikad.\nBentley Systems played a central role in generating the BIM model of the IfM building. This was then augmented with a highly detailed reality model of the plant room using data captured from a GeoSLAM handheld laser scanner and photogrammetry. Topcon used drone and vehicle-based scanning and photogrammetry to generate a reality model of the West Cambridge campus.\nAsset tagging\nThe next step was to add the contextual layers within the digital twin, classifying objects within the model itself so it\u2019s \u2018aware\u2019 of what it is. This includes the creation of an asset register, along with asset identification tags for critical equipment across the IfM. \u201cIf we don\u2019t know what assets we have, there is no point in having a digital twin,\u201d says Parlikad.\nParlikad is keen to point out the importance of developing a systematic methodology to define the asset information requirements and to ensure that whatever data is collected meets the organisational objectives. \u201cOften we have a problem where we go around collecting a lot of data without knowing what it is for,\u201d he says. \u201cSo this approach ensures that we are collecting the right data in the right way. And through the right asset hierarchy and so on, we can represent the data in the right manner.\n\u201d To date, over 200 assets have been tagged within the building. Barcodes are placed on the physical objects and the associated asset data is stored in RedBite\u2019s asset management solution \u2018itemit\u2019.\n\u201cWhat these 2D barcode tags allow us to do is not only identify these assets uniquely but if you scan the tag, the app will show us an asset profile which provides all sorts of descriptions about what the asset is, but more importantly asset management data as well,\u201d explains Parlikad. \u201cSo, you can see information about when it was last inspected, what the inspection record was, when it is due for inspection next, when the next maintenance is due and so forth. It\u2019s a rich collection of data that you can store on those asset profiles using these tags.\u201d\nUsers can also input information about the assets and Parlikad\u2019s team has been exploring the use of natural language processing, as he explains. \u201cFor any user, given the right level of access, you can scan an asset, say the boiler, and enter in simple plain language like \u2018the boiler is not working\u2019 or \u2018the fridge is not working\u2019 and so on.\u201d\nThis would then inform the asset manager, not only that there is a problem with that asset but the prioritisation of that problem.\nResearchers are also developing an augmented reality (AR) application for maintenance. The idea is that users would wear a HoloLens head mounted display and see assets behind the ceiling, then tap on those assets to get their status. The plan is to take things further and allow maintenance operators to do fault diagnostics online.\nReal-time performance data\nOne of the key aims of the project is to explore the best way to harvest and analyse data in order to understand how the asset is performing. In the IfM, this is done through the building management system, IoT sensors and devices that monitor and control the condition and operation of critical assets and the environment within.\nClose to 60 environmental sensors have been deployed so far, including temperature sensors, humidity sensors, and ones that can monitor whether windows are open or closed.\n\u201cWe are going to deploy C02 sensors and also occupancy monitors as well, within the building,\u201d says Parlikad. \u201cOn top of that there are some condition monitoring sensors that allow us to capture data that is not currently captured through the building management system. So, for instance, we have these vibration sensors that are put on the pumps on the HVAC systems in our plant room.\u201d\nTo create a central repository for the sensor data and building management system data, everything is being pushed to a cloud database on Amazon Web Services (AWS). This is partly because the University\u2019s local data infrastructure is locked down, but more importantly, explains Parlikad, because the project gives an opportunity to explore how emerging cloud-based solutions can be used within the digital twin context.\nTwin platforms\nTwo digital twin platforms are being used within the pilot. One is developed by Bentley Systems, based on its AssetWise platform, and the other is a BIM IFC-based digital twin platform that has been developed in house.\nParlikad explains the reason behind this dual platform approach, \u201cWe want to be able to show that if we have the right data and if we export and exchange the right data in the right form, the ability to exploit the data does not depend on a specific platform,\u201d he says. \u201cAs long as the data is exchanged in a standardised way, we can explore different ways in which we visualise this data through different platforms.\n\u201cThis is also an opportunity for us to explore two different pathways in terms of research \u2013 whether to go on a \u2018geometric modelling BIM IFC pathway\u2019 or a \u2018realistic modelling pathway\u2019 [using reality capture],\u201d he adds.\nOf course, physical assets are continually changing so the project will also need to explore how the model will be updated over its lifetime. To plan for this, the site has been broken down into a number of replaceable tiles. Some are smaller than others, depending on the density of information within them.\nFor new buildings, an as-built BIM model might be the obvious choice, but Bentley believes reality capture has a very important role to play. Hutchinson cites a Bentley R&D project that allows a service company to easily capture assets when digging a hole in the road. A GoPro camera is attached to an excavator boom via a custom rig and the videos are then fed into Bentley ContextCapture to produce a mesh showing the exposed pipes, as well as the surrounding manholes and drains.\nThere are important lessons to be learned about data visualisation but, as Parlikad explains, it\u2019s of no use if you can\u2019t exploit the data to drive decisions. With Bentley\u2019s platform, data from the environmental sensors is pushed into AssetWise every minute, along with data from the building management system. This is then fed into AssetWise Operational Analytics with a view to giving insights into the data and the ability to predict asset failures or operational events.\nThrough a web portal, users can access operational data via a series of custom dashboards with key performance indicators (KPIs). Depending on the permissions or role of the end user, data can be presented in different ways to convey different operational and reliability stories. Everything is fully customisable.\nThrough a hierarchical tree, users can explore all rooms and assets within the building, but the system also offers a model first view, where users can navigate, much like in a video game. \u201cYou can interact with the assets in the room, you can click on them and get a picture in picture idea of what\u2019s happening with that asset at any one time,\u201d says Hutchinson, adding that you can also click to see what\u2019s happened in the past and then predict what will happen in the future. This could be predicting the failure of the boilers within the plant room or monitoring the vibrations coming out of the pump to detect any anomalies.\nSafety operating envelopes can also be created to monitor assets during operation. If an asset is working fine, for example, then a green tick could be displayed on a custom table in the dashboard. But if it goes out of pre-defined limits then an alert could be sent by email or text. To date, the project is restricted to analysis and reporting; it hasn\u2019t gone so far as to create control systems that can interact with the assets directly.\nThe West Cambridge pilot has now been collecting sensor data every minute for over a year, which Hutchinson says gives a good dataset for predictive maintenance.\n\u201cWhat we need to work on is the idea of machine learning and algorithms that Cambridge University are looking at, so if we were to click onto an object, what is its predicted failure rate, is the bolt loose, is it vibrating too much, is it too hot?\u201d he says.\nScaling up\nSo far, the West Cambridge project has focused predominantly on the IfM building but it is now starting to extend its reach. Working with Smart Cambridge, researchers have been exploring how a city level digital twin could deliver benefits to citizens and to the council.\nBy looking at traffic data, for example, that the city has been collecting through sensors and through automatic number-plate recognition (ANPR) cameras, the city-level digital twin can be used to predict the impact of different scenarios. This could be how a new housing development might affect the traffic across the different parts of the city, or how the increased use of electric vehicles might impact the energy demand because people will be charging their vehicles at home or at work.\nThe future\nOver the next two years, there are plans to add sensors for things like traffic monitoring, air quality, parking and vision-based condition monitoring of the road surface. There\u2019ll also be an increased focus on data modelling, further research into IFCs, how GIS data and BIM data can be combined and how reality models can be made more object oriented.\nThere are also many questions that need to be answered, as Parlikad explains, \u201cHow do we reduce the cost of maintenance through predictive maintenance, remote diagnostics and so on, how do we reduce the cost of operations by reducing energy consumption and developing performance management apps that allows us to compare the performance of boilers across the different buildings in the campus?\u201d\nThe pilot will also seek to explore how digital twins can improve citizen level outcomes across cities through things like traffic modelling or air quality modelling using C02 sensors.\nBut most importantly, the project needs to move away from simply looking at assets and into large scale systems and national level digital twins.\nThe success of the National Digital Twin project hinges on its ability to bring together multiple, secure, plug and play digital twins seamlessly in one interdependent, dynamic system. \u201cHow easy is it for one organisation to say, yes we have developed a digital twin of our asset, [but] can we plug our twin into the city twin at all?\u201d says Parlikad. \u201cI don\u2019t think we can do it now so we have to explore how we can.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-nextcomputing-offers-intel-xeon-e5-v3-in-luggable-workstations\/","title":"NEWS: NextComputing offers Intel Xeon E5 v3 in luggable workstations","date":1410393600000,"text":"The \u2018Radius\u2019 portable workstation family from NextComputing offers the power of a high-end desktop workstation\nIf you need to take your high-end CAD, BIM or design viz workstation on the road but are fed up with Styrofoam packaging and annoying carry cases, NextComputing might have something for you.\nThe company has just updated its luggable \u2018Radius\u2019 portable workstation family, which now offers up to 36 CPU cores thanks to support for the new \u2018Haswell\u2019 Intel Xeon processor E5-2600\/1600 v3 product family.\nOne model, the Radius EX, has all the hallmarks of a high-end desktop workstation \u2013 single or dual Intel Xeon processors, up to 256GB DDR3 memory, a choice of Nvidia or AMD GPUs and up to 20 drives \u2013 but comes in a compact case with a handle and an integrated 17-inch 1,920 x 1,200 display with optional touchscreen.\nThe machine is 5.8\u201d (D) x 14.9\u201d (H) x 16.75\u201d in size and weighs between 17 and 21 pounds depending on configuration. It is also available as a screen-less version that can plug into an external display or projectors.\nMeanwhile, if you prefer your portable workstation to be a laptop, but want more than the four CPU cores found in most mobile workstations check out the Eurocom Panther 5. This hefty machine can host one Xeon processor, 32 GB memory, four storage drives and dual NVIDIA graphics (up to a Quadro K5x00M).\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-graphisoft-enhances-grasshopper-archicad-live-connection\/","title":"NEWS: Graphisoft enhances Grasshopper-ArchiCAD Live Connection","date":1516579200000,"text":"New \u2018deconstruct\u2019 function introduces intelligent design details that follow changes in the core design\nGraphisoft has released a significant update to its Grasshopper-ArchiCAD Live Connection. Version 2.0 of the bi-directional \u201clive\u201d link is said to open a new level of intelligent workflows between the two design environments (algorithmic design and BIM).\nNew features include a \u201cDeconstruct\u201d function that allows designers to use BIM models as the backbone of their design, adding algorithmic design logic using Grasshopper. According to Graphisoft, this allows designers to maintain the basic design logic in BIM (such as the base geometry of the building) and extend that with intelligent design details that follow changes in the core design.\nTo demonstrate this capability, Graphisoft has released a video (below) that shows building survey data about the built environment being used to re-create the building information model of an urban environment after a recent earthquake in Italy.\n\u201cThe Deconstruct function in the Grasshopper-ArchiCAD live connection made it possible for us to reconstruct the BIM models of the buildings of Grisciano, Italy, by acting as a bridge between the models and the conceptual design,\u201d said project architects Michele Calvano and Mario Sacco.\nAnother new enhancement to the Grasshopper-ArchiCAD Live Connection allows users to create custom GDL Objects using Grasshopper and then \u2018instantly\u2019 place them in their ArchiCAD project. The newly-created library parts will be placed in ArchiCAD\u2019s Embedded Project Library and all future design changes made in Grasshopper will be instantly carried through the ArchiCAD project as well.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/ai\/atkins-teams-up-with-ai-startup-to-transform-project-delivery-2\/","title":"Atkins teams up with AI startup to transform project delivery","date":1596672000000,"text":"Partnership with nPlan to explore new data driven contracting models for major project delivery\nAtkins has formed a partnership with Artificial Intelligence technology start up nPlan to explore new contracting models driven by technology and data which could transform major project delivery across the industry.\nThe work \u2013 which will be supported through funding from Innovate UK \u2013 brings together Atkins\u2019 project delivery experience and nPlan\u2019s machine learning technology to identify and assess a new approach to contracting that focuses on collaboration, and changes the way risk is measured, shared and managed across major infrastructure programmes.\nnPlan machine learning technology uses multiple techniques to learn how projects perform. At a high level, the algorithm learns by comparing the individual activities in a baseline schedule to activities in an actualised schedule. This process is repeated hundreds of millions of times building out a knowledge base of project performance which, according to Atkins, will allow for impartial and collective forecasting, increased transparency between project partners and a shared, improved view of project risk.\nThe team will work alongside asset owners such as Heathrow Airport to analyse the current contracting models before developing and piloting data and technology driven alternatives over the next 12 months. Findings will be shared in autumn 2021 and published in an open industry report.\n\u201cThe industry has historically been slow to adopt new technology, in part due to the risk and rewards mechanisms of its contracting models,\u201d said Richard Robinson, Atkins UK & Europe CEO. \u201cAs such, there needs to be a step-change if we\u2019re going to unlock and encourage innovation which can serve to increase the predictability of project performance and deliver whole-life value to ensure better returns on investment for our clients.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/bricsys-unleashes-v20-bricscad-design-suite\/","title":"Bricsys unleashes V20 BricsCAD design suite","date":1571097600000,"text":"New features include support for McNeel Grasshopper through a dedicated connection\nGhent-based BIM and technical drawing \u2018upstart\u2019 Bricsys has released version 20 of its core design tools. Part of the Hexagon suite of companies since 2018, Bricsys develops an AutoCAD-compatible drawing application, BricsCAD and a relatively new BIM solution called BricsCAD BIM.\nNew features of this release include support for McNeel Grasshopper through a dedicated connection enabling geometry to be driven by the popular visual generative design solution. There is a new stair model which can create parametric staircases with just a few clicks. The real time rendering application Enscape is now supported. IFC support has been upgraded to export and import IFC 4.0, which supports NURBS and meshes.\nThe BCF format is now included to communicate model-based issues with other applications. BricsCAD BIM has a unique approach to modelling. Architects can model with pure geometry and then have machine learning identify objects, such as walls doors and windows, applying the necessary IFC tags. v20 promises and more of that capability. Automatch will automatically match and complete BIM information which is missing.\nMeanwhile, HOK has just announced it will be looking at BricsCAD BIM as a Revit alternative and has also become a BIM Alliance Partner of Bricsys.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/a-q-a-with-vectorworks\/","title":"A Q&A with Vectorworks","date":1486598400000,"text":"AEC Magazine speaks to Vectorworks CEO Dr. Biplab Sarkar about the company\u2019s bold BIM strategy and its plans to support future customer needs\nWith the launch of significant updates to its suite of BIM apps in the second half of 2016, Vectorworks made clear its determination to confound market perceptions of the company as \u2018just\u2019 a CAD supplier. A whole range of features caught AEC Magazine\u2019s eye in the new offering and as we noted at the time: \u201cWith this release, you can really start to see something special happening in the product\u2019s BIM modelling capabilities.\u201d\nSince then, we\u2019ve had the opportunity to interview Vectorworks CEO Dr. Biplab Sarkar, to find out how he feels the BIM market\u2019s developing and how that will impact the directions that the company takes in future.\nQ: Biplab, Vectorworks comes from the Nemetschek stable of building design products, which also includes Allplan, Graphisoft, MAXON and many other known brands. How does Vectorworks differentiate itself from stablemates like Allplan and Graphisoft \u2013 both BIM tools by function and geographical coverage? And how is Vectorworks integrating and cross-pollinating the technologies from these other brands?\nA: Vectorworks differs from its BIM competitors in several ways. First and foremost, its flexible and robust 3D modelling tools allow architects to design the way that they think, rather than forcing them to follow certain processes or procedures. Second, when creating a BIM model, you aren\u2019t limited to the default BIM objects; you can design any aspect of your building through digital sketching. This could entail using Vectorworks\u2019 direct modelling tools, or using subdivision modelling tools and then transforming it into a fully developed BIM model. Finally, our advanced 2D and presentation tools give architects more control over the graphic quality of their work.\nAlso, worth noting is the collaboration between Vectorworks and its many sister brands from the Nemetschek Group. We work with many of our partner brands to describe BIM workflows, connecting various products and can demonstrate how models\/data can be exchanged via open international standards such as IFC and PDF. These products include SCIA for structural engineering; Solibri for BIM data validation, quality checking and data mining; Data Design Systems (DDS) for MEP engineering; and Bluebeam for PDF-based document management. Additionally, Vectorworks uses MAXON\u2019s Cinema 4D as its engine for our integrated rendering solution, Renderworks.\nThe various brands within the Nemetschek Group work together by offering a strong, unified support of OPEN BIM, which bodes well for open exchange with products from other companies.\nDr. Biplab Sarkar has been with Vectorworks since 2000. After first serving as a manager of geometry and rendering, he was the CTO from 2008 to 2016.\nQ: When creating BIM models, automated 2D output can leave a lot to be desired and this leads to a lot of reworking in 2D in order to get that output in the state that customers want to see it. Some users opt to break up the process and take their drawings into AutoCAD, which means they lose the benefits of quick changes driving new drawings . Vectorworks has extensive documentation editing tools: do customers accept the default output or is document editing of sections and elevations still a large part of the process?\nA: It varies among our customers and their practices, but we\u2019ve seen it both ways. The default output of drawings from Vectorworks 3D models meets the majority of most common drawing standards. And our users can customise the look of this default output. Since drawing standards vary from country to country, and even from office to office, we provide these extensive documentation tools to allow our users to have better control over drawing output. This eliminates the need for outside applications, empowering you to still take advantage of the automation that the BIM tool provides.\nQ: Model sizes are a common problem in BIM projects, as more detail is added. What\u2019s a typical file size for a customer project? What kind of RAM would a typical user require and does Vectorworks have methodologies for breaking up models into smaller work packages?\nA: Anywhere between 50MB (for a small residential project) to 2GB (for 300,000 sq ft institutional project) is typical. The minimum RAM requirement for Vectorworks 2017 is 4GB. However, we recommend 8GB to 16GB for large files and for complex renderings. Generally, we don\u2019t recommend that users break up their models. If you have the recommended RAM and graphics card, you should be able to handle fairly large files. We also provide a comprehensive multi-user environment that allows users to keep a single, unified file. However, if a project needs to be organised into multiple files, we do provide different methods for live referencing.\nQ: It\u2019s no secret that learning a BIM process is a massive undertaking and can be a barrier to adoption. What strategies does Vectorworks use in order to speed up training and keep the product from becoming excessively complicated?\nA: To ensure the successful adoption of BIM, we offer a number of resources and types of training. We build on the user\u2019s current knowledge base, in digestible steps, helping them transition from basic BIM implementation to advanced techniques, as they become more familiar and comfortable with the tools and processes. It is not an \u2018all or nothing\u2019 proposition for our users.\nNearly all of our resources and training are customised based on the audience. We normally provide training to decisionmakers first. For example, the first type of \u2018training\u2019 that we recommend is for principals on an executive level to ensure that they are prepared and equipped to manage complex change within their firm. As long as there is \u2018buy-in\u2019 at the top, the barrier for BIM adoption is much lower than expected. We offer on-demand resources for those who choose either self-paced training or live, hands-on training.\nQ: Vectorworks is one of the few BIM products to have a real solid modelling kernel, Parasolid, at its core. In the past, solid models have been deemed too \u2018heavy\u2019 for architectural models, which are mainly concerned with surface detail. What does having Parasolid at the core enable Vectorworks to do \u2013 and will this help downstream with capabilities like fabrication?\nA: BIM fundamentally begins with a 3D model, so having a world-class modelling engine like Parasolid at its core is essential. It allows us to provide far more robust modelling tools, including the support of complex objects and freeform elements, which in turn allows users to design more freely and without limitations.\nThe Parasolid implementation in architectural products is quite different from MCAD products. The architectural scene can be conceived as a massive assembly, with hundreds of thousands of parts that can make the design heavy. Vectorworks\u2019 implementation takes this into account, by storing the Parasolid model with some objects, and in some others, Vectorworks generates the model \u2018on the fly\u2019, thus keeping a balance between storage and performance.\nAdditionally, Parasolid modelling plays into BIM data\u2019s need for being strong and reusable to allow for a variety of workflows, from design to simulation and analysis to fabrication. A growing trend in the industry is embracing the Design for Manufacturing and Assembly (DfMA) idea from industrial design, where a design model is the basis of fabrication and the path is short and direct, without requiring many steps of translation or transformation. For example, the freeform design of the Versailles School of Arts by Platane Beres in France was done in Vectorworks, including the stone panels in the fa\u00e7ade, which were robotically fabricated using CNC stone-cutting machines.\nQ: Vectorworks has recently partnered with BIMobject, a content creation firm in Sweden. The industry has a reputation for a rather random approach to quality and standards for downloadable content. How are your customers using downloadable content and how can BIMobject help them? Can they share their own libraries?\nA: Currently, our customers use downloadable content from multiple sources. One such source is our Vectorworks Service Select portal, where we provide numerous resources we create ourselves, to ensure consistency and optimisation. We also support direct import of multiple file formats such as SKP, DXF\/DWG and RVT\/ RFA. Downloading and utilising resources for these file formats from forums such as 3D Warehouse is at the user\u2019s discretion. BIMobject helps ensure further consistency and optimisation of these different file formats. Currently, you can\u2019t share your own library in BIMobject, but you can share your library on our community board at forum.vectorworks.net.\nQ: Architects seem to be broadening their tool palettes by mixing numerous products together to enable the realisation of their designs. What kind of capabilities does Vectorworks offer for collaborative open BIM working and how do you see this changing in the future?\nA: Vectorworks provides architects with numerous software tools to streamline workflows without compromising capability. Our modelling and drafting tools, along with our presentation capabilities, eliminate the need for additional programmes like SketchUp, Rhino and even Adobe Illustrator. Also, we have added drop shadows for 2D objects, alpha channel support for images and Camera Match in Vectorworks 2017, which further reduce the need for Adobe Photoshop. Without the need for other programmes, you get a more efficient and streamlined design process. Although, if designers choose to use other tools, Vectorworks has extensive import\/export capabilities that create a more efficient and seamless process.\nVectorworks can be a hub for ideas, no matter the form, and those ideas can be transformed into the desired BIM data. Vectorworks also supports, and is certified for, the import and export of the IFC file format. As architects adopt BIM processes, the need for more interoperability will arise in order to accommodate all of the different needs and end goals of the architect, including design, development and documentation. Being able to accommodate varying workflows and tools through file interoperability and IFC support becomes a key aspect or foundation for how Vectorworks will move forward for future adoption of BIM.\nQ: There is a lot of buzz about architects adopting Computational Design, using products like Grasshopper and Dynamo. What\u2019s Vectorworks\u2019 approach to Computational Design, and do you think this capability will become more commonly used as software developers and users increase their skillsets?\nA: Grasshopper, Rhino3D and SketchUp may be popular modelling tools, but even these platforms have limitations in how they can address needs in a wider BIM context. However, Vectorworks is the only BIM software to provide a native, built-in computational design tool that is referred to as Marionette.\nAlso, Marionette saves time, because you can generate not only geometry, but also scripts and plug-ins, so you can produce intelligent building models with Marionette all within one platform. Marionette can automate routine tasks in the design process, such as drawing analysis, database creation and file organisation. These factors can save you hundreds of hours on projects! We see Computational Design being adopted by more and more students and younger designers entering the workforce. This capability is well on its way to becoming common in architectural practices.\nQ: In the UK, we have some very specific industry initiatives to make BIM a country standard. What developments and capabilities has Vectorworks included in the software to assist in customers meeting standards such as the UK version of COBie and interoperability? How does this compare to what you see globally?\nA: As a provider of BIM software to architects, our focus at Vectorworks is ensuring that designers have all of the necessary capabilities to participate in the BIM process. We are also very customerfocused, developing our product as a direct response to our customers\u2019 needs and requests. In fact, 70% of updates to our latest release were introduced in response to customer feedback. Moreover, our BIM tools and IFC support were developed with industry initiatives in mind, such as those in the UK.\nWith Vectorworks 2017, we\u2019ve included support for IFC 4, the newest version of the IFC file format. We\u2019ve also introduced a more advanced IFC data mapping tool, which allows more customisation and control over the data export. This means that there is better support for the various versions of COBie that need to be met.\nWe\u2019ve noticed there has been greater support and adoption of standards such as the UK\u2019s BIM initiative among various countries and government bodies. In the US, BIM has been a significant part of the industry for some time now and this has allowed us to be better able to support this global embrace of BIM.\nQ: Software developers have been very keen to jump on the cloud bandwagon. At AEC Magazine, we can see how this helps collaboration, but also that there are still hurdles to be overcome. How does Vectorworks handle group working and distributed team collaboration and what use of the cloud (or planned use for the cloud) does the company have?\nA: Vectorworks handles group working with a feature called \u2018project sharing\u2019, which is essentially a singular file where multiple people can concurrently work on different parts of a project. The singular project file that everyone accesses is typically kept on a shared server to which all team members have access. In Vectorworks 2017, we have integrated support of cloud-based storage in our project sharing environment. This means that rather than hosting that file on a server, the project file can be kept on Dropbox, Google Drive, OneDrive and Box to be easily accessed by various team members. Overall, this feature enables employees who are remote, travelling or located in the field to work on their projects without being dependent on something like VPN.\nQ: Looking at the BIM market today, what technologies and processes do you think will drive adoption and improve the benefits already delivered by BIM?\nA: We will see Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) used more frequently in different parts of the process to communicate in ways where renders and drawings are insufficient. For example, customers may use VR\/MR systems for design to show a client how the building will look in context, as well as inside, and contractors may use VR\/AR to verify site conditions against design intent.\nFurthermore, other technologies that will become more relevant include robotic 3D printing, an emphasis on high-performance buildings, as well as smart city initiatives.\nThe smart city initiatives strive for more livable cities, tackling challenges such as reducing traffic congestion, fighting crime, fostering economic growth, managing the effects of a changing climate and improving the delivery of city services.\nIn closing, the adoption of BIM standards and initiatives by governing bodies and industry organisations will drive the use of BIM further and further.\nFor most designers, BIM technology needs to adapt to the design process and not try to replace it. This adaption will greatly increase adoption and improve the benefits of BIM.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/www.wired.com\/story\/the-big-interview-podcast-matt-garman-ceo-aws\/","title":"AWS CEO Matt Garman Doesn\u2019t Think AI Should Replace Junior Devs","date":1765843200000,"text":"Amid the breathless coverage and relentless AI hype of recent years, one of the world\u2019s biggest tech companies\u2014Amazon\u2014has been notably absent.\nMatt Garman, the CEO of Amazon Web Services, is looking to change that. At the recent AWS re:Invent conference, Garman announced a bunch of frontier AI models, as well as a tool designed to let AWS customers build models of their own. That tool, Nova Forge, allows companies to engage in what\u2019s known as custom pretraining\u2014adding their data in the process of building a base model\u2014which should allow for vastly more customized models that suit a given company\u2019s needs. Sure, it doesn\u2019t quite have the sexiness of a Sora 2 announcement, but that\u2019s not Garman\u2019s goal: He\u2019s less interested in mass consumer use of AI and more interested in enterprise solutions that\u2019ll integrate AI into all of AWS\u2019s offerings\u2014and have a material impact on a corporate P&L.\nFor this week\u2019s episode of The Big Interview, I caught up with Garman after AWS re:Invent to talk about what the company announced, whether he feels behind in the AI race, how he thinks about managing huge teams (and managing internal dissent), and why he\u2019s not convinced that AI is (or should be) the great job thief of our era. Here\u2019s our conversation.\nThis interview has been edited for length and clarity.\nKATIE DRUMMOND: Matt Garman, welcome to the Big Interview.\nMATT GARMAN: Thank you. Thanks for having me.\nWe always start these conversations with some very quick questions, like a warmup. Are you ready?\nGo ahead. Fire away.\nIf AWS had a mascot, what would it be?\nWe have a big S3 bucket sometimes that goes around, so we'll call it that.\nSorry, what is an S3 bucket?\nAn S3 bucket is like a thing that you store your S3 objects in, but we actually have a large foam big bucket that walks around and actually looks like a paint bucket.\nSo you do have a mascot.\nWell, S3 has a bucket, it has a mascot. It's probably the closest we have, and I like it.\nWhat's the most expensive mistake you've ever made?\nPersonally or professionally? That\u2019s a good question. Personally, the most expensive mistake I ever made was playing basketball too long and I tore my Achilles. So that cost me about nine months of being able to walk. I probably should have known that into my thirties I was well past basketball-playing age. I lost a little bit of time there.\nThat sounds personally expensive.\nYes.\nIf you could rename the cloud today, what would you call it?\nWhat is it called today?\nThe cloud.\nI actually think the cloud is a pretty good name. So I don't know if I would rename it. I would rename what we called Amazon AWS, or Amazon Web Services. Now, no one knows what web services are. So that, I might rename a little bit, but I actually like the name of the cloud, so I'm not sure I would rename it.\nMaybe Amazon Cloud Services.\nYeah, maybe.\nWhat's the part of your job you would love to outsource to an AI agent?\nI try to outsource a lot of the parts of my jobs to AI agents, if I can. But I find that I still haven't figured out how to outsource answering my day-to-day emails yet. That still, I find, takes up a lot of my time. If I could figure that out, I think that would be great.\nBut it sounds like you've tried, and I know we're supposed to be doing quick questions, but I was going to ask about this later. Tell me a bit about how you've tried to incorporate artificial intelligence into your workflow, into your personal and professional life.\nThere's a number of ways that I've done it. In particular for me, though, a lot of the benefits that I get in my job are taking a lot of information inputs and then kind of sharing those out to either the same or other people and connecting a lot of those dots.\nFor my particular role, I haven't yet found a huge shortcut for being able to do that, particularly with regards to the medium in which we communicate, like email or other places like that.\nBecause I find that all of the shortcuts lose some of that nuance. There's some summaries and things that work. There's definitely some tools that allow me to summarize content more quickly or learn new content more quickly, which I find to be super useful. But I haven't found a huge time win for my role in particular, where there's not as much repetitive work or other things like that.\nIt largely is knowledge that I'm trying to get from a bunch of sources and then consolidate to send out to others.\nThat's actually very interesting to me, and reassuring in a way, because I sometimes feel like I should have found a bunch of shortcuts by now.\nSo, from one former AWS intern\u2014that's you\u2014to a future one, what is your best piece of advice?\nI find that people always overestimate how much technology has already been invented and think that there's nothing left to do. What I find is that we continue to be at the early stages of evolution. As long as you're curious and looking and willing to try new technologies, new areas, we're always at that early stage of what can be invented.\nSometimes I run into interns who are like, \u201cWhen you started AWS it was small, but now it's a big company and there's not the same opportunity.\u201d And I'd say that that's just not true. I think there's just as much, if not more, opportunity than there ever has been.\nWhich leads me to my next question: How do people know when you're unimpressed?\nWhen I'm unimpressed, I would tell them directly.\nFair. So, direct feedback.\nLargely, though, it's not about impressing me. I like when people are thoughtful, when they've come up with the right sets of decisions. It's not like people are coming up with something that's super novel every day. I like it when people have done the work so that we have all the information, whether it's customer input or data input or sales input or whatever, so that as a group we can make thoughtful decisions.\nI\u2019m impressed when people have done that work ahead of time so that when we do get together, we can make good decisions, and thoughtful decisions. I'll often tell people it's not necessarily the recommendation that I'm going to be impressed by, but I want us to have all the information so that as a team, we can move forward and make great decisions.\nWhat is a hobby you wish you had more time for? I'm assuming it's not basketball, based on what we just learned about you.\nIt's not; I've switched to golf. I caught the bug a couple years ago and quite love playing golf. I don't get to play as much, but I enjoy it.\nLet me set the stage a little bit. We're here to talk, in particular, about a bunch of announcements that you recently made around AWS and AI and agentic AI that WIRED covered. I'm biased, but I thought we did a great job with that coverage. But I also want to learn a little about you in a professional context. So tell me, and tell all of us, about your career journey thus far. Now you\u2019re the CEO of AWS, but you had a very long career at Amazon before that. How did you end up where you are now?\nI'll start at the beginning. I worked for a couple of startups early on in my career. None of them did particularly well, but I learned a ton from them, which was great. After my second startup, my wife and I both quit our jobs and went to business school, which was a fantastic opportunity.\nWhen I was there, one of the things during my internship that I wanted to try to explore was what entrepreneurship looked like inside of a company. My goal was always to go back and do a startup again. So as part of that, I looked at a bunch of different companies and I ran across Amazon and actually talked to [president and CEO] Andy Jassy, and they talked about building this technology services capability inside of Amazon. I thought that was exactly what I wanted to see. I wanted to see what it would look like for a successful technology company to try to build something new inside of it, because I just wanted to see what that motion looked like and how I could learn from experienced entrepreneurs and what was different than at a startup.\nGot it.\nSo I did my internship for what turned into AWS in 2005 before we launched. I was fascinated by it. I thought it was an awesome opportunity, and I said, \u201cGreat, I wanna come back and work here for a couple years,\u201d and then I would go back and do a startup.\nSo then I started full-time in 2006, effectively as the product manager for all of AWS. It was largely defining all of the services as we launched them. So I started a couple weeks after S3 launched, and before the rest of our services launched, and helped launch them and name them and price them and do a bunch of things.\nThen I kept getting more and more responsibility. I focused on EC2, which was our compute service. I started taking on engineering teams, actually launched our block storage service, kind of wrote the PR\/FAQ for that and hired the first engineering team and launched that.\nThen I grew to lead most of our core compute and networking and storage product areas. So all of the product and engineering teams for that. It was fun. I got to learn a lot along the way. I'm not necessarily, or I wasn't originally, a deep technology person, but I got to learn about hypervisors and kernel engineering and a bunch of these really low-level core technology pieces, which was cool. It was super interesting. It's just such a fascinating space, and as AWS grew really rapidly, we grew along with it, and we grew the team pretty significantly.\nWe were fortunate enough to work with some of the best technology people in the world as we built the service, and a bunch of services, as the business group. I can't remember the exact time, but it was after about 12 or 13 years, so it would've been like 2019, something like that, Andy Jassy called me in his office one day and asked if I would lead sales and marketing. I literally didn't know anything about sales and marketing. In fact, I was kind of looking around to see if he was talking to someone else. But it was a great opportunity to learn that space, and it was a unique opportunity, right?\nYes.\nI was basically handed one of the world's two or three biggest enterprise sales and marketing organizations, having never done any of those jobs before. I think Amazon's a bit unique in that we trust people. If you're smart, you know how to operate, you know the business, you don't necessarily have to know the exact thing that you're going into. The team was gracious and helped me learn that. That was a great opportunity to get to learn how to run a field organization at scale and get to spend a ton more time with customers, to really understand the nuances of what a startup customer was looking for versus enterprise, versus the governments, which was awesome.\nThen I took over as CEO two years ago, or a year and a half ago. So I've spent almost 20 years here at Amazon, all in AWS.\nHow big is the organization by employee size?\nI don't know the exact numbers, but it's in the hundreds of thousands. We have large data centers. We run a very large operational organization. We have data centers all around the world. So it's a pretty big team.\nI love management, and it was clear fairly early in my career that that is where I wanted to go. Was that always clear for you, the idea that you wanted to be taking on more and more of the enterprise? That you were doing what you were meant to be doing?\nYes, I like it and I think that I'm reasonably good at it, I guess.\nWe\u2019ll ask some of your employees.\nI took on more and more as it went. You can ask others. I guess that's not for me to say, but I liked it. You know, it was a ton of learning. What I really love is when I get to take on roles where I get to learn more, and get to stretch myself.\nFrankly, I love building and love having an impact on what the business is doing and what our customers are doing. It's hard because you don't get some of the joy of actually physically getting to build the thing or deliver the thing yourself.\nIn my world, that\u2019s like not getting to write the story.\nThat's right. But you can write a lot more stories in that case. And you learn to do that through others, which is an interesting and useful skill. You learn how to communicate to teams, first through direct management, then through layers of management.\nSo you gotta think about how you build mechanisms. This is one of the things I enjoyed learning, which is how do you think about building mechanisms that allow you to help those individual contributors make some of the right kinds of decisions, and the strategy that you're trying to drive for the team or the business or the company? I've quite enjoyed learning how to leverage some of those mechanisms at different scale. That's been fun to do, too.\nYou just talked about going from managing people to managing managers, and I swear I'm asking for a friend, but do you have any particular mechanisms that you have picked up in those 20 years that stand out to you as particularly effective strategies? Because I will say there is something very specifically different about managing someone who then manages people who then manage teams of people.\nYeah.\nIt has the potential to be a very unproductive game of telephone. What mechanisms do you employ to make it much more effective than that?\nI mean, look, everybody has their own way of doing this. So you a little bit have to find what works, and I do think that as your team in your organization gets larger, you have to change some of those things. I think that's one of the common pitfalls that I see people fall into is that they will assume that the thing that worked great when they were a line manager, managing a team of six to 10 people, will work the same as when they're managing a team of a hundred people.\nThose same things won't work. There's another shift that's like when you don't know the name of everyone in your team. Or your organization, which I can't unfortunately know today. Usually that breaks somewhere around a hundred to 200 people, somewhere in there.\nRight.\nYou'll run into people who are in your team that you don't know or don't know their names. You just have to think about all of those things differently. You want to give the broadest set of people in your team as possible mental models on how you would make decisions in their place as opposed to what the decision actually is.\nBecause if you send down edicts all day\u2014like, we're going to do this, or you wanna do this, or I'm going to make a decision on that\u2014it's not as empowering to your team. And there's a chance that whatever you say gets miscommunicated. But if you can give people mental models of, for example, \u201cIf I was going to approach this situation, this is how I would think about it, or these are the ways in which I would make trade-offs,\u201d then you can empower tens of thousands of people to make decisions.\nThen you can focus on making sure that you hire smart people and don't punish them when they make bad decisions, but instead course-correct. That's how I've kind of learned at scale, where your real leverage points are ensuring that you have those right mechanisms at place.\nI appreciate the free management advice. I'm sure many of our listeners do, too, so thank you.\nI want to now ask you about AWS in a broad context. I would describe WIRED's audience as sort of curious generalists. Obviously, they're listening to this, or reading this, because they're interested in technology and where it's taking the world. They probably have some sense of what AWS is. But they probably don't know how big AWS is and how vital it is in terms of just infrastructure. Can you explain it to us?\nNot like we're five, but like we're 21, we just graduated with a humanities degree, trying to understand this thing that you are in charge of.\nAt a high level, our idea behind AWS hasn't changed in the last 20 years, which is that there are a bunch of pieces of technology that were really non-differentiating that companies had to do on their own for a long time. It used to be that they had to build a data center. They had to go find servers, they had to take care of the servers. When a disk drive broke, they had to go fix it. They had to set up their networks, et cetera. There's a whole bunch of work that they had to do before they could ever write a cool application that would let them stream a movie or connect individuals with people who had rooms they wanted to rent out.\nWhen you think about all of that work, our goal was: What if we could do that work for companies so that they didn't have to do that? That was the thesis when we started. So that someone could come and say, \u201cGreat, give me servers, give me storage, give me databases,\u201d and we'd provision it for them. Then, through an internet connection, they could have access to that. It turns out that was a very powerful idea. I think we did a good job executing on some of the abstractions that made it really powerful.\nNetflix and Airbnb and Pinterest are all some of these early customers that we had that built their business from the beginning on AWS and the cloud. They don't own data centers, they largely just run inside of AWS. Initially, when we first launched the business, we thought this was going to be incredibly compelling for startups and some technology companies, which it was, but as we grew, we found out that enterprises and really large organizations were equally compelled by this value proposition, and we had to build a lot more capabilities for them. Whether it was like encryption capabilities or audit logs or abilities to hit particular compliance things or whatever it is.\nGot it.\nBut now we have customers like Pfizer and JP Morgan, and the United States government and the intelligence agencies. That was a big win for us when we kind of convinced the US government that we could build a top-secret region and that they could run intelligence workloads inside of AWS.\nI would've loved to have been a fly on the wall in those meetings. What does it take to convince the United States government that they should run on the back of AWS?\nWe walked through some architectural questions that they had, and \u2026 you know, there was a whole [request for proposal] process and there's a lot of work that we did, but it was also just some whiteboarding where we kind of walked through how it would work, and at the end of the day, the cloud sounds like it's a magical technology, but it's data centers and it's networks and it's servers, and other things that we run at very high reliability, at very high security.\nWe found some forward-leaning technology folks that wanted to figure out how they could get the benefits to the government. It turns out that if you can find the right person in an organization, even somewhere as large and bureaucratic as the US government, you\u2019ll find people who want to lean forward, go faster\u2014they want to deliver value for citizens. So, finding that right person and then being able to collaborate with them, their eyes light up just like they do for a startup company, right?\nThese people are oftentimes very mission-driven, and if they can see an opportunity to deliver more value for the mission that they're on they're super excited about that.\nSo today, now it's across almost every country and every industry that you think about throughout your daily life. There's still lots more to go, but I would say there's a bunch of companies that now use AWS as their core infrastructure. NASDAQ trading markets run on AWS; financial services companies run on AWS; hospitals run on AWS; media and entertainment\u2014live broadcasting or streaming\u2014we power a lot of that technology.\nJust when you thought you were done learning the job, along comes artificial intelligence. Which brings me to my next question. Obviously, we are in this new era for you and your organization. You held this re:Invent conference in December, and you streamed it on Fortnite, I might add \u2026\nYep.\nYou announced some pretty significant changes to AWS, to your mission, to your priorities, and to what you would be offering to your consumers. I'm hoping you can talk us through that transformation.\nTechnology is always iterating and going through transformations. So for us staying at the forefront of every innovation is incredibly important. I think there's been almost no technology leap since maybe the cloud, and the internet before that, [as big as AI]. So we\u2019ve been investing in AI, in AWS and Amazon, for the last decade-plus, two decades maybe. But definitely with the leap forward from generative AI over the last three years, we've just seen a massive change in what's possible for customers.\nSo we've had this vision that it's not just going to be, AI is over on one side and then the rest of your business is going to be over on the other side. Basically AI is going to be built into what everyone does. In order for that to happen, number one is you have to have all of your data in the cloud world. And we\u2019ve built this whole platform of tools that then allow you, once you have that data in the cloud, to deliver differentiated value to your customers.\nSome of the things that we launched\u2014that I'm quite excited about\u2014at re:Invent are really around AI agents and the difference between the first generation of AI tools [and the next]. The first generation were really around summarization and content creation, right? We were all quite excited and got a lot of value out of those. But there's only so far that goes. I think the next stage is these agents. The real value is they can take access to your data\u2014they can still do some summarization and content creation, but they can actually accomplish tasks. They're able to reason.\nWhen you have these agents that can reason and accomplish tasks on your behalf, all of a sudden you can kind of force-multiply what you're able to do. So we launched a couple of things. One was called Nova Forge, where we allow customers to actually take their data and integrate it in at the early stages of training one of these frontier models. So that gives enterprises one of these AI models that deeply understands their data and their domain. Then we launched a number of these frontier agents that allow customers to really deliver big bodies of work, whether it's in coding or operations or security.\nWe've spent the time to really build this platform where it could actually deliver value. I think broadly people have understood what that long-term strategy was. They really get it. As they see projects really delivering, where there's real value, they see that AWS is the platform where they want to go do that. That's what customers were telling us over and over again [at re:Invent].\nI feel like 2025\u2014which we are, thank God, almost done with\u2014was a confusing year in the narrative for AI. I say that because I feel like in January I went to some conferences, talked to some people, and the sentiment was \u201cThis is the year of the agent. Agentic AI is here. It's about to change everything.\u201d That was almost a year ago.\nYeah.\nAm I using agentic AI right now? Absolutely not. Has that come to fruition in the way people were talking about in January? No. At the same time, you\u2019ve seen reports from MIT, for example, finding that 95 percent of gen AI pilots in companies are failing to yield the productivity gains that I think those corporate leaders thought they would see.\nHow do you make sense of all of those narratives coming together? Were companies just moving too quickly?\nIf you jump forward and don't build that strong foundation of I have my data, I know the workflows, I really know how some of these things are going to tie together, then you're not going to get any value out of it.\nYou're going to have just a chatbot, which is kind of cool and looks neat. Then everybody has a chatbot and then what? So it is those differentiated workflows, you know, and they're less sexy and interesting for people to look at, but a workflow that can help you automate insurance claim processing is super valuable, right? You can actually get people\u2019s claims processed faster, make sure that you cut your costs, have a better customer experience, make sure you have better accuracy\u2014you can deliver all these things.\nSome of the technologies weren't available in January. This technology is moving so fast that the capabilities are much better today.\nI will tell you, at re:Invent I sat in a room of executives for a broad set of companies, and I asked for a show of hands of who is either now starting to see positive ROI on their AI investments or see a clear path to meaningful positive ROI in the next six months. I think 90 percent of the hands went up.\nHmm.\nThat's not the answer I would've gotten a year ago, but it's because we've done a bunch of this work, and it's because we've done a bunch of this work together with customers to understand exactly what they want, how do we solve their problems, and how do we deliver solutions that deliver them real value and not just, you know, clickbait headlines that sound good.\nOn that note, I will say candidly, Amazon has not been a key part of a lot of these AI narratives, right? There have been other companies that have been out there making announcements. I can tell you, leading WIRED, it's this constant stream of news, \u201cWe're doing this, we're doing that.\u201d Does that worry you? Do you worry about Amazon not being in that narrative, or do you feel like you took your time?\nI think both of those things are true. I do worry about it, because I don't want customers to think we're not innovating or driving the latest technologies they need. I want to make sure that it's not just headline-grabbing stuff and it's actually great.\nJeff Bezos used to have a saying that you have to be willing to be misunderstood for long periods of time. For us, I think that's what some of the last two years entailed.\nI think a lot of that narrative has changed now. If you talk to lots of analysts, if you talk to folks in the press, if I talk to customers they're saying, \u201cLook, actually AWS has by far the strongest agentic platform to go build on. They have the broadest set of models that I can build on. They have the broadest set of security controls and compliance controls that actually, if I go put these agents in production, I can actually audit, know what they're doing, control what they're doing.\u201d That is what we're seeing.\nThat\u2019s not to take anything away from [other AI tools]. ChatGPT is an incredible consumer application, but it's just a different thing. That's not our business. Our business is to make sure that banks and health care companies and media and entertainment companies and energy companies can drive their businesses and deliver more outcomes for their customers or cut costs or whatever they want to do.\nSo, I'm quite pleased with where we are now, and I think we've already seen that narrative largely shift.\nI wanted to ask you a little bit more about Nova Forge. What was particularly interesting to me here is this idea of custom pretraining, as opposed to the idea of fine tuning, as a way that a company can take a model and really make it their own. Can you explain that distinction to everybody before I dig in a little more?\nIt's not new that people have thought, \u201cOK, there\u2019s these out-of-the-box models that I wanna customize,\u201d and the best mechanism that we've had to date to customize are these open-weights models. Meta was great and released that with their first Llama model, but now we've seen a variety of these, whether it's Mistral or DeepSeek or Qwen or whatever.\nThere's a number of these, but they're still black boxes, right? You basically get a fully pretrained model, and then they open the weights and you can tune the weights to focus in particular areas. Or there's new techniques like reinforcement learning where you can start to send more information to these models that train them after the fact.\nWhat we find is that if you put too much new data into these models in the later stages, they forget the early stuff and so they forget what made them great at reasoning. Or they actually forget some of that data because they get what's called overtrained on some of the data you're giving it.\nRight.\nThen they lose what was valuable in the first place. So there's only so far that that can go. What we also find is that those techniques are very ineffective if the model wasn't already trained on your domain. So if you try to teach one of these open-weight models about protein folding and they know nothing about protein folding, it doesn't work, because it doesn't know how to inherently reason about that thing.\nWhat we found is that if you can train them earlier, and I use this analogy in my re:Invent talk about the human brain, it\u2019s like learning languages. You're able to learn new languages early when you're younger. Much easier. Now, if I try to learn a new language, it's much harder to do. So models are somewhat similar to that.\nWhat we've done, which is a unique thing, we refer to it as open training, but really the idea is that you have this corpus of data that is from your domain and your particular company and the ways that you do things. If you're able to insert it into the pretraining stages and then mix in a bunch of the data that was used to originally train that model and then finish pretraining the model, when it's done the model actually inherently knows all of your stuff. It knows about your data, it knows about your company, it knows about your domain, and anything that you do with fine-tuning or post-training after that is actually much more effective.\nIt's just never been possible before. Because open-weight models never would expose their data, and there was no kind of mechanism for them to be able to go do this. So we did this with our Nova models and our Nova 2 models.\nTake a financial services company. You can take all your information, inject your data, mix it with an Amazon-curated dataset. This is the data that we have proprietary use. We'll give you tools to easily mix that together. And then you finish pretraining the model, and you effectively have your own custom frontier model that understands your business that you were able to train for a couple hundred thousand dollars or whatever the cost is to finish that training, versus billions of dollars of doing all the research to actually go and build a frontier model.\nI'm curious about risk in the context of Nova Forge in a few different ways. You already deal with companies that have confidential proprietary information, right? You just mentioned financial services inputting all of that data into this model and into this training. You also offered a really compelling example off the back of re:Invent from Reddit, which is using Nova Forge to develop a model that can be used for content moderation, which essentially means training a model that breaks a lot of conventional rules around how these models are typically designed, right? They would be designed to avoid offensive or violent content entirely. Reddit, on the other hand, needs a model that can lean into that kind of content in order to do the moderation. So those are two very distinct examples that I just offered, but I'm curious what kind of risks exist when we start down the road of this kind of custom pretraining, and are there distinct risks that might be different from what we've seen historically?\nI don't know if there's any particular, different risks. I think from a data protection point of view, we have all sorts of protections around this. That's one of the things we pride ourselves on, and have over the last 20 years, is really protecting customer data and making sure that it's isolated and protected.\nWhen people are building their own model, this is true no matter what, if they're fine-tuning it, if they're doing any of that work, companies have to think about what is the output, and they have to own the output of their own models.\nMmm-hmm.\nEven if they're using an off-the-shelf model, by the way, you have to own the outputs of that.\nI think that's one of the key pieces: You can't just give up responsibility for whatever your technology is doing. You can't give up responsibility because a database makes a particular join and you're like, \u201cI don't know, it's just how the database did it.\u201d AI is no different from that.\nWe deliver powerful tools to customers, but they have to own those outputs and think about them. We still have safety classifiers, by the way, for things. You can't pretrain something and then create it to build a bomb or things like that. We still have a lot of those safety controls, and there's no circumventing those.\nBut with regards to things like content moderation, that's someone else's choice, and they can make a choice and they own what that looks like. They have to be sure that they make great choices for the content that's on their website, that's appropriate for their site.\nSo, we\u2019re talking about this premise whereby AI agents will be much more integrated into enterprise settings, and I'm curious how you think about that in the context of the workforce. I'm looking back at 2025 and remembering comments that other AI executives have made about job disruption, cuts to the workforce, et cetera.\nYou actually made an interesting comment where you said that replacing junior employees with AI is, quote, \u201cone of the dumbest ideas\u201d you've ever heard, which made me laugh.\nHa!\nCould you talk more about that and more about how you see artificial intelligence and agentic AI changing the workplace in the years to come. Because I think you have a point of view on this that some people might find reassuring, and that I think is different from what we hear from a lot of other leaders.\nThat point, by the way, was specifically about software developers, but I think it applies to lots. There was this thought that you'll just replace all of your junior engineers and all of your junior employees and you'll just have the most senior, most experienced employees and then agents.\nNumber one, my experience is that many of the most junior folks are actually the most experienced with the AI tools. So they're actually most able to get the most out of them. Number two, they're usually the least expensive because they're right out of college and they generally make less. So if you're thinking about cost optimization, they're not the only people you would want to optimize around.\nThree, at some point that whole thing explodes on itself. If you have no talent pipeline that you're building and no junior people that you're mentoring and bringing up through the company, we often find that that's where we get some of the best ideas.\nWe get the new fresh blood into the company from hires out of college. There's a lot of excitement. There's a lot of new thoughts. You've gotta think longer term about the health of a company, and just saying \u201cOK great, we're never going to hire junior people anymore,\u201d that's just a nonstarter for anyone who's trying to build a long-term company.\nWhat does this mean, though, for the workforce broadly? What does it mean for Amazon's workforce? What does this look like as AI agents infiltrate\u2014which is not a generous word, but it's the word that comes to mind\u2014the way we work and live.\nOne of the things that I tell our own employees is \u201cYour job is going to change.\u201d There's no two ways about it. If there's only one thing I can promise you, it is that the way that you did your job four years ago is not how you're going to do your job next year. You're going to be able to have a bigger impact. You're going to be able to do more things, you're going to be able to have a broader scope of responsibilities, and it's just not going to be the same things that made you successful five years ago. You're going to have to learn new skills, you're going to have to learn new ways of working. We may have to organize our teams differently. We may have to go after problems differently. So people are going to need to be flexible. There is for sure going to be disruption in how work is done, because jobs are going to change and industries are going to change.\nIf they don't, they'll most likely get left behind by people who move faster and do change. There is going to be some disruption in there for sure. Like there is no question in my mind.\nWhen you say disruption, do you mean in the context of job loss, in a big picture, economic way?\nThat is uncertain to me. I'm very confident in the medium to longer term that AI will definitely create more jobs than it removes at first. I think anytime you find opportunities to create new economic prosperity or build new experiences or things like that, there are more jobs that get created. But they will be different. There are jobs that will be eliminated as part of it, or reduced, almost for sure.\nThere are some things where you just no longer need quite as many people to do a particular job. So our job is to also provide training and upskilling so that we can retrain, so that there are other roles that some of those folks can do. Not all people want to do that. There may be some churn in the short term, as people either are hesitant to learn new skills or don't wanna learn new skills or other things like that.\nBut this is true of almost every single technology change: It was true of industrial automation in the 1930s. It was true when personal computers came around, and when the internet came. It will be true for AI too. There'll be some jobs that there won't be as many of, that is true. And I think there'll be new jobs.\nI'm curious how you're thinking about all of this from an environmental perspective. I think one of the notable pieces of agentic AI is having them run continuously for hours or days. I would imagine there is a sustained energy demand in that context. We're already talking about a very energy-intensive technology. Amazon right now is the single biggest purchaser of new renewable energy contracts in the world, at least for the last five years.\nThat's right.\nHow do you put that commitment together with this drastically increased need for energy?\nLook, part of what that is, is us investing. This is not just going out and buying existing things. That is investing in new projects and bringing new renewable energy projects online. That\u2019s a huge commitment for us. It's something we do every single year and we'll continue to do. Because we think that's important from an environmental impact point-of-view, and frankly, from an energy availability point-of-view. I do think we're going to have to keep looking at other energy sources. I think that nuclear power is one of those that's going to be very important for us to look at in the medium to longer term.\nWe have to make sure that we have enough carbon-zero energy out there, but we need to look at all of those sources of energy, and it doesn't mean that no one's going to use natural gas in the inter-medium term. I'm sure some of that is true, too. Our goal is, how do we continuously, period over period, year over year, reduce the carbon intensity of the energy that we consume? We're still committed to that as a company, and we've spent an enormous amount of time on that.\nHow do you respond to criticism of this venture, which is an enormous undertaking, internally? I'm curious about the management piece there. A few weeks ago you had over a thousand Amazon employees\u2014granted, it's a company with a seven-figure employee base, if I'm correct\u2014but they described the company's quote, \u201call-costs-justified\u201d warp-speed approach to AI development. They say that it will cause, quote, \u201cstaggering damage to democracy, to our jobs, and to the Earth.\u201d That's quite a statement from Amazon's own employees. When you hear that read back to you, how do you address it?\nNumber one, we encourage our employees to have their own thoughts as to how they're thinking about things.\nWell, that's good because a lot of tech companies these days are not happy to see that happen.\nI would also say that when you have an organization of any size, you have viewpoints from a lot of different places. I would say that that is not the majority opinion from our employees or even close. I think most of our employees are excited about the technology we're building. Excited about the value that we're giving customers, and excited about the potential and the climate pledge that we have and the path that we're on. So, I think that's OK. As long as those concerns are respectful, we're willing to listen to them. But I think they\u2019re far from the majority.\nI mentioned at the top of the conversation that in January 2025 everyone promised me this was the year of agentic AI. They were wrong. And so as we look out to 2026, I'm curious for your prediction: In the context of AI, what is next year all about for us? Then in 12 months, I'm going to call you and we'll see.\nJust be super, super clear, I'm awful at predicting the future.\nI am too, but I have to ask.\nYou could call me, I just probably won't be right. But I will say I think one of the big things that we are going to be incredibly focused on for our customers is delivering real business returns for them. Whether that's through agents, whether that's through customized models, whether that's through scalable infrastructure, whether that's eliminating tech debt through using [Amazon\u2019s] Transform to get them off of mainframes or get them off of legacy databases. That, I think, is going to be a big focus. It's not going to be about going and experimenting. It's not going and trying new technology. It is really delivering value to the end customers and to the business.\nSo it's time for the P&L to look a little bit different next year.\nYep. I think that's where customers have relied on AWS to help them improve for the last 20 years. And it's what we're particularly great at.\nHow to Listen\nYou can always listen to this week's podcast through the audio player on this page, but if you want to subscribe for free to get every episode, here's how:\nIf you're on an iPhone or iPad, open the app called Podcasts, or just tap this link. You can also download an app like Overcast or Pocket Casts and search for \u201cUncanny Valley.\u201d We\u2019re on Spotify too.","source":"wired.com"}
{"url":"https:\/\/aecmag.com\/workstations\/review-scan-3xs-wa6000-viz-3rd-gen-threadripper\/","title":"Review: Scan 3XS WA6000 Viz [3rd Gen Threadripper]","date":1575936000000,"text":"AMD has transformed Threadripper from a cost-effective multi-core CPU, to one with leading performance. Greg Corke reviews this impressive workstation for rendering\nWhen AMD first launched Threadripper in 2017, some of the mainstream IT press were left scratching their heads as to what one would do with a 16-core CPU. The truth is, AMD didn\u2019t really appreciate what it had on its hands, so review samples went out to journalists who were more familiar with Killzone than they were with KeyShot, or any other ray trace renderer.\nIn short, Threadripper changed the game for ray trace rendering on the CPU, or at least paved the way for change. Intel finally had some serious competition and prices of high-end multi-core desktop CPUs came tumbling down. It was no longer just the viz specialists who could justify a workstation with more than the standard four or six cores. With CPU renderers built into CAD and BIM software, along with standalone push-button viz tools, architects and designers also had a strong case for investment. I\nn just two years, things have changed dramatically. AMD has just released 3rd Gen Threadripper, with the 24-core 3960X (\u00a31,124*) and 32-core 3970X (\u00a31,582*) shipping now. There\u2019s also a 64-core 3990X coming next year. Intel can\u2019t get anywhere near this number of cores on a single-socket desktop CPU, the closest being the 28-core Xeon W-3175X (\u00a32,416*) and the recently released 18-core Core i9-10980XE (\u00a3917*).\nFor now, the Threadripper 3970X is the flagship model, and it\u2019s this 32-core CPU that sits at the heart of the new Scan WA6000 Viz workstation.\nThe 3970X has a base clock speed of 3.7GHz and a Turbo of 4.5GHz. On paper, this is a big increase over the 32-core 2nd Gen Threadripper 2990WX it replaces (3.0GHz, 4.2GHz Turbo). But frequencies only tell half the story. 3rd Gen Threadripper offers more performance, better efficiency, and significantly higher Instructions per clock (IPC) \u2013 15% more, according to AMD. In other words, even if the CPUs ran at the same frequency, the 3970X would be substantially faster.\nThe results from our rendering benchmarks illustrate this perfectly. In KeyShot, the Scan WA6000 Viz rendered our 4K test scene in a mere 69 seconds, a staggering 41% faster than the Threadripper 2990WX we reviewed last year. We had a similar experience in V-Ray, with a 26% lead. We haven\u2019t tested the Intel Xeon W-3175X, but we expect rendering performance would be in a similar ballpark, although the Intel chip costs nearly \u00a31,000 more.\nOne of the trade-offs with 2nd Gen Threadripper was its inferior performance in single-threaded operations, which are used in CAD software. The difference wasn\u2019t huge, and nowhere near as big as it was with 1st Gen Threadripper, but it was significant all the same. With 3rd Gen Threadripper, this has changed and there really is now very little between AMD and Intel. In Solidworks, for example, it completed our single-threaded IGES export test in 81 seconds, only a touch behind an 8-core 4.9GHz overclocked Intel Core i9 9900K (75 secs) and a 6-core, 3.7GHz, 4.7GHz Turbo Intel Xeon E-2176G (80 secs). The Threadripper 2990WX took 107 secs.\n3rd Gen Threadripper also features a whopping 128MB L3 cache and a new unified memory system which theoretically should further improve performance in applications where there are dependencies between threads and memory needs to be accessed by multiple cores. Simulation software is a good example, although you might want to consider error-correcting code (ECC) memory. Also, not many FEA or CFD tools can actually take full advantage of 32 cores. The sweet spot for Ansys Mechanical, for example, is around 12 to 16 cores.\nThe Scan WA6000 Viz features 64GB of Corsair DDR4 3,600MHz, which should really be considered a minimum for a viz-focused machine like this. 3,600MHz is quite fast for memory and while some workflows will benefit, it certainly won\u2019t be all.\nThe other notable feature of 3rd Gen Threadripper is support for PCIe 4.0, which offers double the bandwidth of PCIe 3.0. Scan has made the most of this advancement by choosing a 2TB Corsair MP600 NVMe M.2 PCIe 4.0 SSD. Rated at 4.95GB\/sec sequential read and 4.25GB\/sec sequential write, this should give significant benefits when working with large continuous datasets.\nThe Nvidia Quadro RTX 6000 GPU is PCIe 3.0. But unless you\u2019re shifting serious amounts of data in and out of the GPU, this shouldn\u2019t make a big difference. In addition, there isn\u2019t much choice at the moment, as the only PCIe 4.0 professional GPU is currently the AMD Radeon Pro W5700 \u2013 more \u2018mid-range\u2019 than \u2018high-end\u2019.\nWith 24GB GDDR6 memory and a hugely powerful processor, the Nvidia Quadro RTX 6000 is a very serious professional GPU, and one with an equally serious price tag (\u00a33,697*). Those pushing the boundaries of real-time viz or VR will benefit most. Then, of course, with dedicated RT cores, there\u2019s also GPU rendering. But if you\u2019ve invested in a 32-core CPU, and presumably tuned your render pipeline accordingly, do you really need both? Of course, Scan does offer other GPUs, including the Quadro RTX 4000 and 5000, which significantly brings down cost.\nBy packing so many cores into a single CPU, it\u2019s inevitable that 3rd Gen Threadripper is power-hungry. The 3970X is rated at 280W and, when combined with the 295W Quadro RTX 6000, it\u2019s clear why the Scan WA6000 Viz needs an 850W PSU. Despite the significant power draw, the machine stays relatively quiet even when both CPU and GPU are being hammered at the same time. The sizeable Fractal Design Define R6 case and the Cooler Master MasterLiquid ML360 TR4 hydrocooler cope well with the high thermal output. The CPU maintained a very impressive 3.9GHz (O.2GHz above the base frequency) when rendering for 30 mins.\nConclusion\nWhen you consider just how far AMD was behind Intel a few years ago, it\u2019s incredible to see it now leading the pack in high-end desktop CPUs. And with the 64-core Threadripper 3990X coming next year, this lead is set to get even bigger. At the moment, it\u2019s only specialist manufacturers like Scan that offer Threadripper, but surely Dell, Fujitsu, HP and Lenovo can\u2019t ignore it for long.\nOf course, not everyone needs 32 or 64 cores on a desktop machine, and with increased competition from GPU rendering in applications like V-Ray and KeyShot, architects, designers and viz specialists have a big decision to make. But if you\u2019re heavily invested in workflows that can take advantage of so many CPU cores (and lots of memory), not forgetting simulation or point cloud processing, the Scan WA6000 Viz has much to offer.\nProduct specifications\n\u25a0 AMD Ryzen Threadripper 3970X 3.7GHz, 4.5GHz Turbo) (32 cores) CPU)\n\u25a0 Nvidia Quadro RTX 4000 GPU (8GB GDDR6 memory)\n\u25a0 128GB (4 x 32GB) DDR4 2,666MHz memory\n\u25a0 500GB M.2 NVMe SSD (Samsung)\n\u25a0 Fractal Design Define R6 chassis 543mm (l) x 233mm (w) x 465mm (h)\n\u25a0 Microsoft Windows 10 Professional 64-bit\n\u25a0 3 Years \u2013 1st Year Onsite, 2nd and 3rd Year RTB (Parts and Labour)\n\u25a0 \u00a36,667 (Ex VAT)).\n*All prices Ex VAT. Taken from scan.co.uk on 4\/12\/19.\nCPU benchmarks (secs \u2013 smaller is better)\nCAD\n(Solidworks 2019 IGES export) \u2013 81 secs (smaller is better)\nRendering\n(KeyShot 8.1) \u2013 69 secs secs (smaller is better)\n(V-Ray Next Benchmark) \u2013 23,714 45,426 kSamples (bigger is better)\nGraphics benchmarks ((frames per second @ 4K res) (bigger is better)\nVIZ (Enscape)\nMuseum \u2013 35\nVIZ (Lumen RT)\nRoundabout \u2013 38\nGPU rendering benchmarks\nGPU rendering\n(Solidworks Visualize 2020) 1969 Camaro car @ 4K\n\u2022 1,000 passes \u2013 199 secs\n\u2022 100 passes + AI denoising \u2013 21 secs (smaller is better)\nGPU rendering\n(V-Ray Next Benchmark)\n366 mpaths (bigger is better)\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/esri-gis-meets-bim\/","title":"Esri: GIS meets BIM","date":1590969600000,"text":"AEC Magazine talked with Marc Goldman, director AEC Industry Solutions at Esri about how the company is working with BIM developers to better integrate GIS for AEC professionals\nAEC Magazine: Why should your average architect be interested in GIS?\nMarc Goldman: In my short stint as a designer (I can\u2019t call myself an architect or engineer because I don\u2019t have licences nor ever did) I recall the challenge of putting my first line on paper or screen. Having geographic context allows you to overcome that inertia, provides you with greater understanding, brings insights to the surface, and allows designs and engineered projects to begin with the most amount of valuable information to be accessed, managed, and used for decision-making.\nAEC: Revit is a key application for readers of AEC Magazine. On a technical level, how does ArcGIS work with Revit data \u2013 both in terms of geometry and attributes?\nMG: In ArcGIS Pro we can import RVTs, store the geometry and attributes in a geodatabase that captures some of the semantics of the Revit file, and then use the geometry and attributes in GIS workflows, such as by converting them to I3S Building Scene Layers that can be streamed over the web to mobile and desktop experiences.\nAEC: The first stage of the Autodesk \/ Esri partnership was about connecting Autodesk\u2019s desktop applications to ArcGIS. But the real prize comes from cloud \u2014 connecting Esri ArcGIS and Autodesk BIM 360. What\u2019s the latest on this and why is it so important to AEC firms?\nMG: We\u2019re working on a cloud-to-cloud integration which will allow GIS dashboards, maps and analysis to be performed using any files and data in BIM 360, and from other cloud environments. This \u2018cloud to cloud\u2019 communication, whereby the data in either cloud is accessible to users in browsers and mobile apps, opens to door to workflows we\u2019ve only dreamed of in the AEC industry.\nAEC: Esri previously told us that BIM \/ GIS integration isn\u2019t just about giving BIM models a GIS context, it\u2019s also about giving AEC firms access to Esri services. Can you explain more about these services and how AEC firms can benefit? And what are the challenges of big data?\nMG: Let me first talk about civil BIM and then get onto the \u201cbuilding\u201d BIM side. On the civil BIM side, within [AutoCAD] Civil 3D and Infraworks, engineers have access to the layers and data you\u2019d use or manage in ArcGIS Pro, or which you\u2019d visualise and interact in ArcGIS Online. One benefit is streamlining workflows, so as data is collected in the field with Esri field-based apps, that data appears in real time on the engineer\u2019s desktop in Civil 3D or Infraworks.\nAs to improving quality, in Civil 3D and Infraworks you have access to the GIS data layers to better understand the project\u2019s constraints. All this means individuals and teams can make informed design and engineering decisions by visualising collaboratively, the Infrastructure and GIS data directly.\nLooking at Building BIM, once the BIM (RVT or IFC) has been imported to ArcGIS Pro, the information is managed like other geodatabase or shapefile data sources. This allows the BIM objects themselves to be understood visually \u2013 sliced and diced on screen.\nUsers can tap into the properties of the BIM objects and of the project itself, getting deeper understandings than any \u201cviewer\u201d and easier than any modelling app. Architects, engineers, contractors and owners collaborate with a common view into the intelligence which can be gleaned from the BIM.\nThe challenges of \u2018Big Data\u2019 for any AEC industry technology provider are the same as those of most any industry. How do we address the goal of transforming massive amounts of spatial data into manageable information? And what do we do with this ability and how do we deliver value from the data? The answer is, we\u2019ve been doing this for decades and now we\u2019re also ingesting real-time data from sensors, and IoT systems and delivering actionable information that automatically scales to the analysis and data storage needs of the facility or infrastructure project.\nThrough our solutions, users reveal spatial patterns that animate and automatically aggregate as real-time data streams. Using time as a variable you can visualise massive datasets in four dimensions and identify data patterns that were previously hidden in the noise of spreadsheets, revisions and endless logs. It\u2019s also possible to find clusters and hot spots of activity. Using regression tools, we\u2019re able to find relationships between datasets and predict future events which drive building control systems and other systems effecting performance and optimisations.\nWe know how to make sense of massive imagery collections. We\u2019re experts in using distributed processing and deep learning for object detection, classification, terrain analysis, and change detection on any scale.\nWe don\u2019t see many unique challenges; we see many opportunities in fact.\nAEC: Last year Autodesk admitted that it needed to work closer with Esri and engage with AEC customers together, rather than as separate entities. Is that now happening and what have Autodesk and Esri learnt from this?\nMG: We\u2019re working very closely together. I have two or three calls weekly with Autodesk and I\u2019m one of a several at Esri working with Autodesk. We\u2019re hosting a series of Joint Customer Council meetings in the US and Europe, and we\u2019re working on various tactical efforts and strategic initiatives with our shared customers.\nOur technical teams work closely on the ArcGIS Connector for Civil 3D and the ArcGIS Connector for Infraworks, as well as Revit & ArcGIS interop and a number of code-named projects at various stages of exploration and development.\nWe\u2019ve learned where our efforts are bringing value to users across a number of workflows and segments of AEC. Currently the greatest value, the most compelling integrations and user adoption is in Civil related projects. However, building projects are certainly benefitting as well from the earliest stages of planning through building operations. Esri and Autodesk are working together with our combined customers delivering new capabilities, higher productivity and better collaboration at the intersection of GIS and BIM.\nAEC: What\u2019s Esri\u2019s strategy in the digital twin market, this mixes GIS, BIM, point cloud and all project data?\nMG: Digital twin is one of those industry terms that means something different to everyone. But that\u2019s OK \u2013 a digital twin can mean various things to different people. At this point in time, the term is not used always the same nor is a digital twin comprised of the same information from one project to another, one client to another, and one portfolio to another.\nWe don\u2019t throw the term \u201cdigital twin\u201d around as much as some others in this industry. If there\u2019s ever been a platform suited to deliver the promise of a digital twin it\u2019s the ArcGIS platform. The platform can be a critical component of, or even the hub of, a Connected Data Environment which collects, analyses and provides insight on a built project from the earliest stages of planning through to ongoing operations and maintenance. We connect to data repositories and document management systems as part of a digital twin strategy, and we connect to the data streams from beacons and IoT devices. We bring this together to deliver the capabilities which our industry is associating with the idea of a digital twin. We\u2019re just not throwing that term around so widely.\nAEC: How does ArcGIS work with IoT information and what instances have there been of customers implementing this in construction\/building that you can talk about?\nMG: We have a product called Analytics for IoT, which is fully integrated with ArcGIS. It enables users to ingest, visualise, and analyse spatial real-time and big data.\nWithout getting too specific, the US Dept of Energy is one of our customers; we\u2019re working with several of the DoE laboratories exploring projects where the intersection of IoT meets energy modelling and proactive building performance management with 3D interaction and easy to use dashboards. We are still exploring.\nAEC: Outside of Autodesk, which other application developers has Esri been working with? Nemetschek, Trimble, Bentley? And what level of integration can we expect? Any interesting cooperation projects going on? (Vectorworks is the leader in landscape software for instance)\nMG: Nemtschek \u2013 yep! Their landscape leadership with Vectorworks is of great interest and we have a partnership with them announced back in November. Vectorworks has a built an integration to ArcGIS \u2013 we\u2019re a logical starting point, reference point and deliver insights to Vectorworks users. And, it doesn\u2019t hurt Esri\u2019s founder and our president, Jack Dangermond, has a Master\u2019s Degree in Landscape Architecture. To this day as the company grows, he is involved in the location and orientation every bit of landscaping at the most lush and welcoming campus I\u2019ve ever visited, Esri\u2019s in Redlands, CA.\nTrimble \u2013 yep! We have worked collaboratively for years. The most successful collaboration has been between their Trimble Geospatial Solutions group and our Field Apps team. We have worked together to bring high accuracy GNSS solutions for Esri products (such as Collector).\nAEC: What roles are AI and simulation playing in GIS and what kind of applications have we seen\/can we expect to see in digital construction, civil and urban planning?\nMG: The industry has a need to capture the condition, location and other statuses, and expect intervention through AI and ML \u2013 we\u2019ve developed and delivered this in multiple cases.\nWe\u2019ve applied recognition of visual assets through photo and video capture for civil engineering and urban planning. Specifically, classification of infrastructure assets along miles of urban and rural environments.\nI envision we\u2019ll see operations managers, project superintendents, and machine operators use AI to fill in the gaps in planning and scheduling databases, track materials assets in real time, accurately predict arrival times, installation, and anticipate future services and maintenance needs to stay one step ahead. There\u2019s really nothing to keep us from delivering this today.\nAEC: What are the product development key focus points for Esri in the building\/construction space for the next couple of years?\nMG: There are a number of key focus points: obviously, enabling digital project delivery for projects of all size.\nWe aim to deliver solutions that bring the field and the office closer together through the aggregation, curation and delivering insights through the combination of: reality capture data, BIM \/ VDC and schedules.\nWe will continue to bring the worlds of BIM and GIS together to realise the full potential of integrated workflows between what were once competing technologies.\nAlso, we want to replace paper-based workflows with digital solutions that aggregate information into insightful common and connected environments.\n\u25a0 esri.com\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/bentley-be-forum-prague\/","title":"Bentley BE Forum, Prague","date":1151452800000,"text":"Bentley Systems recently held a major week-long user event in the Czech Republic to launch the long awaited MicroStation V8 XM, unveil ProjectWise StartPoint for entry-level collaboration and announce major enhancements to its Select program.\nPrague is a beautiful city, of that there is no doubt. The majestic castle and bridge never fails to impress and every little turn always offers another architectural gem. Even though in summer it\u2019s buzzing with tourists and British stag parties, this well preserved city, combined with the famous Czech hospitality, makes the destination an excellent one for a European user event, and that works doubly so when the World Cup is on.\nBentley has had a European jamboree for its users once before, in Rome, but this was many, many years ago. Since then it has kept its yearly main event strictly in the US, moving about the East coast through Philadelphia, Baltimore, North Carolina and Florida. Prague represents a significant return of focus to Europe for the company, running its Prague \u2018mirror\u2019 event only a week after the US BE (Bentley Empowered). Many of the company\u2019s executives were in town for the event, headed up by Bentley CEO, Greg Bentley.\nThere were a large number of product and business strategy announcements at the event, with the core focus on the latest version of MicroStation V8 \u2013 called XM. MicroStation still acts as the core platform for most of Bentley\u2019s vertical products for Process Plant, AEC, Civils and GeoSpatial. Strangely enough, Bentley is one of Autodesk\u2019s biggest developers (although that\u2019s an unofficial title), also owning a number of applications that run on, or alongside, AutoCAD.\nMicroStation V8 XM\nV8 was the last release of MicroStation and it was a major reworking of the product, especially with regards to the database and capabilities behind the scenes of the CAD product. The work resulted in a major change to MicroStation\u2019s file format, DGN but provided Bentley with an interesting edge, offering dual format support for its own and AutoCAD\u2019s DWG format. For years, Bentley\u2019s customers had complained about problems working within a DWG world and in V8, Bentley pretty much removed many of these issues, allowing MicroStation to work in either format \u2013 a major engineering challenge.\nThe downside was that while V8 was a lot of engineering work, it still looked the same as the previous editions of MicroStation and really was falling behind the competition in user interface and graphics capabilities. V8 XM is the completion of this major product overhaul, entirely replacing the graphics pipeline and significantly whizzing up the 3D performance. XM by the way doesn\u2019t stand for anything.\nFor those of you that have never used or owned MicroStation, Bentley has a continuous improvement development process as part of its Select subscription, which means that it\u2019s always hard for journalists to tell you what it can and can\u2019t do as it\u2019s continually having functionality added to it!\nIn the past Bentley has lead the way with many innovations like Xrefs, image manipulation tools and CAD management aids. While Autodesk has recently picked up the innovation gauntlet, XM has a few cool new things in it beyond the 3D graphics enhancements:\nElement Templates: It\u2019s now possible to integrate CAD standards with MicroStation Tasks to align features and tools with design and production workflows, so that teams can create consistent work.\nProjectWise StartPoint: Document management is now built-in providing an entry-level collaboration tool to manage, find, and share CAD and geospatial content using Microsoft Office SharePoint technologies. The product works with MicroStation and AutoCAD.\nReference Enhancements: You can attach a PDF reference to a design file, dynamically manipulate reference clipping boundaries with handles, and attach multiple instances of the same model at different stages of development using Design History \u2013 a unique V8 feature that lets you roll forwards and backwards through design milestones.\nLink Sets: By placing special links between documents and across formats including DGN, DWG, PDF, and Office formats, it\u2019s possible to collate related documents and this can be leveraged by ProjectWise.\nPANTONE and RAL colour systems: Catching up with AutoCAD, MicroStation now supports 24-bit colour to deliver richer, more consistent presentations.\nDirectX graphics system: As we pointed out recently, there\u2019s a new display subsystem in town and it uses Microsoft DirectX technologies which are used to drive high-speed graphics technology in the video gaming industry, brings a significant increase view and navigation speed in 2D and 3D designs. It also places MicroStation in an easy to port to Microsoft Vista\u2019s operating system, whenever it gets released.\n3D modelling advancements: A big beef up in the 3D department to coincide with the new speedy graphics engine, MicroStation now offers 3D parametrics, mesh modelling for creating lightweight structures, and new handles for intuitive and interactive editing. Visualisation and animation have also been improved.\nAnd finally, Keyboard Position Assignments: Bentley has introduced \u2018Patented keyboard position assignment\u2019 to provide immediate access to any MicroStation command at the stroke of a key, and programmable mouse functionality to increase the performance of view and model navigation. While this sounds grand, MicroStation has always had powerful keyboard shortcuts.\nEven though Autodesk released AutoCAD 2007 in March with a new DWG file format, MicroStation V8 XM doesn\u2019t yet support the new DWG format. Bentley is reliant on the Open Design Alliance (ODA) to reverse engineer the format and hand over the libraries. We were told that once the ODA had done this, MicroStation V8 XM will have that functionality added to it, probably within one of the regular Select upgrades.\nOverall, MicroStation has undergone a big change over the last two releases. It\u2019s taken quite a while to get there, XM having been in beta for the last year alone. The inclusion of ProjectWise Sharepoint promises to be a significant benefit for MicroStation customers that couldn\u2019t justify the cost of implementing full ProjectWise and could lead to wide-spread adoption of Bentley\u2019s PDM tool within its base. Bentley has also targeted Autodesk customers with a special port of the application at what sounds like an attractive price per seat.\nWhile it\u2019s hard to get excited about a vanilla CAD system these days, it\u2019s pretty obvious to see how Bentley\u2019s vertical product portfolio will make great use of the new 3D technologies within MicroStation v8 XM. At first we are expecting to see mainly flat ports of existing applications but over time, XM will drive 3D functionality and adoption.\nGenerative Components\nLong standing readers of AEC magazine will be well aware of my fascination with Bentley\u2019s Generative Component technology. There\u2019s very little out there to compete with this parametric form-finding application that is built on MicroStation. The event included a number of end-user presentations describing how Generative Components has already been used on a number of major projects, the largest of which is a soon-to-be-built London skyscraper designed by KPF.\nWhile the software has been in development and beta for almost three years, it\u2019s now getting to be proven in the field. Generative Components offers a programmatic way for designers to scope out their designs and interactively adjust and interact with complex frameworks to derive optimal forms. At the event Lars Hesselgren of KPF demonstrated that its latest tower project had a number of last minute height changes which were easily catered for by the Generative model which he had created.\nIt looks like Generative Components will get its official launch in September of this year. A recent popular event at the British Design Museum attracted many London and New York based practices, who are all experimenting and looking at these sorts of generative designs.\nRobert Aish, Bentley\u2019s Director of Research gave me a one-on-one demonstration of how a Generative Component model is created. In the workshops I have to admit that it appeared as if extensive programming knowledge was required but I\u2019m glad to say that it\u2019s fairly straightforward to build simple frameworks, using standard MicroStation geometry and a point and click interface. Once the concepts have been mastered, some programming knowledge will be required but I think the early and useable results will drive people to skill up to drive the system to solve more complex problems.\nSelect update\nSelect is Bentley\u2019s subscription program. For a set fee, customers get support, regular upgrades, pooled licences and many other things. This year Bentley upgraded Select to offer even more and one unique benefit.\nFrom September 1, Bentley adds three new three offerings within Select, now nine in total:\n1 Annual license exchange (new)\n2 Pooled and Trust licensing\n3 Continuous product development and updates\n4 OnDemand eLearning (new)\n5 BE Conference registration\n6 Help desk\n7 ProjectWise user subscription (new)\n8 Additional software to extend the enterprise\n9 Select membership in the Bentley Developer Network\nThe main event here is the annual licence exchange. Bentley says that this protects an organisation\u2019s investment in Bentley software. It does this by providing subscribers with the opportunity to rebalance their technology portfolio in the year ahead. To do this, Bentley has first stated that all software on maintenance is worth the current list price. So while you may have bought MicroStation six years ago, it\u2019s worth the current list price of MicroStation when you go to trade it in for another Bentley product. That\u2019s like owning a car for six years and never losing any value off it \u2013 which is pretty cool. Once a year as you re-subscribe you can re-allocate licenses and products from the Bentley portfolio for the value of the software you trade in. This is an industry first. Software licenses can be exchanged to accommodate changing business requirements, replace underutilised licenses or as part of a \u2018trade-up\u2019 to new Bentley technology.\nOnline eLearning courses should help keep users skill up or move from one release to another. I think this is great technology although I am unconvinced that it\u2019s as good as one on one training but then again it\u2019s free and as it\u2019s part of Select you can sit through it as many times as you want. There is definitely a skills shortage with MicroStation users few and far between. I don\u2019t think this is the solution for that but anything to assist increase the pool of users would be welcome.\nFinally it\u2019s worth mentioning the new trust licensing model. In the past, Bentley customers have been able to use pooled licenses, allowing a company to buy fewer licenses than users, and let MicroStation be a shared resource. The new strategy is to remove this limit, which impacts Bentley customer\u2019s productivity. So with the new XM Select server, there will not be a limit to the number of licenses that can be handed out. Instead, usage will be sent to Bentley and over many, many months (Bentley is being a bit vague here but the indication is a year), the company will gauge your usage and come back if you regularly use more than the number of licenses you own. It\u2019s kind of like an honesty bar but Bentley is watching.\nI had a number of conversations with users over this and the trust licensing caused the most concern. The pooled version meant that the first thing users did when they got into work was to start a copy of MicroStation, so they could hold onto the license all day. Despite trying to get users to use the system properly, MicroStation license scarcity drove users to hoard! The concern is that with trust licensing users will continue the culture of hogging and then run up a bill with Bentley. Bentley has tried to limit this worry, saying that only obvious abusers will be picked out. It\u2019s also possible to time out MicroStation sessions. I can see what Bentley thinks it\u2019s offering users here and if used correctly, could be of great benefit. There\u2019s just an element of big brother about usage reports and users fear additional expenditure. I am sure this will work out over the next year.\nWhile on the subject of Select and subscription models, I have talked with users of many different CAD systems and there seems to be a growing wariness of subscription, in that it\u2019s a honey pot designed to tie you in and become more reliant on one technology vendor. I can see their point. Rapidly, users are questioning the value they get from the yearly membership fees, as CAD systems become more and more like a service than a product. Bentley has an enviable subscription model in the CAD industry and from these recent announcements is undoubtedly aware of the on-going value assessment.\nConclusion\nBentley BE Forum, Prague was a successful event in all, with lots of good product and end-user benefits announced. Bentley is continuing to develop its technology portfolio and broaden its flexibility with customers. One wonders if Bentley\u2019s marketing will increase, as its presence has certainly been absent in the European market. Having a major launch event in Europe once more, is a start. We hope to review MicroStation V8 XM soon.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/microsoft-launches-hololens-2-to-enhance-mixed-reality\/","title":"Microsoft launches HoloLens 2 to enhance mixed reality","date":1551139200000,"text":"Microsoft uses Bentley Synchro XR to demonstrate benefits of mixed reality to AEC professionals\nThis week at the Mobile World Congress in Barcelona Microsoft introduced HoloLens 2, its much anticipated next generation wearable holographic computer. Microsoft was joined by Bentley Systems who showed how with its Synchro XR for HoloLens 2 software, users can interact collaboratively with digital construction models to plan, visualise, and experience construction sequencing.\nHoloLens 2 features more than double the field of view than the original mixed reality device, which many found to be a significant limitation. There\u2019s also a new visual display system, which Microsoft says makes the holograms even more vibrant and realistic.\nThe updated device includes new eye-tracking sensors that are designed to make interacting with holograms even more natural. For example, when someone\u2019s eyes land on a particular part of a machine, it can call up useful digital information about it.\nInteraction with holograms has also been made easier. With natural hand tracking, users can now grab and rotate them as they would a real-world object, rather than having to learn gestures that mimic mouse movements.\n\u201cFor the first time, you\u2019re going to feel what it feels like to touch a hologram, to interact with a hologram and to play with it, almost where you forget that this is a piece of digital content you\u2019re looking at as opposed to it just existing in the real world,\u201d said Alex Kipman, technical fellow in Microsoft\u2019s Cloud and AI group.\nHoloLens 2 also includes iris recognition with Windows Hello authentication to make it easier for multiple users to log in and share the device. A new flip-up visor lets users switch easily between physical and holographic worlds.\nComfort has also been addressed. Microsoft says the new dial-in fit system makes the HoloLens 2 comfortable to wear for \u2018hours on end\u2019. Users can also wear glasses with the mixed reality headset because it now adapts by sliding right over them.\nIn terms of practical applications for construction, Bentley Systems showed how a project digital twin can be visualized with the HoloLens 2 via Bentley\u2019s connected data environment, powered by Microsoft Azure. With the mixed reality solution, construction managers, project schedulers, owner operators, and other project stakeholders can gain insights through immersive visualisation into planned work, construction progress, potential site risks, and safety requirements. Additionally, users can interact with the model together and collaboratively experience 4D objects in space and time, as opposed to traditional interaction with a 2D screen depicting 3D objects.\nNoah Eckhouse, senior vice president, project delivery for Bentley Systems, said, \u201cOur Synchro XR app for HoloLens 2 provides a totally new way to interact with digital twins for infrastructure projects. Users benefit from a new perspective on the design and a deeper, more immediate understanding of the work and project schedule. Instead of using a 2D screen with a mouse and keyboard, the user can now walk around the model with their body and reach out and grab digital objects that appear to co-occupy physical reality. This is a powerful way to review work that is completed and to prepare for upcoming work at the jobsite.\u201d\nMenno de Jonge, director of digital construction for the Royal BAM Group, said, \u201cWe are currently using Synchro and HoloLens 2 mixed reality solution for the construction site for a large museum project in the city of Rotterdam. The real need for a digital transformation in our industry is about avoiding rework at our construction site. Using this technology, we can easily visualise the construction schedule. Then, we can see if we are behind in schedule, we can flag any potential problems or issues, look into the problems, and get back on track.\u201d\nBentley emphasised the importance of the cloud to track all the moving parts on massive infrastructure projects. By connecting each HoloLens device on a site to a master model that\u2019s constantly updating in Azure, it helps ensure that everyone works from the same shared reality, with the latest information to sequence jobs, plan crane movements, track progress and keep workers safe.\n\u201cThe cloud connectivity is critical because in these large projects the amount of information going back and forth between the field and the engineers and designers is continual,\u201d said Eckhouse. \u201cAnd the consequences of working on infrastructure projects in the physical world are very real.\u201d\nMeanwhile, Trimble has introduced a certified wearable hard hat compatible device, the Trimble XR10 with HoloLens 2, that enables workers in safety-controlled environments to access holographic information on the worksite. The company also announced that it is continuing to develop its cloud-based collaboration platform, Trimble Connect for HoloLens, to help site workers get more value from constructible 3D models and transform daily work such as assembly and inspections.\nEpic Games also announced that Unreal Engine 4 support is coming to HoloLens.\nFinally, Microsoft has introduced new Azure mixed reality services that are designed to help developers build mixed reality applications. One of these is Azure Remote Rendering, which helps reduce the need to \u201cdecimate,\u201d or simplify a 3D model so it can run on target hardware. This service will render high-quality 3D content in the cloud and stream it to edge devices, all in real time, which will help retain a higher level of detail for scenarios like design review. Azure Remote Rendering is currently in private preview. Meanwhile, Azure Spatial Anchors allows people to create holograms that persist in a specific physical space.\nMicrosoft has also released mixed reality applications for Microsoft Dynamics 365 to help companies use HoloLens 2 without needing to hire developers. Dynamics 365 Guides, for example, helps companies move training materials from flat paper and screens into an immersive three-dimensional experience. It joins Dynamics 365 Remote Assist and Dynamics 365 Layout which were announced last year.\nHoloLens 2 will be available this year at a price of $3,500.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/corona-renderer-gets-performance-boost-for-interactive-rendering\/","title":"Corona Renderer gets performance boost for interactive rendering","date":1543449600000,"text":"New release of the photorealistic renderer boasts better IR performance, reduced memory usage and \u2018instant\u2019 GPU-based denoising\nThe latest release of Corona Renderer 3 for 3ds Max, which is popular with arch viz studios, features faster interactive rendering (IR) and an enhanced version of the Nvidia AI Denoiser.\nThe new IR system is said to offer quick impressions of scenes, helping artists assess key elements like lighting early on in the design process and iterate more quickly, cycling through objects, materials and compositions.\nThe enhanced AI denoiser uses deep learning to remove artifacts and grain, leaving a clear image. According to the Render Legion, the developer of the software, the unique integration of the AI denoiser in Corona Renderer can preserve more image details, including reflections and refractions, than any other implementation.\nIn addition, improved displacement tools are said to now create better quality imagery with no increase in memory usage or time. Render Legion estimates that memory savings are around 50 percent for artists, with no loss in quality or details.\nCorona Renderer 3 is Render Legion\u2019s second update of 2018, with the next update to hit studios early next year. Among the planned features: an implementation of lightweight, fully automatic caustics and a new Light Solver.\nCorona Renderer 3 is available now for Autodesk 3ds Max x64, versions 2013-2019. Pricing is subscription-based, with monthly rates set at \u20ac24.99 and yearly rates at \u20ac289.99. Render Legion, who is owned by Chaos Group (the developer of V-Ray), also offers a free 45-day commercial trial.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/a-q-a-with-space-group-3ddesign-modelling\/","title":"A Q&A with Space Group","date":1502064000000,"text":"Space Group CEO Rob Charlton shares his thoughts on the level of maturity within the built environment in the UK, digital construction methods and processes from design stage through to asset handover\nNow in its 60th year, Space Group has seen significant change in our industry and was a very early adopter of 3D design and modelling. Rob Charlton tells us not just from an industry perspective how far we\u2019ve come, but from personal experience what influences change and most importantly the desire to change.\nQ: What level of maturity do you think we\u2019re at right now in the UK in our ability to truly meet Level 2 BIM requirements?\nA: The reality is we are a long way from the entire industry achieving level 2. I even doubt we will ever get there. The standard is very structured for obvious reasons which may mean consistent delivery will be a challenge.\nHowever, I believe there is commitment from most that construction is digitising and very few are now ignoring it. I think Level 2 provides a great framework for the industry to work within and to use to suit their requirements.\nI\u2019m a great believer in that the tail should not wag the dog.\nUltimately, we are all striving to improve outcomes across the construction sector. If Level 2 standards help us achieve this then that\u2019s great.\nQ: It\u2019s clear that the art of collaboration and sharing knowledge, skills and expertise is something you feel very strongly about. Is this a key driver for Space Group?\nA: At Space Group we are driven by \u201cMaking Buildings Smarter\u201d We can\u2019t do this alone, and if we are to make a contribution to the industry we need to work with a wide range of companies and people. All of our most successful projects, which have achieved the best outcomes have had great teams all working together and sharing knowledge to achieve a common goal.\nTraditionally we know design and construction has been contractual and siloed. Digitisation helps to break down these barriers and to share information. Fortunately, now we have projects which have completed successfully by focusing on collaboration through a digital framework. This provides the opportunity to turn talk into reality.\nQ: Do you see Space Group as initiator of change?\nA: I have never seen our role to drive change in our industry. There are many great groups and institutions far better placed than we are. We are driven by improving the outcomes of our clients. Innovation helps us achieve this.\nWe need to work with great companies and organisations to help us achieve this. We want to bring all of these great people together which was why we started BIM Show Live. This is a great place for sharing and learning and over the years we have built some fantastic relationships and partnerships through the show.\nQ: How do you engage with built environment stakeholders to offer new technical solutions and smarter ways of working?\nA: We spend time trying to truly understand the challenges faced by our clients. This often means you need to understand their business. When we work with real estate investment trusts their priority is to maximise shareholder investment. Greater predictability and the reduction of risk is all important.\nWhen working with discount retailers their priority is pace of delivery therefore we need to consider standardisation and information exchanged.\nWe are continually developing new ideas and testing these on our clients.\nQ: What is the next step on the UK construction industry\u2019s digital journey?\nA: We have proven over the past decade what can be achieved across construction in a short space of time. The industry is now full of clever digital experts who would not have joined the industry a few decades ago. There is now a now a culture of innovation and challenge. We are already looking at technologies from other sectors and how they can improve our industry.\nThe increasing influence of millennials will continue to challenge and change the culture, digital skills will increase exponentially as new people enter the industry and software will become increasingly intelligent, by using data to allow artificial intelligence to influence decisions.\nHardware will also continue to develop. This will be beyond processing and will start to include hardware such as robotics, reality capture and 3D printing.\nIt is certainly an exciting time for construction and hopefully we will be able to compete with other sectors to attract the best talent in the years to come.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/transitioning-to-bim-pt3\/","title":"Moving from AutoCAD to Revit: part 3","date":1438300800000,"text":"Sean Bryant gives more practical advice on how a CAD manager in a medium-sized, multi-disciplinary practice can help smooth the path from AutoCAD to Revit. This month he tackles data transfer\nIntroduction\nThis is the final article in a series of three about making the move to Revit.\nUsing the scenario from part one of the series\nA medium sized, multi-disciplinary CAD practice, involved in architecture, structure and services, which often works with external contractors. Based in London, UK, it has fifteen core users, with anywhere up to twenty-five users when contract CAD personnel are brought in to make up capacity. A CAD manager is in place who acts as liaison between management at director level and the users in the CAD team. The team is currently using Autodesk AutoCAD for all of its work and is up to date with the latest version, due to an active Autodesk subscription agreement.\nThe practice has decided to use Autodesk Revit as its BIM tool of choice. There is a need to manage both the implementation and training required to make the practice both effective and efficient.\nThe article is written from the CAD manager\u2019s perspective.\nAddressing data transfer\nCAD manager to director level\n\u201cWe need to transfer large amounts of data to a database related product so that large-scale repetitive changes can be exported, edited and imported back to Revit. I can look for free apps on the Autodesk App Store, but we will need to look at paying for a suitable proprietary app to work with, so will need budget\u201d\nCAD manager to the CAD team\n\u201cAs we move forward with Revit we will need to work with large data exports. We don\u2019t have the skills in-house to automate these imports and exports, so we will be using an app we can all use to work in this way.\u201d The CAD manager needs a budget to ensure s\/he has a proprietary app that works. As the practice grows, so will the projects. Free apps have limitations and are aimed at the VSB\/SME sector. This does not work for large scale commercial\/residential projects that require large data transfer. A professional app is required to transfer Revit data to products such as MS Excel and MS Access. This will enable quick and easy data transfer which can be edited by a non-Revit trained user. Once edited, data can be imported back in to Revit.\nDeciding on commercial Revit data transfer software\nCAD manager to director level\n\u201cThere are a number of products out there in the marketplace and we need to make sure we get the best we can for what we can afford. I don\u2019t want to spend thousands of pounds on an app we only use once\u201d\nCAD manager to the CAD team\n\u201cI need you to evaluate these Revit data transfer products. They will need to be tested against our existing projects and some past projects. This will allow us to set a benchmark of whether we would have saved time on completed projects and whether we can save time and beat deadlines on ongoing projects.\u201d\nThe CAD manager is approaching the team to test and evaluate the products. This will give the practice a good idea of what app it needs, and allow the entire team to make a group decision on what is best. Delegating tasks is a good managerial skill, empowering the team and allowing the CAD manager to utilise the skills the team already have. It will build familiarity with the products before purchase, saving learning time when using the chosen app in a live environment.\nChoosing the app\nCAD manager to director level\n\u201cWe will make a decision on the app to be used based on what results the CAD team present us with. They will be testing the apps on past and existing projects to assess time savings and ease of use.\u201d\nCAD manager to the CAD team\n\u201cWe will decide on an app that YOU want to use. It will need all of you to thoroughly test and assess all of the products we are looking at, not only on an individual user basis, but on how they will enhance the performance of the practice as a whole.\u201d\nA CAD manager has to delegate tasks to his or her team. That is the art of good management. Delegate tasks and empower the team, and then manage the results the team provides. A good CAD team will give the CAD manager the information needed to make the right purchasing decision. The CAD team are on the ground at the coal face every day. They will quickly assess which app will make their lives easier.\nUsing the app\nCAD manager to director level\n\u201cWe now have a working Revit data transfer app in the practice. We should now see time-savings on our existing and future projects. It has been fully assessed by the team and will be used as and where necessary.\u201d\nCAD manager to the CAD team\n\u201cThe Revit data transfer app is now in place for all of our projects. An investment has been made in the team to assist us with the use of data in and out of Revit on our projects. We will make sure that we use the app to our best advantage.\u201d\nAnother role of the CAD manager is to lead. The above quote to the team is encouraging them to take advantage of the investment made in them, and to use to app to increase team performance on their Revit projects. This, in turn, will make their Revit projects more profitable and benefit the whole practice.\nConclusion\nIn this final installment about transitioning to BIM, we have looked how to bring in third party data transfer apps that work with Revit.\nRevit, and other BIM-related applications, create large amounts of data, especially when scheduling. Sometimes, updates to this data via Revit, can be time-consuming, so the ability to export out that data to a proprietary application, such as Microsoft Excel, can save large amounts of time, with editing work being able to be done by a non-Revit trained member of staff. Then, when the editing is done, the data app will allow for the edited data to be imported back in to the Revit project, thus saving time.\nThird party data apps such as Ideate BIMLink fulfil this need, allowing practices to handle large amounts of BIM data that they might not be able to handle otherwise.\nThese types of third party apps allow you to build up your BIM app inventory, making your BIM toolkit bigger and better, enhancing BIM performance and empowering your CAD team.\nAbout the author\nShaun Bryant is an Autodesk Certified Professional with twenty-six years total industry experience using AutoCAD and Revit.\nThe first part of this article can be read here.\nThe second part of this article can be read here.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/new-imodel-army\/","title":"New iModel army","date":1512000000000,"text":"At the Year In Infrastructure event, held in Singapore in October, Bentley Systems CTO Keith Bentley unveiled the company\u2019s most significant technology update in a decade, writes Martyn Day\nTen years ago, Bentley Systems introduced iModels as a format to exchange information between design, construction and operations environments. As a bespoke format, it was designed to hold information pertinent to the construction industry, supporting business properties, geometry, graphics and relationships.\nThese optimised files are self-defining; in other words, they don\u2019t require a CAD system to view precise model geometry (there is a free viewing application Bentley View on mobile or desktop), they know their source and when they were produced and can conform to multiple schemas, supporting industry standards. Companies such as Trimble embraced the format and plug-ins were developed for popular products such as Tekla Structures, ArchiCAD, Rhino, AutoCAD and SketchUp.\nAs the software industry moves to the cloud, Bentley continues to harmonise the data structures of its multitude of products within the DGN format. While DGN is still the most stable and consistent format in the industry, internally developed Bentley applications and acquired technology do not necessarily store data in the identical structures within DGN. This can become an issue as Bentley\u2019s product stack accelerates to enable seamless digital workflows between all its products on its common MicroStation platform.\nWith that in mind, the company has taken a fresh look at the iModel format, its capabilities and how it can homogenise its own internal data structures to deliver unparalleled digital workflows and apply multi-disciplinary analysis and machine learning to consistent project data.\nAt Bentley Systems\u2019 Year in Infrastructure event, held in Singapore in October, Keith Bentley, the company\u2019s CTO and chief software architect, explained in his keynote presentation the reasons behind this development and why he believes the benefits will make the new technology a game-changer for distributed project teams.\nDigital workflows\nDespite CAD, document management systems, the internet, the cloud and BIM, the industry hasn\u2019t really travelled very far in the last twenty years. We still buy large format printers and the industry has adopted PDF as a digital replication of the printing substitute. But just because a PDF is electronic it doesn\u2019t mean that it provides a much better alternative to older workflows. Much time is spent creating data-rich models and then flattening them out into a format that can be emailed. That just adds to a noise of files that muddies the waters when it comes to team-wide understanding.\nTo build a format fit for the cloud generation, Keith Bentley and his team have come up with a way to share data with seamless elegance.\nKeith Bentley first started his presentation by talking about what the cloud is \u2013 and what it is not. Many people think of the cloud as a large supercomputer with infinite resources, that can solve any problem instantly. While you might be able to buy a machine like this, the cloud isn\u2019t actually one big computer, but lots of smaller computers running multiple applications. In fact, most applications running\nin the cloud use similar resources to those written for a single personal computer. In the days before personal computers, there was typically one computer for many humans. When the personal computer arrived, the ratio changed to one to one. This was progress. However, in the cloud world, the ratio changes again, to many computers per user, and this is where the greatest opportunity lies. So, when Keith Bentley talks about cloud platforms and cloud services, there can be many programs and services running all at the same time and that can be of real value to users.\nHistorically, iModels were a way to collate large sets of project data into a single file or container. You could practically put anything you wanted into an iModel. While that has worked out pretty well for users, Bentley feels it can improve on that, and specifically, provide more convenience and closer collaboration than PDFs provide today.\nIn coming up with iModel 2.0, Keith Bentley explained that there were three issues in iModel 1.0 that needed to be resolved in order to leverage the cloud.\nIssue 1: dark data\nThe first is a concept that Bentley calls \u2018dark data\u2019. With many projects, there is a lot of this. It\u2019s information that is rarely used and only understood by the applications that created it, because intelligence and data are separated.\nAccording to Bentley, the real problem here is that data considered to be dark is stored in a format by the application that created it, developed by a programmer who was thinking only about one job in particular. The data needs to be saved out to disk, so that a user can read it back in the next session. Bentley equates this to that data being stored on a disk through a \u2018filter\u2019 that only the programmer knows or understands.\nAnother file on the same disk from a different programme will be written by a different programmer who stored their own data in a slightly different way. With different angles of filters running across a large project, you could have six or seven \u2013 or even a dozen \u2013 different applications. In other words, when you put all those filters together, that\u2019s why data is dark. It\u2019s a tangled mess.\nFor iModel 2.0, Keith Bentley says the company has undertaken the alignment of this dark data. Alignment involves lining up units, deciding whether distances are stored in metres or feet, whether temperatures are in Fahrenheit or Centigrade, and angles in radians or degrees. By aligning data, it becomes digestible and can be shared and used as an input for many other systems, such as machine learning.\nThis is not a pretty problem, Keith Bentley acknowledged, but it\u2019s real and the company knew it needed to realign its data before it could be repurposed for digital workflows.\nIssue 2: making changes\nThe second issue is another common problem that occurs when someone creates an iModel from a number of disparate files and another person updates one of those source files. This requires a re-run to make a new iModel. That\u2019s a lot of work, especially if the second iModel is 97.7 percent the same as the original. Keith Bentley describes this limitation as \u201cinsane\u201d. Change is the lifeblood of projects, and iModel needed to be more change-tolerant. The net result of this thinking has been to store changes, rather than the results of changes, as per the previous file-based methodology.\nChanges, of course, happen in sequence. A person changes one thing, a subsequent team member then changes something else. Bentley Systems has addressed this by coming up with a cloud service it calls iModelHubs. This is a single, centralised hub where changes to iModels are reliably and securely stored and time stamped. According to Keith Bentley, it\u2019s a bit like the bank statement you receive every month, with a start date and an end date, and the balances for those days. The list of transactions in between show how you got from opening to closing balance.\nSo, in effect, iModelHub is an accounting system for projects. You can see from the list what happened at any point in time in a project. You create an iModel 2.0 at the start and the iModelHub will synchronise it forwards or backwards to get to any point in time in the project\u2019s duration. Some points on the timeline will be milestones and these can be named. It\u2019ll give an indication, for example, of when approvals, reviews or submittals occurred. One important aspect of this is that you can\u2019t go back and revise history \u2013 so if you remember a point on the timeline in your iModelHub, you can always be guaranteed if you go back to that point, you\u2019ll get the exact same version of the iModel.\nIssue 3: accessibility\nThe third and final issue Bentley addressed is accessibility. If iModelHub stores only the changes to an iModel, where\u2019s the master iModel, you might wonder? The answer is that there is no master iModel. Instead, there are lots of copies of the iModel and they are all synchronised through iModelHub with changes. Each one of these iModels is as reliable as the next as they are derived from the same recipe of changes. The master copy, in fact, is the timeline itself.\nAs iModels are files, they can be stored on disk, but need to be synchronised to iModelHub to stay up-to-date. However, you don\u2019t need to connect to the iModelHub in order to perform any operation on an iModel. Should you have an iModel on your iPad, for example, it doesn\u2019t need connectivity to iModelHub to still be of value. It only needs connection to iModelHub to change its point on the timeline. When iModels are stored on the cloud, they can be connected to services such as tools for analysis, quantity surveying, or facilities\/asset management. These can be used by many individuals, for many different applications, simultaneously. This is exceptionally liberating for CAD data.\niModel bridges\nSo how can firms start using this technology? Bentley explained a concept that the company calls iModel Bridges. These iModel Bridges connect to ProjectWise, so ProjectWise customers just need to install an iModelBridge for every application or format that they want to map into their iModels. The bridge runs automatically when anybody checks a change into ProjectWise and it marks the change in the iModelHub, too, which in turn updates the iModel 2.0 files. Users have no idea this is happening, as it\u2019s all done behind the scenes. Bentley explained that the development team has been working on iModel 2.0 for a long time, but now believes it will change Bentley\u2019s business and help customers change their own businesses, too, by adopting and benefiting from digital workflows. By embracing change, change becomes a first-class citizen in project ecosystems.\nWith a change-based system, it\u2019s also easier to keep track of changes on a project from data edited across multiple applications. Notifications can be sent when something important changes, allocation of changes and responsibility for changes become more identifiable. In some ways, and to carry on the banking analogy, the system can alert you when changes are made, similar to when the bank notifies you that your credit card has been used in a different country.\niModels also feature a timeline slider, so you can see what changed in a model through its lifecycle \u2013 red denotes deleted, green denotes additions and blue denotes modifications. Users can also spot areas where there have been many changes, identifying design problems via \u2018a heat map\u2019 that provides an indication of issues that may delay a project or that have proved contentious. The system is also not limited to geometry, but highlights property changes too.\nBentley hopes that machine learning will be used to analyse project changes, so that successes and failures can be identified and carried forwards into subsequent projects, as a lesson to designers based on their past errors.\nWith iModel 2.0, iModelHub and iModel Bridges, Bentley has come up with a unique solution to project management using the cloud. Keith Bentley was asked about what other firms were doing to solve this, and said that other solutions focus on a centralised database with other applications reading and writing directly to it. This approach, he said, addresses the issue with alignment, because you are not really able to use any other data, but doesn\u2019t embrace change at all, and can become a bottleneck with little scalability. And, at the end of a project, a massive database gets handed over, which the customer has to adopt. With iModel 2.0, the client simply takes over the timeline for the lifecycle of the building.\nConclusion\nWhile on the face of it, iModel 2.0 may sound complicated, with its hubs and its bridges, it is underpinned by some very elegant thinking when it comes to cracking the problems that BIM designers and engineers face with today\u2019s project workflows. It collates data from multiple packages from different vendors, keeps data from going dark, enhances portability and accessibility, tracks changes, helps identify problem areas in design, 2D sections and elevations, drives collaboration and frees CAD data for others to use.\nThe notion of a single massive centralised database on the cloud may have seemed like an inevitability, but Bentley has taken a more revolutionary view, by boiling down what is important and how this can be driven using the cloud, in what is almost a blockchain approach. The company\u2019s decision to focus on change and to distribute rich content in a managed and secure environment has great potential to also overcome some of the industry\u2019s most painful BIM collaboration problems.\nSimilarly, PDFs may be the industry standard format, but they just replicate old processes and offer little more than dull digital prints. With version 2.0, iModels come alive, to be fed by traditional project management tools. They can be taken offline and re-synchronised later, while still offering a deep insight not only on the current state of a project but also into the \u2018who, why and what\u2019 of changes. Milestones are stored in the file, which means no more transfer of files and no more mushrooming file management issues across project teams.\nI\u2019m very excited about how iModels now have a life of their own, with access to applications, analysis and intelligence through connection to cloud services. They could act as an interface to many asset-based applications, perhaps even real-time systems. It will be interesting to see what new functionality comes online.\nSo how might you start to use them? Bentley is not quite there yet, but iModel Bridges will be available in the first quarter of next year and the company is looking to find beta sites to help it decide on final functionality.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/autodesk-plans-sdc-in-india\/","title":"Autodesk plans SDC in India","date":1061942400000,"text":"Autodesk Corporation is planning to develop software in India. \u201cWe are strongly considering developing software in India and it is all in a planning stage. Nothing has been finalised yet. But it is soon to happen,\u201d said Andre Pravaz, Regional Director, South Asia Pacific, Autodesk.\nAt present the company has no software development team in India and has about 10 software engineers based in the Asia Pacific office in Singapore. The company presently has 100 employees in the country spread across jobs such as sales, marketing, support, reseller support, etc. The software development is meant to start in a small scale before scaling up. However, numbers were not disclosed.\n\u201cWe are expanding the strength of our staff in India. There is no specific numbers, but business is certainly growing from the region,\u201d added Pravaz.\nThe California headquartered company, entered India is 1987 and was represented by partners in the country. After inaugurating a liaison office in 1996, Autodesk opened a subsidiary of its South Asia Pacific operations in India during 2002.\nAutodesk has also revamped its channel strategy from a three -tier layer to a more efficient two -tier layer. \u201cPayment was always a problem in India. Earlier, there were three value -added resellers for the company in Neil Automation, Hope Technologies and DRS Computers. We had a huge amount of outstanding payments with them. Now the whole equation has changed and we have appointed distribution major Tech Pacific first as a fulfillment partner and then now, as an exclusive distributor. This has helped us a lot as the payment issue is taken care of, products stocking, customs clearance and most importantly, geographical reach,\u201d commented Pravaz.\nSource: Siliconindia News Bureau","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/intelligent-design\/","title":"Intelligent design","date":1506297600000,"text":"The construction industry may be behind on adopting artificial intelligence and machine learning, but their transformational potential in our sector should not be underestimated, says Adam Ward from Space Group and BIM Technologies\nIt comes as no surprise that the high tech, telecoms and financial services sectors are the leading early adopters of machine learning and artificial intelligence. After all, these industries are well-known for their willingness to invest in and integrate new technologies, in order to gain competitive advantage and achieve internal process efficiencies.\nBut it\u2019s painfully noticeable that the construction industry is missing from the list. Why is it taking so long for construction to catch up with advanced technology? As an industry, we amass huge amounts of data \u2013 but it\u2019s only now that we are finally starting to understand what we might do with it and where it might take us.\nTake, for example, machine learning: this technology might allow us to generate progressive and independent thinking within the machines and computers that drive our industry. In other words, machines are learning to think for themselves, but it\u2019s only by feeding them vast quantities of data that we can enable them to learn. From there, computers might analyse that data, to predict patterns and behaviours within occupied buildings and help designers to make more informed choices in the design and prototype phases of a project.\nMachine learning primer\nMachine learning is a branch of the wider theory and development of computer systems known as artificial intelligence, or AI. It has many potential applications in construction for the design and operational stages of a project, but it is perhaps building operations that might be our best target. In effect, it\u2019s about giving a building a brain.\nMachines are very good at consuming and analysing large amounts of seemingly unrelated data and finding patterns in the chaos. Once sufficient quantities of data have been collected and cleaned, they can then be used for training the machine learning algorithms that enable building systems to react to detected patterns and make informed decisions. These huge volumes can also drive a core set of algorithms that use neural networks with many hidden layers \u2013 sometimes referred to as deep neural networks or DNNs \u2013 to assist in learning, classification and prediction.\nLearning curves\nTo put this issue in a more everyday context, we might look at child development. When a child is born, it doesn\u2019t know how to walk, talk, or feed itself. These are traits that are learnt, usually through repetitive patterns that the human brain processes and interprets as common functions.\nAdam Ward is a digital construction professional with close to 20 years\u2019 experience. As Technology Director at Space Group, he is responsible for developing the processes and technologies that Space Group uses to solve complex problems in the construction industry.\nMachine learning operates in a very similar way to the neurological processes of the human brain. By looking at patterns within data, it can make informed decisions on what will most likely happen next and adjust accordingly.\nWhen a child eats sugar, an electrical signal is fired to the brain that says \u2018I like this\u2019. Over time, when a child eats more sugar, this emotion is triggered again, and over time, a neurological connection becomes burnt into the brain. The brain has essentially been \u2018trained\u2019.\nThis is very similar, broadly speaking, to how machine learning and neural networks work. The main difference is that a machine gathers information from large existing data sets much quicker than a human can.\nAnother similar example: If a computer programme is given thousands of images of frogs, and it is told thousands of times \u2018This is a frog\u2019, it will learn to recognise other frog pictures or objects that look like a frog. This is why companies such as Google, that have already collected vast amounts of data, are leading the way in machine learning.\nIn the same way, if a computer programme sees thousands of architects selecting a particular type of door handle \u2013 for use on a particular door type, in a particular building type, in a particular country \u2013 it can use this knowledge to make future recommendations to architects automatically about which door handle they might select. It will also know why it\u2019s making that recommendation, because its selection is based on the choices of previous architects.\nThis is why many software companies are transitioning their platforms to the cloud, so that they can start to combine information from many different data sources, and start to connect the dots.\nSmart buildings need smart data\nThe development of smart buildings is on the rise, due to the availability of the cheap sensors and connected devices that make up the Internet of Things (IoT). These are becoming commonplace in our homes, with many everyday gadgets and appliances now available as smart products. Likewise, many buildings already have sensors that measure light, heat, energy consumption, footfall, movement, capacity and more. Many are part of proprietary, closed systems that don\u2019t talk to each other, but this is changing as the systems become increasingly able to share data in open standard formats or via application programming interfaces (APIs).\nOnce data is connected, we can layer on top those machine learning algorithms that allow us to predict the behaviours of a building and its occupants, not only for efficiencies in operating and maintenance, but also to inform the way we design future buildings to optimise space and flow and select more appropriate building materials.\nEqually, many clients are now asking to know when an asset is likely to break down so it can be fixed or replaced ahead of time. Once we have enough data, we can use a linear regression algorithm on the data to predict asset failure. Machine learning is also used in more everyday tasks, such as providing more relevant search results, by using previous user search trends to influence results. This technique has been used for years by companies such as Facebook or Netflix, which use machine learning to push news stories and recommendations to your feed.\nA similar approach is now seen with BIM object provider bimstore, to provide manufacturers with insights into not only what buyers have selected, but what factors may have influenced their decision.\nEarly days\nThe computers in a modern car or aeroplane collect and analyse vast amounts of data every second. Companies like Tesla use this technology to analyse our driving patterns, diagnose problems before they happen and make recommendations on when to change the tyres \u2013 and which tyres to select \u2013 based on predicted driving patterns, distance travelled and road surfaces. That\u2019s just the tip of the iceberg for machine learning capabilities. Construction is still way behind.\nWith many buildings now been designed, delivered and operated using digital processes and technologies such as BIM, machine learning is also starting to appear in many of the tools that architects and engineers use daily. Companies such as Autodesk are investing heavily and their technology previews such as Project Dreamcatcher and Project Fractal hint at a future where machines are able to \u2018design\u2019.\nFor the past few years, there has been a strong message that in the future we will not tell the computer \u2018what to do\u2019, as we do now. Instead, we will tell it \u2018what we want to achieve\u2019 and leave it to find the optimal solution and design.\nAutodesk is also looking at how machine learning might make construction sites a safer place. It has recently acquired Smartvid.io, a company that has developed intelligent video and photo-tagging software to enable cameras to highlight potential hazards and issues on a construction site.\nRapid interest in, and development around, machine learning has been created by a perfect storm of emerging technologies across all industries. Data is becoming more accessible through cheap IoT devices, open standards and public APIs. These allow devices and services to talk to each other. This is then combined with ease of access to cloud storage and infinite computing, where we can collect, store and analyse vast amounts of data. And the more data we can process in this way, the more accurate machine learning will become.\nThe new oil\nThe insight we can derive from data, and the patterns it reveals in our actions, selections, behaviours and choices, combine to bestow great power on those companies that can own, control and interpret it. Data has become \u2018the new oil\u2019 that drives growth for organisations such as Facebook or Google that are competing for your personal information.\nWith this great power comes high risk, which is why the new rules known as General Data Protection Regulation, or GDPR, are being introduced. Companies must also adjust to new threats. In digital construction, we must contend with specific standards such as PAS 1192-5:2015.\nNew power, new opportunities, new risks and new threats: they are all changing the way our industry operates. In others, they are already combining to become the \u2018new normal\u2019.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/autodesk-updates-autocad-based-solutions-for-architects-civil-and-mep-engineers\/","title":"Autodesk updates AutoCAD-based solutions for architects, civil and MEP engineers","date":1171411200000,"text":"San Francisco, USA, 13 February 2007 \u2013 At its World Press Day event, Autodesk, Inc. unveiled updates to its comprehensive portfolio of discipline-specific software solutions based on the AutoCAD software platform for architecture, engineering and construction (AEC).\nAutoCAD Architecture 2008 (formerly Autodesk Architectural Desktop), AutoCAD MEP (formerly Autodesk Building Systems) and AutoCAD Civil 3D software applications incorporate new features and functionality to help architects, mechanical, electrical and plumbing (MEP) and civil engineers improve productivity and increase efficiency. Enhancements in these AutoCAD-based applications also pave the way for architects, designers and engineers to streamline fundamental tasks and redefine traditional design process. The latest application updates are built on the AutoCAD 2008 platform.\n-: AutoCAD Architecture 2008\nAutoCAD Architecture provides the best AutoCAD-based design and documentation productivity for architects. AutoCAD Architecture 2008 makes it easier for users familiar with AutoCAD to automate tedious drafting tasks so design documentation can be completed more easily and more quickly.\nNew features and functionality in AutoCAD Architecture 2008 include:\n\u2013 Automatic scaling of drawing production allows users to simply change the scale of the design, and the annotations including dimensions, tags and leaders are automatically updated.\n\u2013 Changing building elements and components display (such as a door and its swing) is now as easy as modifying AutoCAD linework.\n\u2013 New Drawing Compare feature, available through the Autodesk Subscription program, uses color-coded displays to show items on a drawing that have been changed, added or deleted by other members of the design team. Changes to items such as styles, fire ratings or other non-graphical properties also can be tracked.\n-: AutoCAD MEP 2008\nAutoCAD MEP 2008 is AutoCAD-based software for mechanical, electrical and plumbing (MEP) engineers, designers and drafters. AutoCAD MEP 2008 software brings efficiency to AutoCAD-based workflows for greater productivity and accuracy, which also helps minimize coordination errors between architecture and engineering teams.\nNew features in AutoCAD MEP 2008 include:\n\u2013 Automation of MEP systems design, layout and documentation, including single line plumbing, electrical layout and piping.\n\u2013 Part wizard to speed creation of new parts with predefined parametric templates.\n\u2013 International metric content, for documentation in global projects using international metric measurements.\n\u2013 Improved display control for construction documentation and display.\n-: AutoCAD Civil 3D\nAutodesk provides civil engineers, designers, surveyors and drafters with a comprehensive AutoCAD-based package that uses a dynamic model to link design and production drafting for site development, road design and more. This capability enables changes to update across the project, so that all team members work from the same consistent, up-to-date design. AutoCAD Civil 3D 2008 is helping civil engineers redefine design by using proven technology to eliminate much of the tedious step-by-step development of plans. At the same time, Civil 3D helps speed execution of design changes and facilitates evaluation of multiple design scenarios.\nKey features in AutoCAD Civil 3D 2008 include:\n\u2013 Enhanced multiuser environment for work on larger, more complex projects.\n\u2013 Interactive design capabilities and feature automation for greater productivity, such as automatic generation and updates to roadway plan and profile sheets.\n\u2013 Increased survey functionality to create base geometry faster and move data from and to the field.\n\u2013 Interoperability and migration for multiple data formats, including Google Earth mapping functionality and the DWF file specification.\n\u2014 Availability\nProduct availability will vary by country. Details on purchasing options will be available in the spring at: https:\/\/www.autodesk.com\/purchaseoptions.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/look-around-you-closely\/","title":"Look around you (closely!)","date":1165449600000,"text":"James Cutler, CEO at eMapSite explores how the explosion in the ability to capture, store and mine all kinds of data digitally allows CAD and GIS practitioners to play an ever more valuable role.\nThis article, about pervasive data networks and what they mean to GIS and CAD practitioners in the context of the business they work in, has been handily interrupted late in its preparation by the Information Commissioner\u00dds \u00fdReport on the Surveillance Society\u00af1 . In it much is made of the ubiquity of the active and passive data capture and utilisation technologies that impact every aspect of everyone\u00dds life. This article explores how the explosion in the ability capture, store and mine all kinds of data digitally allows CAD and GIS practitioners to play an ever more valuable role.\nSo, what do we mean \u2013 what is being \u00fdcaptured\u00af?\nAnd what does it mean? Perhaps it is this that is best illustrated initially:\n- we grumble (as the British are wont to do) about CCTV (4.2m cameras), speed cameras (6,000), junk mail (billions).\n- we wonder (or perhaps we don\u00ddt) at personally targeted advertising on iTunes or Amazon or in our mail as a consequence of our loyalty card membership (an incredible 50% of the population are in the Nectar card private data network).\n- we are glad to create customised RSS feeds to sift out the daily detritus of our online worlds.\n- we are grateful that number plate recognition allows for pay as you drive in Central London.\n- we reluctantly undergo biometric, psychometric and psychological testing in or for the workplace.\n- we marvel at the supply chain that keeps the shelves of our shops stocked with just what we like (and our roads replete with just in time delivery vehicles of every shape and size).\n- we watch awestruck as the Thames Barrier glides into operation as a consequence of river, rain, lunar and tidal monitoring.\n- geofencing of goods and people makes us safe.\n- we shiver at the thought that 1\/6th of the former East German population were informants on their fellow citizens.\n- we pay more for houses in reportedly low crime, high educational attainment areas.\n- we wonder who carries out chemical or radiation sensing (and how).\n- intelligent housing \/ domestic robotics (domotics anyone?) \u00b1 automated energy conservation, sound and lighting control, security management, garden maintenance (no smirking please).\n- we rarely remember that Google reserves the right to store all your web activity to search it at a later date.\n- we relax knowing that an elderly relative\u00dds drug regimen will be administered exactly according to prescription (medical sensing).\n- we worry that we might be wrongly delayed, detained, arrested or otherwise inconvenienced online, at the checkout, at border controls or on the road due to some clerical error or historic folly (and trust that the right ones are apprehended and that we won\u00ddt become a credit risk).\n- we read that despite the investment in CCTV, many types of particularly public nuisance type crime have barely diminished.\n- we are delighted to gloat to colleagues about the latest real time traffic service that got us home without running into any jams at all on a Friday evening.\nWhere\u00dds this taking us, why\u00dds it relevant to us, what\u00dds involved?\nApart from recommending the report to you, it is very relevant to us as the cadre of professionals whose task it is to design and build Britain\u00dds infrastructure. Any activity in any sector (transport, communications, health, education, residential and commercial development, utilities, leisure and entertainment et al) has for in some cases many years either required or included or both the use or integration of technologies that allow them to function. The technologies themselves are beyond our scope other that to list some of them by broad technological area for your further research should you be interested (but some are of course already integral to our activities):\n- tagging and tracking technologies (GPS, RFID, ANPR , transponders, mobile phone triangulation, SiRF location enhancement)\n- audiovisual recording (CCTV, webcam, traffic cam, camera phone etc)\n- telecommunications (from cell phones to Bluetooth, Zigbee and much more)\n- digital computer technologies (databases especially)\nMuch of this should be no surprise to us at all as professionals \u00b1 we do after all depend on data, and the tools that translate that data into information that supports the decision making process. We are glad of that data and of the people (statisticians, analysts, actuaries) and technologies that collect and crunch the raw material into useful nuggets.\nSo, we can agree, these technologies are necessary, desirable even \u00b1 improving entitlement and access to public and private services, enhancing healthcare, fighting the bogeymen (terrorism and serious crime) \u2013 and certainly useful in context. As professionals we have more data at our fingertips as we seek to design and build infrastructure that will directly influence the lives of those using it. Surveillance is not a conspiracy theory made real and is unlikely to diminish; the technologies with us now and due to arrive over the next decade are large scale, woven into the fabric of our lives, almost or actually invisible and we need to understand and use them intelligently.\nWhere we as professionals must be vigilant is in our participation in the design of the data capture systems and of the systems that store and analyse this information and especially in how we then pick up the results of those analyses in the development of solutions to them. OS MasterMap affords a geographical framework into which we can connect this data, over which we can display our analyses and with which we can create new (local) worlds. There is of course an extensive series of identifiers in the geospatial world:\n- OS MasterMap feature, address or transport network TOID (Ordnance Survey)\n- Postcode Address File now including Multi-Residences Without Postal Address (Royal Mail)\n- Unique Property Reference Number (UPRN) from the National Land and Property Gazetteer (NLPG) \u00b1 see below\n- Unique Address Reference Number (UARN) \u00b1 from the Valuation Office Agency\n- Land Title reference\n- Administrative units \u2013 Enumeration District, Ward, Parish\n- Sampling Units \u2013 Census Output Areas\n- Media boundaries (phone, internet, cable, TV, news, radio \u2013 used to determine what you receive in terms of advertising, media content and market research)\n- Postcode sector (underpins most expenditure, lifestyle and neighbourhood classification and segmentation tools for profiling, catchment area and site location analysis including ACORN, CAMEO and PERSONICX GEO)\n- Geographic position \u2013 latitude and longitude, OSGB grid coordinates\nThis list excludes all reference to the other systems that might identify us but does serve to demonstrate both the diverse operational context for planning and decision making and the basis for the news-friendly sound-bite of the \u00fdpostcode lottery\u00af.\nIt also illustrates that there are multiple and in some cases competing (owing to commercial interests), conflicting or even complementary mechanisms by which we (and those that watch over us) can reference us in the landscape.\nAs local authorities retain significant powers in the planning and development framework, it is worth looking a little closer at the tools they use to help govern decisions that involve our industry extensively.\nThe National Land and Property Gazetteer (NLPG)\nThe NLPG covers the whole of England and Wales and contains more than 30 million residential, business and non-mailing addresses. It is a comprehensive and continually updated database, created by each local authority, the body with legal responsibility for street naming and numbering of property. The NLPG uses the BS7666 standard (Parts 1 and 2) to ensure data format consistency across the country. Each record includes a unique property reference number (UPRN) which provides a reference key to join related address records across different datasets.\nLocal authorities are the creators of address data (obtained through their activities such as planning applications, building and environmental control, licensing, the electoral roll, council tax and non-domestic rates) and manage the address throughout the \u00d9property lifecycle\u00dd. The NLPG includes separate units of properties in multiple occupation (like flats, retail units or office blocks), land, buildings within complexes (such as hospitals, schools or universities) and other structures (such as churches, scout huts and war memorials) and the history of all changes to those properties. There are currently more than 4 million such objects in what is as yet a far from complete NLPG.\nBeyond the Local Authority\nMany local authorities are under considerable pressures for planning and development \u00b1 from limited resources, from central government, from lobbyists, from e-government targets and more. Plentiful information and research, well presented in support of a particular proposal or policy direction might well be considered helpful. Inevitably perhaps the ability to harness and wield that information can skew the development process. For example, automated systems of classification or segmentation \u00b1 \u00fdprofiling\u00af \u00b1 can result in or support a particular view or preference.\nProfiling is dependent upon the collection, aggregation and mining of vast databases of individual data, often held by private enterprise and its analysis can be determined by the user. The results may meet a narrow even opportunistic agenda but they may also blur the bigger picture, particularly in an increasingly risk-averse culture where social stratification provides its own answers (as Disraeli would have it, \u00fdlies, damned lies and statistics\u00af).\nSo instead of visionary planning of for example traffic free city centres or focused health and education interventions the case may be made for more CCTV or failing public services \u00b1 it is all a matter of perspective.\nWe are smarter than that and we should seek to ensure that we are part of the system. \u00fdEverything happens somewhere\u00af, and it is the inevitable outcome of all these real time data collection systems that they will be viewed in their geographic context (be it demographics, psychographics or behaviour). This is ideal when extending or removing flood control infrastructure certainly, but is ideal also in identifying the nature of new health or education interventions (programmes, plans, projects), and ideal too in ensuring that holistic approaches are taken to planning residential and commercial development.\nPart of the irony of these hugely rich information streams is that they do allow for the creation of an incredibly well planned society. As pivotal participants in both the upward assimilation of data and in the downward translation of the resulting decisions into the landscape we must not be passive, but constructive, contextual and visionary.\nThere are many \u00fdIT\u00af words \u2013 ambient, immersive, pervasive \u00b1 for the digital surveilled world around us. We do not need to be distracted by these but rather ensure that our tools (for data collection, aggregation, assimilation, management, analysis and critically, presentation) enable us to interpret the consequences of these systems in order to ensure that we construct a world that we are proud of (think of positives in the initial list).\nThis article was written by James Cutler, CEO at eMapSite, a platinum partner of Ordnance Survey and online mapping service to professional users","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/graphisoft-s-new-ceo\/","title":"Graphisoft\u2019s new CEO","date":1558310400000,"text":"With the London launch of ArchiCAD 23, AEC Magazine had the chance to catch up with Graphisoft\u2019s new CEO of two months, Huw Roberts, to capture his thoughts on the company and the BIM market\nGraphisoft was originally founded in 1982 by Hungarian software developer G\u00e1bor Boj\u00e1r. Boj\u00e1r was the head of a mathematical department in the Geophysical Institute of Budapest, where he developed terrain modelling software on microcomputers. Apple\u2019s Steve Jobs saw the early developments in 2D\/3D architectural CAD that Boj\u00e1r had made. He was so impressed that he shipped him an Apple computer, which had to be smuggled into the country, as it was behind the Iron Curtain at the time. This development became ArchiCAD, the first desktop BIM system.\nSince then Graphisoft was sold to Nemetschek, a holding company for many brands, such as Solibri and Maxon, and has gone on to be a key global player in the architectural movement from 2D drafting to 3D modelling. Now in its 23rd release, ArchiCAD runs on both Mac and PC and competes head on with Autodesk Revit.\nHuw Roberts recently took over the reins of the company from Viktor V\u00e1rkonyi, who has since moved up to become chief division officer at the Nemetschek Group. In his ten-year tenure as CEO, V\u00e1rkonyi tripled the company\u2019s revenues and took it into new regions, primarily in Asia and South America.\nAEC Magazine has known industry veteran Huw Roberts for almost 20 years, as previously he was global VP of marketing for Bentley Systems\u2019 portfolio of AEC products, from TriForma to MicroStation and Projectwise, and was responsible for forming strong partnerships in the industry. Roberts is a trained architect and still proudly keeps his licence to practice. He is a former chairman of the Technology in Practice committee at the AIA and prior to joining Graphisoft last year (initially as VP of Americas) he was Chief Marketing Officer at BlueCielo, a European CDE developer. He brings a lot of American marketing experience to the role.\nAEC Magazine: What are your plans for Graphisoft?\nHuw Roberts: I started with Graphisoft in September (2018) and was initially focussed on learning the company, learning our customer base and seeing how we position ourselves in the market. Two months ago I took over as CEO, so I\u2019m still early in my observations, but we are going to keep our approach to business by continuing to focus on the core of what we\u2019re great at and that\u2019s delivering great software for architects and designers, expanding the capabilities of ArchiCAD and our commitment to OpenBIM.\nThe company is in a really good place; not just Graphisoft and ArchiCAD, but as a key component in the Nemetschek portfolio of companies. We have a broad range of technologies for the desktop, mobile and cloud, adding value across the AEC market. Looking at where we\u2019re going to grow, we have to anticipate where the market is going and what will benefit our industry next and be ahead of that change.\nAEC: What got you into the BIM world?\nHR: Before I worked in the architectural software world, I was an architect and I am still an architect. I ran a studio in one of the largest US firms based in Philadelphia. I used to design hospitals, sports facilities, large and complex projects \u2013 and I pushed the available technology to its limits. My interest in technology grew and I got involved with all the professional organisations in my space, especially the AIA, in trying to work out best practices for how we should collaborate and share data between disciplines. I\u2019m a strong believer in community and open standards. While at Bentley Systems, I continued this with international organisations like RIBA, Construction and Owner organisations, the Smart Geometry group and buildingSMART.\nAEC: What are the key barriers to successful implementation of BIM?\nHR: Being a global company we can see how there are different levels of success with and adoption of technology in all geographies, and we have the opportunity to see those different pieces and start connecting the dots. I like the William Gibson quote that \u2018the future is already here, it\u2019s just not evenly distributed\u2019, and I think that\u2019s certainly the case in our industry. There are fantastic things happening, but they\u2019re spread out. Our job is to see the larger landscape and share that knowledge with our customers through our relationships and see how we can fold that back into the software to bring benefits to all our customers around the world. So in effect a lack of awareness or education is the main issue.\nAEC: Graphisoft scores highly on customer satisfaction. What\u2019s the secret?\nHR: I think it\u2019s central to Graphisoft\u2019s culture to be customer focused. We\u2019re not going to succeed by playing business games with our customers or trying to lock people into software. We\u2019re not trying to manipulate the market with a business model. Our central focus is to understand what we can do to help our customers succeed. It\u2019s actually fundamentally really refreshing to me, to see how dedicated the company is to that approach, across the team in Graphisoft HQ and around the world. I think that\u2019s kind of a driving principle for us to just add value as our approach to opportunities in the market.\nAEC: What changes will you make to the company?\nHR: Graphisoft, as a company is strong, we are having good business success, we have good financial performance, good customer satisfaction, good growth -but perhaps we are not particularly publicly bold or proud of that. We don\u2019t market strongly; we could better leverage that strength. I\u2019ve been in this industry for 30 years and when I look at the numbers and I look at the status of ArchiCAD in the architectural BIM market as number one or number two in most of the major markets in the world and, in many of those number two spots, we are on a trajectory to take over number one.\nI think it\u2019s our job to actually leverage and build on that strength, and to promote that strength to validate our customer\u2019s choice. We should do a better job of broadcasting and being proud about that, so we\u2019re going to start cranking things up to 11 to go Spinal Tap on you!\nIn addition, while there\u2019s a lot of really successful implementations of ArchiCAD in the professions, we need to see how we can accelerate this out to other users, and here maybe, the most powerful force is not actually the technology it\u2019s the human brain, helping people learn from each other, understanding how to apply these technologies and methodologies. So we\u2019re looking at aligning the learning offerings we could provide customers to where they are in the process \u2013 and delivering them on demand, in person, online in a large format, short format, just in time, all to help customers make the most from applying the technology\nThe other thing that I think is important to the industry (and I\u2019m proud of the Graphisoft attitude) is support of OpenBIM workflows. The Nemetschek group at a recent buildingSMART international conference, held in Dusseldorf in March, helped drive through a set of principles an extension of the OpenBIM approach and philosophy to all the participating software companies that were there. This wasn\u2019t just about software or APIs, but about the behaviour of the software companies. We all know that a software company could say \u201cYes we support IFC, it only takes 40 minutes to open that file in our software\u201d That\u2019s not real, that\u2019s fake. That\u2019s a checkbox on a marketing brochure, to pretend that you are supporting Open BIM.\nSoftware firms have to actually commit to it and make it practical and promote it as the preferred method of sharing, rather than something a software company feels it has to do. We propose that members of the buildingSMART organisation internationally commit to that approach. Forget thinking of IFC as a just a file format, start thinking of intelligent workflows for sharing information amongst software tools, technologies such as smart devices, digital twin buildings, AI, digital manufacturing. There\u2019s all sorts of things beyond BIM models that need the same open mindset across the industry if it\u2019s going to advance properly.\nAEC: Any particular areas where BIM workflows needs to do better?\nHR: As you move information through a building\u2019s lifecycle there are these discontinuity points where information is \u2018thrown over the wall\u2019 to the next phase or the next participant in the design. There are all sorts of reasons for this; legal, process, and technical reasons through all the phases as information moves between subcontractors, designers, engineers, it goes out to bid and tender, comes back again, then it drops down to the general contractor and the construction team, and finally gets to the owner for operations. It\u2019s a very well understood and inefficient challenge across our industry.\nI think that the goal is to try to enable as integrated teamwork as possible. Achieving full workflow transparency and free flow of information at these points in the process and to solve these challenges will require a combination of technologies to find a solution in conjunction to changes in the legal and contract\u2018ing systems, especially concerning liability. Most of the thinking in this regard has been done at a document level bid documents, design documents, engineering calculations, building codes, whatever\u2026 but these big transaction dumps are a big kind of block to information flow.\nOur vision is to not only think about the problem at that level, to not think of these big monolithic transactions as the primary method of getting asset information through that lifecycle. We think it\u2019s not just at a project level, or even at an element level. Our approach is to make the information exchanges at the attribute level and to give customers dynamic control in a practical and effective way.\nAEC: There have been changes within Nemetschek with a more coordinated front at exhibitions. What\u2019s driving this?\nHR: To date, the Nemetschek Group has kind of been a holding company for an array of different businesses that operate on their own and we are still discreet and separate businesses. However, simultaneously to my taking over as CEO at Graphisoft, we\u2019ve done a reorganisation of the Nemetschek Group which better integrates the brand CEOs for better collaboration. It will allow us to figure out our synergies between the brands with a focus on our shared customers. We can better discover areas that are not internally competitive where we can add value and share information, we can share our roadmaps and some of our \u2018secrets\u2019 a little bit more than we can with the outside world. We\u2019re going to take advantage of this in a big way. Of course, we also want to continue expanding our integration with companies that are not in the Nemetschek Group as well.\nAEC: What will be your approach to development?\nHR: On the first day I was in Budapest, G\u00e1bor Boj\u00e1r, the company founder, gave me a copy of his book about the history of Graphisoft and very proudly highlighted that Graphisoft invented BIM and so we are founded on innovation. But 1984 is a long time ago and we are still innovating at a healthy pace. Great things are coming! I want to look at how we can accelerate that and how we can make sure innovation meets and anticipates customers\u2019 needs.\nWe\u2019re going to be tackling this on three fronts. First, within the product, where we have a great team understanding the feedback wishes from our customers and the market, and we will continue advancing the product as you are familiar with. Second, we have an exciting set of new technologies which we will be presenting at our Key Customer Conference in Vegas in June. And thirdly, we are going to do a lot more with our Labs initiatives, looking at all sorts of interesting areas like IoT, Artificial Intelligence, and more where there are some amazing potentials. It\u2019s all about us learning how we can best apply these disruptive opportunities in useful ways to add value to what customers are trying to achieve. So, our job is to continually add value to what we offer our customers so that they can be successful in producing great buildings and competing more effectively. We do that by continually learning about new technologies, new industry trends, new challenges and opportunities architects face, and delivering innovations that really add value to our customers. I\u2019m very excited for the future we will be building together.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/civil-engineering\/4d-in-the-driving-seat-bim-academy-4dmodelling\/","title":"4D: in the driving seat","date":1599955200000,"text":"BIM Academy\u2019s Peter Barker explains how 4D modelling helped deliver the complex A19\/A1058 Coast Road junction upgrade project and explores future trends we can expect to see in highways design\nThe A19\/A1058 Coast Road junc tion is one of the busiest road intersections in the North East of England. In 2016, work start- \u2013 ed on a major upgrade to combat congestion and improve consistency in journey time. The \u00a375m improvement project was a part of the UK Government\u2019s \u00a315 billion roads investment programme, which aimed to foster economic growth in the North East as part of the Northern Powerhouse initiative.\nThe challenging project required the installation of a three-level flyover \/ underpass and roundabout. To help plan construction and support the critical need to maintain traffic flow throughout, while minimising disruption, it was one of the first major highways infrastructure projects in the UK to utilise 4D BIM.\nConstruction in a live environment\nIn 2017, BIM Academy was approached by the JV supporting Highways England to work on the project. This was initially as a digital construction expert to assist in achieving compliance with the UK Government\u2019s mandate for BIM Level 2, but the role quickly expanded to focus on delivering greater practical value throughout the two-year project and helped to bring the project in 31 weeks ahead of schedule.\nDesign team WSP and construction JV partners Sisk\/Lagan worked with BIM Academy on this incredibly complex project. The size and scale of the upgrade were of course a factor, but the real challenge lay in the fact that the junction is a major regional component of the Strategic Roads Network (SRN) \u2013 and had to remain active throughout the course of construction. Working in a busy, and potentially hazardous live environment involved heavy restrictions and constraints on the construction process, posing significant challenges to the timescale and budget of the project \u2013 and perhaps most importantly, the safety and logistics considerations of improving road user experience with minimal disruption to transport routes.\nAn innovative solution was required to enable the project team to meet the safety requirements of a live environment, while still delivering a high-quality end result. As well as tackling appropriate standards and protocols, providing guidance to design and construction teams, BIM Academy brought forward a proposal to use 4D technologies on the project.\nIntroducing 4D\nThe idea was to link programme planning tools to the existing 3D digital design and terrain models, in order to more clearly visualise and communicate the construction sequence and potential barriers. Reviewing and assessing the potential construction sequences in a virtual environment became one of the project\u2019s standard design steps before anything reached the live environment on site. Using programmes such as Revit, AutoCAD Civil 3D and Synchro, the team was able to visualise and analyse how construction sequencing would affect the live environment at any given time, by creating a visual dynamic sequence from the 3D model and planning software.\nNot only did this approach aid in the planning coordination and buildability, it also significantly improved design of temporary works, which reduced the risk and negative impact of the project as well as streamlining and optimising the end solution, which incorporated smart traffic monitoring systems (STMS) and automated signalling, in a nod to future highways technologies.\nThe end result of the approach was that the project team was able to visually communicate and plan activities in the context of time and space, taking account of resources, procurement and spatial constraints. This created the thinking space \u2013 and provided the data \u2013 to assess alternative approaches to site layout, scheduling and logistics during the construction phase, providing a number of alternative options for contractors to assess all risks and opportunities, and make fully informed decisions around construction costs and sequencing.\nThis innovative approach to \u2018optioneering\u2019, as well as access to new and better data, was influential in the logistical improvement of the A19\/A1058 Coastal Road project \u2013 so much so that 14 months were cut from the original estimated 42 month construction programme, saving over a year of road disruption and a huge amount of cost.\nThe future of highways design\nThis of course raises the question \u2013 should this approach be applied to all major highways improvements moving forward?\nThis optioneering process allowed the contractor to assess all risks and opportunities. The future of highway design is certainly focused on leveraging digital tools to assess and optimise alternative solutions whilst factoring in legislative design standards.\nThere are also other future trends we can expect to see in highways design. Systems such as Motorway Incident Detection and Automation Signaling (MIDAS) are being developed and implemented by Highways England right now, as a distributed network of traffic sensors.\nRadar technology and magneto-resistive wireless sensors are also being trialed and are designed to alert local regional control centres (RCC) of traffic flow and average speed, as well as automating variable message signaling and advisory speed limits with minimal human intervention.\nImproving traffic flow with the use of digital technology is something BIM Academy definitely sees being introduced in more and more projects such as this one, introducing the ability to proactively predict and control traffic with all of the consequent benefits on safety, environmental impact and user experience.\nHighway design based purely on design of the physical infrastructure can be risky. Introducing a combination of sensors and 4D modelling is the way forward in smart infrastructure design, helping to measure the impact before it happens, and allowing for a smoother, less invasive, greener \u2013 and cheaper \u2013 project.\nIn the future, I believe we will start to see trends including the increased adoption of in-vehicle technology. The consequent reduction of roadside tech will follow, seeing the introduction of so-called \u2018Naked Roads.\u2019 We expect to see this concept escalating over the next two years as the government presses ahead with initiatives promoting vehicle-centric intelligent technologies.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-2017-updates\/","title":"Autodesk 2017 updates","date":1463011200000,"text":"With updated versions of AEC tools now available, and more features coming in the Autumn for subscription customers, Martyn Day takes a look at what\u2019s new from Autodesk.\nThe announcement of AEC updates from Autodesk comes around each year like clockwork for customers \u2014 but the year ahead looks set to be one of momentous change. From 31 July, Autodesk will go subscription-only for new seats and suites, in a decisive and swifter-than-expected shift away from a more traditional business model based on selling perpetual licenses.\nThe impact will be felt by everyone in the Autodesk ecosystem \u2013 its customers, its resellers and third-party developers.\nWhat\u2019s clear is that customers who have previously purchased perpetual licenses for Autodesk software will continue to own and have full usage rights for those products. If they\u2019re still tied into maintenance contracts, they\u2019ll continue to get software updates and other benefits, for as long as they continue to renew. No limits will be imposed on how long they can keep renewing.\nFor others, the picture is more mixed. For new customers and start-up companies, it\u2019s good news, because accessing professional CAD tools will no longer involve the hefty capital expense hit associated with forking out lump sums for perpetual licenses.\nLongstanding Autodesk customers, however, face the prospect of paying more for subscriptions over a three to four-year period than they would previously have done for perpetual licenses and regular upgrades. To cushion the blow, Autodesk is promising more frequent updates throughout the year, along with access to new online services. And it\u2019s hoping to make the proposition more attractive still with Autodesk Suites, which offer broader collections of Autodesk products for relatively low subscription fees.\nEither way, it\u2019s happening: subscriptions make good business sense for Autodesk, just as they have done for Adobe. The company may take a short-term revenue hit as those big lump sums for perpetual licenses dry up, but subscriptions mean a more regular flow of revenues, plus increased cost of ownership for users over the longer term, the result of which should make itself felt on the company\u2019s books in three to four years\u2019 time.\nCarrots and sticks\nThere are carrots here, as well as sticks. With its move to subscription-only fulfillment for new seats, Autodesk has realised that introducing updates that break end-user customisations on upgrade are not a great idea and its executives have conceded that file-format changes are a nightmare for customers.\nThe company now recognises that subscription is more about providing a continuum of improvements, rather than big dollops of new but hard-to-digest functionality. There\u2019s a definite impetus to smooth the upgrade process and maintain ongoing compatibility, which actually benefits the AEC industry as the work it does becomes more collaborative.\nAutodesk is also making efforts to provide more online e-learning opportunities, help forums and more online support specialists as part of the service.\nExecutives at the company also highlight the added flexibility of subscriptions, in that users can access current or earlier versions at home or at work, be sure they\u2019ve got the latest version when they need it and be set up for single or multi-user (shared) access. Another potential benefit is that software licenses can be better managed by customers according to the budgetary and other economic constraints that they face \u2013 although it might equally be argued that other software companies with subscription policies don\u2019t always make it easy for customers to exit subscriptions or retire licenses.\nIn other words, should there be another economic crunch, it will be interesting to see how easily customers can divest themselves of subscriptions if staffing levels fall. (Obviously, I hope we don\u2019t see another debt bubble implode. Once in a lifetime was more than enough, thank you.)\nRevit 2017\nThe first major change for AEC customers coming down the line from Autodesk is that Revit is now a single product. Its architecture, structural and MEP elements have been combined, so everyone gets everything and the distinction of disciplines is no longer an issue.\nFor customers not yet on subscription, Revit 2017 features all of the updates included with the 2016 R2 releases of October 2015, details of which can be found here (tinyurl.com\/z3eplko). This was a significant release in terms of Revit performance, which will be one of the big instant benefits of upgrading.\nRevit 2017 has been the focus of over 200 improvements. Since these touch many commonly used functions, this update should prove popular with expert users.\nIn visualisation, depth cueing allows enhanced visualisation of elevation and section views, using a lighting gradient to indicate which elements are furthest away and which are closest to the front of the view.\nThe ability to view or hide selected point cloud regions has been improved, so scan regions can be selected and hidden in each view, which ultimately helps performance.\nAutodesk Raytracer is now used for all rendering functions, so users no longer need to select a rendering engine. This is a high-quality and exceptionally fast physical rendering engine, introduced in last year\u2019s release and capable of using more than 16 CPU cores for processing.\nEnergy Analysis has been simplified, too: users only need to enter in the location of the model and the system will intelligently determine the rest of the default settings, to achieve optimal results with minimal input. For more advanced Energy Analysis, the Advanced Energy Setting dialogue offers all the parameters that were formerly found in the Energy Settings dialogue, with a bit of renaming. The export to gbXML tool can now export the energy analytical model created using the built-in energy settings or the model using volumes based on rooms or spaces.\nOther enhancements include:\nRailings: These can now be sketched on top of faces of floors, slabs, slab edges, walls and roofs. Balusters and railings adjust to the slope of irregular surfaces.\nDisplay of walls: To improve the system regardless of detail level assigned to a view, Revit regenerates walls only for those visible in the drawing area.\nExport to FBX: When exporting a 3D model view to FBX, compatibility is provided for FBX Files for import in 2016+, FBX 2015 and Previous for import in 2015 and earlier version products.\nFormIt 360 Converter: This converts FormIt 360 files for use with Revit, and vice versa.\nIn-place elements: Users can now create an in-place stair element in the context of a model.\nDynamo: This is a graphical programming interface, similar to McNeel Grasshopper for Rhino, that lets you customise your building information workflow. It is now available as part of the Revit installation.\nA360 Collaboration for Revit: This is now installed with Revit.\nThere are also a host of updates and enhancements to Revit\u2019s existing text, schedules, user interface, workflow and collaboration tools.\nStructural Fabrication Suite\nAutodesk launched the Structural Fabrication suite last year. This includes Revit, AutoCAD, Advance Steel and Navisworks and aims to connect up engineering and construction by bundling tools for design, detailing and fabrication.\nRevit has always been strong in structural design and engineering, while the AutoCAD-based Advance Steel (a product Autodesk acquired from French structural software developer Graitec in 2013) is better tuned for fabrication (drawings, bills of materials (BOMs) and CNC). Navisworks Simulate adds estimation, co-ordination and construction simulation of steel projects.\nIn the past, Autodesk has been criticised for poor interoperability between key products within its suites. The good news is that the company is already addressing this for the Fabrication Suite with the ability to synchronise not just the structure but also the fully parametric steel connections between Revit and Advance Steel.\nThe new Steel Connections for Revit extension also enables Revit 2017 users to model connections with a higher level of detail, including a built-in steel connection design engine based on US and European codes. As a result, this looks to be an important step in streamlining design-to-fabrication workflows.\nMost of the other highlighted developments this year centre on Advance Steel 2017, with new connections and profiles for international suppliers; new user-defined formulas for BOMs; new automatic and parametric tools for inserting cold rolled sections; more tools for cleaner documentation; and options to manage fabrication status data. Interoperability has also been improved with vanilla AutoCAD, thanks to an object enabler that allows users to open Advance Steel 2D drawings in AutoCAD 2017 and AutoCAD LT 2017.\nCivil 3D and InfraWorks 360\nLooking at the range of AEC applications as a whole, it\u2019s clear to see that InfraWorks 360 is the product boasting the greatest development velocity. Every year, Autodesk adds impressively big chunks of additional capability to what started out as a conceptual civils tool, and in the process, is creating something that will potentially replace the AutoCAD-based Civil3D detailing application.\nInfraWorks 360 2017 edition provides support for more complex road configurations \u2013 for example, multi-lane (merges and diverges), complex intersections, more advanced roundabouts and weaving lanes.\nThe major update here has focused on simulation capabilities, enabling InfraWorks 360 to be much more useful in the conceptual phase of a process. As well as supporting the analysis of more complex road conditions, the system has been expanded beyond the simulation of vehicles to include pedestrians and public transport. When combined with stunning graphics pulled from real-world GIS, 2D drawings, OSM maps, raster images and satellite data, InfraWorks can provide traffic simulation that blows SimCity out of the water. With the user almost in \u2018God mode\u2019, they can get really useful data out of the system.\nThis release at last provides InfraWorks with much better interoperability with Revit and Civil 3D. In fact, it appears to work better with Revit now than Civil3D does. So much detail can be pulled across to InfraWorks, in fact, that it almost rivals Navisworks as a viewing option.\nNavisworks\nAmong a number of enhancements to its desktop coordination application, and its ability to work with Revit and more notably Recap (which has seen a drastic price reduction recently), Autodesk has taken this release to integrate Navisworks with its cloud-based equivalent, BIM 360 Glue. As a result, views created in Navisworks can now be shared with teams working in BIM 360 Glue. The displays look alike and share model elements such as transparency and section planes.\nAutoCAD 2017\nStill going strong, 2D giant AutoCAD and its sibling LT roll ever onward. This year\u2019s updates include the ability to import geometry, TrueType fonts and raster images from PDF files.\nThere are also improvements to Smart Centrelines\/Centre Mark creation, with new options for associative editing of these.\nThe new Autodesk Desktop App replaces Autodesk Application Manager, seamlessly delivering product updates and learning content in a subscription-aware one-stop shop. The idea here is that security patches and updates can be delivered for all 2015, 2016 and 2017 versions of Microsoft Windows-based Autodesk products without disrupting workflow.\nConclusion\nAutodesk watched from the sidelines with admiration as Adobe pulled off its own move to subscription-based pricing. That transition wasn\u2019t without its problems: Adobe stumbled at first, upsetting many customers with the sheer abruptness of the move and its pricing mechanisms, but soon recovered.\nSo it\u2019s no surprise that Autodesk hopes to emulate the positive elements of that shift \u2013 but with so many historical products and different interfaces, the Building Design Suite runs the risk of being seen as a bunch of \u2018islands\u2019, lacking Adobe\u2019s level of convergence in user interface and data formats. There is still much work to be done.\nIn its favour, Autodesk now has a formidable range of AEC applications. The most concentrated development effort seems to be focused on Infraworks 360 (and civils in general), the new 360 collaboration tools and Revit. A recent price drop for Recap, meanwhile, would seem to indicate that Autodesk is trying brute force to take market share in the surveying and reality-capture market. With Formit and Dynamo, it\u2019s still working on the conceptual phase of the problem.\nSo there are still unknowns in what was once a far more predictable upgrade cycle \u2013 but customers can expect further updates from Autodesk\u2019s AEC team in September and October this year.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/digital-fabrication\/the-path-to-industrialised-construction\/","title":"The path to Industrialised Construction","date":1593648000000,"text":"Project Frog\u2019s Dara Douraghi takes a closer look at Industrialised Construction and how technology and manufacturing can bring new efficienices to a mature industry\nThe construction world is currently in the midst of its own industrial revolution, attempting to bring automation, improved machinery and offsite manufacturing to speed production in a world where costs have skyrocketed, while skilled workforces have disappeared. This Industrialised Construction (IC) approach is converging industrialisation and software solutions to enable an end goal of mechanisation and systemisation in the construction industry. It\u2019s combining offsite fabrication techniques and processes with manufacturing principles, similar to the automotive, consumer electronics and aerospace industries, in a workshop or factory, then transporting and assembling the components at the job site.\nFragmented solutions\nThe Architecture, Engineering and Construction (AEC) industry is no stranger to technology or implementation of software solutions for design, documentation and construction management. Unfortunately, the integration of technology is often in the form of disparate point solutions that are more than likely incompatible, leading to a fragmented overall process. These varying and incompatible solutions can impact a portion, or even all, of the stakeholders and project members during any given project. Under these circumstances, there will be the inevitable errors and omissions, forcing constant remediation of design and revision of project scope due to that lack of integration. In addition, the lack of communication within project teams, and throughout the full chain, can result in failure to follow agreed upon, or mandated, standards or codes.\nWhat\u2019s missing is an overarching process with a comprehensive suite of software solutions linking design, engineering and construction processes from the onset, to successfully translate the content into a physical building. The integration of platform with software services and tools creates an end-to-end solution that enables mass customisation at scale across the global AEC industry.\nEnabling scalability\nThat comprehensive suite of solutions that links the process from end to end and delivers scalability must begin with the seamless integration of a kit-of-parts platform and a technology platform. A Kit-of-Parts is a repository of building components that are pre-designed and pre-engineered, allowing architects to focus on project specific design work and it also facilitates the coordination effort with other disciplines. Additionally, that repository would wrap rules and constraints around each part or series of parts, which could then be shared with all stakeholders from design to build.\nIn this approach, buildings are treated as a product and must address the wide amount of variation the market demands. The focus is on developing, iterating and refining reusable components that can be mixed and matched to design and create a wide variety of buildings. These components are designed for manufacturing and ease of assembly on site.\nBeyond the focus on Design for Manufacture and Assembly (DfMA) approach, each component needs to be designed for flexibility, automation and usability, and, as you might imagine, requires data management to handle the range, scale and complexity of a building. It must also allow for rapid design and engineering, provide immediate feedback on cost and schedule, and effortlessly manage that dataflow through manufacturing and construction. Not a small feat.\nTo enable scalability, IC also needs a technology platform that allows all users within a system to contribute to the dataflow while simultaneously benefiting from other user\u2019s data. This data-centric connected workflow must start upstream and make its way from design through construction. With a data-centric workflow, data standards are required to make interoperability between all systems and components possible.\nSuch a workflow allows teams to automate the process, from design to manufacturing, eliminating manual effort wherever possible. This would include construction documents, shop drawings, engineering calculations and detailing of repetitive building elements, all typically consuming tremendous amounts of time from highly skilled professionals.\nBy integrating these innovative techniques, processes and technology, components can be manufactured in controlled environments, and, once manufactured, they are transported to the final location and assembled. By leveraging the many synergies of manufacturing, a notable increase in speed and scalability is achieved from design through manufacturing and construction. The upside of building this way is enormous \u2013 build schedules can be cut in half, key parts of the process are automated, and schedule and quality control are drastically improved when ad-hoc processes are removed.\nPutting IC to the test\nThere are a number of examples where this approach to IC can significantly impact both cost and project schedules. An example is national build programs, where major chain or franchise businesses are attempting to expand into new markets or increase presence in existing regions. The increasing costs and risks associated with stick-built construction, coupled with a dwindling workforce, creates uncertainty around whether costs will skyrocket or labour will be available when it becomes time to build.\nWith an integrated process, companies can easily manage design, development of shop drawings and fabrication of building components. Implementing a systematic and holistic approach towards structural engineering and platform design allows for consistency to be maintained across the program at a national level. This includes the development of building platforms broken into wall panels, with integrated mechanical, electrical and plumbing (MEP) components, and panelised composite roofing systems along with unitised glazing. These can be easily shipped to each site, where businesses benefit from unlimited design options, shorter build cycles and improved quality. This approach can also consolidate and optimise the network of suppliers and number of resources needed, allowing development of potentially hundreds of buildings more quickly, at less cost, with less waste and fewer mistakes, even as the skilled labour pool shrinks.\nSchools are another sector that benefit immensely with the IC approach to construction. With long and rigorous review processes, coupled with very short build windows, school construction has become inefficient, expensive and inflexible. Architects can create their own, customisable library of parts, allowing them to design flexible learning spaces based on specific needs. Coupled with the ability to manufacture off-site and deliver the superstructure to the site, school buildings can now easily be built in a summer while students are off campus.\nFor districts building across multiple campuses, structures can be designed for each school\u2019s specific needs and have their own look and feel yet stem from the same building system. Utilising an Industrialised Construction approach allows development of a building system that provides consistency and efficiencies during design and engineering as well as during construction.\nFinally, construction of apartment buildings, condominium complexes and even office buildings can benefit from an Industrialised Construction approach. While the cost for a unit or space in a building varies, the underlying design elements of things such as bathrooms and kitchens remain the same. It\u2019s the finishings that differ at each price point.\nGeneral contractors utilise repetitive content, such as bathroom pods, in almost 75% of their projects, yet every time a new project starts, the bathroom pods are being re-generated within the design models from scratch. Because every project has unique stakeholders and design constraints, countless documents must be organised and updated, with changes applied whenever design guidelines are changed.\nOften, changes are controlled by a combination of tools and functions in Revit or any other design and documentation software, but if there is no process in place to implement a feedback loop downstream during manufacturing and construction, teams end up resorting to manual change management and documentation control. This approach is prone to user error and can result in significant delays, and lead to inaccuracies and lack of coordinated content management.\nGoing back to the bathroom pod example, by breaking pod options into a defined set of parts, GCs can create a repository of reusable components that can be mixed and matched to concoct almost endless combinations. This enables the bathroom pods, down to the component level, to be managed and tracked easily, all the way from design to manufacturing. If a change occurs to the pod itself, including, for example, changes in ADA requirements, teams can make the change within their library, allowing each project manager at the factory floor to refer to the latest content and be confident it matches the requirements coming from the product development team.\nAll this means significant reduction in design time, immediate starts on new projects, reduced unnecessary loopbacks from the factory floor to the design team, all shortening the entire design\/build cycle and cutting costs.\nIn order to truly industrialise the construction process, firms from design to manufacturing and construction must embrace technology solutions that will allow them to begin to automate the process wherever possible. A technology platform that connects and integrates a comprehensive Kit-of-Parts, along with the associated rules and criteria, is essential to drive us to a mechanisation and systemisation approach to architecture, engineering and construction . While there are many software solutions available to the industry, the fragmentation and lack of integration are hurdles that must be overcome in order to reap the benefits of Industrialised Construction.\nDara Douraghi AIA is VP of Architecture and Engineering at Project Frog. Douraghi leads Frog\u2019s architecture and engineering studio, responsible for designing and developing Frog building platforms across various typologies using Industrialised Construction solutions.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/choosing-a-workstation-it-s-not-that-straightforward\/","title":"Choosing a workstation for BIM? How to future-proof your investment","date":1517184000000,"text":"With so many powerful new technologies coming online, choosing a workstation for design and engineering is no longer the straightforward decision it used to be. Greg Corke explores several ways to help future-proof your investment\nI\u2019ve been writing about workstations for 20 years now and this is by far the most exciting time I can remember. That\u2019s not because the CPUs and GPUs are more powerful than ever (which they are, of course), but because the software that designers, engineers and architects use is changing dramatically.\nIt used to be all about CAD or BIM and a little bit of rendering. Now, we are starting to see core design workflows augmented with technologies like game engine viz, Virtual Reality (VR), GPU rendering, generative design and reality modelling.\nThis presents a big challenge when buying a new workstation. It was always a good bet that a machine with a high-frequency quad core processor, mid-range pro graphics card, 256GB SSD, 2TB HDD and 32GB of memory would serve you now and well into the future. After all, CAD\/BIM software and its hardware requirements have not changed dramatically for many years, and this should remain the case for some time.\nBut with so many new technologies coming online, almost all of which demand a much faster workstation, it\u2019s now a lot harder to choose a machine that you know will be able to support your changing workflows in years to come. There are a few options here.\nThe first is to buy a workstation that is faster than you currently need. A GPU that is \u2018VR Ready\u2019 or a CPU that has more cores, for example, would give you some built-in headroom \u2013 and the flexibility to try out new tech. However, getting budget approved may be hard, particularly when firms have long-held views on how much a CAD\/BIM workstation \u2018should\u2019 cost.\nThe second is to make sure the workstation is upgradeable. This could mean checking that there are spare DIMM slots available to increase RAM when datasets get more complex or the number of applications you use or their memory footprint increases. Or, that the workstation chassis and power supply can accommodate a more powerful graphics card \u2014 or multiples thereof for GPU rendering or some viz and VR applications.\nGPU, memory and storage upgrades are very easy to do, even for novice users. CPU upgrades, on the other hand, are harder and require more specialist knowledge. In some cases, depending on your motherboard, the additional benefits are also very small.\nUpgrades do not have to be made inside the machine. With new I\/O standards like Thunderbolt 3, a tiny desktop workstation or mobile workstation with limited expansion, can be easily upgraded with fast external storage. More recently, external GPUs can also be added. Nvidia\u2019s Quadro eGPU solution, for example, can give users access to a powerful \u2018VR Ready\u2019 professional desktop GPU. Importantly, the Windows OS can see this as the primary GPU, so it can be used for game engine viz or VR and not just as a co-processor for GPU rendering or processing reality models.\nThere\u2019s also a third option, which is to invest in a workstation that can handle 95% of your workload, then offload the other 5% to the cloud \u2013 or to a shared local resource. Why buy an expensive estate car for the city when you can instead hire a roof rack for occasional holidays away? With this approach, you can reduce your capital investment and pay for additional CPU or GPU power on demand.\nThe obvious candidate for this \u2018pay as you go\u2019 approach is rendering, which may only be needed at certain times in the design process. However, it could also extend to virtual desktop services from companies like Frame, which allow you to run powerful 3D apps in a web browser. This is not a viable solution for pro VR, however, as the GPU needs to be local.\nConclusion\nIt\u2019s very hard to predict how your design workflow is going to change over the life of your workstation but, with so many new technologies coming online, it could certainly look very different in the next few years. With this in mind, it\u2019s never been more important to be prepared. Kitting out every member of your design team with an ultra high-end workstation is unrealistic, but if you don\u2019t have a solid plan in place and the flexibility to adapt then you could get left behind.\nThis article is part of an AEC Magazine workstation special report. To read articles and reviews in this report click on the links below.\nDesktop Workstation Buyer\u2019s Guide\nEssential advice for those looking to buy a workstation for product development\nBoxx Apexx S3\nThe overclocked six core \u2018Coffee Lake Core i7 CPU makes this CAD workstation fly\nWorkstation Specialists WS-1160A\nAMD CPU and AMD GPU combine for a powerful workstation for CAD,viz and VR\nAMD Radeon Vega GPUs\nWhat does AMD\u2019s Vega GPU architecture bring to CAD, VR, viz and GPU rendering?\nArmari Magnetar R80 (Pre Production Unit)\nThis dual Intel Xeon Gold workstation delivers the goods in single and multi-threaded workflows\nBoston Venom EPYC (Pre Production Unit)\nDual 32 core AMD Epyc CPUs make this rendering beast fly, but it\u2019s at the expense of single threaded performance\nMobile workstation buyer\u2019s guide\nA rundown of the things to look out when buying a professional 3D laptop\nWacom MobileStudio Pro 16\nWacom\u2019s legendary pen technology is embedded in a pro 3D tablet\nHP ZBook x2 G4 (Pre-Production Unit)\nThis detachable 2-in-1 combines Wacom pen technology with a Quadro GPU for 3D CAD\nPNY PrevailPro P4000\nThis slimline 15-inch mobile workstation breaks all the rules by putting a powerful \u2018VR Ready\u2019 GPU at its heart\nDell Precision 7720\nWith powerful processors, impressive cooling, good serviceability and excellent build quality, this 17\u201d mobile workstation is hard to beat\nRendering beyond the CAD workstation\nWhen rendering work is on the cards, what\u2019s the best way for a firm to get its machines and its workflows up to speed?\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/revit-structure-3-74545492\/","title":"Revit Structure 3","date":1156291200000,"text":"With Revit Structure an analytical model is automatically generated as you create your physical model. The analytical model can then be linked to structural analysis applications and the physical model automatically updated from the results.\nRevit Building was only ever meant to be one part of the much bigger picture that is BIM. The parametric building modeller, which provides the architectural element to Autodesk\u00dds Building Information Modelling philosophy, has now been joined by a structural engineering partner, Revit Structure. The product was launched in the US last year, but it has taken 12 months for it to make its way across the pond, maturing into third release and picking up localisation for the UK market on the way.\nRevit Structure is a design and documentation solution specifically developed for structural engineers. Like Revit Building, it is a database-driven application built around a single model. You can view your model in 2D, 3D, elevation, section, and detail and each will give you a live view taken from the master database. All of these views are stored in a central project browser rather than in individual files and any changes made, such as moving the position of a member, will be reflected across the whole project. This isn\u00ddt just restricted to geometry however, as the master model can also be amended by making changes to linked schedules \u00b1 by altering the size of a column, for example.\nThis is a different approach to many 3D systems which derive 2D drawings and schedule data from a master 3D model, but not the other way around. With Revit Structure\u00dds change management engine everything is linked \u00b1 a change made in one view is propagated everywhere.\nWhereas Revit Building uses a single model, Revit Structure uses two and automatically generates an editable analytical model alongside the physical model you create. The physical model, which shows a true representation of what structural elements look like, is used for layout, drawing production and documentation, whereas the analytical model can be bi-directionally linked to third-party structural analysis applications to enable a more integrated design process. Any design changes made in your analysis application can be brought back into Revit Structure and both the analytical and physical model (and of course any related documentation) automatically updated.\nWorking with structural models\nIn an ideal world the starting point for any Revit Structure model would be a 3D Revit Building model sent direct from the architect. This can streamline the whole model creation process as engineers can share the same underlying database and define structural members directly from the architectural model.\nWorking alongside Revit Building also has benefits downstream and interference checking can be carried out between architectural and structural elements and once the models are linked both architect and engineer can be notified of any changes made to the architectural\/structural model using the copy\/monitor tool.\nFor example, if an architect moves a column, the structural engineer will be automatically notified when the models are next linked up. The change can then be accepted, postponed or rejected.\n3D models can also be imported from Architectural Desktop, and engineers can use plan views as a reference when starting their structural layouts. To help co-ordinate designs structural engineers can also export their Revit Structure models to Architectural Desktop, creating true ADT objects. Links to Autodesk Building Systems (used by mechanical and electrical engineers) and IFCs (Industry Foundation Classes) are also provided.\nRobot Millennium from Robobat\n(www.revit.robobat.com)\nRAM Structural System from RAM Intl\n(www.ramint.com\/support\/revit.jsp )\nRISA-3D and RISAFloor from RISA Technologies\n(www.risatech.com\/partner )\nSmart Modeller from CADS\n(www.cads.co.uk )\nFastrak and S-Frame from CSC\n(www.cscworld.com )\nGSA from Oasys Software (Arup)\n(coming later this year)\n(www.oasys-software.com\/products )\nDespite the additional benefits that come from working with a complementary 3D architectural product, a more common starting point is to import a 2D DWG, DXF, or MicroStation DGN file. Here, structural elements such as walls and grids, can be created simply by clicking on the appropriate 2D lines.\nOf course all the sharing of 2D and 3D data relies on the premise that the structural engineer fully trusts the integrity of the architectural model, which is still not often the case in the construction industry. There is still a fair amount of distrust of CAD data and the standard deliverable continues to be the printed drawing, complete with the legally binding signature. As a result, many structural engineers may prefer to start modelling from scratch, though this sharing of data may prove to be very popular in design\/build companies, where there is a collective responsibility.\nWhatever the starting point for the project, Revit Structure provides a range of parametric structural modelling tools, which can be used to generate your model in either a 2D or 3D view. The depth of tools is fairly extensive, including grids, beams, beams systems, intelligent wall families, bracing, slabs, and foundations to name but a few. One notable omission is cold rolled steel and Autodesk is currently working with UK manufacturers to deliver a library of Z purlins and rails.\nElsewhere, structural steel components have been localised for both UK and European standards in Revit Structure 3 and these are selected from a pull down list, along with material type and size.\nWith Revit Structure\u00dds object\u00b1based architecture, columns automatically snap to grids and beams snap to columns or gridlines. If the grid is changed at any time during the project, all the related components will follow. Of course, the beauty of Revit Structure is that many components can be modelled parametrically. For example, secondary beam systems can be defined not only by their spacing, but by the number of beams within a defined length. Then, if this length is changed, the system will automatically update the position of the beams.\nIn a typical building, the structural engineer will complete a single level, then copy and paste the structural components onto the other levels, quickly building up a complete structure.\nWhile this entire modelling process is centred around the physical model, at the same time Revit Structure automatically generates a fully associative analytical model, consisting of elements and nodes, and it is this that the structural engineer can use for analysis in a range of third party applications.\nStructural analysis\nWhile the modelling functionality in Revit Structure is excellent, the feature that sets the system apart from others is its associative analytical model, which enables a bi-directional link to be formed with a range of third party structural analysis applications. With multiple analysis applications often used in a single project, the creation of a master analytical model in Revit Structure means that structural engineers do not need to re-model the same data in each individual application. This not only streamlines the whole process, but helps reduce errors and ensure that the engineer is always working on the latest model.\nThe level of integration varies from application to application and it\u00dds important to check with your chosen structural analysis software provider to see how tight this is. Developers that currently offer plug-ins for their analysis products through Revit Structure\u00dds API include Robot Millennium from Robobat, RAM Structural System from RAM Intl, RISA-3D and RISAFloor from RISA Technologies, Smart Modeller from CADS, and Fastrak and S Frame from CSC. The list is still growing and Oasys, the software arm of Arup, will be launching a plug in for GSA later this year. In addition, Revit Structure also supports the CIMSteel CIS\/2 interchange standard.\nUsers can choose to link the whole analytical model or a subset to their chosen analysis application. This depends on the type of analysis that they want to carry out, and can be done at any time in the design process with a single mouse click. For example, the entire model could first be used for a global analysis to get reactions, sheer, force and bending moments, with a section then exported to another application to design beams or slabs.\nIn terms of loading, you can apply basic area and point loads inside Revit Structure and these can also be exported to your chosen analysis application. However, users may find it more efficient to set up loads and load combinations inside their chosen analysis application.\nOnce your analyses have been solved any modifications can be fed back into Revit Structure, where it can track changes such as which members have been altered, which have been added and which have been removed. The system can then automatically update both the analytical model and physical model.\nOf course, the process described above is an ideal example. The reality is that many analytical models need to be different from their physical counterparts so that they can be solved efficiently or indeed accurately. For example, as you step up through a building, the column size is typically reduced and the user will need to alter the analytical model in Revit Structure before it can be analysed accurately. This, of course can mean than the analytical and physical model are out of sync, and despite built-in tools that can check and warn if tolerances are exceeded, users need to be very aware of the implications of any changes, particularly when receiving updates from their third party analysis application.\nDesign documentation\nOnce the design is finalised, the creation of plans, elevations and sections in Revit Structure is very easy. Views simply need to be dragged and dropped from the project browser onto each drawing sheet. As each drawing view is still a live representation of the master model even last minute changes to the design will be automatically reflected throughout all documentation. Schedules, section labels and callouts will also be co-ordinated even if they are on separate drawing sheets.\nSchedules themselves are incredibly easy to generate and include sorting, grouping filtering and counting functionality. As mentioned earlier, any changes made to the schedule will automatically be made to the master model and hence any related views.\nStructural details can be created from views of the model, or generated from scratch either using Revit\u00dds 2D drafting tools or importing them from AutoCAD (which incidentally ships with AutoCAD Revit Series \u2013 Structure). AutoCAD can also be used to finish off your design documentation, but you lose the associativity with the master model.\nConclusion\nWith Revit Structure Autodesk is offering an extremely exciting proposition for structural engineers. The modelling tools are easy to use and the powerful change management engine, which means a change anywhere will be reflected across the whole project, is a major strength of the product. This not on only streamlines the whole design documentation process but helps reduce errors, particularly when any last minute design changes are required.\nThis level of functionality has been at the heart of the architecturally focussed Revit Building for some time, but the coordination of design data in Revit Structure goes beyond the integration of 2D, 3D, section views and schedules. The product\u00dds links to Revit Building and ADT also help bring the architect and structural engineer closer together by enabling the re-use of data, interference checking, and in the case of Revit Building 9 and Revit Structure 3, the monitoring of any changes made between structural and architectural models.\nWhile this co-ordination of architectural design data will appeal to those who trust their source data, the major draw for structural engineers is certain to be the ability to establish dynamic links with a range of structural analysis applications, with Revit Structure acting as a central repository for all design updates.\nAnd while close attention will have to be paid to any changes to the analytical model during the analysis round trip and the knock on effect to the physical model, the fact that you only need to model once to produce physical model, analytical model, and all design documentation has the potential to make Revit Structure an attractive proposition for any structural engineer.\nwww.autodesk.co.uk\/revitstructure","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/in-pursuit-of-cool-surveying-laser-scanning-bim\/","title":"In pursuit of cool","date":1426032000000,"text":"As the surveying industry develops, a greater number of ingenious applications of the technology are making waves\nLaser scanning is just going to get \u2018cooler\u2019 every year, wrote AEC Magazine\u2019s Martyn Day in January\/February\u2019s Scan-to-BIM article. Hopefully this does not mean that Plowman Craven surveyors insist on the latest designer Hi Vi and safety glasses before starting on site! It does, however, mean that the technology is rapidly developing and the race to understand what is available, and how best it can be applied, intensifies.\nBIM has brought excitement to the surveying industry and with it some uncertainties. Embracing and adapting to the challenges of BIM will be the difference between \u2018opportunity\u2019 and \u2018threat\u2019, and organisations are now under constant pressure to keep informed and on top of advances in laser scanning applications. This is crucial as clients\u2019 understanding and demands evolve.\nYes, scanning is getting \u2018cooler\u2019 and surveyors love playing with the latest toys and trying to find different ways of applying the technology. The skill is understanding where there is real benefit to be gained and the most appropriate technology for the task ahead.\nTerrestrial Laser Scanning\nDevelopment \/ progress in the field of terrestrial laser scanning has been rapid and concentrated in three main areas \u2014 range, speed and reliability. Range has increased gradually, however the speed of laser scanners has increased dramatically over the last ten years. They are now more reliable, smaller and cheaper, allowing greater flexibility in application and increased market exposure.\nTerrestrial Laser Scanning is part of the surveyor\u2019s standard tool kit and used for a multitude of applications whether for urban streetscapes, refurbishment and retrofit projects and new developments. It is the surveyor\u2019s way of bringing the existing environment into the BIM world.\nKinematic \/ Trolley Scanning\nLaser Scanning is as applicable to rail as it is in the built environment. The combination of laser profiling and a railway trolley has allowed surveyors to capture point cloud data while on the move. This complex data capture technique allows surveyors to push a trolley at walking pace along the fixed rail, capturing data as they go. Point cloud data is captured at the same time as traditional track measurements such as Cant and Gauge. The recent integration of inertial measurement units (IMUs) to the trolley systems has allowed surveyors to increase the distance between control points and to capture more data in a given track possession.\nMobile mapping\nThis can be applied to both aerial (Lidar) and vehicle mounted applications. Achievable accuracies have considerably improved over the last few years, particularly relating to vehicle-mounted, which are similar to terrestrial capabilities. The ability to cover large areas in a short space of time, safety for operators and negating the need for infrastructure closures lends itself to highways and bridge applications.\nA vehicle mounted mobile mapping system is usually built up of six core components. These are laser profilers, cameras, video cameras, an IMU, an odometer and GPS (GNSS) receivers. Data from all of these sensors in combination with ground control points allows point cloud data to be captured from a moving vehicle. This is typically a car but can, in principle, be any moving vehicle.\nThe complexity of these systems makes them very expensive, however developments in this field are starting to bring pricing to an accessible level for a less specialist market.\nUnmanned Aerial Vehicles (UAVs)\nDrones are riding a wave of popularity at the moment, and are the real \u2018cool dudes\u2019 of the industry, but they also play a real part in reality data capture. Reduced cost has made this technology commonplace in a wide range of sectors including agriculture, mining, military, surveillance and consumer recreation. We are seeing real benefits for site investigations, construction program assessments, condition surveys and surveying areas which were previously inaccessible.\nUAV systems typically carry a range of sensors including GPS (GNSS), photographic, infra-red, hyperspectral and thermal cameras. The quality and size of the sensors being carried is typically limited by the lifting capacity of the UAV itself. At the cutting edge of this industry are those developing UAV systems able to carry laser scanning systems. Systems typically lift lightweight laser scanners such as the Faro x330 and Riegl VUX-1.\nThis technology is still in its infancy and still has to overcome barriers from authority and public suspicions plus increased safety education, but it has a great deal of potential within the industry as a whole.\nHandheld devices\nIndoor mapping is another arena where there has been rapid progress. This technology is driven by a range of players including developers of bespoke devices and smartphones.\nThe industry is underpinned by the development of Simultaneous Localisation and Mapping (SLAM) technology. In simplistic terms, this technology relies around computing the sensor\u2019s location and using this to map the environment around the sensor in real time. This technology was initially developed for robotics applications and is currently being used with autonomous vehicles, UAV and planetary rovers.\nSystems such as the Zebedee developed by CSIRO and the MID by ViAmetris are known systems in this arena, and large players like Google are entering the market with its recently-launched \u2018Cartographer\u2019, a backpack equipped with SLAM technology.\nWe will see a great deal of development in this technology which has considerable potential and the possibility of a massive effect in how data is captured quickly and effectively inside a built environment.\nThe future\nScanning technology is developing and expanding at a fantastic rate and is going \u2018hand to hand\u2019 with the rapid development in digital photogrammetry and image-based modelling. Plowman Craven has worked for a number of years in the entertainment industry providing 3D laser scanning and photogrammetry solutions for film and TV such as Game of Thrones, Life of Pi, Skyfall and very recently Marvel\u2019s Guardians of the Galaxy.\nIn the surveying industry we have seen how both technologies moved forward together in a very short space of time, and became associated with mass market deliverables. Laser scanning and image based modelling does have a long way to go yet, but we are seeing a greater number of ingenious applications of the technology and many of them are indeed really cool!\nAbout the author\nPeter Folwell is a director at Plowman Craven with responsibility for survey deliverables within the BIM environment. He is involved in Survey4BIM and a founder of BIMnet.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/ai-at-heart-of-new-asset-data-management-tool\/","title":"AI at heart of new asset data management tool","date":1573689600000,"text":"Microdesk ARID uses ML and AI to analyse assets names and values, identify inconsistencies and make recommendations\nARID is a new cloud-based asset registry and information database application from Global AECO consulting firm Microdesk that uses machine learning (ML) and artificial intelligence (AI) for enhanced asset data management.\nThe software is designed to enable organisations to capture and organize equipment assets in a central location and uses ML and AI to analyze assets names and values, identify inconsistencies and make recommendations based on industry standards.\n\u201cAsset information is pivotal for construction, operations and maintenance teams. We\u2019re excited to offer an application that will streamline the process and reduce the effort required to manage asset data information, while maximizing the quality of data,\u201d said George Broadbent, vice president of asset management, Microdesk.\nARID is the fourth product Microdesk has brought to market that expands on its BIM and BIM to Facility Management (FM) product suites which include ModelStream, BIMrx, and the BIMrx Accelerator.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/rover-mission-aims-to-find-if-life-exists-beyond-earth\/","title":"Rover mission aims to find if life exists beyond Earth","date":1073606400000,"text":"The photos trickling out of the Mars Rover Spirit are a lot more than an $820 million scrapbook. To an untrained eye, the images may look like just a bunch of nondescript rocks. But the pictures, combined with up-close analysis of those rocks, may hold stunning answers to questions about the origins of life on our own planet.\n\u201cThe reason that rocks are important is that, if you want to read about an ancient civilization here on Earth, you go and read history books. If you want to read the history of a planet, you read it in the rocks,\u201d Mars program manager Firouz Naderi said.\n\u201cNow, the key thing is to take with you the tools that would allow you to read the rocks,\u201d he said at NASA\u2019s Jet Propulsion Laboratory. The rovers have those tools, and once they start rolling around on the surface, they\u2019ll produce a lot more than photographs.\nThe rovers aren\u2019t digging for water itself. They are analyzing rocks for evidence they formed in water. Evidence that water once flowed on Mars would mean the planet might have been favorable to the existence of life. Future missions can look for fossils where the rovers find water-related rocks. It\u2019s difficult to describe the entire red planet based on four landing sites \u2014 the two Viking landings in the 1970s, the Pathfinder mission in the \u201990s, and now the first rover.\nIt would be like landing spacecraft in the Nevada desert, the Italian hills, Antarctica and the Costa Rican rain forest and trying to come up with one theory to explain Earth\u2019s history. Even a mile could make a tremendous difference in what the spacecraft sees and what people conclude. And Mars missions have been limited by the need for safety; no mountains have been explored yet.\nSpirit is in the middle of Gusev Crater, which may have been a lake. Opportunity will land on the other side of the planet, on Meridiani Planum, where minerals suggest water once existed. Results from the Mars rovers will fit NASA\u2019s overall objective, Naderi said: to answer the question of whether life exists outside of Earth. Powerful space telescopes look beyond our solar system and try to draw conclusions about whether life could exist elsewhere by identifying materials in distant star systems. In Earth\u2019s neighborhood, robots do the exploring. The most likely candidates for life are Mars and the icy Europa, one of Jupiter\u2019s moons, which may hide an ocean beneath its surface. Robotic missions may help pave the way for human exploration.\nThe challenges of a human mission to Mars are great. It\u2019s going to be expensive. Better propulsion would help. And scientists don\u2019t yet understand how humans can survive the harsh radiation there. International Space Station crews stay just six months in the relative safety of low-Earth orbit, where radiation doses are high but nothing like what people would get flying between here and Mars.\nThe rovers are just one among several planned or discussed robotic missions that would expand human knowledge of Mars, including one that would return samples to Earth for analysis.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/reality-capture-modelling\/matterlab-launches-bim-360-archiving-solution\/","title":"Matterlab launches BIM 360 archiving solution","date":1593561600000,"text":"ArchiveHub lets users download and browse complete project data for handover, analysis or archiving\nLondon-based AEC software developer Matterlab has released ArchiveHub, an archiving solution for Autodesk BIM 360 that lets users download and browse complete project data for handover, analysis or archiving at project milestones.\nThe software comes with a desktop viewer application that lets users view and browse their archive, keeping all metadata and file structures intact so that it\u2019s \u2018easy to explore\u2019.\nAccording to Matterlab, manually downloading every file on BIM 360 is difficult, takes a long time and can leave you with only partial project information. With ArchiveHub, all information and file structures are kept intact, \u2018minimising any risks of error or incomplete data\u2019.\nArchiveHub can also help with compliance and legal requirements for archiving data, such as holding onto construction project data for several years after a project has been completed.\nThe software keeps every bit of project information intact, including data on issues, comments and all other metadata and relationships between items.\nWith the enterprise version of ArchiveHub, archives can be automated, set at user-defined intervals. The system only downloads the content and metadata that has changed between archiving operations.\nPrices start at $600 per archive.\nEarlier this year Matterlab launched Unitize, a generative design tool and Revit plug-in designed to help architects assess residential masses in seconds.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/au2015-day-two\/","title":"Live from Autodesk University 2015 - Day Two","date":1448928000000,"text":"Join us for AEC Magazine\u2019s live coverage from Autodesk University in Las Vegas.\nWe\u2019re starting with an Inventor breakfast and then it\u2019s straight into some keynote sessions, so stick with us as everything will be coming thick and fast below the line!\n8.45 \u2013 It\u2019s a presentation breakfast \u2013 burritos, ALL the coffee and some news from the Inventor team.\nFirst up is that releases are no longer going to be once a year \u2013 expect more updates throughout the year, \u2018making sure the product meets the needs of the most demanding problems\u2019.\n11,500 beta participants are checking everything before the stuff hits your desks, which is important as over 2,000 customer improvements have been made over 9 releases in the last six months.\nThe Inventor team are pushing to build the most open, connected and professional grade tool \u2013 it\u2019s all very confident.\n8.50 \u2013 AnyCAD is exactly what it says on the tin \u2013 users can now grab any non-native CAD file from any system and insert it into Inventor. The user can recompute all the downstream features too, so in action it\u2019s very slick. In a two window desktop, the part in another CAD package \u2013 SolidWorks, say \u2013 if the file is changed, the Inventor part changes too.\nInventor is now cramming in a lot of new features \u2013 topology optimisation powered by Nastran simulation looks pretty trick, with direct STL file output for instant 3D printing.\nForce Effect, an iPad app for kinematic analysis, can take data straight into Inventor \u2013 pushing some fast and loose conceptual design into the system.\n9.00 \u2013 Inventor 2016 R3, the latest release, is focussing two capabilities \u2013 expanding the BIM workflow, and adding \u2018connected design\u2019, more of which later.\nIFC export capabilities means Inventor can churn out BIM-ready files \u2018cementing Inventors status as the leader for BIM content\u2019.\n9.05 \u2013 Connected Design is all about letting people collaborate on a design in real time \u2013 which will usually be held in the cloud, and working via A360 \u2013 which is a file viewer, letting project workers get by without the need to have Inventor, giving the usual mark-up capabilities, but giving the sharer a lot of control over what they\u2019re sharing.\n10.00 \u2013 Breakfast over, it\u2019s back in to the giant hall with everyone else for the main session \u2013 I fear the word \u2018innovation\u2019 is going to be used a lot in this, so I\u2019m apologising in advance\u2026 Roman Mars is on stage, introducing the Innovation Forum.\n10.05 \u2013 First up is Footprint, a couple of graduates that are designing and manufacturing custom footwear. EVA used in making midsoles can last up to 1,000 years in landfill, and no two feet are alike, so there\u2019s a lot to be improved on just there. 3D scanning, modelling and 3D printing are all used to get the best results.\n10.15 \u2013 Back to the architects \u2013 LMN Architects to be specific \u2013 which has set up a tech studio within the business to get its staff working on new tools, from generative design to CNC machining.\nThe team redesigned Cleveland\u2019s Civic Core on a tight budget, and they built a plug in especially for Revit to factor in materials and cost savings. \u2018We weren\u2019t just using a software to make things look good. We were able to use it all the way through the process\u2019.\nAnother project, using aluminium cladding, saw the team bust out the CNC machines, develop the facade, and work closer with the end part fabricators.\n10.28 \u2013 Taylor Dawson, GE\u2019s community manager, is telling us about First Build \u2013 an online community designing, engineering, building, and selling the next generation of major home appliances \u2013 which GE has partnered with.\nGE are working with outside makers and hackers to \u2018make the products that they want\u2019. The vision is to revolutionise how products are brought to market \u2013 shredding the usual \u2018four years to market\u2019.\n\u2018A prototype is worth a thousand meetings\u2026 openness ignites passions\u2019\nSo far they\u2019ve created a countertop ice maker ($3M raised on Indigogo \u2013 for an ice maker \u2013 the world has gone mad), and are now developing a home pizza oven.\n10.43 \u2013 Architect Ulrich Homann is presenting on connected data \u2013 specifically the \u2018Age Of Data\u2019. Take that Bronze Age\u2026\nWe\u2019re collecting more data than ever, and predictive maintenance for buildings is fast becoming a reality \u2013 with suppliers like ThyssenKrupp elevators putting this in place already.\n10.54 \u2013 BAC Mono are introducing the audience to their high performance car \u2013 if you were at DEVELOP3D LIVE you\u2019ll have already seen it.\n\u2018We weren\u2019t designing a car\u2026 we were designing a piece of equipment for the extreme sport that is driving.\u2019\n44 carbon fibre parts with a Cosworth engine and a Formula 3 gearbox \u2013 it\u2019s a bit of a beast. \u2018A purist supercar\u2026 for nothing but driving\u2019.\n11.09 \u2013 From amazing autos, to incredible images from Industrial Light and Magic \u2013 creators of some of the most memorable movie special effects. Their showreel has a drum soundtrack that is probably not being loved by many in the audience with sore heads from the night before\u2026\nThe aim of \u2018not being better than the competition in their industry, but to develop things that create their own industry \u2013 making the competition irrelevant\u2019.\nIts new X-Lab is working on the next generation technology for filmmakers \u2013 such as Scout, a VR enabler to allow filmmakers to explore a virtual set before anything has been built, and the Open Bionics Star Wars themed prosthetic limbs.\n15.00 \u2013 We\u2019re back for the afternoon, with some interesting user stories, beginning with ITAMCO \u2013 a company that make gears, clutch plates and connectors, but a wide variety of them, featuring everywhere from the international space station to offshore oil rigs.\nOddly enough, the presentation is about IoT and how they\u2019ve created an education centre for advancing the technology from an industrial perspective, along with VR, in their design and manufacturing operations.\n15.12 \u2013 Next is Voorbij Prefab, on how its investment in Revit helped save it from closure \u2013 it now has the best prefab factory in Europe, producing concrete parts for the construction industry.\nThey turn out enough prefab parts for 18 houses every day \u2013 \u2018every item can be 100 per cent customised without changing the process\u2019.\nUsing robots, they 3D print the moulds for the concrete parts, with the entire process digital, slashing time to delivery with the ability to be very flexible.\nThey also feed into the process a ton of other data, using Google transport data and even monitoring the weather, so they can produce the parts just before they\u2019re needed to be on the building site \u2013 that way they cut out a lot of the hassle of storage.\n15.31 \u2013 Next on stage is ConXtech, which produce modular structural steel framing for the AEC industry, and their drive to find a better way to build.\nIt arrived at the \u2018digital chassis\u2019, making the frame modular to a point, that slots into place, using gravity to shore up the structure. This also benefits architects, as they can take it into the design much earlier, and the construction teams, as it\u2019s much easier to assemble, requires fewer workers, and is a safer way to build fast.\nThe manufacturing of the parts is done in a big robotics and milling facility, which looks fully loaded. Both this and the previous speaker show the merging worlds of product design and architecture as buildings bring in more prefabricated components.\n16.15 \u2013 Time for the media session to wrap up the day.\nAutodesk senior VP Andrew Anagost is starting by reflecting on what we\u2019ve seen so far \u2013 most of it around the future of making things.\n\u2018We\u2019re relentlessly building out the ecosystem for how things are made\u2019\nWhat will happen to the channel once Autodesk moves to a subscription service? \u2013 \u2018Our best partners are already figuring out what they need\u2026 they\u2019re going to deliver a whole new set of services\u2026 it\u2019s an inevitable part of the change\u2019.\n16.35 \u2013 CEO Carl Bass and CTO Jeff Kowalski are on stage taking questions \u2013 we\u2019ll bring you this in full later in the event.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/hybrid-manufacturing-machine-goes-live\/","title":"Hybrid manufacturing machine goes live","date":1549324800000,"text":"LASIMM project machine said to pave the way for 3D printing parts and structures for construction\nOne of the world\u2019s largest hybrid manufacturing machines, which features metal additive and subtractive capabilities, is now ready to build, and will be capable of 3D printing large pieces of metal and large parts and structures for construction. The team behind the EU-funded LASIMM project say the machine offers a 20 per cent reduction in time and cost expenditure, as well as a 15 per cent increase in productivity for high-volume additive manufacturing production.\nThe machine includes a modular configuration of industrial robot arms and a specialised milling robot \u2013 the first for additive manufacturing of aluminium and steel, and the second for machining away surplus material to provide the final finish. According to the LASIMM team, the process will enable entire large-scale industries to move away from standardised components and towards bespoke solutions for industries such as aerospace, renewables, energy, transport, construction and many more.\n\u201cFor Europe\u2019s future industrial competitiveness, the LASIMM project represents a mighty leap forward for hybrid manufacturing and will enable many countries to produce far larger materials, both more quickly and cost effectively,\u201d said Johnny van der Zwaag, project manager research and innovation projects at Autodesk, the lead software partner on the project. \u201cThe project has brought hybrid manufacturing to a truly global and industrial scale. To date, 3D printing has been limited to smaller components and is often seen as an expensive option. But the technology, both software and hardware that has been implemented within this project, shows that it is now ready for bigger things.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/graphisoft-big-in-japan\/","title":"Graphisoft big in Japan","date":1502150400000,"text":"Graphisoft held a key customer conference in Kobe, Japan, to launch ArchiCAD 21 and to highlight increasing BIM usage in the Pacific Rim. Martyn Day attended to hear the latest on the software, processes, collaboration and to drink a little sake.\nThree years ago, Graphisoft held a similar conference in Tokyo, Japan, which highlighted the company\u2019s decision to focus on Asia as a place for BIM growth. That decision was a calculated one as these markets are culturally distinct from the west and have their own processes and workflows. While Autodesk was dominating with Revit in other geographies, Asia was a tougher market to break as off-the-shelf BIM modelling tools are less fit for purpose. Graphisoft was willing to work with large firms to incorporate their requirements and react to requests for solutions.\nAt the time this was seen as a risky strategy, as the BIM market was hot in the West but sluggish in Asia and there was a danger that a focus on one market could pervert development of the core product for existing users. Three years on, it\u2019s clear that this was a gamble worth taking and the additional input has not impacted development of ArchiCAD \u2013 only contributed to help drive the application to deliver even more value for modellers and especially in the quality of the 2D output.\nAt the time, Graphisoft\u2019s CEO, Viktor V\u00e1rkonyi, explained that another danger was to overcommit to providing capability and to under deliver. Especially in Japan, this was culturally unacceptable. Firms which have done this in the past have found themselves shut out of the market as reputations matter.\nSoftware firms are typically not the best at delivering on their promises, as they are pulled in many directions by global customers. I\u2019m glad to see that, three years on, Graphisoft\u2019s reputation is intact and ArchiCAD usage is starting to spread across the region, with customers from China, Singapore and Australia presenting at the Kobe event.\nViktor V\u00e1rkonyi\nV\u00e1rkonyi sees ArchiCAD as essentially a design tool and recognises the gap between BIM and construction models, identifying that more work needs to be done by all vendors to better connect both parts of the process. This will be done also through other Nemetschek products such as Solibri and Bluebeam.\nArchiCAD is intended to be the master model, defining everything to LOD300, with associated 2D drawings and non-graphic construction data. We can expect much tighter \/ seamless integration with products which Nemetschek has acquired to serve the design and building industries. Graphisoft\u2019s Asian clients appear to be driving this development.\nOn the business front, Graphisoft appears to be to doing well, with customers spread throughout Europe, Asia, and North America. In Asia, usage has gone from 17% to 42% in a year, with 55% of revenue coming from new customers. By my estimates, Graphisoft has doubled in 3 years. The company is benefitting from the increase in the appetite for BIM.\nOne of ArchiCAD\u2019s biggest strengths over Revit is its ability to handle large models without becoming unwieldy. In many demonstrations throughout the conference this was clearly shown, in one instance an ArchiCAD customer loaded a 14GB hospital model into the BIMcloud with 40 people accessing it simultaneously. On that note, Graphisoft has stated it will create an API for BIMcloud, allowing other firms to access the BIM model.\nMobile devices are much more memory- limited than desktop machines, which restricts Graphisoft\u2019s BIM model sharing application BIMx. To get around this, the next version of BIMx will enable streaming, so there will be no limit to the models which can be shown on phones and tablets when there is a data connection.\nThe link with McNeel Rhino Grasshopper is going from strength to strength. With each successive release of ArchiCAD, Graphisoft is refining and deepening the ability to computationally control ArchiCAD\u2019s BIM elements.\nThe alliance between McNeel and Graphisoft will see Rhino\/Grasshopper with ArchiCAD operating as a pre-design combination, allying the easy script generation of Grasshopper with the powerful modelling of ArchiCAD components. 50% of existing ArchiCAD large customers use Rhino and Grasshopper in their design mix. This integration is also winning new fans in competitors\u2019 installed bases and will lead to ArchiCAD being used by a whole new set of users.\nV\u00e1rkonyi also told the press that this link to Rhino will not only be at the start of the design process but will also remain connected for downstream changes. For now, it can work with walls, parametric elements and stories. In the future the API integration will get extremely deep between these two products, with Grasshopper being able to access GDL objects and design optimisation loops.\nHowever, V\u00e1rkonyi sees the next big challenge of the BIM market to be really all about workflow, concluding: \u201cThose who have the best workflow win\u201d. BIMcloud adoption has been slow but it\u2019s a step change in the way clients connect their offices and teams and has required firms to rethink their set-up. Using BIMcloud Manager with BIMcloud Servers enables a flexible mix of deployments but has required Graphisoft to work towards improved integration with its own suite of add-on applications to make is more seamless.\nArchiCAD 21\nThe latest release of ArchiCAD is a bit more eclectic than previous themed releases but I\u2019m not going to hold that against Graphisoft. The main and outstanding feature is the complete reworking of the stair creation tool. I\u2019ve never seen such a powerful BIM component generation tool and it\u2019s very easy to use. Prior to going to Japan, I sat in on an hour session in London that just covered what the stair tool can do! If you want to design something funky, complex, curvy, highly customised with a hand rail, this is your program. The software will even check how ergonomic it is, to get the optimal design.\nGraphisoft always picks a customer\u2019s project to appear on the box and this year\u2019s is the Charles Perkins Centre, Sydney, Australia by Francis-Jones Morehen Thorp studio (fjmtstudio.com). The amazing organic staircase says it all really.\nGraphisoft made a lot of noise about this being the first real application of Artificial Intelligence in a BIM application, using a predictive algorithm to assist in the designing of complex and standard stair casing. As the designer creates the staircase the system also warns if any design may infringe on local standards. The system can suggest alternatives and out of thousands of possible options, suggests four that will work.\nIFC files can now be placed as hotlinks into projects as reference content. IFC reference content can be filtered by categories, such as Structural or MEP, or by element selection. IFC model content can be updated from the linked source file and if the link is broken, elements within the IFC can be edited as regular ArchiCAD elements.\nIn the past, ArchiCAD only allowed a fixed set of Element Classifications. In version 21, it\u2019s not possible to classify elements to match any national or company-standard classification system. Classifications can be transferred between projects via XML file format.\nThe CineRender engine by MAXON (also owned by Nemetschek) has been updated. The latest version introduces Light Mapping and Secondary GI methods for more realistic, yet fast rendering. Expect Lens distortion, reflectance enhancements and more shading modes.\nCollision Detection is now a standard part of the feature set. There have also been enhancements to doors, windows, labels, curtain walls, IFC publishing and sections and elevations.\nWe will have a full review of ArchiCAD 21 in the next edition of AEC Magazine.\nCustomer presentations\nOver the two days of the conference, the stage was dominated by a wide range of customer presentations from the likes of: LWK & Partners, HDR | Rice Daubney, Surbana Jurong, FJMT, Kajima Corp, GS E&C and John Robertson Architects.\nThe striking similarity was the toolsets that these firms were deploying in addition to ArchiCAD: Solibri, Rhino \/ Grasshopper, Bluebeam and even dRofus (planning and data management application owned by Nemetschek).\nDuring my previous trip to Tokyo, three years ago, I noted that each firm had a distinctly different approach to their internal BIM design process and also used various product mixes. Three years later and the toolsets are standardising and products like Solibri (also recently acquired by Nemetschek) seem to have become the de facto standard model checking software both for internal and external checking of complex BIM data. As these firms which operate in the Pacific Rim work together more connecting up their tool sets and processes at quite an impressive pace.\nLooking back to Europe and the Americas, while the UK is the most advanced in demanding BIM deliverables and processes, this has taken a long time and is still far from perfect. While the West spends a lot of time codifying and developing standards, China, Japan and Singapore are making considerable strides to move to BIM with less general reticence.\nConclusion\nSometimes small decisions in software product design can make huge differences to their appeal to customers. From what I can see, Graphisoft has made two decisions which have ignited growth of sales and which also appeal to those who had perhaps been understood to have already made their BIM tool choice. A focus on Asia and a decision to not develop but to partner and integrate McNeel\u2019s Grasshopper have proven to be pivotal.\nWith Bentley having GC (Generative Components), Autodesk developing Dynamo, Graphisoft could have thrown valuable resources at creating its own programmatic design language and integrated it deep into ArchiCAD. But it would only have appealed to existing customers and would have required years of work to get anywhere near the complexity of the competition.\nComputational Design is still only used by a small proportion of designers and this would have been a major investment. By teaming up with McNeel, Graphisoft taps into an army of young architects and designers, immediately sparks interest in leading Architectural firms and makes the job one of software integration through APIs, as opposed to reinventing the wheel.\nRhino and Grasshopper are big in Japan, but they are also big in London, New York, San Francisco, Chicago, Paris, Barcelona and the Nordics. From release to release the level of integration is delivering on Graphisoft\u2019s promise and this is getting the company in through the doors of mature Revit customers that want to use Grasshopper with BIM components and love the idea they can spin huge models within 16GB of RAM.\nLarge American corporates selling volume product are a lot less likely to tailor products in markets which will not guarantee high volume return. Graphisoft\u2019s size and approach to customers means that delivering more tailored solutions and functionality is less of an arduous task. Understanding that Japanese clients are demanding and that they expect to get what\u2019s been agreed is a serious commitment.\nFrom the numbers Graphisoft is quoting, it\u2019s surfing on a wave of BIM adoption and is benefiting from the respect the firm and its employees pay to their Japanese customers. It\u2019s amazing to meet so many Hungarians that can speak Japanese! At one of the events even the Hungarian ambassador appeared and gave an English and Japanese address, with an obvious sense of pride that here was an small Hungarian software firm that doing so well in such a complex market.\nArchiCAD 21 is yet again another significant addition to the product feature set and I say in all honesty, it\u2019s worth buying a copy just to create funky stairs in any mature BIM process, there\u2019s nothing on the market quite like it.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-slr-part-of-global-bim-challenge-winning-team\/","title":"NEWS: SLR part of global BIM challenge winning team","date":1447718400000,"text":"International environmental consultancy awarded for collaborative working practices in a BIM environment\nSLR Consulting has contributed to a collaborative project that claimed the top award at this year\u2019s Build New York Live event.\nLandscape and GIS specialists from the environmental firm formed part of the 40-strong winning Ryder Alliance team to scoop the virtual design and construction competition, with its innovative approach to the redevelopment of Hudson Yards in Manhattan.\nThe award is dedicated to improving working practices in a BIM environment and the team, operating from six international locations; Newcastle, Manchester, Barcelona, Madrid, Sydney and Melbourne, were linked by digital communications systems \u2212 reflecting this new way of collaborative working.\nWith a tight deadline of just two days, the Ryder Alliance\u2019s winning concept included an indoor sports facility, a 60-storey residential tower and connection into the extended High Line linear park.\nClose cooperation with the architectural team enabled SLR to overcome the challenges presented by the site and the need to design proposals over significant variations in level. SLR\u2019s contribution involved landscape and architectural design within a BIM environment, exploiting the benefits of Virtual Reality (VR) using Oculus Rift, as well as Geographical Information Systems (GIS) mapping and the creation of a presentation video.\nAssociate, and BIM Manager, Kevin Smith, who headed up the SLR team, explained: \u201cThe building information model was displayed to three public focus groups, who viewed the project throughout the design process in an immersive virtual environment using Oculus Rift.\n\u201cKey metrics were collected using Visual Experience (VEx) scales, relating to the spatial and experiential qualities of the project. Once recorded, these were fed directly to the design team to refine the evolving design. This method was a real-time example of live public participation with the design process, utilising BIM and virtual environment technology to facilitate research-based design.\u201d\nThe project won the award for \u2018the best overall BIM effort within the brief as voted by the judges\u2019 and recognises the value of collaboration, working within a multidisciplinary team and contributing based on knowledge specialisms.\nThose participating from SLR included; Kevin Smith, Andrew Scambler, Andy Forbes, Rob Myers and Ben Blakeman. Ryder Architecture, BIM Academy, i2C and Ravetllat Ribas also formed part of the Ryder Alliance team, which included services engineer Cundall, BIM consultant Rapid 5D, Sir Robert McAlpine, construction consultant Summers Inman, and Willmott Dixon Construction.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-3d-repo-announces-real-time-bim-change-detection-tool\/","title":"NEWS: 3D Repo announces real-time BIM change detection tool","date":1517875200000,"text":"Web-based solution features a \u2018unique\u2019 algorithm that can detect changes between 3D models from different sources\n3D Repo has released what it describes as the first web based, real-time change detection software for 3D construction models. The cloud-based \u20183D Diff\u2019 solution is said to detect changes to 3D models, regardless of file type, in real time, via an encrypted web browser. It also allows users to share visualisations with project partners and stakeholders irrespective of location or time-zone.\n\u201cThe uniqueness of our change detection algorithm means that users can mix and match models from different modelling tools and pipelines and still be able to spot exactly what has changed over time between any two revisions,\u201d commented Dr Jozef Dobos, CEO of 3D Repo. \u201cThis becomes invaluable, not only during the design coordination stages, but also throughout construction when contractors introduce as-built models. It is also important for litigation when the proof of who did what and when may result in legal liability and financial claim.\u201d\nBy comparing the actual geometry of a model rather than underlying, software specific object IDs or labels, 3D Diff can detect changes between models from different sources. For instance, a user may pass 3D models from PDMS to Navisworks as well as to Tekla Structures due to project specific requirements. Without 3D Diff, 3D Repo says there is no way of verifying what data has been lost in translation from one package to the next. The company adds that 3D Diff also runs in real time, regardless of the model file size, so even the largest and most complex projects can be \u2018instantly differenced\u2019.\n3D Diff builds on 3D Repo\u2019s cloud-based BIM platform and \u2018online knowledge base\u2019 that allows users to access, via the web, the latest 3D models and make real time changes. 3D Repo says its platform is different from other collaboration tools as it uses a component-based database \u2013 meaning that information is live, useful and accessible throughout the entire project lifecycle.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-collaboration-to-enhance-building-energy-efficiency\/","title":"NEWS: Collaboration to enhance building energy efficiency","date":1422316800000,"text":"Schneider Electric and Autodesk sign Memorandum of Understanding to enhance Building Lifecycle Management through BIM\nAutodesk and Schneider Electric are working together to explore new ways to make buildings more energy efficient from the design and construction through operation and end-of-life phases. The two companies will use Building Information Modelling (BIM) to help enhance current practices for building lifecycle management.\nThe joint effort looks to combine Schneider Electric\u2019s expertise in electrical distribution, energy and building management solutions with Autodesk\u2019s portfolio of BIM-based design and construction software such as Autodesk Revit and Autodesk BIM 360. The collaboration may include new solutions and services in the areas of energy management, building automation and control, and workspace management.\n\u201cThe journey to sustainable high performance buildings starts with taking a comprehensive holistic view of a building lifecycle, from design, construction, and operation including adaptation to ever changing needs of businesses and organizations,\u201d says Jean-Luc Meyer, senior vice president, strategy and innovation, Schneider Electric. \u201cToday\u2019s announcement builds on decades of Schneider Electric integrated approach to building management that can reduce energy, reduce capital expenditures, decrease operating expenditures and improve overall business performance. We see a huge potential in digitizing buildings project lifecycle. We believe that the cooperation with Autodesk will help drive a deep and long-term transformation in the construction industry, providing greater value to each user and contributing to solve the energy challenge.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-mariana-popescu-block-research-group\/","title":"Video: NXT BLD 2019 \u2013 Mariana Popescu, Block Research Group","date":1565049600000,"text":"Addressing the increasingly urgent requirement of decreasing embodied energy and waste in construction\nMy research brings together advances in digital fabrication, computation, and structural design. It looks at reducing material demand both in terms of structural volume and during the building process, specifically for concrete. Designing structures that intelligently include structural performance and architectural geometry leads to beautiful, economical and structurally optimised systems that use very little material. However, their expressive, intricate and bespoke geometries can be challenging to build with traditional formwork methods that rely on single-use cut timber or milled foam. To harness the full potential of non-standard and non-repetitive efficient concrete structures, the formwork systems used for construction need to be rethought. Using 3D weft-knitted technical textiles as stay-in-place moulds is a novel type of flexible and cost-effective solution for casting concrete structures. Through tensioning the custom-tailored textile is formed into the desired shape and coated with a special cement-paste to obtain the mould, which becomes a basis for efficient, lightweight structures. With an inhouse developed computational pipeline consisting of algorithms and design tools to translate any given 3D geometry into a knitting pattern in an automated way, commonly available CNC knitting machines produce intricately knitted textiles. These textiles are light, compact and can be effortlessly transported to the construction site. The system was deployed at an architectural scale with KnitCandela, a four-meter-tall curved concrete shell with a knitted textile formwork supported by a steel cable-net built at the Museo Universitario Arte Contempor\u00e1neo (MUAC) in Mexico City in 2018.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction.\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bim-show-live-announces-2018-dates\/","title":"NEWS: BIM Show Live announces 2018 date and new awards","date":1499731200000,"text":"Conference returns to Newcastle from 28 Feb to 1 March 2018, plus BIM Awards on 27 Feb 2018\nBIM Show Live 2018 will return to Newcastle\u2019s Boiler Shop on Wednesday 28 February and Thursday 1 March, 2018. The two-day conference will feature a series of keynotes, panel and interactive seminar sessions from BIM and digital construction experts from across the globe, addressing new digital construction technologies; from artificial intelligence to predictive data.\nBIM Show Live will also be hosting its inaugural BIM Awards on Tuesday 27 February*, also at the Boiler Shop. The awards feature 17 categories, containing a mixture of submissions from organisations and nominations for individuals.\n\u201cWe\u2019ve been running the show now for six years, and each year we never fail to present the new and the wonderful,\u201d said BIM Show Live founder and Space Group CEO Rob Charlton. \u201cOur industry is progressing at an accelerated rate and we believe at BIM Show Live we have an obligation to bring our visitors the very best our industry has to offer for technology and digital construction, plus what has changed year-on-year and what this means for future projects.\n\u201cAnd this year we\u2019re introducing a new BIM Awards programme. This is an opportunity to recognise and reward the achievements of construction industry professionals, those pioneers who are leading the way for younger generations and to pass on the mantel of BIM-excellence.\u201d\n*The BIM Awards were originally scheduled for November 2, 2017, and were to be held in London.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/municipal-software-corporation-moves-up-on-profit-list\/","title":"Municipal Software Corporation moves up on profit list","date":1054944000000,"text":"Municipal Software Corporation, a developer of local government business process automation software and a wholly owned subsidiary of Municipal Solutions Group, Inc. is proud to announce that they are listed at # 83, a move up from last year\u2019s #94 position, on Profit magazine\u2019s List of Top 100 fastest-growing companies in Canada. The company had revenues of $4.4 million last year. Headed by co-founder Robert Bennett, the company has enjoyed a healthy five-year revenue growth rate of 692 per cent. Municipal has grown from five people in 1996 to 46 today. The Profit 100 is Canada\u2019s authoritative ranking of high-growth companies, celebrating the achievements of the best and brightest business stars. Results of the magazine\u2019s annual survey, with stories profiling top-growth leaders, are published in the June issue of Profit.\nMunicipal Software Corporation provides CityView 8.NET Enterprise, a true enterprise solution that integrates functions across local government organizations and enables e-government. CityView 8.NET helps government automate business processes such as Building, Planning, Code Enforcement, Business Licensing, Public Works, Emergency Management and more. Award-winning CityView integrates with existing systems, including financials, other departmental automation software, document management, IVR, cashiering, field data collection and GIS. With clients in 30 states and five provinces, and in business for over 20 years, Municipal Software Corporation is the one-stop provider of everything government needs for an enterprise solution, including the software, the training, the services and the post-purchase support.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/bentley-be-inspired\/","title":"Bentley Be Inspired","date":1360886400000,"text":"In November 2012 Bentley Systems held its annual meeting for press and customers in Amsterdam to celebrate its Be Inspired gala awards. AEC Magazine attended to cover the latest technology developments.\nWhile in the news the recession grinds on, there are efforts by European governments to kick-start their economies through investment in infrastructure. The UK is a case in point. London\u2019s Crossrail will be completed later this decade and plans for HS2, the high-speed rail link between London, Birmingham and Leeds, are now underway.\nIf there is one software firm that is almost guaranteed to be involved in the majority of the world\u2019s large-scale infrastructure projects it is Bentley Systems. The company covers pretty much every facet of building design, engineering and infrastructure and has developed or acquired an unenviable collection of software and technology.\nEach year Bentley hosts Be Inspired, a two-day event designed to honour the work of its many global customers.\nLast November\u2019s event stood out for the number of UK winners, all of which centred on the capital. Crossrail scooped the award for Innovation in Government, while John McAslan + Partners was rightly recognised for the fantastic work it did for the redevelopment of London\u2019s Grade I listed King\u2019s Cross Station.\nA few stops down London\u2019s Victoria tube line, Taylor Woodrow\/BAM Nuttall JV were honoured for their role in the Victoria Station Upgrade, a hugely complex project that included a series of sprayed concrete tunnels connecting new and existing parts of the station.\nRobin Partington Architects was also acknowledged for its work on Park House, a 1.04-acre city block redevelopment on the edge of Mayfair. The London practice was singled out for its use of Bentley\u2019s GenerativeComponents to solve a number of extremely difficult design challenges, including pushing the boundaries of glass bending.\nFull details of the 2012 Be Inspired Awards Winners can be found at tinyurl.com\/BE2012AEC.\nBeyond the awards, Be Inspired serves as an opportunity for Bentley to update the world press on its recent developments in technology and business.\nChief executive officer Greg Bentley gave an annual insight into the company\u2019s finances and due to its comprehensive SELECT management system, he also revealed how much design is actually going on, through the hours logged in its applications.\nDespite the economic slowdown, Bentley Systems revenues have risen 10% year on year, to $523 million. Looking at the customer usage, annual growth is back and steadily recovering, with a big increase in the number of users applying Building Information Modelling (BIM) processes to their architecture and infrastructure projects. Greg Bentley stated that out of all the hours logged, now 47% of them were what he termed, \u2018BIM\u2019 hours.\nBentley also announced a few new acquisitions, namely SpecWave for authoring AEC specifications and Microprotol, an analysis tool for pressure vessels and heat exchangers, which will add pipe stress analysis to AutoPIPE.\nBentley CONNECT\nEveryone is talking about the cloud \u2014 well every software vendor is talking about the cloud. Most customers are not and many do not even know what it is, despite relying on it everyday.\nBentley has a unique take on the cloud, mainly because it has its own global private network, or extranet, in Bentley SELECT. Customers have had on-demand access to Bentley software, training and services within their companies since 1994. The cloud, if anything just adds an extra layer to this already impressive infrastructure.\nThe CAD market was late to the world of mobile apps but now all the vendors provide applications for tablets and smart phones. Bentley offers Navigator Pano Review, ProjectWise Explorer Mobile, Structural Synchronizer View, Bentley Map mobile \u2014 to name but a few.\nWith Bentley\u2019s DNA firmly in the Windows OS camp, this foray into mobile adds support for iOS, Android, Blackberry and of course Windows 8. All flavours will always be released simultaneously says Bentley.\nBentley CONNECT is the next generation of SELECT and connects users, projects and enterprises with Bentley services through a personalised interface. Users have profiles, a message centre and community membership within CONNECT. Through the service they can download Bentley iWare apps, access support services, training, software updates, content, shared services and powerful simulation services. Additionally contractors and project participants can also be exposed to services within the Bentley Connection Space.\nOne of Bentley\u2019s key collaboration technologies, i-model, is leveraged to enhance the sharing experience within project communities. Using the i-model Composition Server, design information can be \u2018thinned out\u2019 and automatically rendered as i-models, on demand or to a planned schedule.\nAt the event Bentley launched a new i-model plug-in for Revit, enabling notoriously large models to be used in MicroStation, Bentley Navigator, AECOsim Building Designer, and additional products from Bentley and others, while retaining Revit model properties (compatible with 2010-2013 versions of Revit products).\nOn demonstration was a 100MB Revit model running in Bentley Navigator Pano for iPad. The models can be viewed and inspected on mobile, desktop or web and include support for the industry de facto standard PDF, with a new i-model plug-in. CONNECT is a complete environment for users that need to collaborate at HQ or onsite with an off-the-shelf system.\nCivils \u2014 OpenRoads technology\nOne of the biggest developments to come out of Be Inspired 2012 was the introduction of OpenRoads, a new umbrella technology that will soon form the backbone to Bentley\u2019s civil design tools \u2014 InRoads, Geopak and MX.\nOver the years there has been some cross-pollination between Bentley\u2019s three civils products, but they have largely retained their individual code-streams.\nWith the introduction of OpenRoads technology in the V8i SELECTseries 3 versions of the products this will change.\n\u201cOpenRoads technology is a common thread that runs through all of our civil products,\u201d said Ron Gant, global marketing director, civil engineering at Bentley Systems. \u201cIt is a common thread that gives us great capability in data acquisition, in immersive modelling, in design intent, in design interaction, in design time visualisation.\u201d\nDo not be fooled by the name; OpenRoads is not just for roads. While it will initially be integrated into Bentley\u2019s road design tools, there are already plans to roll it out to other areas. \u201cIt is a technology that will reach across rail, road, water and everything we do in that civil environment,\u201d explained Mr Gant. \u201cIt will even find its way into the bridge products as well.\u201d\nMr Gant detailed some of the key features of the new technology, starting off by explaining how InRoads, Geopak and MX will now share the same data acquisition tools \u2014 from point clouds and survey data to GPS, LiDAR and field data input from any kind of binary or ASCII file.\nOpenRoads will also place a big emphasis on what Bentley describes as \u2018real 3D design\u2019, while recognising the continued importance of standards-based 2D deliverables.\n\u201cThe challenge we have in our civil products is to be able to deliver smart intelligent 3D design with parametric constraints that allows you to have dependencies throughout, but also to be able to support the 2D horizontal and vertical requirements of the local and federal agencies that are controlling what your design standards are,\u201d said Mr Gant.\nAcknowledging that a lot of road design tasks are based on variations of a theme, OpenRoads technology has a major focus on repurposing design.\nThe new dynamic civil cell functionality enables users to create 2D and 3D geometric configurations, which can then be re-used and adapted to their new context.\nAccording to Mr Gant, a civil cell is anything that you can re-purpose and use again and again and define with a set of constraints and relationships. \u201cYou can build a set of dynamic parameters within an intersection so that it becomes a 3D cell. But it\u2019s more than just a cell because it has design intent. It has an engineer\u2019s decisions built into it,\u201d he explained. \u201cSo if I\u2019ve built it once I can take it and drop it into another intersection and all I\u2019ve got to do is change the changes of that intersection plus the parameters \u2014 how high is it now, where is the ground, where did it intercept the ground, is there something I need to move or be relative to?\nGant explained that relationships can also be maintained between different civil objects. Move a road and the other civil objects can move with it \u2014 a building pad, drainage pipes, or a ditch, for example.\nA new design intent capability in OpenRoads builds associations and relationships between civil elements, so if a change is made to one object, related elements will recreate themselves based on the stored relationships.\n\u201cWe\u2019ve put intelligence into software, and into the objects it\u2019s creating, that each object remembers how it was created, by whom it was created and what was created from it,\u201d says Mr Gant, who then explained the concept by way of a simple example \u2014 a stop sign located at a junction.\nWith the stop sign there are requirements in terms of how far back from the crossing road and how far clear of the approach road it needs to be. Line of sight also needs to be taken into account so road users have a clear view. \u201cIf the stop sign is placed relative to all of those objects, if the interchange moves, so will the stop sign,\u201d he said.\nDynamic cross sections, which Bentley says takes immersive modelling to the next level, enables users to adjust a cross section and any changes will automatically reflected in the 3D model.\nThe user interface has been enhanced. Tools are now available at cursor location and are context sensitive, designed to anticipate the next task, so you do not end up with a screen full of dialogue boxes. Grips now appear when hovering over geometry, making it easy to dynamically edit a road alignment, for example.\nVisualisation has also been overhauled and can be used on-demand, without having to go through an additional piece of software, says Mr Gant. As OpenRoads technology uses real world objects, users do not have to apply materials as the software automatically knows this already.\nOpenRoads is the result of a long-term strategy of Bentley to develop a common technology across all of its civil products, Gant explained how the transition has been subtle so as to eventually ease users gently into one common product.\n\u201cEvolution not revolution,\u201d he said, proud of the fact that Bentley has never suddenly pulled the plug on any of its civils products.\nHaving acquired the civil division of Intergraph (InRoads) in 2000, Geopak in 2001 and MX in 2003, some might argue that it has taken Bentley too long for this transition. Chief operating officer and senior-vice president Malcolm Walter admitted that OpenRoads should probably have been done earlier, adding that rationalisation of products is now very much on the agenda for Bentley.\n\u201cWe recognise that having a broad portfolio is great but, when there\u2019s redundancies and overlaps, that can present some confusion to users,\u201d added Huw Roberts, vice president, core marketing at Bentley Systems. \u201cSo where we see that, that\u2019s what we want to tackle first and OpenRoads in a great example.\u201d\nWhile Mr Roberts makes the point that Bentley is a provider of solutions, highlighting the flexibility of the Bentley Passport subscription programme, the company has already begun thinking about rationalising its crowded structural portfolio. Bentley\u2019s Integrated Structural Modeling (ISM), facilitated by Bentley\u2019s i-model technology, has done an excellent job of streamlining the flow of structural information between its key structural applications, but there is still room for consolidation, admits Bentley.\n\u201cWe\u2019ve got our fantastic STAAD product line, our RAM product line, there are some parts that overlap,\u201d explained Roberts.\n\u201cWe have to rationalise RAM and STAAD and we have to figure that out. It\u2019s really useful that the company is now agreeing that we need to do that. It\u2019s an implicit thing to reduce those things and now it\u2019s getting done,\u201d added Mr Walter.\nConstruction monitoring\nFaraz Ravi, director of product management for point clouds at Bentley Systems, gave an update on the point cloud functionality built into the MicroStation platform, the ProjectWise point cloud streaming service and the recently released Bentley Pointools V8i.\nIn the September \/ October 2012 edition of AEC Magazine we spoke with Mr Ravi at length about how Bentley now recognises point clouds to be a fundamental dataset. At Be Together 2012 he introduced a new technology, construction monitoring, which uses point clouds to record and monitor construction progress.\nConstruction monitoring works by comparing laser scans at various stages of the construction process. Laser scans can also be compared to the original BIM model to help ensure that the project retains the original design intent.\n\u201cThis is a new technology that\u2019s we\u2019re developing for Bentley Pointools V8i and will also be migrated to the MicroStation platform,\u201d said Mr Ravi. \u201cI think it\u2019s going to greatly advance the use of point clouds for construction. We are able to identify what changes have been made from one state to another \u2014 what has been newly constructed, what has been newly demolished, and so on.\u201d\nRavi also explained the importance of capturing any changes that do occur during the construction process \u2014 issues that may have been solved on site, and not recorded in the original design model.\n\u201cAlthough we try to minimise [these changes] they are typical in any complex project. The key thing is we want to capture it and understand what happened.\u201d\nRavi envisages that laser scanners could become permanent fixtures on construction sites \u2014 not only used to help maintain design intent, but for sign off and to provide an audit trail to protect against litigation.\nWhen construction has finished the end result would be an up-to-date asset model. This could be an edited BIM model, a laser scan model or a combination of the two, said Mr Ravi.\nConstruction monitoring is currently just a toolset but AEC Magazine asked Mr Ravi if Bentley is looking to link into any of its construction scheduling software. \u201cAt the moment it\u2019s just technology and we\u2019re going to put it into Bentley Pointools V8i, but we\u2019re also migrating that into the platform [MicroStation] and once it\u2019s in the platform then any vertical could take advantage of it. ConstructSim for example, could do just that.\u201d\nTrimble\nIn the week prior to Be Together, Trimble and Bentley announced the formation of a strategic alliance designed to connect the virtual and physical environments for infrastructure projects.\nThe idea is as follows: Trimble provides the field positioning technologies such as robotic total stations, 3D laser scanners, and global navigation satellite system (GNSS) positioning solutions. Bentley provides the information modelling software while work sharing and dynamic feedback is securely managed in Bentley ProjectWise.\nThe possibilities are huge: feedback loops from as-built construction to Building Information Modelling (BIM) using technologies such as laser scanning; streamlined 3D design to construction site workflows for GPS controlled grading.\nThe two companies have worked closely together before, with Trimble subsidiary Tekla supporting the ISM (Integrated Structural Modelling) methodology to share structural BIM models.\nSpeaking with AEC Magazine, Malcolm Walter shared his vision of where this partnership could lead, using the panoramic capability of an iPad to deliver an augmented reality experience.\n\u201cWhat I want to imagine is you would walk into this room, or a construction site, or anything that has been \u2018sensored\u2019, and you\u2019d be able to look at what\u2019s behind the wall,\u201d he said. \u201cBeing able to know what rebar is in a pillar so that I can know where I can drill through. To be able to look through things because it knows exactly where you are on the planet, it knows exactly where that is, and it can overlay the drawings because they are geo-coordinated too.\nBhupinder Singh, senior vice-president, Bentley Software, gave a glimpse of this capability at Be Inspired 2011. At the time Mr Singh said that in all augmented reality applications, the accuracy of the GPS is the big challenge but Bentley has \u201cmanaged to figure it out\u201d. It would now appear that the Bentley \/ Trimble partnership may have already been in the pipeline back then.\nMr Walter believes that augmented reality will change the way \u201cwe interact with our facilities\u201d, but that it will only become truly useful when you can have cm and mm accuracy, which you are not going to get from typical GPS \u2014 instead relying on a fully-sensored environment.\nHowever, Huw Roberts believes the required accuracy can be achieved through a number of sensing devices. \u201cThey [Trimble] have all sorts of automated laser layout systems; they have querying systems, they have laser measures and identifiers,\u201d he said. \u201cSo, if you have a Trimble device and you come into this room, and you shoot that corner and you shoot that corner and now you\u2019re registered to where you are in this room.\n\u201cThere is no sensor in this building, but I have a model of this building and I have a point cloud skin of this building and I have this digital cursor with the Trimble device and I can align these with each other.\n\u201cSo Greg [Bentley] calls it, the Trimble device is your cursor in the physical world, into the virtual world in the Bentley environment. Once you\u2019re connected to that \u2014 what\u2019s the cost, what\u2019s it made of, what\u2019s inside it, what\u2019s its engineering stress level, who installed it, when was it last maintained, all of those things become possible.\u201d\nAECOsim update\nLast year Bentley combined its BIM applications for building design (architectural, structural, mechanical and electrical) into AECOsim Building Designer. This allows anyone in a project to have access to Bentley\u2019s suite of tools and BIM objects, created in a model. Clash detection was improved and hypermodelling was added to create links between models and drawing sheets.\nAECOsim has direct links to Bentley\u2019s RAM and STAAD analysis tools and works with AECOsim Energy Simulator .\nThe November 2012 event saw the first update to AECOsim Building Designer with a raft of new features, updates and fixes. Of special note are 100 new components for HVAC, fire protection, plumbing and drainage. There is support for ISM 3.0, which is Bentley\u2019s excellent structural interoperability solution. There is also a new \u2018Microstation mode\u2019, which defaults to the standard MicroStation interface and wall area centre gross and net have been added for quantification.\nBentley Institute Press has a comprehensive new book called Practical Architectural Modelling with AECOsim Designer written by the UK\u2019s very own BIM guru, Nigel Davies. (tinyurl.com\/AECObook)\nAutomating Hypermodels\nIntroduced at Be Inspired 2011, Hypermodels act as a 3D index for all related project documentation. Fabrication drawings, quantity takeoffs, connection details, RFIs, schedules, images and AVIs can all be embedded in the interactive 3D model. But Gant believes Bentley is only scratching the surface of what can be achieved, estimating that Bentley is only 40% of the way to fulfilling the true potential of the technology.\nAt the moment, creating a Hypermodel is very much a manual process, as Mr Gant explains. \u201cIf I\u2019ve got a set of specifications, and I\u2019ve got a manhole [which has] a standard sheet, then some draughtsperson or engineer has to go and make a link to that.\u201d\nBut he believes this process is ripe for automation. \u201cWhat I see for the vision for the future is that an object type always has its specification assigned so that every time I create one, and set one, every time it goes into the ProjectWise database it grabs every associated file that goes with that and builds that model for me \u2014 that Hypermodel.\u201d\nFactory design\nBentley also announced a strategic collaboration agreement with Siemens to advance the integration of digital product design and manufacturing processes design with information modelling for factory lifecycle design. This work is an extension of earlier collaborations between the two organisations that resulted in each deploying technology offerings developed by the other.\nIn the longer term, the companies will explore opportunities for jointly developed technology to expand industry-centric solutions and further collaboration interoperability between Siemens\u2019 Teamcenter and Bentley\u2019s ProjectWise software.\nConclusion\nWhile many in the industry are expecting the move to cloud-based solutions to be an \u2018extinction level event\u2019 for some big names in CAD, I do not think there is any other AECO CAD firm that this move would benefit as much as Bentley Systems. Extending the data reach to mobile devices and updating its SELECT infrastructure places the firm well ahead of the curve in offering point solutions, backbone (ProjectWise) and managed collaboration. It is an extra layer to what the firm already has versus a whole new programming and data storage model.\nThe big fight is going to be in the civils and construction market, that is where the dollars are being spent, and where all the AECO software firms are making the biggest improvements. The new OpenRoads technology, the addition of excellent point cloud capabilities and a close working partnership with Trimble demonstrate that Bentley Systems is preparing to take a bigger slice of the infrastructure software market, while holding off Autodesk, who has recently stepped up its focus on civils. Trimble and Bentley seem to make good bedfellows in both of their key markets. Tongues are already wagging.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/big-in-bim-bond-bryan-digital\/","title":"Big in BIM - Bond Bryan Digital","date":1489536000000,"text":"While the adoption of BIM is growing, many practices still struggle to fully adapt to new processes. Bond Bryan, by contrast, has embedded BIM at its core \u2013 and has been able to expand into offering digital services to other companies as a direct result, writes Martyn Day\nIn the early days of BIM, its key selling point (at least according to software vendors) was that it gave users a 3D model that would automatically generate coordinated section and elevation drawings, even while editing.\nNot only did this woefully undersell the benefit of the \u2018I\u2019 in BIM, it also misled many architects into thinking that BIM is a replacement for CAD \u2014 which it isn\u2019t. The industry has been struggling with this misconception for ten years.\nBIM\u2019s true value is becoming clearer now. The UK government\u2019s requirement for all publicly procured projects from 4 April 2016 to be completed in BIM has required a herculean effort on behalf of companies trying to adhere to rapidly developed national standards, to collaborate effectively, to deliver data at the right times, in the right formats. But from that effort has come a wealth of new insights.\nIn short, modelling and generating sections and elevations are not where the true value of BIM lies. To achieve BIM Level 2, the focus must instead be applied to the management, definition, and coordination of project data.\nThis isn\u2019t something that comes \u2018out of the box\u2019 from any vendor. It requires knowledgeable and experienced personnel, using their skills and insights on subjects such as IFC (Industry Foundation Class), data structures, data wrangling and so on. Companies must also understand what they hope to achieve from adopting these processes.\nIn this respect, Bond Bryan is ahead of the game. The firm switched to using BIM way back in 1994 and, in addition to creating exciting buildings, now plays an active role in educating clients and sharing its BIM expertise with the wider industry. It has recently expanded to offer a range of digital services, through which it shares its over two decades\u2019 worth of project-proven BIM expertise.\nIntroducing Bond Bryan\nToday, with offices in London, Sheffield, Birmingham and Kuwait, Bond Bryan is an award-winning, research-led practice offering architectural, interior and landscape design. It offers services ranging from strategic estates advice, master planning, feasibility studies and BIM-related digital services, too.\nIts staff of 120 people has significant expertise in education, research, manufacturing and workplace environments. It works on public-sector and commercial projects, to create mixed-use, commercial, residential and retail developments. It enjoys a strong reputation in the further and higher education sectors, having worked with over 50 major institutions, on projects with a combined value of well over \u00a3500 million. These include the Glasgow School of Art and the Universities of Sheffield, Nottingham, Chichester and West London.\nBIM lies at the core of the company\u2019s coordinated design process. It uses Graphisoft\u2019s ArchiCAD as its core BIM weapon of choice. The practice\u2019s expertise in collaboration has led to work with Bam Construct, Willmott Dixon, Wates and Balfour Beatty.\nArchiCAD BIM\nIn the UK, Bond Bryan is seen as a poster child for its expert use of ArchiCAD, and even had its work, The David Hockney Building for Bradford College, featured as a signature building on the box of ArchiCAD 19.\nThe firm has used ArchiCAD since 1994 and has become closely involved in Graphisoft\u2019s development programme, feeding back feature requests and testing beta versions of forthcoming releases. The firm has also presented at a number of Graphisoft events, including openBIM 3D, BIM in Practice and Key Client Conference, as well as giving talks at the BIM Show Live and RTC.\nIt therefore follows that Bond Bryan was one of the first Graphisoft clients to deploy the company\u2019s BIMcloud collaboration technology, enabling its design teams to share models through the cloud, while enabling easier management of projects, templates and content in a centralised location. The company currently uses version 19 but will be upgrading to version 21 once it\u2019s released.\nDigital services\nHaving built such a wealth of internal expertise in information handling, the firm has added digital services to its business model. The company says it prefers to think of BIM as standing for \u2018Better Information Management\u2019 and aims to assist clients in building standard processes that benefit all parties involved in the design, construction and maintenance of built assets.\nBond Bryan Digital\u2019s information management services offer firms assistance in achieving BIM Level 2 capability, working towards BIM level 3 (due in 2020), helping to define Asset Information Requirements (AIR) and Employer\u2019s Information Requirements (EIR), together with providing documentation management, coordination and processes. These are based around open standards, to support design and construction teams, with a focus of delivering high-quality clean data to other post-occupancy fields such as facilities management and lifecycle asset management. Bond Bryan Digital has delivered complete BIM execution plans to clients, from analysing complete supply chains to providing common data environments with model validation for COBie production [Construction Operations Building Information Exchange].\nDespite the UK having tens of thousands of architects and thousands of architectural firms, the BIM hardcore is still quite a small, tight-knit community of advocates, practicing what they preach and trying to spread the BIM religion by handing on their dark arts, a bit like Jedi Knights but without the robes.\nRob Jackson, BIM expert\nEven with social media sites like Twitter as places to exchange information and so many events and forums at which BIM advocates share their knowledge, some individuals still stand out for their passion and knowledge.\nRob Jackson, associate director at Bond Bryan Digital (@bondbryanBIM) is one of those experts. He\u2019s been championing the concept of BIM, data openness and collaboration from its earliest days and has specifically examined and implemented standards for BIM data.\nThrough Bond Bryan Digital\u2019s blog, Jackson has come up with a novel idea on how to explain the intricacies of BIM through the medium of Lego, having modelled Le Corbusier\u2019s Villa Savoye and used it to explain core BIM concepts.\nTalk to Rob Jackson for long enough and you will be left with no doubt that BIM software, while improving, is far from perfect. The inherent inefficiencies with which the AEC space has struggled are still ever-present. This is a challenge \u2013 and Jackson likes to solve problems.\nBIM\u2019s data problems start with the basics. Firms that don\u2019t have standards are inconsistent and don\u2019t check the quality of their work. In many countries, there are now emerging standards with which to comply. Here in the UK, for example, a key deliverable is COBie, a non-proprietary structured format for quantity, location, installation and maintenance of building assets. Ideally, designers would understand these requirements.\nThen there is the software developer\u2019s interpretation of this format, which can be very hit-or-miss. Some packages offer a COBie export button, for example. Users click on this and think they have complied with the standard. But this function may be fundamentally limited by how the data is structured in the core architecture of the BIM software, which may not have the granularity for proper definition, or may simply not have a correlating data field.\nAdded to all this, software developers insist on upgrading and changing their applications every year, or even more frequently, which can change the way they structure data. National standards continue to evolve, sometimes at a bewildering pace.\nThe one thing users can be sure of is that simply clicking on an COBie export button will not pass a data integrity test. Things get even more messy when they need to generate a COBie drop from federated databases, which originate from software created by different developers from different firms and with different standards, some of which may not quality-check their output. As Jackson explains, \u201cI see my role as helping designers deliver the right information, and by developing a process, you consistently deliver models created to standards. It\u2019s easier to tackle at source, than [to] fix when we get to the stage of merging IFCs. It\u2019s all about getting information to the client that is good and to do that consistently. However in BIM, everything is constantly changing, it\u2019s a never-ending cycle, it\u2019s constantly moving \u2014 but we need to establish levels of confidence.\u201d\nAcquiring this knowledge may mean breaking things: Jackson regularly carries out geometry and data tests on many industry tools, to identify what elements go in but don\u2019t come out, to understand the limitations and to seek out remedies.\nWith so much BIM data flying around between authoring and visualisation tools, common data environments, COBie, IFC readers and so on, it can be surprising what data gets filtered out in the process. BIM data exchange is rarely a lossless process. Jackson and his team are so skilled at this that they\u2019re hired by software vendors to test their code.\n\u201cThe industry is struggling to deliver consistent reliable information and it\u2019s no surprise,\u201d says Jackson. \u201cFor instance, Autodesk Revit has an \u2018IFC out\u2019 and a \u2018COBie out\u2019, when COBie is a subset of IFC \u2014 but they are developed by different teams. \u201cThen there are user inconsistencies. We have worked on projects where all participants are using the same authoring tool, but there are huge discrepancies. We end up spending a lot of time mapping stuff. We have to clean it up into a format that is usable.\u201d\nHere, Bond Bryan Digital\u2019s weapon of choice is Solibri, Jackson explains. \u201cAs far as we are concerned, working in an Open BIM environment is about conforming to IFC, COBie and BCF formats. We perform element classification tests, merge federated data, perform clash detection and build rules to validate models using Solibri. We make sure we achieve high-quality information take-off and create filters on classifications to check the quality of data.\u201d\nWhile Solibri acts as a common data environment for wrangling IFC data for validation, Bond Bryan Digital is currently investigating the quickly developing world of generic BIM common data environments, with clients using disparate systems from Dropbox and Buzzsaw to dRofus, which Nemetschek acquired at the end of last year. Jackson has yet to find a truly outstanding one, but is increasing his exposure to the options available by working with clients\u2019 different choices.\nWith so many firms struggling to move BIM data around their offices, I asked Jackson about Bond Bryan\u2019s own set-up. \u201cWe have ArchiCAD BIMcloud across all offices. We have never required data compressors. As to file sizes, I don\u2019t really know anymore; with BIMcloud, I don\u2019t see them and BIMcloud just sends the data that changes, not the whole model.\n\u201cI guess as some indication, our IFCs get to be 150MB. We use standard off-the-shelf Macs. Nobody complains about speed. When creating renderings is perhaps the only time users wait but, day to day, the computer operates faster than the users. Speed is not an issue on our project work. If you asked our users for their top 20 issues, speed would be at the bottom. The number one issue would be the complexity of my templates \u2013 I won\u2019t deny it!\u201d\nBond Bryan has a single Revit Suite and while the firm has considered setting up a small Revit team in the past, it decided that supporting two BIM tools was crazy.\n\u201cI think ArchiCAD is the best design tool as it\u2019s less structured,\u201d says Jackson. \u201cRevit is very structured, so great as an engineering tool, but as an architectural tool, I would personally rather create workflows in ArchiCAD as you can have flexibility, while still being able to produce structured data.\u201d\nOne of my regular questions to BIM experts is how much 2D work they do to their auto-generated sections. The more 2D detail is added, the less benefit they get from automatic coordinated updates when the model changes.\nAccording to Jackson, \u201cWe embellish the model with 2D over the top, I guess to make it what people would expect the output to be. Everything we try not to do! It\u2019s [about] level of information, which goes back to not trusting contractors, adding detailed specifications to the drawing to ensure everything is unambiguous. There\u2019s always a fear that if you don\u2019t add that detail, mistakes will happen. But I\u2019d love the industry to move away from being drawing-based.\u201d\nWhen it comes to hiring staff, does Bond Bryan automatically look for experienced ArchiCAD users? \u201cObviously, at the moment, the industry is massively constrained with a skills shortage. We don\u2019t advertise for ArchiCAD users specifically, we just look for good architects or technicians. As far as we are concerned, those are the core criteria and our chosen authoring tool, ArchiCAD, can be learnt in a week. It\u2019s all about attitude and mindset.\n\u201cWe get a lot of interest from young students, because we go out and give lectures and talks. They see what we\u2019re doing and the level of expertise we have with the tools and [they] approach us,\u201d he says.\nEvolving BIM\nIn many ways, the BIM market is still evolving, trying to sort itself out. The problems include proprietary file formats, inconsistent data formatting, poor compliance to data formats, low levels of openness and users who probably don\u2019t achieve good quality or standards in 2D, then going on to replicate bad practices and bad habits in BIM.\nWhile there has been a rush to adopt BIM, there hasn\u2019t necessarily been a rush to adapt to it, or spend much time ensuring that the data that gets delivered is correct. Again, just because your software has a COBie export button, that doesn\u2019t mean it conforms to the standard.\nArchitectural practices need a new breed of people \u2014 IT-savvy people who have the time and interest to experiment in order to address shortfalls in software and the skill levels they encounter in their dealings with collaborators and clients. Many firms are setting up internal R&D teams to ease the deployment of new technology and to wrangle data. From talking with Rob Jackson, it seems clear that firms need to identify their natural puzzle- solvers to drive standards internally and interface with federated teams.\nIt\u2019s also clear that getting ahead in BIM knowledge opens new business opportunities. Bond Bryan can now bid on a number of levels: design, strategy, coordination, quality assessment and many others. By working with clients in a wider range of capacities, other opportunities arise and the company acquires even more knowledge. In other words, it\u2019s a virtuous cycle.\nLooking at BIM from a data-centric viewpoint requires asking different questions of internal processes and introducing a culture of self-checking. Creating a model and coordinated drawings really is the most elementary part of BIM. The bigger picture is getting a handle on all the data that BIM models possess and fathoming out how to get that data in the right format, in order to meaningfully share it with someone \u2014 as a schedule, as an IFC, as a COBie sheet, or whatever else is required.\nIt\u2019s not enough to be using the same BIM modelling tool: inconsistent standards, training and quality can wreak havoc on a collaborative project in the design and construction phases and can go on to negatively impact the occupancy phase. Add to the mix constantly evolving software, often released on yearly upgrade cycles, and the shifting sands of national and international BIM standards, and you\u2019ve got an even bigger challenge with which to contend.\nBut tackling that challenge \u2014 and doing it smartly \u2014 is exactly how Bond Bryan Digital has found its niche.\nThe Bond Bryan Digital Blog\nBond Bryan Digital\u2019s blog is a mixture of practice news and technology updates \u2013 but it\u2019s also where Rob Jackson has created a series of very informative and easy-to-comprehend explanations of BIM through the medium of LEGO and Le Corbusier\u2019s Villa Savoye (featured on the front cover of this edition of AEC Magazine).\nThe series, now 22 posts in length, offers advice to clients and architects, as well as BIM laypeople, covering all the key concepts of BIM.\nThese make it clear that, while many people continue to think of a model and its geometry as an end in itself, these really are only the start of many important processes that deliver value throughout a building\u2019s lifecycle.\nWith each entry, Jackson explains the concepts, processes and benefits of modelling, model views, visualisation, object libraries, Levels of Detail (LODs), augmented reality, schedules, and so on.\nThe series is a great starting point for any beginner to the concept of BIM. Our understanding is that Jackson intends to continue with the series and has even bigger plans for additional LEGO-based informative materials.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/construction\/bentley-systems-acquires-voice-based-site-automation-developer\/","title":"Bentley Systems acquires voice-based site automation developer","date":1591315200000,"text":"NoteVault\u2019s technology to be integrated with Synchro to feed live site updates into 4D model\nBentley Systems has acquired NoteVault, a developer of mobile construction management software which is used to capture site information using the power of voice.\nThrough the acquisition, Bentley plans to expand its Synchro digital construction environment with mobile field applications to track and manage labour, materials, and equipment. Immersive 4D models of the construction progress will be combined with detailed up-to-date reports on resource expenditures, enabling more effective management of cost, schedule, and risk.\nNoteVault\u2019s automated speech-to-text technology stands out because it has been specifically developed to recognise construction language for accurate transcription. It also enables automated translation, so non-native English speakers can submit daily reports more easily using their native language.\n\u201cDigital twins continue to transform the way projects are delivered and operated. Inherent in every digital twin is a stream of continuously updated data, and for construction, automating the semantic interpretation of field reports can now be one of the richest sources of live project information,\u201d said Dustin Parkman, vice president, project delivery, Bentley Systems.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/taking-bim-out-of-the-specialist-only-zone\/","title":"Taking BIM out of the \u2018specialist-only\u2019 zone","date":1563840000000,"text":"At Willmott Dixon, visual collaboration software Revizto is bringing projects to life for wide groups of stakeholders, generating ideas, opinions and insights that will help them run more smoothly, writes Jessica Twentyman\nAt UK construction company Willmott Dixon, the fusion of BIM models with reality-capture data is transforming project planning and review meetings, sparking new insights and driving new conversations.\nThat\u2019s what happens when the results of this powerful combination can be presented to even the most non-technical of users in a compelling, understandable visual format, delivered directly to their familiar mobile device or via an immersive virtual reality (VR) experience, according to Antony Brophy, senior digital manager at Willmott Dixon.\nThe technology at work here is Revizto, he explains, which fuses multiple data sources together in one collaborative environment to make it simple for all stakeholders to understand a project, to help spot issues and to track them through to resolution.\nIn Willmott Dixon\u2019s case, the BIM data comes largely from Autodesk Revit; the reality-capture data of the site is from 3D scanners and drones, imported into Revizto as point cloud and mesh data. The end-user viewing device is most frequently an Apple iPad, although a wide range of other mobile devices are used too, as well as Oculus Rift head-mounted displays (HMDs). Either way, the aim is the same, says Brophy:\n\u201cIt\u2019s about opening up 3D models and making them accessible to more people, so that they can understand a project site and what\u2019s happening there and you can benefit from their ideas and opinions,\u201d he says. \u201cWe don\u2019t want to restrict anyone with a useful point of view from seeing what we see. Revizto helps us take BIM out of a \u2018specialist-only zone\u2019.\u201d\nComments raised in meetings, especially those that focus on issues, can easily be captured, stored and shared through Revizto\u2019s mark-up tools and subsequently tracked through to resolution. The system\u2019s search engine, meanwhile, makes it easier to locate these issues (and all information relating to them) further down the line.\nIt\u2019s a democratic approach that works very well in discussions with clients, according to Brophy. \u201cWe can show them what they\u2019ve got on the site already, what will be new there, and we can take them on a guided tour around the model,\u201d says Brophy. \u201cThey can float into a Revit model, take a look around, and where we\u2019re incorporating that drone data, they can look out the windows, see what the view will be like \u2013 and this could be a year or more before we even build anything.\u201d\nWe don\u2019t want to restrict anyone with a useful point of view from seeing what we see. Revizto helps us take BIM out of a \u2018specialist-only zone\u2019\nAntony Brophy, senior digital manager, Wlilmott Dixon\nWhile Oculus Rift headsets have proved a popular way to showcase planned projects to clients, in a recent technology pilot, clients were walked through a virtual model of their project projected onto the interior wall of the Soluis Reality Portal, for an ultra-immersive, joint exploration. \u201cIt was a superb experience for everyone involved and it\u2019s really got me thinking about using that kind of technology more in future,\u201d says Brophy. \u201cIt\u2019s a great way to take people out of the silo of a VR headset.\u201d\nInternal collaboration time\nRevizto is proving equally valuable for overcoming internal collaboration hurdles, especially in conversations that involve office-based workers and onsite teams.\nIt was used, for example, on a recent hospital project, which involves a lot of refurbishment work, as well as some rebuilding. Plant room refurbishment is an important part of the plan here, making it vital to understand before the project began what equipment was already installed in three different plant rooms across the site. The best way to capture that information was via 3D scanning, in order to create a point cloud that was brought into Revit and then shared via Revizto to various MEP and design teams involved in the planning stages.\nSays Brophy: \u201cThis allowed us to basically have the discussions around where we needed to put the handling units, the pump sets and so on. We also needed to understand which pipework could and couldn\u2019t be stripped out, because obviously, this is a hospital that needs to be kept running, so we needed to find ways of doing that. And then we looked at where the new work should start and how the rip-out should begin, so we could plan that all ahead of time,\u201d he says.\n\u201cIt was a big eye-opener for the team to realise that we\u2019ve got a model designed in Revit for MEP, plonked in the middle of a point cloud, allowing everyone to clearly see where the issues were likely to be. It allowed us a bit of forward-thinking before anyone even got to the site.\u201d\nMore recently, as part of that same hospital project, drone data has been integrated with Revit, in order to capture an accurate record of the condition of the roof where work will be taking place, so when it comes to handover, Willmott Dixon will see what dilapidation was evident when the project began, compared to what was handed over.\nReality mesh data captured by drones has also been integrated with design data from both Revit and AutoCAD in the case of an entirely new building, intended as student accommodation. \u201cThe idea here is can we get a feel for a building from the very earliest planning stages \u2013 from the outside, from the inside, looking out the windows. The fact that you can experience a building in its context and its surroundings before it\u2019s even begun is amazing.\u201d\nMoving forwards\nWillmott Dixon currently has an allocation of 250 licences for Revizto and regular usage now accounts for around 200 of these. These include employees working on the planning and construction phases of projects, as well as employees in the field and construction managers at the handover stage. Members of Brophy\u2019s technical team are among the heaviest users, he says. \u201cAs the firm wins new work, projects come in through the doors and we\u2019ll generally push them up into Revizto to show a wider audience in the business and get them talking about the project. From there, the specific project team takes over, taking ownership of pushing their models to Revizto as and when required. So we get them started, but they take it from there.\u201d\nMore recently, the firm has started loading images captured on a Ricoh Theta V 360-degree camera for the aftercare process. The data lacks the detail of a laser scan, but it\u2019s much quicker to capture and doesn\u2019t require specialist skills. One example of this is in the installation of fire doors in a new school, for which a checklist needs to be created to ensure the right seals and door furniture are in place as the installation progresses.\nA 360 photo was taken in every room in the school, taking just two days to complete the task for three separate buildings. Each photo is attached to its respective room in Revizto, as a 360 stamp, and for each stamp, a task is created for the aftercare team. Subsequent progress on the installation can be captured in incremental 360 photos and attached in the same way, creating a complete record.\n\u201cThis is still in the R&D stage at the moment, but the question we\u2019re looking to answer is, \u2018Can we do this as the norm?\u2019,\u201d says Brophy. \u201cAnd the comments that we\u2019re getting back from users is that they\u2019re loving it. They can see it in the field, on an iPad and that really works for them.\u201d\nHe can see this working for many different aspects of construction work. If issues arise some time down the line, the firm has a complete record, in the form of these 360 photos, of what was done and when \u2013 especially in the case of features, such as services ducts, that may now be hidden behind walls or under floors.\nIn terms of VR, Brophy is now toying with the idea of creating stereoscopic views for groups of stakeholders. The idea here is to equip them with 3D glasses that they would use to watch a walk-through together on a large screen. \u201cSo if we had a Revizto-enabled stereoscopic view, people could sit together at a screen, ten people in a meeting room, for example, pop on their glasses and they\u2019re all immersed in it together,\u201d he says. \u201cI love the simplicity of that. There\u2019s obviously technicalities to be resolved around that, in terms of software compatibilities, but that\u2019s where ideally I\u2019d like the technology to go.\u201d\nFor him, it\u2019s the perfect counter to the downside of VR, in that participants are immersed in their own experience and cut off from those around them. In short, VR can be a highly individual, but potentially isolating experience. The 3D glasses approach, he believes, has the potential to make it far more collaborative, in a relatively low-cost way.\n\u201cGiven that we\u2019ve already got large screens on all of the sites these days, it\u2019s a logical next step,\u201d he says. And another step, it seems, on Brophy\u2019s mission to bring construction issues \u2013 and their resolution \u2013 to life for as wide a group of stakeholders as possible \u2013 and benefit from a wider and earlier pool of ideas, insights and opinions as a result.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/vr-treadmill-used-to-explore-virtual-worlds-on-foot\/","title":"NEWS: VR treadmill allows virtual worlds to be explored on foot","date":1437350400000,"text":"New facilities in Milton Keynes to explore the challenges of sharing public walkways with autonomous vehicles\nThe Transport Systems Catapult (TSC) has opened a \u2018Visualisation laboratory\u2019 in Milton Keynes. The new facilities will allow designers and engineers to use Virtual Reality to test out new transport technology networks and explore proposed buildings and civil engineering projects.\nThe laboratory includes the UK\u2019s first commercially available omnidirectional treadmill built by Swedish company Omnifinity. The 6 metre wide Onmideck6 is mated to an Oculus Rift DK2 VR headset to allow users to walk at freedom in any direction within a Virtual Reality environment.\nOne of the first projects to be undertaken in the Visualisation Laboratory is a joint project between the Catapult, Omnifinity and Virtual Viewing, which explores the challenges of sharing public walkways with autonomous vehicles.\nVisitors can walk inside a virtual Milton Keynes populated with pedestrians and Autonomous Pods from the Lutz Pathfinder project.\nBy collecting information about the physical responses and subjective responses when subjects come into close proximity with the pods in VR, it may be possible to identify how to improve the Pod\u2019s control algorithms. For example how the Pod responds when it detects a human on the path, how it indicates this externally and how it changes its direction.\nOther planned applications for the Omnideck6 include investigating and signing off the designs of buildings and big civil engineering constructions.\nProviding the ability to walk around the outside and inside a new structure will help decision makers to better understand the scale and context of an engineered design, says TSC.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/christmas-comes-early-at-epic-games\/","title":"Christmas comes early for users of Unreal Engine as Datasmith made free","date":1573516800000,"text":"The developer of Unreal Engine makes Datasmith free forever, Twinmotion free until 2020 and makes Megascans assets free to use. Greg Corke talks with Epic Games\u2019 Marc Petit about the big giveaway.\nEpic Games is continuing its aggressive push into the AEC sector with the news today that Datasmith, its toolkit for importing and prepping CAD and BIM data for use in Unreal Engine, will now be completely free.\nPreviously, Datasmith was part of Unreal Studio, which was about to come out of free beta and cost $49\/month on subscription. But now it will be a core part of the free Unreal Engine, starting with version 4.24, due for release next month. Other parts of Unreal Studio, including static mesh editing, and the variant manager, which allows users to switch between different configurations at the click of a button, will also be brought into the core engine.\nEpic Games has also extended the free availability of Twinmotion, its architect-friendly real time viz tool based on Unreal Engine, that the company previously said would only remain free until November 2019. AEC firms will now be able to download the software for free until the next release, which is anticipated to ship in the first quarter of 2020. Once downloaded, the free version can be used indefinitely.\nMarc Petit, the general manager of Unreal Engine\u2019s Enterprise business, told AEC Magazine that there have been 300,000 downloads of Twinmotion in six months, with only a 25% overlap with the 350,000 users that have downloaded Unreal Studio.\nEpic Games acquired Twinmotion from Abvent in May 2019 and then promptly gave it away for free. Before then, it cost \u20ac1,650 a licence, a price that Petit admits, despite the good download figures, has been a barrier to adoption. \u201cSome people are worried about the price, because it used to be an expensive piece of software,\u201d he says. \u201c[The larger firms] are testing it, but not deploying it en masse.\u201d\nBut this will change, says Petit. He is keen to stress that when Epic Games starts charging for the software, it will be \u201cconsiderably cheaper than anything out there.\u201d\n\u201cOur commitment is to keep Twinmotion the most accessible and the most affordable visualisation solution,\u201d he says.\nPetit did not share prices, but when one considers that real-time viz tool Enscape starts at $469 per year, we imagine it could be as little as a few hundred dollars per year.\nThe 2020 version will also bring new functionality to the software. There\u2019ll be a direct link to Rhino and a direct export to Unreal Engine. This is an important piece of the jigsaw as it will give AEC firms a viz workflow from early design validation all the way through to marketing and sales.\nThere\u2019ll also be tools to facilitate collaborative workflows and much improved visual fidelity, as Petit explains. \u201cWe have upgraded all of the assets \u2013 we have upgraded the characters, we have upgraded the vegetation. It really looks beautiful,\u201d he says.\nPetit did not mention support for Nvidia RTX real time ray tracing, but we know this is on the roadmap and would be surprised if it wasn\u2019t included in the new release.\nEpic Games also has some important developments for the core engine. Release 4.24 will add enhanced support for mobile and AR devices. Also, for the first time, users can model inside Unreal. \u201cYou can actually create geometry from within the engine, so it makes it a more rounded DCC tool,\u201d says Petit, adding that users will also be able to import point clouds.\nThe new release will also include visual data prep, which Petit describes as a \u201ckind of completion for Datasmith.\u201d\n\u201cYou can actually automate the process of converting your CAD, BIM or GIS data into a digital twin,\u201d he says. \u201cAll the functions are scriptable and automatable. Of course, you still have to do the recipes yourself, but then the whole process can be automated. And using our visual scripting interface it\u2019s very easy for people.\n\u201cFor example, you could have all the urinals in the BIM file, and you want to strip them out [to get performance in VR] \u2013 all of those processes can now be automated.\u201d\nImportantly, all of the functionality within Unreal Engine is available as source code, so third parties can build applications on top of it. Off the record, we\u2019ve heard about some very interesting tools in development.\nBringing scenes to life\nEpic Games is not stopping with software; it recognises that assets are extremely important to viz workflows. Today, the company has announced the acquisition of Quixel, a Swedish firm that owns Megascans, a huge library of 2D and 3D photogrammetry assets that are used by leading game developers, filmmakers, and visualisation specialists.\nAll 10,000 library assets \u2013 which include anything from concrete and pipeworks to fabrics and plants \u2013 will be made available for free, for use within Unreal Engine. Ten high-resolution packs can be downloaded now on the Unreal Engine Marketplace and the rest will be made available before the 4.24 release in December.\n\u201cFor architecture, it\u2019s a big piece of news because a lot of people were buying Megascans to create nice environments for visualisation,\u201d says Petit. \u201cWhen you download Unreal Engine 4.24 in December, it\u2019s going to feel like Christmas. Not only you get all of the software for free, but you also get the assets now.\u201d\nBut will Megascans be available for TwinMotion? \u201cNot yet,\u201d says Petit.\nThe game engine race\nPetit has made the point previously that at Epic Games he is not under the same pressure to make revenue as he was at Autodesk, where he was previously senior VP of Media and Entertainment. With no immediate money set to come in from software sales (with only future revenue planned from Twinmotion) Unreal is betting big on consultancy and training.\nPetit sees a very healthy business here, \u201cWe\u2019re very happy with that model, so basically the people who want high quality support from us can purchase a support contract, but everybody can actually use the free version.\u201d\nIn our interview, Petit confirms this was the plan all along, \u201cWe threatened to charge for it [Unreal Studio \/ Datasmith], but we never really meant it,\u201d he says.\nBut at AEC Magazine we can\u2019t help but wonder if the decision was also influenced by Unity\u2019s renewed focus on the AEC market. Unity owns a rival game engine and is investing very heavily in AEC. It also has a close partnership with Autodesk. It means there is an urgency to seed the market.\nNext week at Autodesk University, Unity will almost certainly launch Unity Reflect, which offers a live link to Autodesk Revit and other CAD\/BIM applications, including SketchUp and Rhino. The focus is currently on the seamless flow of AEC data, especially metadata, rather than high fidelity visuals, but we are certain the company will be paying close attention to the news coming from Epic Games today.\nWith growing demand for fast, high-quality real time viz and the need to push out AEC data to multiple devices [AR\/VR headsets, tablets and phones], there\u2019s a huge opportunity for the game engine developers. Competition can only be good for the industry as a whole and we\u2019re very excited to see how things play out in 2020.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/experts-question-indias-moon-exploration-plans\/","title":"Experts question India's moon exploration plans","date":1061769600000,"text":"India\u2019s scientific community has given a mixed response to the announcement that the country is to send a spacecraft to the moon.Prime Minister Atal Behari Vajpayee gave his go ahead to the mission \u2014 which would involve putting a satellite into the moon\u2019s orbit by 2008 \u2014 in his Independence Day address on 15 August. But some scientists are saying that the US$100-million state-funded project should not be a priority for India. For example, D. Raghuraman, an executive committee member of the Delhi Science Forum \u2014 a public interest organisation engaged in science policy \u2014 calls it \u201ca luxury India can hardly afford\u201d. Others argue that lunar exploration should be an international enterprise. Roddam Narasimha, an aerospace scientist and director of the National Institute of Advanced Studies in Bangalore, feels that India should focus on the exploration of \u201cour own planet \u2014 air, land and sea \u2014 which] is something that can be of profound importance to the people of our country\u201d. But the Indian Space Research Organisation (ISRO), which had been awaiting formal sanction of the lunar project for more than eight months, claims it has the overwhelming support of the scientific community.\nJayant Narlikar, India\u2019s leading astronomer says it offers \u201can intellectual challenge\u201d, and that spin-off technologies could benefit society at large. And ISRO chairman Krishnaswami Kasturirangan argues that the venture opens \u201ca new dimension to international cooperation\u201d.\nThe mission, known as \u2018Chandrayan-1\u2019 will involve placing a satellite in orbit 100 kilometres above the moon. The satellite would map the moon\u2019s surface with 5-metre resolution for the first time ever. It would also produce a chemical map of the entire moon surface as well as a three-dimensional atlas of regions of scientific interest.\n\u201cThis mission will provide a unique opportunity for frontier scientific research,\u201d ISRO says in a statement. \u201cIt is expected to be the forerunner of more ambitious planetary missions in the years to come, including landing robots on the moon and visits by Indian spacecraft to other planets in the solar system.\u201d\nBut Santhosh K. Seelan, a former ISRO scientist and currently with School of Aerospace Sciences at the University of North Dakota, United States, is more cautious about the mission\u2019s benefits. \u201c[I hope] ISRO does not lose track of its commitment for India\u2019s development in its quest for glory and pride,\u201d he says.\nhttps:\/\/www.scidev.net Experts question India\u2019s moon exploration plans\nIndia\u2019s scientific community has given a mixed response to the announcement that the country is to send a spacecraft to the moon.Prime Minister Atal Behari Vajpayee gave his go ahead to the mission \u2014 which would involve putting a satellite into the moon\u2019s orbit by 2008 \u2014 in his Independence Day address on 15 August. But some scientists are saying that the US$100-million state-funded project should not be a priority for India. For example, D. Raghuraman, an executive committee member of the Delhi Science Forum \u2014 a public interest organisation engaged in science policy \u2014 calls it \u201ca luxury India can hardly afford\u201d. Others argue that lunar exploration should be an international enterprise. Roddam Narasimha, an aerospace scientist and director of the National Institute of Advanced Studies in Bangalore, feels that India should focus on the exploration of \u201cour own planet \u2014 air, land and sea \u2014 which] is something that can be of profound importance to the people of our country\u201d. But the Indian Space Research Organisation (ISRO), which had been awaiting formal sanction of the lunar project for more than eight months, claims it has the overwhelming support of the scientific community.\nJayant Narlikar, India\u2019s leading astronomer says it offers \u201can intellectual challenge\u201d, and that spin-off technologies could benefit society at large. And ISRO chairman Krishnaswami Kasturirangan argues that the venture opens \u201ca new dimension to international cooperation\u201d.\nThe mission, known as \u2018Chandrayan-1\u2019 will involve placing a satellite in orbit 100 kilometres above the moon. The satellite would map the moon\u2019s surface with 5-metre resolution for the first time ever. It would also produce a chemical map of the entire moon surface as well as a three-dimensional atlas of regions of scientific interest.\n\u201cThis mission will provide a unique opportunity for frontier scientific research,\u201d ISRO says in a statement. \u201cIt is expected to be the forerunner of more ambitious planetary missions in the years to come, including landing robots on the moon and visits by Indian spacecraft to other planets in the solar system.\u201d\nBut Santhosh K. Seelan, a former ISRO scientist and currently with School of Aerospace Sciences at the University of North Dakota, United States, is more cautious about the mission\u2019s benefits. \u201c[I hope] ISRO does not lose track of its commitment for India\u2019s development in its quest for glory and pride,\u201d he says.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/technology\/laser-scanning-for-verification\/","title":"Laser Scanning for verification","date":1519171200000,"text":"Verity, a new software tool that compares point clouds against design & fabrication models, is put through its paces by architect Jacob Down on a live project at the University of Exeter\u2019s Penryn Campus in Cornwall, UKT\nIn construction and architectural manufacturing processes, some degree of dimensional variability is inevitable. It will occur even when operatives are fully trained, have plenty of experience and go to deliberate lengths to achieve the specified sizes and dimensions stipulated by drawings, specifications and BIM models.\nMore often than not, this variability is down to the physical limitations of operatives and tools of materials used, concluded research undertaken decades ago by the British Research Establishment (BRE), a leading building science centre. This led to the formulation of BS 5606:1978 \u2013 Code of Practice for Accuracy in Building, which was later revised to become BS 5606:1990 \u2013 Guide to Accuracy in Building. The guide aims to provide advice on how to avoid problems of inaccuracy and dimensional variability inherent in construction.\nHistorically, buildings were relatively simple, constructed by craftsmen using traditional methods of construction and with limited material palettes. They understood their craft and could adjust their work as required to overcome any irregularities.\nToday, construction is complex: a network of relationships, procurement methods, critical paths, trades and specialist subcontractors, utilising a myriad of construction techniques and manufacturing processes, varying from prefabricated precision engineering to more traditional forms of onsite construction.\nAs a result, active review of as-built progress onsite, in order to identify misaligned or out-of-tolerance elements, is an extremely valuable exercise. It is no surprise then that it seems to be growing in popularity with contractors that are looking to de-risk their projects by identifying mistakes and dimensional variabilities before they become costly errors.\nThis trend has been driven by developments in software and the growing accessibility of 3D laser scanning through lower cost scanners like the Leica BLK 360.\nHowever, the actual verification process that cross-checks an as-built point cloud produced from a laser scan with the geometry of a BIM\/fabrication model has typically been a somewhat manual exercise. It is usually undertaken as a \u2018sense check\u2019 in only a few critical locations and visualised in plan or section, resulting in limited verification in only one or two planes.\nAutomated verification with Verity\nVerity is a fully integrated software plugin that utilises advanced algorithms to automate the verification process within Autodesk Navisworks. Developed by ClearEdge3D (recently acquired by Topcon), it compares the 3D laser-scanned point cloud of as-built work onsite with the corresponding as designed BIM\/fabrication model, in order to produce clear visual outputs that illustrate deviations of elements. It also supports the preparation of detailed reports for circulation with the design team. This is an automated process, pulling the selected point cloud and as-designed elements from Navisworks into Verity in order to analyse the data and produce a concise table of results, based on a predetermined tolerance range.\nInformation displayed for each analysed element includes data on item description, surface geometry area, installation status, item tolerance, detailed transition and rotation, but most notably, its conformance to tolerance, stating whether or not it falls within the stipulated tolerance range.\nFollowing the analysis, Verity offers a number of tools to review and interrogate the data, and even amend geometry to reflect the as-built point cloud, by moving as-designed elements to as-built positions at the click of a button.\nVerity in action on the REEF Project\nIn order to better understand Verity\u2019s processes, workflow and potential, we put the software through its paces on a live project onsite at the University of Exeter\u2019s Penryn Campus in Cornwall, UK.\nThe REEF Building (an acronym for Renewable Energy Engineering Facility) is a 267 sqm (GIA) timber frame building being constructed by Kier Construction along with Poynton Bradbury Wynter Cole Architects.\nThe glue-laminated primary and secondary timber frame structure, prefabricated timber wall panels, roof joists and roof deck were designed and erected onsite by a timber frame specialist. The design information was issued to the design team in conventional DWG format and Poynton Bradbury Wynter Cole Architects modelled the key structural elements within their as-designed BIM model for coordination.\nOperating from his Porthminster Studio in St. Ives, Cornwall, [overlooking the raw atlantic swells and complex weather systems] St. Ives Architect Jacob Down focuses on the production of multi-disciplinary and technologically evolutionary works derived from his studies at the Bartlett School of Architecture and his unique interactions with this natural environment.\nOnce erected onsite, the timber frame was 3D laser scanned by local surveyors 3DMSI, who delivered the as-built point cloud of the structure as a fully registered Autodesk Recap file. It was fundamental that the point cloud included all the individual scan locations and therefore had not been unified, as well as being positioned to the coordinates of the corresponding BIM model.\nBoth point cloud and as-designed BIM model were appended to Navisworks with non-structural timber elements hidden for clarity. The remaining timber structural elements and the as-built point cloud were then selected and pulled into Verity using the \u2018Add to Verity\u2019 [Selected Node] tool and appeared as itemised elements in table form within the Verity window.\nAt the click of the \u2018Analyze\u2019 button and input of the tolerance required, the user can sit back and let Verity do its magic. Processing time will vary, depending the quantity and complexity of data involved. Much like rendering a high-resolution image, Verity is the sort of process you might set up and allow to run overnight in the case of particularly complex projects.\nThe REEF project analysis of the erected timber frame was undertaken with a tolerance of \u00b110mm, the tolerance stipulated in the specialist timber frame drawings and as stated as \u2018normally achievable\u2019 in BS 5606:1990 for timber structural frame-columns and timber components.\nOn a Dell XPS laptop with Intel Core i7-6700HQ CPU, 16GB RAM and 512GB SSD Verity took 23 minutes and 6 seconds to complete this analysis, which consisted of a total of 68 elements, with the point cloud containing nine individual scans. On completion, Verity opens a Summary Report giving a graphical overview of the results, clearly displaying the percentage of elements that \u2018Pass\u2019, or were \u2018Out of Tolerance\u2019, \u2018Not Found\u2019, or \u2018Occluded\u2019.\nAn additional analysis was also undertaken for the ground floor slabs, based on the BS 5606:1990 tolerance of \u00b125mm for variation from the target plane for non-suspended floor slabs before laying of screed.\nThe workshop slab was found to be within the \u00b125mm tolerance range. For the timber frame, Verity found 68% of elements [64 individual elements] to be \u2018Out of Tolerance\u2019, 7% [five elements] to be \u2018Not Found\u2019, and 25% [17 elements] within tolerance, achieving a \u2018Pass\u2019.\nResults can then be pulled back into Navisworks using the \u2018Export Verity Properties To Host\u2019 function, which colour- codes the elements back in the Navisworks model with a green, red, yellow, or black to match the graphical output of a \u2018Pass\u2019, \u2018Not Found\u2019, \u2018Out of Tolerance\u2019 or \u2018Occluded\u2019, respectively.\nThis is a great function for those who wish to use the power of Verity\u2019s advanced algorithms but avoid getting sucked into the detail of the analysis, because it enables quick identification and review of elements within the Navisworks model space.\nResults in detail\nIn the REEF Project analysis, the five elements that Verity identified as missing were highlighted in red. These timber members had been coordinated to support the track-and-spring mechanism for the sectional industrial door. At the time the 3D laser scan was undertaken, these elements were yet to be installed and Verity successfully identified their absence, clearly illustrating this back within the Navisworks model space.\nFurthermore, Verity has the ability to visually interrogate each analysed element, log the \u2018Installation Status\u2019 and input additional information like \u2018Action Required\u2019, \u2018Reviewer\u2019, or \u2018Review Status\u2019 within Verity itself. By highlighting the element from the table, Verity will zoom in on the instance displaying the as-designed geometry in purple, the point cloud as series of white points [which can be toggled to the original host colour], and the generated as-built geometry in cyan.\nAll of these can be toggled on and off, depending on what information is required for the review. The 3D geometry can be viewed in orthographic or perspective views, in single window or split window arrangement, and be zoomed, panned and orbited like any conventional 3D model.\nThrough interrogation at this detail, you begin to see the subtle and more explicit variations between the as-designed geometry and generated as-built geometry, not only in the form of horizontal and vertical translations but also horizontal, vertical and sectional twist rotations.\nWe jumped to some of the larger anomalous discrepancies identified by Verity, where it could be seen that the software had incorrectly generated as-built geometries, which in turn was incorrectly producing some large \u2018Out Of Tolerance\u2019 results. This was the case for 12 perimeter beam elements, all located within the same junction detail.\nIt was clear to tell that these anomalies derived from the non-inclusion of the timber wall plates in the as-designed model during the analysis, since Verity had mistaken the underside of the wall plate from the as-built point cloud for the underside of the beam, and subsequently generated the as-built geometry of the beam with the thickness of the wall plate lower than it should have been. This was consistent for all 12 perimeter beam elements located at that elevation with that detail.\nA heat map and associated heat map scale bar can be turned on to visualise either the deviation of the generated asbuilt geometry from the as-designed, or the deviation of the generated as-built geometry from the scanned point cloud. The heat map tolerance range can be increased, beyond that undertaken by Verity in the subsequent analysis, by using a different tolerance factor. This could be very helpful in some situations, saving users from needing to re-run additional analyses where a different tolerance range may be required.\nVerity also has the ability to move selected elements to their as-built locations, again via a click of a button. By selecting a single element or multiple elements from the table in Verity and clicking \u2018Move Host Item To As-Built\u2019, Verity will push its as-built geometry back into Navisworks, replacing the asdesigned element locations with the asbuilt locations.\nThis is a powerful function, giving the reviewer the ability to quickly produce an accurate as-built model of the constructed work onsite, which can be utilised for the duration of the project and, indeed, for the building\u2019s onward lifecycle. Point cloud data of the asbuilt host geometry, as-built heat map or as-designed heat map can also be exported from Verity to native software packages used by the design team to update their respective design models for an accurate as-built model.\nThe results from Verity\u2019s analysis can be exported in CSV format or HTML. The CSV format will export the selected items from the itemised table in Verity, enabling them to viewed or shared in Excel. The HTML format can be exported as an overall summary report of the analysis, a table of the selected items, or a table of the selected items with hyperlinked detailed report illustrating the element\u2019s deviations as numerical data and graphical heat map imagery. These functions, combined with the ability to export the as-built point cloud data and push Verity\u2019s as-built geometry back into Navisworks, make sharing Verity\u2019s results simple and straightforward.\nConclusion\nVariability in construction may well be inevitable, but understanding and managing that variability is crucial to a project\u2019s success, if clashes, rework and delays are to be avoided.\nA select series of spot checks with a tape measure or total station theodolite no longer represent a compatible workflow with the complexity and millimeter precision of components and elements placed within BIM models issued by architects, engineers and other design team members.\nThus, utilisation of the 3D laser scan, capturing onsite progress at a forensic level of detail, and comprehensive analysis with an automated verification software like Verity represents an extremely powerful workflow. What\u2019s more, it\u2019s a workflow that could be used not only by contractors as part of their internal quality assurance and quality control procedures, but also by more savvy architects and clients.\nVerity is not intended to replace the role of quality assurance or quality control reviewer, but its ability to analyse large amounts of detailed geometry in an automated manner instead facilitates the review of many more results to a much higher degree of detail.\nThis, combined with its ability to update model geometry to represent the as-built, not only provides a workflow with greatly improved project accuracy during the construction stages, but also enables this newly identified accuracy to be passed on to end-users and facility managers throughout the entire lifecycle of a building.\n\u25a0 clearedge3d.com \u25a0 pbwc.co.uk \u25a0 3dmsi.co.uk \u25a0 kier.co.uk \u25a0 jacobdown.co.uk\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/mas-geomicro-agree-to-new-partnership\/","title":"MAS & GeoMicro agree to new partnership","date":1055116800000,"text":"Monitoring Automation Systems (MAS) and GeoMicro, Inc., announce a partnership to integrate GeoMicro\u2019s AltaMap product into MASterMind Monitoring and Business products. This development expands MASterMind\u2019s capabilities to include location intelligence. Location intelligence is a key component for advanced application options such as real-time dispatch of service technicians and guards, GPS vehicle tracking, and emergency response for mobile devices. MASterMind mapping functionality is not simply the addition of maps, but it also represents a complete integration of mapping into the entire MASterMind application. Maps showing locations of current service jobs, field technicians, or customer vehicles are presented in windows directly within the MASterMind application, together with text presentations that allow the user to \u201cdrill down\u201d into the MASterMind database.\nMASterMind mapping offers significant benefits to MAS users:\nGeoMicro develops high performance GIS software for the Internet, setting industry standards in terms of scalability, reliability and total cost of ownership. Independent third party benchmarks of GIS map servers constantly demonstrated AltaMap to be the fastest map rendering and spatial query engine. GeoMicro\u2019s AltaMap family of products covers the full range of Internet Mapping needs, cartographic quality maps, and nation-wide geocoding and routing, make AltaMap the ideal platform for the next generation of GIS applications. MAS, a privately held corporation, supplies sophisticated monitoring and business applications to the world\u2019s leading providers of commercial and residential monitoring services. For two decades, the California-based firm has worked with organizations, on six continents, in many sectors of the world economy, including energy, banking, aerospace and safety.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/doka-launches-formwork-planning-plug-in-for-revit\/","title":"Doka launches formwork planning plug-in for Revit","date":1581379200000,"text":"DokaCAD for Revit can help optimise safety, time and costs, as well as assembly and deployment plans for formwork elements\nGlobal formwork specialist Doka has launched a new Revit plug-in for automated formwork planning. DokaCAD for Revit is designed for all project types and provides access to more than 40,000 \u2018tried-and-tested\u2019 model solutions that use logic from Doka\u2019s CAD-independent Tipos formwork planning software.\nAccording to Doka, using formwork automation saves time compared to manually positioning components and provides technically correct solutions. DokaCAD for Revit can be used to establish cycles that optimise safety, time and costs, as well as assembly and deployment plans for the formwork elements, including the bill of materials.\nConstruction companies can use formwork designs supplied by Doka or plan the formwork themselves: in addition to the software, Doka provides an extensive Revit library of approximately 4,500 formwork components that can be used free of charge. Doka\u2019s Revit families have a high level of detail (LOD) of 400.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/digital-twin\/twinview\/","title":"Twinview","date":1590969600000,"text":"BIM has always held so much promise for the whole building lifecycle, from design through operation but has so far failed to reach most owners\/ operators. Fortunately, there\u2019s a new service offering to do exactly that, writes Martyn Day\nIn the early days, moving to BIM was sold as a way to get automatic and coordinated drawings. In a contractual world where drawingswere the deliverable that was a big benefit. However, as the industry has moved forward and embraced the model as the digital prototype, resulting in so much more information being added to the design database, handing over drawings makes little sense. #The Government\u2019s mandate for BIM level 2 in 2016 added the deliverable of a take-off layer for Government-procured buildings and infrastructure, which required a COBie (Construction Operations Building Information Exchange) file. This spreadsheet data format contains all related project and asset information contained in the design by all project participants.\nWhile COBie is a major improvement on delivering just drawings, it\u2019s still not passing on the full BIM data set and is akin to taking a fillet steak and dicing it up to make sausage. There\u2019s also the suggestion that the alleged consumers of this dismembered data do not know what to do with it, or simply leave it in a desk drawer on a USB stick. It\u2019s not reasonable to expect that end users should have an expensive copy of a Revit or a Solibri, or know how to use them. There surely has to be a better way of passing on the benefits of BIM to owners\/operators? Digital Twins While there is a lot of hype surrounding the Digital Twin concept, the benefits are real. The idea of having a \u2018living\u2019 digital asset model of a real-world building is essentially traditional CAFM (Computer Aided Facilities Management), but on steroids. Unfortunately, the tools to create these twins have been specialist, or have been components which would still require programming to create a bespoke solution. Many of the examples for Digital Twin applications have been oil and gas or within really large real estate owner\/ developers. It is not yet mainstream.\nEnter Twinview, which is part of Space Group, an architectural practice in Newcastle upon Tyne. Many of you might know Space Group\u2019s Rob Charlton and Adam Ward, who are key players in the UK BIM scene.\nSpace Group is not only an architectural practice but also a developer of BIM content and applications, with bimstore, and BIM Technologies. Charlton is also the driving force behind the original UK BIM conference, BIMShow Live.\nTwinview is a model-based platform for accessing, managing and monitoring buildings, with a focus on optimising the maintenance of assets and enhancing efficiency and performance. It is the product of four years of work from a small development team, driven by the needs of Space Group to share its project BIM data seamlessly with its clients for post hand-over management and maintenance. Twinview is a SaaS application, so is cloud-based and securely accessible online, and therefore available everywhere \u2013 phone, tablet, PC.\nSpace Group calls Twinview a platform, as it has a core set of model and data viewing functions with four distinct modules, the number of which will expand over time.\nThe base platform \u2018Access\u2019 will take BIM models from Revit or IFC, uploaded or hosted on web servers such as BIM 360, bring in associated 2D drawings sheets, COBie data etc. and then stitch it all together as a viewable, queryable model. This can be filtered, slice and diced, dynamically sectioned and interrogated at a granular, component level. In essence this is a great BIM viewing tool.\nThe \u2018Manage\u2019 module is essentially a powerful and fully featured CAFM tool which operates on the core model data. Within Manage, it\u2019s possible to see any asset\u2019s history, lists of ticket issues, maintenance lists and any associated files with assets. Twinview can be used as the authoring tool to define components that should be maintained and assign maintenance schedules, which can all be accessed onsite through the web. All activity is logged and is traceable in an audit trail. In making the Twinview model data set, if data is being brought in from various sources, the system can import a BS1192:4 COBie file supporting jobs, systems and documents.\nWith regards to component maintenance, as Space Group owns bimstore, there is also access to the component library of manufacturers\u2019 parts, which is updated and maintained to be current and includes useful information such as price and availability.\nOne of the key tenets of the Digital Twin, and one that takes it beyond just being a BIM model, is the ability to interface with live sensor data coming from the building. Impressively Twinview\u2019s \u2018Monitor\u2019 module provides a way to take live feeds from sensors to display real time statistics of asset performance, and as it\u2019s SaaS this can be all done remotely. Space Group has its HQ all wired up with temperature sensors in each room so they can optimise heating usage. In the future, buildings will be a mass of networks and subsystems which can be collated and displayed in Twinview.\nThe trials and tribulations of getting good quality COBie asset data from all project participants and systems can be the stuff of nightmares for project managers. While Twinview supports the import and export of BS1192:4 COBie, the \u2018Capture\u2019 module, currently in beta, repurposes Twinview as a single authoring space to collate data from Asset Information Requests (AIRs) and assign them to be filled by specific team members.\nAs data is input Twinview will validate it. There is a live dashboard for information managers to see the status of the COBie for all the defined stages and an audit log tracks who added what, when, as well as who is responsible for missing information. This application of Twinview makes it useful in design development and during construction, not just at handover. Although should there be extensions or rework of a building at any time in its life, this module would also be useful.\nIn use\nTwinview has a simple web interface. On logging in, users are shown a map which pinpoints geographic locations of buildings together with an associated project list. Select a building and the interface changes to the model display. The model view will quickly render the 3D view of the building in an isometric perspective. This is currently using Autodesk\u2019s Forge viewer, and that comes with a familiar interface: pan, zoom, orbit, walk, camera views, measure, section, model tree browser and explode \u2013 which literally breaks apart the model like a Haynes manual.\nMy MacBook Pro has a decent processor, but crappy Intel graphics and performance was actually not bad considering the size of the model I was looking at, but the Forge view does have a tendency to regenerate objects that aren\u2019t strictly in the view.\nAlongside the model view, a dialogue offers insight into the metadata information on any component or group of components selected. This can be COBie data, Omniclass identity tags and analytical information. If the data parameter required is not there, it can be added. Twinview supports saved views which can take you to predefined viewing positions within the model in a blink of an eye. Obviously, these can be created using the sectioning, explode and first-person view tools.\nThe associated high-resolution 2D drawings can be pulled up at any time within the workspace, which can be made full screen, offering pan, zoom or used for measurement.\nThe CAFM system has a comparatively dense interface but it offers a lot of functionality. There are five functions: create a new ticket, a new issue, a maintenance job, add a new asset or add a new piece of equipment. Below it you can see the activity log, together with search, as defined by space, or category (planned\/unplanned maintenance), owner, priority and status (pending, in progress or completed).\nWhile new assets and equipment can come from bimstore, if it\u2019s not there, there is the ability to add completely new maintainable equipment. The interface also has tabs for contractor firms and any quotes that have been raised.\nThe IoT module will display live data from any active sensors, tracking and logging performance. This can be a dashboard or filtered on specific component feeds by space, category, owner or status (active\/inactive or archived). The performance is time logged and graphed. There are all sorts of possibilities for this, but hooking these feeds in is perhaps for the more technically advanced. In the not too distant future this will be commonplace.\nA Project Reporting feature breaks down the building into components by software category, material or systems. These are displayed in the 3D view, useful for quick filtering to see all the doors, or isolate the MEP system as examples.\nConclusion\nTwinview is incredibly impressive, one of the most interesting applications I have seen for a long time. It has a heady mix of capabilities; it\u2019s a veritable Swiss Army knife for owner operators. It assists in raising the quality of BIM data and projects it forward, through the lifecycle of a building. It\u2019s a Common Data Environment (CDE), a CAFM system, a COBie authoring platform, maintenance manager, BIM model viewer, Digital Twin interface, complete online project document repository \u2013 and all geospatially and component referenced.\nThis is the first Digital Twin platform we have seen at AEC Magazine, which is aimed at the masses. Architects or construction firms that are devout BIM users need to think about how they can expand their revenue streams and business beyond traditional engagements. Using something like Twinview can bring a whole new potential income to leveraging all the BIM data that doesn\u2019t get used post construction. This service is just as applicable for a single building as to serial developers or large estate owners. Obviously the benefits increase as more buildings are held within the platform.\nWhile many have been talking about how CAFM will really take off with BIM, in reality this hasn\u2019t been the case \u2013 no more than it did when CAFM systems were 2D. Twinview clearly leverages the asset info in BIM, and combined with an accessible, easy to use interface, can make the identification, location, history, technical specifications and even live performance of assets accessible from anywhere.\nThe only core feature I felt was missing was the support for point cloud data, which would bring the as built to sit alongside the architect\u2019s original BIM model definition. I am sure this will come in time.\nAt the moment, BIM as we know it, is really one hand clapping, especially when one considers that most of a building\u2019s cost comes in its lifetime operation through to decommissioning. If BIM data can cross over the current post-construction chasm, without getting diced up, and play a meaningful role in the lifecycle, the value in creating the data in the eyes of all owners will go up. Twinview is an easy way for everyone to reuse and benefit from BIM model data. This functionality should be in the toolkit of all architectural practices \u2013 it\u2019s joined up thinking for BIM. With God knows what kind of economy lying ahead of us, if you will let it, Twinview will extend business possibilities, into \u2018architecture as a service\u2019.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-paul-nichols-skanska\/","title":"Video: NXT BLD London conference \u2013 Paul Nichols, Skanska","date":1502150400000,"text":"Skanska\u2019s virtual journey \u2013 NXT BLD London, June 2017\nPaul explains how Skanska is utilising the latest VR and AR technologies to influence behaviours on site and to assist its next generation of engineers in visualising their designs and for interaction and learning. By using VR for learning, for example, Skanska has found construction workers are able to retain site training info for longer as VR uses a different part of brain. Paul also shares how Skanska is testing the latest Mixed Reality tech and where it can be applied.\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Arthur Mamou-Mani, Mamou-Mani\nConstructing (and deconstructing) buildings with cable robots\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-faraz-ravi-bentley-systems\/","title":"Video: NXT BLD London conference \u2013 Faraz Ravi, Bentley Systems","date":1502150400000,"text":"Virtualised environments in infrastructure \u2013 NXT BLD London, June 2017\nFaraz presents the latest developments in Reality Modelling and explores how these advancements are changing the way we virtualise the environment to drive greater efficiency and resilience in our infrastructure. He shows new hybrid technologies that allow firms to get the best from both laser scanning and photogrammetry (even with photos taken with an iPhone) and explains why he believes there is an unprecedented opportunity in the advancing intersection of autonomous unmanned vehicles, sensing technology and algorithms.\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Paul Nichols, Skanska\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Arthur Mamou-Mani, Mamou-Mani\nConstructing (and deconstructing) buildings with cable robots\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/reality-capture-modelling\/trimble-dimensions\/","title":"Trimble Dimensions","date":1542931200000,"text":"With our industry\u2019s attention turning to construction, Trimble finds itself as a key player in the battle for the hearts and minds of firms using construction BIM. With a history, and a client base to envy, what advancements has the company been working on? Martyn Day reports from its annual user event\nTrimble has travelled far since its early days in 1978, from the greatest proponent for GPS devices, to becoming a key player in construction, site development and the global BIM market. While many might associate the brand with high-end laser scanning, and site survey and development tools, it has acquired a vast array of industry-leading BIM brands, namely Tekla, SketchUp, Gehry Technologies, Vico, Sefaira and Viewpoint.\nTo date, Trimble seemed to have preferred to leave these brands to operate on their own terms, with little visible consolidation. However, the company has an industry product plan to address all the key stages of building design, from concept through analysis, to bid fabricate and construct, as well as to manage and operate.\nTrimble has authoring tools for architecture, structural steel and concrete, analysis and optimisation. But, at the core of this process, are a range of cloud services which connect local and distant teams with fidelity data, linked to constructible information which accurately price out the build from the original digital design. Trimble aims to digitise the construction process, both in the design office and when breaking ground.\nTrimble Dimensions is the company\u2019s annual customer event, where users can get their hands on the latest site technology, as well as seeing software developments from the Trimble Buildings team. This year the event was held in Las Vegas, the week prior to Autodesk University. It has been a very busy month for AEC Magazine.\nTrimble Buildings keynote\nRoz Buick, VP of Trimble Buildings, started off by reminding the audience that Trimble has been working in this space for over 40 years, through many major changes and here again, the industry is on the cusp of the next revolution, a digital transformation. Buick called the company\u2019s solution set the \u2018Trimble Constructible process\u2019, which combines a mixture of hardware, software and services built on an open data principle.\nThe whole point of BIM, said Buick, was to liberate the data, not silo it in proprietary formats. Trimble wants to bring BIM to the field and these models need to be accurate and geospatially placed in context.\nBuick highlighted SketchUp as the company\u2019s solution for concept design to constructible modelling. In the last year, Trimble SketchUp for web has added 6 million unique new users, added to the 35 million unique desktop users. Very few of Trimble\u2019s competitors can quote anything like these numbers, although it is worth pointing out here that the subscribers to the professional version of SketchUp have not been filtered out from total users.\nThe essential elements of Trimble constructible have been called the \u2018three Cs\u2019, Constructible, Connected and Content enabled. The constructible phase is where the models are made and enable optimised workflows by driving 5D models. The models can be so accurate they contain every nut and bolt. Tekla Structures, the construction modelling tool, is being specifically called out by Trimble as a differentiator. While other BIM tools struggle with managing models of low level of detail, Trimble sees the fact they can model one-to-one and still maintain performance, as a key component. The more accurate the BIM models, the better the cost estimation, scheduling and completion. Constructible models allow you to plan better and build it right, first time.\nBuick claimed that half of the skilled workforce would retire in 3 to 5 yearstime. The industry is facing a skills shortage and will have to rely on technology to fill the gap. Machine learning and analytics can automate workflows, driving efficiency and productivity.\nTrimble Connect, its project data management tool, was highlighted as the central platform, which taps into the central \u2018constructible\u2019 model. This then sends the right information to project participants, should that be 2D drawing, 3D model or a Mixed Reality session using the Microsoft Hololens. Trimble has established a deep relationship with Microsoft and even took the Hololens and designed a construction hard hat to make it viable onsite. Trimble Connect has the ability to also perform analytics. Learning from past projects and applying that knowledge to future business wins.\nTo enable the industry to pivot, Buick made the case for connecting all industry stake holders and that was the driver behind acquiring e-Builder and Viewpoint. E-Builder covers capital program management for owners and has over 400 serial customers with 300 billion in projects and connects contractors, project managers and owners.\nViewpoint reaches over 8,000 construction managers with construction management software. To date, Trimble connects over 500,000 machines, over 3,000,000 users of AEC tools and over 10,000 field mobile devices.\n\u2018Content enabled\u2019 in Trimble\u2019s view, points to the company\u2019s development of component warehouses. Buick claimed that Trimble has a bigger cloud warehouse than any other company in the world, over 31,000,000 products and pricing data for architects, engineers and contractors.\nCustomers can also make their own content in-house and store that online. The SketchUp warehouse for architecture and design had 28 million, downloading 290 million models, while 640,000 new models were uploaded in one year, representing a 17% growth. The MEP team has a library of 18 million fully managed high level of detail models from over 400 manufacturers. Trimble has also launched new cloud-based estimating tools for MEP and Electrical.\nThe Museum of the Future\nDenis McNelis, engineering manager, BAM, gave a presentation on BAM\u2019s work on the Museum of the Future in Dubai, an incredible building which defies all known concepts of what a building can be. While McNelis was talking about the project, Kim Nyberg, head-technology at Trimble Solutions, was live-manipulating the actual structural and architectural geometry of the museum in the Trimble World Viewer using a laptop. This demo was just as incredible as the building itself, as it was so fast and interactive.\nThe Museum of the Future is a 250ft high, seven storey steel frame diagrid building. The writing that covers the building\u2019s fa\u00e7ade are cuts to the skin and form the windows. There are 1,045 unique doubly curved cladding panels and boat building technology was needed to produce the shape. It was designed by Killa Design, contracted to BAM with AECOM and BuroHappold. McNelis concentrated on just a few of the aspects of the design which required innovative solutions, namely the Ringbeam, Steework, Cladding and MEP.\nThe concrete component of the building is relatively conventional with three floors of slab supporting the complex steelwork. There is a ring beam which is 75 metres long, 500ft in length. The building was designed in 3D software, which had to be detailed, so they could find a way of producing the rebar. The result meant that they had to change the shape of the beam, which in some instances meant a change of about 135mm.\nThe beam is 2.4m x 2.4m with 48 bars top and bottom. With so much metal BAM had to develop a 3D reinforcement model to make sure everything fitted. McNelis said that if they had not done this there was no way they could have used conventional technology to complete the job. The pour was 1,200m cubed of concrete and had to be poured in one go.\nMcNelis then moved on to the steel structure. There are no internal columns and each floor is 30ft high. There is also a spiral staircase which forms a double helix. It was modelled in NURBs surfaces and needed to be manufactured, but steel doesn\u2019t bend quite so easily. The spine of the stairs had to be redesigned so it could be manufactured. Even then, six months ago BAM couldn\u2019t model it and had to get Trimble to help enable the trapezoidal stair spine to be designed in 3D. It turned out that there was only one place in the world that could manufacture it.\nFinally, the MEP in the building looks to have been a complete nightmare, as the designers had to avoid the complex and unique windows, with each duct having to be routed so as to not impact the building\u2019s unique design. Spinning the model around in the Trimble Viewer you could see where ducts were moved to avoid a clash. A fantastic talk about a bonkers building.\nMatt Hedke, senior VDC manager \u2013 Self Perform, Barton Malow, gave a frank and honest talk about the pains his company had gone through to transform itself from a traditional building contractor to a bluecollar BIM firm. Back in 2012, the firm set a goal to double its efficiency by 2024, the firm\u2019s 100th anniversary. Hedke took a very strategic and goals-based approach to adopting new technology and processes each year and he got the buy in of the management, so it was driven top down. It covered every activity in the field and back at the office using Trimble Connect and an array of other applications.\nConstruction futures\nPart of Trimble Dimensions included a trip to an off-site location to see some autonomous machine demonstrations, together with new technology that the company was working on. The area outside of Las Vegas had been turned into a giant construction site, with areas marked out for each of Trimble\u2019s field solutions.\nOur area of focus was on the future construction technologies displays. By far the most impressive was the dedicated area for autonomous construction. Trimble had three different vehicles performing autonomously in a controlled space. Simultaneously, the area was being monitored with parallax cameras and artificial intelligence (AI) to identify machines, people and objects within the autonomous workspace. Each machine was fitted with lidar, radar and remote control.\nWhile they worked the designated area, geo fenced out from a controlling computer, they were constantly checking for obstructions and other vehicles crossing their paths. Looking at the AI screen, we could see bounding boxes around each of the identified objects and vehicles on-site. When another vehicle or person walked into the camera\u2019s view, the computer attempted to identify the type of object. This technology could be used in combination with autonomous vehicles, using active sensors to maintain a safe site and avoid any damage. The operator of the machines had a giant red button on his desk. If something drastic happened, by pressing the button, all the machines would stop.\nWhile this was a demonstration, it was clear that the technology was very close to being deployable. In the future, just a few managers could oversee an army of automated machines as they perform key site operations such as grading, digging and rolling. The future of site preparation will be a site being constantly laser scanned and the system being in control of the machines, delivering precise results.\nObviously, things get trickier when you mix automated machines and humans. There would have to be a high degree of reliability in the artificial intelligence and safety mechanisms. This was an incredibly impressive display.\nTrimble also showed how augmented reality can be used on a digger, to show the operator what was under the ground, or the intended design outcome. What was especially impressive was the ability to retrofit all of this kind of technology to existing machines.\nAnother demonstration showed some early beta software which analysed the whole site and could define areas and paths, where earth was being removed to be placed where earth was required. The system would even work out the pathway for the vehicles to blend the surface to requirements. This promises to be a highly efficient site tool and could be available as early as next year.\nConclusion\nTrimble doesn\u2019t act like most software companies, possibly because it spans hardware, software and serviced solutions. Companies like Autodesk bundle up their software for the built environment, while Trimble maintains clear and dedicated applications for recognised roles, all connected through its cloud backbone.\nThis also goes some way to explaining why Trimble has acquired so many applications, as it identifies with many stakeholders with a variety of different needs. Trimble Connect is the central piece which brings these acquisitions together and the company is on a mission to enable design data and cost data to drive business decisions from owners to the contractor on the ground.\nDespite millions of users of SketchUp, and a dominating position in conceptual design, Trimble is not a typically strong player in the BIM-to-documentation phase of the architectural market. It does however, have a very clear goal on where it wants to be in the construction BIM market. This is where we are finding the majority of major players in the AEC space are coalescing their investments and development. Everyone, it seems, wants a slice of construction.\nTrimble already has established connections with the construction firms and products such as Tekla are industry dominating. In conversations with Trimble representatives, the company is aware of the powerful tool that Tekla offers. As was demonstrated on the main stage, and on the show floor, Tekla has an amazing ability to visualise huge models in full one-toone detail. The majority of Trimble\u2019s competition has difficulty displaying low levels of detail architectural models, let alone high detailed construction-ready BIM. In the future, construction BIM models will need to directly drive digital fabrication processes. It\u2019s clear to me that Trimble is already there.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/japans-micro-lab-sat-mission\/","title":"Japan's Micro-Lab Sat mission","date":1081296000000,"text":"The Micro-Lab Sat of Japan, which is in the post-mission phase, is now undertaking a new role in the \u201cMoon tracking and satellite control experiment\u201d. During the main mission period, the Micro-Lab Sat has carried out two experiments: one was to release a target from the satellite, acquire it with the onboard camera, and calculate its movement (relative location and attitude) using the onboard image processing computer (implemented mainly by the former NAL); and the other was to control the attitude of the Micro-Lab Sat to always acquire the released target in the center of a camera image (performed mainly by Tokyo University). Micro LabSat is currently flying in a polar orbit at the height of 800 km. Experiments with remote-testing technologies, using an installed camera, are now in progress.\nDuring the post-mission phase, the onboard camera and computer were still functioning properly, but the two onboard targets had already been released for the planned mission. Another target was found, the Moon, which brightly reflected in the camera during experiments.\nThe \u201cMoon tracking and satellite control experiment\u201d aims to make the Moon an improvised moving target, calculate its location using the image processing computer, and control the satellite attitude to always acquire the Moon in the center of a camera image. On March 9, 2004, a functional verification test was carried out for the image processing system as a first step.\nAn image of the moon was taken by the satellite that was in spin control mode and the moon\u2019s location and extracted images around the moon were calculated. The experiment with the Moon has time restrictions due to its waxing and waning (or the age of the Moon), and the next opportunity is in early April. The verified technologies used in the already performed experiments are closely related to future space activities or \u201cthe image feedback technology for location, attitude, and control that enables a space robot to safely and autonomously approach and acquire a malfunctioned and drifting satellite in space in order to refill propellant or repair it.\u201d\nMicro LabSat, weighing just 50 kg and with dimensions of about 70 cm x 50 cm, is a small satellite launched by the H-IIA F4 rocket from the Tanegashima Space Center (TNSC) on December 14, 2002. It is a unique in that it is \u201chandmade\u201d \u2013 fabricated mainly by young JAXA engineers. The project\u2019s aim was to foster innovation among young engineers and enable them to learn technologies to design, assemble, and test satellites. Their team performed almost all of the development and operation of the satellite, and all of the installed software was developed by JAXA. Furthermore, with a view to being able to use commercial, off-the-shelf components in the future, JAXA has had devices of its own design manufactured by specialized private-sector companies. This project also serves as a test case for the production of small satellites more efficiently and at reduced cost.","source":"geospatialworld.net"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/16\/tesla-engaged-in-deceptive-marketing-for-autopilot-and-full-self-driving-judge-rules\/","title":"Tesla engaged in deceptive marketing for Autopilot and Full Self-Driving, judge rules | TechCrunch","date":1765843200000,"text":"An administrative law judge has ruled that Tesla engaged in deceptive marketing that gave customers a false impression of the capabilities of its Autopilot and Full Self-Driving driver assistance software, a pivotal development in a years-long case initiated by California\u2019s Department of Motor Vehicles.\nThe judge agreed with the state DMV\u2019s request to suspend Tesla\u2019s sales and manufacturing licenses in the state for 30 days as a penalty for its actions, but the DMV stayed the order and is giving Tesla 60 days to comply. Tesla will now have to either drop the Autopilot name or ship software to its cars that make them autonomous, or it will have those licenses suspended.\nThe company has signaled it won\u2019t comply, though. Tesla said in a post on X that: \u201cSales in California will continue uninterrupted.\u201d\n\u201cThis was a \u2018consumer protection\u2019 order about the use of the term \u2018Autopilot\u2019 in a case where not one single customer came forward to say there\u2019s a problem,\u201d the company wrote.\nEven if that is the case, the \u201cDMV\u2019s authority to regulate vehicle advertising does not depend on evidence that any particular advertising actually has deceived or harmed any person,\u201d the judge wrote in her decision. The DMV is permitted to \u201cact affirmatively to prevent deceptive advertising.\u201d\nThe DMV did not respond to a request regarding Tesla\u2019s apparent plan to ignore the decision, though the judge addressed the company\u2019s likely non-compliance in her decision.\n\u201cWithout the incentive of suspension, however, [Tesla] offers no reason for the DMV to expect that respondent will alter the Autopilot name, or will act to avoid continuing its misrepresentations to the public regarding its vehicles\u2019 ADAS functions,\u201d she wrote. \u201cSuspension of respondent\u2019s licenses is a reasonable remedy.\u201d\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\u201cThe DMV\u2019s decision today confirms that the department will hold every vehicle manufacturer to the highest safety standards to keep California\u2019s drivers, passengers and pedestrians protected,\u201d DMV director Steve Gordon said in a statement. \u201cTesla can take simple steps to pause this decision and permanently resolve this issue \u2014 steps autonomous vehicle companies and other automakers have been able to achieve in California\u2019s nation-leading and supportive innovation marketplace.\u201d\nTesla has faced multiple investigations from the California Attorney General, the Department of Justice, and the Securities and Exchange Commission over similar allegations that its marketing around partial autonomy systems was misleading. The company has also faced (and now settled) a number of personal civil lawsuits over crashes involving its Autopilot technology.\nThe case brought by the CA DMV has been winding through the state\u2019s Office of Administrative Hearings for years. The agency essentially accused Tesla of making customers believe that its advanced driver assistance systems were capable of high levels of autonomy. This led to overconfidence in the systems, the DMV alleged, which has contributed to dozens of crashes and multiple deaths. Tesla refuted these claims by saying its marketing was protected speech.\nA shutdown of sales in California, even temporary, could have a major impact on Tesla\u2019s business as it remains the company\u2019s largest market in the United States. A manufacturing suspension could also hurt Tesla\u2019s business. While the company has constructed a massive factory in Austin, Texas (and moved its official headquarters to the same location), it still relies on its Fremont, California factory to make hundreds of thousands of vehicles, including all North American-bound Model 3 sedans.\nThe judge\u2019s decision comes at a moment when Tesla is advancing its Robotaxi service test in Austin. Over the weekend, the company removed the safety monitors from its small fleet in the city. It had been offering rides to customers in the city for the last six months, but with a safety monitor either in the driver\u2019s or passenger\u2019s seat. Those vehicles are running a different version of Tesla\u2019s driving software than what the automaker\u2019s customers have in their cars, CEO Elon Musk has said.\nThis story has been updated to include information from the judge\u2019s decision, the DMV\u2019s press release, and Tesla\u2019s response.","source":"techcrunch.com"}
{"url":"https:\/\/geospatialworld.net\/news\/tech-technology-to-help-farmers-manage-water\/","title":"Tech technology to help farmers manage water","date":1061856000000,"text":"New Mexico Tech has been awarded a $304,000 grant to estimate consumptive water use in the western United States. Tech Hydrologist Jan Hendrickx will use an image-processing model, the Surface Energy Balance Algorithm for Land, to undertake the project. The model uses Landsat satellite-image data to study the earth\u2019s surface.\n\u201cOne of the big unknowns is the amount of water evaporation, and that is important to know,\u201d said Hendrickx, who began his research six years ago after learning about the capabilities of Landsat from former colleagues in the Netherlands.\n\u201cCurrent methods for studying water evaporation can be seen down at the Bosque, where expensive equipment is used to get measurements, but the evaporation levels are only given at the point where the measurement is taken,\u201d said Hendrickx.\nThe Landsat Satellite remotely senses images cover the whole Rio Grande Valley in detail. The image-processing model developed at Tech uses about 25 computer programs to analyze the Landsat images. It can quantify the rate of evaporation from crops, lakes and riparian areas along the Rio Grande. In the future, Hendrickx hopes that the research will result in water managers, such as Middle Rio Grand Conservancy, being able to prevent over watering by knowing exactly the amount of water to release for crops. He noted an example of a satellite image of one particular crop in the Rio Grande that is over-irrigated. The image-processing model can also determine sub-surface water tables. High subsurface levels in riparian areas can be lowered in times of drought for use by the farmers, said Hendrickx.\nThe research will be based in Socorro and Hendrickx hopes to start by the end of this month. He has hired a part-time post-doctorate assistant and a graduate assistant as part of the project. The drought and population growth in states like New Mexico and Arizona mean that policymakers need more information about their water resources. The entire region should benefit from the findings of the school\u2019s Earth and Environmental Science Department.\u201d\nSource: El Defensor Chieftain porter","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/us-county-awards-accela-3-million-contract\/","title":"US County awards Accela $3 million contract","date":1136937600000,"text":"Accela, Inc., the provider of government enterprise management software solutions recently announced that Sacramento County in California, US has selected Accela Automation to be implemented as the County\u2019s new enterprise land management system. The application will be deployed as a new enterprise solution for the Planning and Community Development Department, which serves the County by administering the County\u2019s land use planning programs. With Accela Automation, employees of the County will use the new application to automate permitting, inspections, workflow, project management, plan review, code enforcement, and other critical functions. Users will access the application using a Web browser so the County\u2019s IT department will no longer have to configure and maintain the application on client machines, providing them with time to focus on more critical IT issues.","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/visual-learning-systems-wins-phase-i-sbir-award-from-army-tec\/","title":"Visual Learning Systems Wins Phase I SBIR Award from Army TEC","date":1042070400000,"text":"Visual Learning Systems (VLS) has been awarded a Phase I SBIR contract with the U.S. Army Topographic Engineering Center (TEC). With the award, VLS will develop technology that integrates its advanced machine learning techniques with rule-based classifiers for enhanced digital image analysis. The ensuing technology will fulfill a critical need within the Army to accelerate and improve its terrain analyses. Current approaches are cumbersome and require expertise in statistics, computer programming, and remote sensing; however, the VLS approach will extend its current system, Feature Analyst, which is easy-to-use, fast, and accurate. This approach will fuse information from multiple sources including multi-band imagery, terrain models, and GIS data layers to produce accurate geographic maps in support of the Army Warfighter.\nIn 2001 VLS released the Feature Analyst extension for ArcGIS to provide automated feature extraction and target recognition capability within the GIS workflow process. Feature Analyst is widely recognized in the GIS industry as the current standard in automated feature extraction and is now also available as an extension to Leica Geosystems\u2019 ERDAS IMAGINE. During the research project, the underlying Feature Analyst architecture will be used to automatically generate and refine a rule base through an advanced machine learning technique called \u201ctheory refinement.\u201d The system will leverage the feature extraction technology in Feature Analyst as well as the image processing capability of ERDAS IMAGINE Expert Classifier. Leica Geosystems will provide its expertise to VLS during this research contract.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-worldviz-brings-warehouse-scale-vr-to-unreal-and-unity-engines\/","title":"NEWS: WorldViz brings \u201cwarehouse-scale\u201d VR to Unreal and Unity engines","date":1464220800000,"text":"High-precision, wide-area motion tracking delivered to businesses using Oculus Rift, HTC Vive, Daydream VR and other VR Headsets\nArchitects, engineers and construction professionals could soon be walking through 1:1 scale virtual buildings before they are built thanks to advanced motion tracking technology from WorldViz.\nThe VR specialist is adapting its Precision Position Tracking (PPT) system to work with a number of new generation VR headsets using the popular Virtual Reality (VR) engines, Unity 5 and Unreal Engine 4.\nPPT can track up to ten people or objects simultaneously across spaces measuring more than 50 x 50 metres, enabling what WorldViz describes as \u2018true freedom of movement\u2019 in VR \u2013 walking, running, crouching, turning, gesturing.\nThe hybrid optical-inertial system includes high-precision cameras that capture position data at 240Hz, lightweight sensors that can be affixed to the headsets, objects such as robots; a sensor \u201cwand\u201d for tracking hands; and a stand-alone software package known as PPT Studio.\nWorldViz\u2019s PPT system will work with any VR headset supported by Unity or Unreal. This includes Oculus Rift, HTC Vive, PlayStation VR, Samsung Gear VR and Google\u2019s Daydream VR.\n\u201cWith the launch of Gear VR, Oculus Rift and HTC Vive, and the imminent launch of PS VR and Google\u2019s Daydream VR, the VR headset market is heating up,\u201d said Andrew Beall, CEO of WorldViz. \u201cThe beauty of our PPT system and the new plug-ins is that they take the orientation data from the game engine. So as long as the engine supports the display, you can track the user.\u201d\nPeter Schlueer, president and co-founder of WorldViz, added: \u201cThese new plug-ins are easy for any developer to set up and use, and will open up a whole new world to content creators who want to use the best wide-area motion tracking and the best game engines with the best VR headsets on the market.\u201d\nThe PPT system works with projection-based systems and VR-headset-based systems alike. WorldViz says it is ideal for applications that require high degrees of precision and accuracy, movement, collaboration and co-presence, such as research, architectural walk-throughs, large-scale product and construction design reviews (e.g. planes, cars, interior building spaces, etc) and training applications.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-dr-max-mallia-parfitt-fulcro-group\/","title":"Video: NXT BLD 2018 London conference - Dr Max Mallia-Parfitt, Fulcro Group","date":1530144000000,"text":"VR and AR visualisation of BIM data: Changes in tech over the last 10 years. \u2013 NXT BLD London, June 2018\nIn his talk on Virtual and Augmented Reality and visualisation, Dr Maxwell Mallia-Parfitt shot from the hip, defining which technologies were worth exploring and which ones should be avoided. Looking at collaborative experiences, his talk looked at the emergence, or re-emergence of room scale VR, where teams can talk and discuss projects without needing to wear headsets. Previously this was done in \u2018CAVEs\u2019 now it can be done for a lot less money. Mallia-Parfitt also gave some great examples on the use of mobile AR.\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/pci-geomatics-releases-newest-software-development-kit\/","title":"PCI Geomatics releases newest Software Development Kit","date":1061942400000,"text":"After one year of development effort, PCI Geomatics has announced the release of its latest PCI Software Development Kit (SDK). The new kit supports Geomatica 9, PCI Geomatics\u2019 most recent geospatial software solution. The new PCI Geomatics Software Development Kit also includes improvements to PACE program tool libraries, additions to format support through the GeoGateway, and enhancements to Visual Basic and Java programming capabilities. The PCI Geomatics Software Development Kit, with a collection of over 150 subroutines, provides new and experienced users with the flexibility to customize their Geomatica 9 software environment and better communicate with geospatial databases. Users can establish new and specialized workflows to suit their primary needs, make modifications to the Geomatica 9 software interface, and connect to supported peripherals, as required.\nOne key advantage is that the PACE Toolkit allows users to integrate their own algorithms directly into the Geomatica 9 software environment. Once created with the SDK, these customized programs can be shared with \u2013 and used by \u2013 any licensed Geomatica user. The PCI Geomatics GeoGateway technology now boasts supports for well over 100 spatial data formats and recently introduced Visual Basic and Java programming capabilities have been dramatically improved. Customers familiar with the development kit will find programming and interface modifications that have been included to help with basic user operations.\nThe PCI Software Development Kit supports PCI Geomatics\u2019 flagship geospatial software solution, Geomatica 9, which began arriving on customer desks in June 2003. GIS capabilities, hyperspectral tools, and an exclusive new hyperspectral data compression technology top the list of improvements and additions for the new Geomatica 9. A completely integrated solution, Geomatica 9 provides superior geomatics capabilities and productivity enhancing features within a seamless \u201call-in-one\u201d environment for remote sensing, GIS, photogrammetry, and cartographic processing. The PCI Software Development Kit supports Geomatica 9 and all of the built in Geomatica 9 software code libraries.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-z-f-s-new-blue-workflow-registers-point-cloud-data-on-the-fly\/","title":"NEWS: Z+F\u2019s new \u2018Blue Workflow\u2019 registers point cloud data on the fly","date":1431388800000,"text":"Integrated laser scanner and scout software means crews can leave the job site with fully registered point cloud data.\nZoller + Fr\u00f6hlich\u2019s new 3D laser scanner, the Z+F IMAGER 5010X, is said to come with a unique navigation system which works both outdoors and indoors. This is made possible by GPS and MEMS-based inertial measurement unit (IMU) sensors, combined with an automatic registration capability, which estimates the scanner position and orientation without any external targets. The scanner will also track the movements while carrying the device on to the next scanning position.\nThings appear to get even more interesting when combining the Z+F IMAGER 5010X with the new Z+F LaserControl Scout Windows tablet software. This resulting so-called \u2018Blue Workflow\u2019 allows users to check data and target quality and register scans before leaving the site. If any gaps are found in the data users can then fill them in with more scans, helping ensure the point cloud dataset is complete.\nAs data registration and verification is traditionally done back in the office this could be a huge time saver by reducing the need for site re-visits.\nZ+F LaserControl Scout software automatically synchronizes all scan data locally and, after registration, updates all scans on the scanner.\nData transfer between the Z+F LaserControl Scout software and Z+F IMAGER 5010X is carried out over high-speed WiFi (802.11n with dual antennas), which can move 200MB of data in 30 seconds.\nBoth devices can be used in parallel, which helps minimise time on site: the Scout software can register scans while the laser scanner captures new data.\nThe Z+F LaserControl Scout software runs on a Window 8 A5 or A7 core touch tablet with 8GB RAM.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nAutodesk allows virtualisation and cloud-based deployments\nAecom acquires AI start-up Consigli\nNvidia RTX Pro Blackwell workstation GPUs launch\nNEWS: HP introduces HP Z1 G3 all-in-one workstation\nAEC Magazine March \/ April 2024 Edition\nAutodesk targets BIM with Forma Building Design\nCorel Designer Technical Suite 12\nNEWS: AMD Ryzen 7 CPU good news for CAD users who render\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/into-the-fold\/","title":"Into the fold","date":1401148800000,"text":"Using robots to bend, shape and fold sheet metal is adding freedom to architectural fa\u00e7ade designs. Stephen Holmes visits Robofold and becomes transfixed by robotic origami.\nIn the metallic arms of a pair of assembly robots an evolutionary change in architectural cladding is underway.\nAt Robofold\u2019s South London headquarters two 6-axis ABB robots are moving in tandem, gliding into their pre-programmed positions, their suction cups attaching to a sheet of pre-cut aluminium.\nThey arch and twist till the metal is bent and precisely formed into a giant petal, a replica of the one displayed on the screen of the nearby controlling laptop.\nThis process is capable of creating individual cladding panels \u2014 something that until now has been too expensive and time consuming.\nWith tools that can now liberate the form of each and every individual panel, architects can transform the overall shape of building faades using Robofold\u2019s design and manufacturing technique.\nRobots rock\nThe system works off a plug-in for designs built in Rhino 3D CAD with the software coding developed by Robofold.\nThe designs are translated into folding simulations in the King Kong software. In turn these results are linked to the Godzilla robot software, to simulate robots folding, before the physical folding begins.\nThe folding software can be purchased on its own, but Robofold has also developed 3-axis CAM software, 6-axis CAM software, and other robot inputs to make best use of the hardware it drives.\nFor the construction team, the individual nature of each programmed sheet being formed by robots at the building site means any last minute changes can be quickly and easily rectified.\n\u201cIf you want variation it becomes really valuable,\u201d Gregory Epps, founder of Robofold said. \u201cYou can program that in up-front and say what are the parameters that you want to vary.\n\u201cIf you want to tweak the design a little bit you don\u2019t even have to reprogram the CAM software and the robot software \u2014 if you had to make a new mould then it would be crazy \u2014 so this is super efficient.\n\u201cYou can prototype in production materials and then manufacture using the same equipment, so you know what you\u2019re going to get.\u201d\nFold over\nMetal is best for bending, typically using 1.5mm aluminium, cut with the CNC router in the factory. It can also work laser cut steel or stainless steel up to 1.5mm.\nThe steel can be finished with paint or dipped in zinc, while aluminium should be anodised; giving a range of options for finishes.\n\u201cNormally in architecture we get requests like, \u2018can you make it 3mm?\u2019 as they\u2019re used to a flat faade panel,\u201d Mr Epps said. \u201cBut once you put the curvature in it and the fold in it you\u2019ve added so much rigidity that you don\u2019t need all that material \u2014 you can halve the amount of material that you need.\u201d\nAccuracy of the panel is defined in the cutting of the flat sheet, and because the process does not stretch the material like a standard press does, holes can be pre-drilled and trimming is not needed.\nLocal stretch can be calculated as normal with sheet material around the fold, so the designer knows that it is going to fit to the building\u2019s exterior substructure.\nThe speed of the process is slower than that of pressing, but offers labour-saving benefits \u2014 it is a single stage process, whereas pressing could involve several stages before being drilled and trimmed by a 5-axis laser-cutter.\n\u201cNormally you would say that you have to make the forming part to be perfect, but in this case it\u2019s not too critical about the forming, it\u2019s more critical about cutting it right first,\u201d Mr Epps explains.\n\u201cYou can fold something in paper and go and make it \u2014 it\u2019s the ultimate goal.\u201d\nWorking with desktop paper printers to verify and tweak sheet designs allows mini versions of an architect\u2019s design to be scaled up for production in metal.\nShaping Venice\nThe largest piece of work to date created by this method was for Zaha Hadid Architects \u2014 the Arum sculpture for the Venice Biennale.\nA six metre high, tulip shaped, self-supporting structure, the design was created with Robofold\u2019s input and manufacturing service.\n\u201cWe helped the Zaha team to develop their own software so that it would generate all of the data we\u2019d need for production.\n\u201cThey were able to generate valid, folded surfaces, and then out them to us and we would automatically create the production data from it.\u201d\nThe sheets were cut in the UK and transported to the site in Venice where two robots were ready waiting to fold them.\nSeeing the process in action was a big benefit for the Zaha Hadid team, although this did add pressure.\n\u201cWe still had them on site, changing their mind while we were building it!\u201d Mr Epps explains.\nThis problem only served to highlight the benefits of the process \u2014 new parts were swiftly modelled, cut and flown to the site, where the robots folded the new sheets without any mass reprogramming or delays.\n\u201cI think it was quite daring of them to use us, as this was the largest piece we\u2019d done to date. It\u2019s a really cutting edge technology, but it shows that they\u2019re a cutting edge practice to be able to embrace it.\u201d\nCutting edge\nIf the future is to grow more dependent on mobile, localised factories then methods of onsite production like these offered by Robofold are destined to lead the way.\nOnce more architects and structural engineers realise that panels formed on site are as accurate as the designer\u2019s 3D model, the increased adoption of this method is inevitable.\nThe next step for Mr Epps and his team is to license the system, supplying two robots, a router, a cutter, a pick-up table, and all the necessary software.\nLinking with others in the Robofold licensed network, it will become part of a linked FabLab system where each facility can pick up overspill work from others.\nSuch a system of robotic manufacturing methods is an exciting method to free up the future of design from mass produced parts, and add further creative flourish to architecture.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/workstations\/dell-precision-7540-nvidia-quadro-rtx-5000\/","title":"Review: Dell Precision 7540 [Nvidia Quadro RTX 5000]","date":1579737600000,"text":"Users of real-time viz, VR and GPU rendering software can now get performance and portability from this beefed-up 15-inch mobile workstation, writes Greg Corke\nEveryone wants their laptop to be slim and light, and there are several impressive 15-inch mobile workstations out there that come in under 2kg. But while the Dell Precision 5540, HP ZBook Studio G5 and Lenovo ThinkPad P1 might be perfect for 3D CAD, they will likely fall short in more demanding workflows.\nWhen Dell launched the chunkier Precision 7540 in May 2019, it followed a familiar template for mainstream 15-inch mobile workstations: top-end mobile CPU (up to an 8-core Intel Xeon), lots of memory (up to 128GB), plenty of storage (up to three drives), but a more conservative selection of GPUs, maxing out with the Nvidia Quadro RTX 3000 (6GB).\nThe machine proved great for CPU-intensive workflows, such as ray trace rendering, simulation and point cloud processing. But the Quadro RTX 3000 was only really suited to entry-level real-time viz, GPU rendering and VR.\nAs Dell\u2019s more demanding customers were led to the 17-inch Precision 7740 and its high-end Quadro RTX 5000 (16GB) GPU, other manufacturers were looking to pick up business by offering the same GPU in a 15-inch mobile workstation, a form factor that\u2019s smaller and lighter and considerably more popular.\nLenovo was the first to break the mould. With its ThinkPad P53, it became the first major workstation OEM to offer such a high-end GPU in a 15-inch mobile workstation. Acer, Asus, BOXX, MSI and Razer also delivered 15-inch laptops with the Quadro RTX 5000.\nIn response to this competition, Dell completely re-engineered the cooling system in the Precision 7540, so that it can take the Quadro RTX 5000. What\u2019s more, Dell reckons it can run it at 80W, which it claims is 10W more than the Lenovo ThinkPad P53. Update 3\/02\/20 Lenovo has informed AEC Magazine that the Quadro RTX 5000 in its ThinkPad P53 also runs at 80W\nThe chassis\nThe Quadro RTX 5000 might grab the headlines, but there\u2019s a lot more to this mobile workstation than simply a powerful GPU. The Precision 7540\u2019s chassis is incredibly well-built and feels extremely solid.\nAt 25mm (front) and 29.95mm (rear), it\u2019s not exactly thin and with a large bezel surrounding the 15.6-inch display, it\u2019s also wide and deep (337.6mm x 251.3mm). However, starting at 2.53kg, it still feels relatively portable, and is significantly smaller and lighter than the 17-inch Precision 7740, which starts at 3.08kg.\nA larger chassis does have its benefits. Unlike the slender Precision 5540, there\u2019s a full-sized keyboard, complete with number pad for punching in dimensions. There\u2019s a little bit of flex in the middle of the deck when typing, but it generally feels nice to use.\nThe trackpad is of a decent size with a good amount of resistance and three clickable buttons. There\u2019s also a trackpoint with its own set of buttons below the space bar. But for precise design work in general, we\u2019d always recommend an external mouse.\nThere\u2019s a choice of four displays: three are FHD (1,920 x 1,080) and one is 4K (3,840 x 2,160). We\u2019re used to seeing nice displays on Dell mobile workstations and the 4K IGZO panel that came with our review unit is no different. Linework in CAD is super-sharp and colours are notably vivid. The panel supports 100% of the Adobe RGB colour space, so you should pick up nice subtleties in your renders.\n4K on a 15.6-inch display means a very high-pixel density. To avoid tiny icons and text that\u2019s unreadable to ageing eyes, Windows ramps things up to 250%, but we found the best balance to be at 200%. If you\u2019re on a tight budget and don\u2019t need 4K, you can save yourself around \u00a3300 by choosing a more basic FHD panel.\nIn terms of connectivity, the machine is relatively well-equipped. There are 2 x USB 3.1 Type A ports w\/PowerShare on the right, 2 x Thunderbolt 3 Type C ports on the left, and a mini DisplayPort 1.4, HDMI 2.0 and Ethernet at the rear. Those with multiple USB Type A peripherals can always plug in a hub or use a USB Type C to A adapter, as we did when setting up an Oculus Rift VR headset with two sensors.\nKey repairs and upgrades are relatively simple. After loosening five Philips screws, the back panel pops off, giving access to the battery, storage and two of the four memory slots.\nCore specs\nThe machine comes with a big choice of CPUs, from the quad-core Intel Core i5-9400H (2.50GHz, 4.3GHz Turbo), right up to the eight-core Intel Xeon E2286M (2.40GHz, 5.00GHz Turbo), which was in our review unit.\nIf your workflows aren\u2019t particularly multi-threaded, you could save yourself between \u00a3120 and \u00a3180 by going down to six cores, but you will lose a little bit of single-threaded performance as well: the six-core Xeon E-2276M and Core i7-9850H, for example, peak at 4.7GHz and 4.6GHz respectively.\nFor GPUs, there\u2019s an equally big choice. This includes the entry-level AMD Radeon Pro WX 3200 (4GB), Nvidia Quadro T1000 (4GB) and T2000 (4GB), the mid-range Nvidia Quadro RTX 3000 (6GB) and RTX 4000 (8GB) and, of course, the high-end Nvidia Quadro RTX 5000 (16GB). If you simply work with 3D CAD and therefore don\u2019t need anything more than the Quadro T2000, you\u2019re probably better served by the smaller and lighter Dell Precision 5540. The machine can be fitted with up to 128GB of RAM, spread across four DIMMs. Our test unit came with 64GB (2 x 32GB) of DDR4 2666MHz Non-ECC memory, fitted behind the keyboard. This should be plenty for most users, but should your needs change, there are two slots free for upgrades.\nThere\u2019s lots of scope for storage with support for up to three drives: either three M.2 PCIe SSDs or two M.2 PCIe SSDs and one 2.5\u201d HDD. There is a caveat to this, however: you can\u2019t have a 2.5-inch drive with a Quadro RTX 3000, 4000 or 5000, as these power-hungry GPUs need the larger 6-cell 97Wh Lithium Ion battery, which blocks off the 2.5-inch bay. With the larger Precision 7740, you can have both.\nFor those who work with very large datasets this an important consideration, as the price per GB of SSDs is still significantly higher than it is for HDDs.\nOur test unit came with single 512GB \u2018Class 50\u2019 SSD, an SK Hynix PC601. We found very little information about this particular SSD, which may or not be used in other Precision 7540s (Dell uses a range of suppliers). However, in Dell speak \u2018Class 50\u2019 means better performance than \u2018Class 40\u2019, where an equivalent capacity SSD is \u00a3200 cheaper. For many professional users, a more important metric is endurance, but we couldn\u2019t find any data on this.\nPerformance\nThe Precision 7540 is built for performance and we were largely impressed with the results from our real-world tests.\nExporting an IGES model from 3D CAD software Solidworks took 82 secs and for this single-threaded process, the Xeon E2286M CPU maintained a clock speed of 4.7GHz. This is on par with most desktop workstations.\nUsing all eight CPU cores, a 4K render in Luxion KeyShot took 381 seconds. This is slower than you\u2019d expect from a top-end eight-core desktop CPU but considering the Xeon E2286M has a TDP of 45W and a base clock speed of 2.4GHz, we were very impressed that it hit 3.2GHz for the duration of the test. In fact, it even maintained this frequency when rendering for over an hour, with fan noise at quite acceptable levels.\nPutting the Quadro RTX 5000 through its paces in Autodesk VRED Professional, an application used extensively in automotive design, 3D performance was good at 4K resolution. We got a smooth 42 frames per second (FPS) with anti-aliasing (AA) off and 26 FPS with AA set to medium. This is slightly faster than we\u2019d expect from a desktop Quadro RTX 4000, but slower than a desktop Quadro RTX 5000.\nIn Enscape, a popular real-time viz tool for architecture, we got an adequate 17 FPS at 4K with our large architectural scene. Here, performance was almost identical to a desktop Quadro RTX 4000.\nOf course, the Quadro RTX 5000 is also VR ready and we had a very good flicker-free experience with an Oculus Rift in both Autodesk VRED Professional and Enscape. With newer, higher resolution VR headsets, you\u2019d likely see a bit of drop in performance here.\nThe machine was a fair bit off the pace in GPU rendering. The Quadro RTX 5000 took about 40% longer to render a scene than a desktop Quadro RTX 4000. This was in both the V-Ray NEXT benchmark and Solidworks Visualize. This could be down to sustained demands on the GPU. With interactive graphics, the GPU is taxed on and off as and when you move the model, whereas with GPU rendering, it is hammered for extended periods. In one eight-minute render, for example, as the GPU heated up, we saw the clock speed drop from around 1,500MHz to 1,200MHz.\nWhat these results show is that while the mobile Quadro RTX 5000 might have the same name as its desktop counterpart and be built from the same silicon, you can\u2019t expect the same performance. This is down to the available power and the challenges of keeping a hot GPU running cool in a small space.\nTo put this in perspective, the Quadro RTX 5000 runs up to 80W in the Precision 7540, up to 110W in the larger Precision 7740 and up to 265W in a desktop workstation. However, it\u2019s important to note that the relationship between power and performance is non-linear and the performance curve will flatten off above 80W.\nTo really push the machine to its limits, we started a CPU render and GPU render at the same time. The CPU clocked down to 1.95GHz and GPU utilisation went down from 95% to around 70-80% but the machine remained stable. We could even launch Solidworks and spin a model without it locking up. Most impressively, fan noise never became too loud and the palm rest stayed cool, although the keyboard did get a little warm, but certainly not hot.\nThe battery drop\nWe also tested performance when running off battery and were quite surprised, if not completely shocked, at how much the GPU and CPU clocked down. In KeyShot, the CPU dropped to 2.2GHz, taking 45% longer to complete the test render. In Solidworks Visualize the difference was even more dramatic, with the GPU instantly dropping from 1,500MHz to 300MHz. It was so slow, in fact, that we didn\u2019t even wait for our render to finish.\nIn short, when running off battery, you can get away with rendering on the CPU, but for GPU rendering, you really need to be plugged in. And, of course, any compute intensive operation will drain your battery much more quickly.\nConclusion\nDell has given its mainstream 15-inch mobile workstation a new lease of life with the powerful Nvidia Quadro RTX 5000 GPU. It might not reach the heights of a desktop RTX 5000, and there are questions about performance when GPU rendering, but it gives those with GPU-hungry workflows a strong alternative to the bulkier 17-inch mobile workstation.\nWhen shoehorning powerful components into a small chassis, there\u2019s always a danger that thermals will get out of control. But Dell\u2019s engineering team looks to have done an excellent job here and in our real-world tests, the machine maintained relatively high clock speeds without too much fan noise or heat.\nConsidering the high-end spec, it will come as little surprise that our review unit costs \u00a34,690 + VAT. Moving down to a Quadro RTX 4000 would save a substantial \u00a3812. We\u2019d be interested to learn how this impacts 3D performance, although the drop from 16GB to 8GB of GPU memory should also be considered. With our largest models in Enscape VR and Solidworks Visualize, for example, we were pushing 10 \u2013 11GB.\nIn summary, those who value performance above anything else would likely be best served by the Precision 7740, or even a desktop workstation, but for a blend of speed, portability and stability in a very well-engineered chassis, the Precision 7540 is hard to beat.\nProduct specifications\n\u25a0 Intel Xeon E2286M CPU (8 Cores) (2.40GHz, 5.00GHz Turbo)\n\u25a0 Nvidia Quadro RTX 5000 GPU (16GB GDDR6 memory))\n\u25a0 64GB (2 x 32GB) DDR4 2666MHz non-ECC memory\n\u25a0 512GB M.2 NVMe PCIe Class 50 SSD\n\u25a0 Intel Wi-Fi 6 AX200 2\u00d72 .11ax 160MHz + Bluetooth 5.0\n\u25a0 M15.6\u201dUltraSharp UHD (3,840 x 2,160) Anti-Glare IGZO, 100% Adobe colour\n\u25a0 From 2.53kg\n\u25a0 337.6mm (w) x 251.3mm (d) x 25mm \u2013 29.95mm (h)\n\u25a0 Windows 10 Pro for Workstation (4 Cores Plus)\n\u25a0 3 year ProSupport and Next Business Day onsite service\n\u25a0 \u00a34,690 (ex VAT)\nCPU benchmarks (secs \u2013 smaller is better)\nCAD\n(Solidworks 2020 IGES export) \u2013 62 secs (smaller is better)\nRendering\n(KeyShot 8.1) \u2013 381 secs (smaller is better)\n(V-Ray Next Benchmark) \u2013 9,651 ksamples (bigger is better)\nGraphics benchmarks (frames per second @ 4K res) (bigger is better)\nVIZ (Enscape)\nMuseum \u2013 17\nVIZ (Autodesk VRED Pro)\nCar model (AA off, med, high) \u2013 42.05 \/ 26.55 \/ 9.85\nGPU rendering benchmarks\nGPU rendering\n(Solidworks Visualize 2020) 1969 Camaro car @ 4K\n\u2022 1,000 passes \u2013 452 secs\n\u2022 100 passes + AI denoising \u2013 56 secs (smaller is better)\nGPU rendering\n(V-Ray Next Benchmark)\n159 mpaths (bigger is better)\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/executive-briefing-bentley-systems\/","title":"Executive briefing: Bentley Systems","date":1145577600000,"text":"Over the last few years it\u2019s all gone a bit quiet on the marketing front at Bentley Systems. Martyn Day caught up with Tony Flynn, Bentley\u2019s VP of marketing, to find out exactly what the multidisciplinary AEC and infrastructure CAD company has been up to.\nWhat\u2019s your view on the AEC technology market today? What are the key trends?\nTony Flynn: I\u2019m more excited now about the infrastructure market than ever. Engineering News Record (the US-based construction industry magazine \u2013 Ed) is predicting over $4 trillion in construction for next year. There is much to improve in the world. And software for infrastructure can do much to help. Here, I see four main advances.\nFirst, general-purpose CAD is morphing and upgrading into specific disciplines. For example, CAD for architecture is becoming BIM (Building Information Modelling), which is boosting productivity and collaboration.\nSecond, products are becoming more integrated and interoperable, which is increasing quality and decreasing data loss. For example, the convergence of design and structural analysis enables building designers to simulate their designs and increase quality. The same holds true for water networks. Another example, the convergence of CAD and GIS results in fewer data translations, and therefore cuts data loss and expense for utilities, communications companies, cities, and others.\nThird, software servers are taking collaboration forward by quantum leaps, cutting costs and improving performance. And that\u2019s particularly fortunate, because most infrastructure work is now performed by distributed enterprises which must tap and connect top talent from multiple locations, often around the world.\nLastly, enterprise subscriptions for software and training are creating much more agile organisations. By removing many administrative barriers, these organisations are realising just-in-time productivity increases and more quickly responding to project needs and opportunities.\nWhere would you place Bentley\u2019s position in the market vs. its competitors? (key differentiators, unique selling points, customer profile).\nTF: Bentley is in a very strong position, and getting stronger. We\u2019re the largest software provider focused on the infrastructure market. More than 80% of the Engineering News Record 500, more than ten of the \u201cUK Architecture 15\u201d, and 47 US state DOTs now rely on Bentley products. And Daratech recently ranked us #1 in revenue in 14 categories of plant software.\nWe are in first or second place in each of our verticals: building, civil, plant, and geospatial \u2013 a testament to the strength and advantages of our solutions over other choices.\nIn building, for example, we offer a single, strong BIM solution for architects, A\/E firms, construction companies, and owners. The other choice is Autodesk, which does not offer a single, strong solution for BIM. In fact, it offers two weaker platforms: Revit, for simpler, smallish projects; and AutoCAD, with a future in building that even Autodesk executives question.\nIn geospatial, we offer solutions with CAD\/GIS integration as well as interoperability. ESRI is a great, isolated GIS product, especially for non-infrastructure projects. But, for infrastructure projects, geospatial users prefer to plan in the context of engineering, and engineers prefer to design and maintain in the context of GIS.\nHere, we\u2019ve just announced our unique Google Earth connection. It\u2019s amazing! We believe that it will become the most popular way to view and navigate CAD and GIS data together.\nAt the high end, we\u2019re unique in offering CAD\/GIS interoperability \u2013 where the two systems communicate in real time. By eliminating never-ending CAD to GIS translations, governments, utilities, and communication companies are slashing data loss and streamlining workflows.\nIn plant, we offer comprehensive solutions for the distributed enterprise, whereas Intergraph is offering a 1980s solution to a 2006 problem. Plant owner-operators need a lifecycle solution that thrives in the distributed enterprise, whereas Intergraph\u2019s solution is a frozen-in-time centralized system that struggles to scale to today\u2019s requirements.\nIn civil, we have lifecycle solutions for road, rail, site, and water infrastructure. In fact, water and road are two of our strongest segments. We have a particularly strong maintenance solution for rail, and site design is an emerging market for us, especially given the opportunity presented by Autodesk\u2019s transition from LDD to Civil3D.\nAEC CAD appears to be mature market with few new users. Where do you see Bentley\u2019s growth coming from? Or are there new customers?\nTF: Again, our market is software for infrastructure. Worldwide, construction of infrastructure is a $4trillion business. By comparison, the GDP of the world is about $40trillion. So I\u2019m not sure I agree that software for infrastructure is a \u201cmature market with few new users.\u201d Quite the opposite, there\u2019s much to improve in this $4trillion investment, and we are eager to help with software.\nTens of thousands don\u2019t even have professional CAD software, particularly in the developing economies of China and India. Current CAD users want more vertical applications in order to be more productive and produce better work. Collaboration software is becoming more attractive \u2013 to the point of becoming a requirement \u2013 in a world of distributed enterprises.\nAlso, we see an expanding market for O&M (operation and maintenance) software, especially for railways, plants, and roads.\nBesides, Bentley is very competitive and always on the lookout for adding more value. So we\u2019ve set our sights on gaining market share in all of our verticals.\nHas BIM been a measurable success, a slow burn or a non-event?\nTF: A definite, measurable success, and gaining momentum. We have many users applying BIM, and many even setting it as their standard of practice. Nearly every architecture and A\/E firm now aspires to BIM and wants a path to BIM in their software solution. This is not just for architects, but for engineers (structural and building services as well), and it\u2019s even quite relevant for construction conglomerates.\nMost importantly, even owner-operators, who write the cheques and are the ultimate decision makers, are increasingly demanding BIM. Good examples of this in the US are the General Services Administration \u2013 the largest owner-operator in the US \u2013 and General Motors.\nBIM unites everyone in the supply chain, thus improving it. BIM is now virtually always a component of any discussion with architects and engineers and contractors, and it is always about \u201chow\u201d, never about \u201cif\u201d.\nI\u2019m impressed with the growing following for Bentley\u2019s SmartGeometry technology which is still in beta. How does SmartGeometry fit into the Bentley\u2019s architectural suite?\nTF: Yes, the technology is inspiring and one that allows people to do something they could not do before. But note that Bentley\u2019s technology is named GenerativeComponents (www.bentley.com\/gc); \u201cSmartGeometry Group\u201d is a non-profit organisation of users\/advocates\/researchers (www.smartgeometry.org). GenerativeComponents is the high-level conceptual tool that allows designers to experiment with building shapes and, by design, stay within the design requirements.\nTo answer your question, GenerativeComponents is 100% integrated with Bentley products. It can drive geometry and coordinate with any of Bentley\u2019s BIM portfolio, including Bentley Architecture, Bentley Structural, Bentley Building Mechanical Systems, and Bentley Building Electrical Systems. It\u2019s part of the beauty of a single, strong BIM platform \u2013 this world-class conceptual tool immediately raises all these boats.\nBentley\u2019s route to market is a mix of direct and dealer (VAR). Moving forward, what\u2019s the channel strategy?\nTF: To major accounts, we sell directly. To small to medium accounts, we use eSales often assisted by channel partner experts.\nIncreasingly, we\u2019re finding that eSales with product specialists is the best and most leveraged way for Bentley and the buyer. As our product line and target geographies expand, it\u2019s the most efficient way to understand needs and communicate value to all corners of the world. It also happens to be our fastest-growing distribution method.\nLast year Bentley ran an aggressive \u2018you deserve better\u2019 campaign against Autodesk\u2019s upgrade policy. How successful was it? Will you be doing the same this year? Isn\u2019t MicroStation more expensive to buy and own?\nTF: It was the most successful campaign we\u2019ve ever run to upgrade Autodesk users to Bentley products. We found that CAD users need their products to natively support the three most popular file formats, DWG, DGN, and PDF. And they need historical, non-retiring support of these formats \u2013 not forced retirement that carries expensive and unfortunate consequences.\nThey also appreciate the superior price\/performance of PowerDraft, the vertical Power products, MicroStation, and MicroStation applications. In hybrid accounts, we also see a move to consolidate DWG\/DGN support using MicroStation \u2013 a move the significantly lowers their cost of administration.\nIn China, our PowerDraft Web site is the most popular offer we ever made. You\u2019ll soon hear about this year\u2019s \u201cyou deserve better\u201d programs.\nBentley doesn\u2019t appear to have a traditional view to marketing brand and technology. What\u2019s the marketing strategy?\nTF:I\u2019m not sure what a \u201ctraditional\u201d view of marketing is. Regardless, we have a focused and strong marketing strategy.\nFirst, we are uniquely and strongly positioned as a company: Bentley provides software for the world\u2019s infrastructure. That is very focused, easy to understand, helps to drive our colleagues every day, helps to draw partners, and it\u2019s clear to our users and prospects that we want to improve their business and our world. Again, we\u2019re the largest software vendor dedicated to this market.\nSecond, we provide important networking opportunities for our users. In their fast-changing world, they want to learn best practices from each other in order to stay their most productive. Here, the BE Conference is wildly popular, with 99% satisfaction rates among attendees. The BE Awards identify and promote the most amazing projects of our users. BE Magazine, as well, has the largest circulation of any digital magazine in our space.\nThird, we sponsor a thought leadership series for executives. Increasingly, technology is becoming a strategic issue in our user organisations \u2013 it is a top reason why they thrive. Executives want to stay abreast of the latest trends and developments of not just our technologies, but also related technologies. They typically think forward two to three years. And they typically enjoy discussing it with a small group of like-minded executives, mostly within their industry. This series give them that opportunity.\nFinally, in identifying and interesting new users, we like eMarketing. Using bentley.com, email, eSeminars, and more as a marketing machine offers us much higher ROI than traditional means -about 100 times higher. And it\u2019s better and more efficient for the prospect. We can communicate directly with them and understand their needs as individuals \u2013 anywhere around the world! And they can quickly get the detailed information and the sales expert they need to make their decisions.\nWe\u2019re excited that our bentley.com traffic is up over 200 percent year-over-year. And recall that eSales, the partner of eMarketing, is our fastest growing channel.\nThe BE Conference appears to be the main event on the Bentley Calendar. This year there\u2019s a European mirror event in Prague. What are the main ingredients of the BE conference? Why Europe now?\nTF:The BE Conference comprises once-a-year learning opportunities for Bentley users and their managers who want to sharpen their skills and expand their knowledge. It has over 300 keynote, training, new-technology, and best-practice sessions. BE Conference 2006 happens in Charlotte, the home of NASCAR, in late May.\nFor our European users this year, we are essentially recreating BE Conference 2006 in Prague in early June \u2013 with one exception \u2013 the BE Awards of Excellence judging and ceremony takes place only in the US.\nWhy in Europe now? It\u2019s a question of demand; Our users are asking for it. For many of them, it\u2019s far easier to attend there.\nWhat are the challenges facing Bentley in the next three years?\nTF: Maybe I should say that my immediate challenge is completing this tough interview! Bentley\u2019s biggest challenge is to continue to increase our value-add to the infrastructure market. Here, we want to continue to define the state-of-the-art in technology \u2013 our heritage. Here, we want to continue to expand our portfolio of solutions through strategic and timely acquisitions. And, here we want to continue to lead the market in interoperability.\nAnother challenge is to continue to expand internationally. The whole world needs infrastructure and we want to fully help all countries increase their infrastructure performance. You may already know that we have moved two senior executives to Asia to better serve those markets. There and throughout the world, we are improving the reach and efficiency of our sales, marketing, and service models. And, before I end, I want to mention that we\u2019re off and running with global services \u2013 with over 500 professionals, we have perhaps the largest service organisation in infrastructure software in the world.\nAnd, ending with marketing, we need to continue to clarify our advantages over Autodesk, ESRI, and Intergraph. That\u2019s a challenge that we welcome because, for the infrastructure market and organisations that we serve, the advantages that we offer are clear and compelling.","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/mapinfo-identifies-where-college-graduates-live-and-what-jobs-attract-them\/","title":"MapInfo identifies where college graduates live and what jobs attract them","date":1056326400000,"text":"Many of this year\u2019s college graduates are leaving behind more than four years of studies to find a job in this tough economy. According to MapInfo\u2019s Predictive Analytics demographers, many are also leaving their college town. In a study of residents with bachelor degrees, MapInfo uncovers interesting demographic trends about the country\u2019s top metropolitan areas. While bachelor degree-holding beach boys and girls of California dominate the top ten list, renowned \u201ccollege town\u201d Boston, Massachusetts did not make the cut. The top ten metro areas with the highest penetration of people with bachelor degrees are clustered in California, Connecticut, New Jersey, Washington, D.C. and Colorado.\nMapInfo uncovered this information while engineering the PSYTE\u00ae U.S. update, MapInfo\u2019s industry-leading neighborhood segmentation system. Incorporating data from the 2000 U.S. Census with MapInfo\u2019s location-enhanced lifestyle and consumer demographic data, PSYTE provides valuable insights into lifestyles and consumer behavior. Connecticut\u2019s Cream of the Crop\u2014It is no secret the richest individuals are often the best educated. The Constitution State is no exception; it is home to some of the most affluent individuals and the highest percentage of residents with bachelor degrees. Graduates are attracted by a thriving financial services industry, led by insurance, finance and real estate companies that employ more than 2.2 million local residents. From the Big Apple to the Garden State\u2014Middlesex County, New Jersey is not just hometown to rockstar Jon Bon Jovi. The Middlesex-Somerset-Hunterdon metro area is also home to the tenth largest metro in MapInfo\u2019s list. Industries such as pharmaceuticals attract graduates, seeking white-collar jobs, away from nearby business mecca New York City. While taking advantage of lower tax structures, these businesses are also closer to employees\u2019 homes, reducing stressful commutes. Trading in the surfboard for an SUV\u2014California has half of the top ten metros. San Jose and San Francisco metros have the nation\u2019s largest numbers of college-educated Asians. Often seen driving an SUV or Asian sports car, these grads keep their college connection and are often members of their school or college alumni associations. PSYTE\u00ae U.S. is a unique combination of MapInfo\u2019s location-enhanced lifestyle and consumer demographics and clustering techniques that result in one of the industry\u2019s renowned neighborhood clustering systems. PSYTE U.S. ties location to the behaviors and characteristics of the diverse American population helping customers make more insightful decisions about market and product potential, store placement and target marketing.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/technology\/review-archicad-19-performance-bim\/","title":"REVIEW: ArchiCAD 19: performance BIM","date":1431561600000,"text":"With significant speed increases in this bumper update to ArchiCAD, Graphisoft is clearly aiming to get the attention of Autodesk Revit customers who are frustrated with slow model performance, writes Martyn Day.\nSpeed is something that designers have always striven for with their tools. Even back in the good old days of 2D CAD, drafters wanted the latest Intel x86 workstations, with \u2018masses\u2019 of RAM (4MB) and powerful graphics cards, albeit ones capable of providing only SVGA in 256 colours.\nJust when we started thinking we had more than enough power on the desktop, and file exchange \u2018just worked\u2019, the industry moved to BIM, which combines 3D geometry, proprietary databases, parametric constraints and rich layers of metadata, requiring lots of RAM and processing power.\nThis move has driven us back to trying to squeeze every last drop of performance out of our workstations. Speed is one of the biggest issues in BIM today and CAD managers have to spend their time working out strategies to handle large unwieldy datasets to keep projects on track.\nIn general the industry is creaking under the combined effect of big data shoehorned into \u2018old\u2019 BIM software. Neither the complexity of models nor the advances of processor architectures were originally planned for in the current generation of applications and many software developers have failed to fully utilise multi core processors, instead relying on a single processor core to do most of the heavy lifting.\nLooking forward, software that does not utilise multi-cores could actually stagnate in terms of performance as the next generation of processors from Intel will have more cores but single threaded performance may not increase significantly. Next generation tools will get acceleration from parallelisation of processes not clock cycles.\nHungary-based developer Graphisoft has been well aware of this change and over the past four releases has been building comprehensive multi-core support into its ArchiCAD BIM platform. This parallelises the computing that\u2019s necessary for handling large BIM models. However, the latest release, ArchiCAD 19 brings a whole new dimension to using the multi-cores available.\nPredictive performance\nPrior to release 19, ArchiCAD\u2019s multi-core capability would break up tasks and allocate those to individual cores on your workstation\u2019s processor. This was very much on demand and related to the current view. With this release, ArchiCAD further enhances its use of multiple cores by constantly using the spare capacity of the workstation CPU and \u2018guessing ahead\u2019, the processing tasks that it \u2018thinks\u2019 you are going to do next. While this might sound fanciful and indeed a touch too much like snake oil or clairvoyance, it\u2019s actually quite a stroke of genius.\nObviously, ArchiCAD can\u2019t actually predict the future but there have been changes to the interface which assists it in getting ahead with the processing workload. With 19, Graphisoft has introduced the concept of tabbed workspaces, which work in a very similar way to tabs in a web browser, but each tab contains a workspace. This makes it much easier to flip between sections, model view, drawing view, render or elevations.\nThe predictive processing uses these as clues as to what you will be doing next, As you work in one tab making edits and updating the design, the software will dynamically allocate the update of the model in those tabbed views to spare cores or processing capacity. The net result is that moving from tab to tab, users will find the model \u2018instantly\u2019 updated or a long way into performing a task \u2014 such as a 3D section. So the software appears significantly more responsive and makes the most of today\u2019s multi-core architectures.\nGraphisoft explained that in addition to the \u2018tabbed clues\u2019 for predictive processing, the new predictive algorithm will do some analysis as to workflow and commonly used tools, to further refine the allocation of processing. To ensure that all this additional \u2018front-loaded\u2019 processing doesn\u2019t impact the overall speed of ArchiCAD, the software leaves one core free. By using this unused computer capacity, even if the software anticipates incorrectly, nothing has been lost.\nAs to how much quicker this makes ArchiCAD 19 over 18? Graphisoft stated the benefit can be from \u2018Instant\u2019 to a 70% improvement in creating 3D views. The opening time for all model-based sections, elevation or 3D documents has dropped by half. Beta testers have told me that it\u2019s a significant improvement and absolutely screams on Apple\u2019s new Mac Pro, which comes with an Intel Xeon processor with up to 12 cores.\nOpenGL\nIt\u2019s not just editing and rendering that have seen speed improvements in this release; graphics performance has also been further optimised with faster and smoother OpenGL navigation. It is now possible to navigate through extremely large models with a \u2018flickerless display\u2019, reaching frame rates well in excess of 30 frames per second. In reality, this kind of performance is games software territory and brings real-time performance to BIM.\nNew ArchiCAD functions\nTabs: Workspace is really important and so many BIM tools today take up valuable screen real estate with menus and frames. 19 does away with many of these menus, making them \u2018pop-up\u2019 when the mouse is moved to the top of the screen and the new \u2018tab\u2019 views make it much easier to navigate.\nRhino: Graphisoft has decided to team up with McNeel, developer of the Rhinoceros NURBS-based modelling tool and Grasshopper computation design environment.\nArchiCAD 19 has been enhanced to better import Rhino geometry and there are plans to further link Grasshopper to drive complex forms in future releases. This saves Graphisoft having to develop this type of functionality and there are thousands of proficient users of Rhino and Grasshopper out there.\nPoint clouds: ArchiCAD 19 now reads industry standard point clouds for the import of laser scanned survey data. This is formative functionality at the moment but will be expanded on in the future. For now it is possible to select and model BIM objects to the XYZ points.\nSurface label: ArchiCAD now has an interactive 3D Surface Painter for drag-and-drop editing on building model surfaces in 3D. This provides instant visual feedback.\nProductivity: In general productivity, Graphisoft has added dimensional improvements for the ease of laying out areas where dimensions and labels can automatically overlay and become confusing.\nArchiCAD 19 has overhauled its Guide Lines function to better match the way you work, making precise input even more intuitive. There are new on the fly snap guides and line smart guide lines which stay as long as you want them (permanent if necessary on a separate layer)\nLayouts can now be exported (not just model views) with their drawing layers to PDF format. In the resulting PDF document, you can show or hide content by layers, or navigate among layers.\nTeamwork and BIM Server have also seen some useful updates, such as a BIM Cloud \/ BIM Server diagnostic tool.\nBIM X: The company\u2019s unique portable model sharing format has been slightly enhanced, where authors can control which element information will be published as part of the Hyper model.\nMEP: Collision detection now works for all elements classified as MEP-type in Element Settings and not just for MEP Modeler elements. As a result, collision detection works on all imported IFC MEP elements (regardless of which MEP application exported the IFC model), and all ArchiCAD project MEP Objects.\nNemetschek\nThe final thing to note is a small corporate change, but Graphisoft also unveiled its new logo, which is actually very similar to the old one but now includes the brand \u2018Nemetschek\u2019 within it. Graphisoft is owned by Nemetschek, a German developer, which started out with Allplan \u2014 a BIM system popular in Germany.\nNemetschek also owns Vectorworks (Formerly Minicad), Maxon (rendering) and Bluebeam (PDF workflow) amongst other firms. Historically the company has tended to \u2018silo\u2019 its acquisitions but now there appears to be a more cohesive approach to push the brand.\nConclusion\nThis is another great release of ArchiCAD with loads of crowd pleasers. Putting performance aside, changes to the interface, giving maximum model room, really makes a big difference and certainly helps make the most of a laptop screen.\nThere are a range of useful new features and capabilities ranging from small productivity enhancements to collision detection and IFC output.\nFollowing Bentley and Autodesk, point cloud support is now the de facto standard in the industry, as more firms experiment with importing survey data to rapidly generate BIM models from real-world measurements. This is definitely a useful future technology.\nThe decision to work with Rhino and Grasshopper is also a no-brainer for getting complex forms into ArchiCAD. It saves the company developing computational design tools, as Bentley (GC) and Autodesk (Dynamo) already have done and Rhino is also exceptionally popular in practice.\nDespite being the most mature BIM application on the planet, you have to admire Graphisoft\u2019s commitment to driving performance and fundamentally re-architecting ArchiCAD to utilize the capabilities of multiple cores on both Windows and Apple OSX. BIM models are big and complex and as models get ever bigger and more detailed it\u2019s important to use whatever hardware is available to accelerate this. When looking forward and knowing that processors will get more cores but single core performance won\u2019t increase at the same rate, software has to be ready for this.\nAutodesk Revit, which only runs in Windows, has some multi-core support, but these aren\u2019t for discreet processes and apply to separate capabilities like rendering. A number of functions are multi-threaded (Save \/ Load etc.) but the software\u2019s architecture is just not designed to work that way, which is a concern, considering the way the next generation processors will work.\nIn a BIM discussion with an Intel employee we were told, \u201cAs processor clock speeds dip, you will see the number of cores greatly increase. We could have six or 12 cores minimum in the future. If you think of these like lanes of a motorway, software that just uses one core will be deciding to only use the inside lane, with all the other \u2018traffic\u2019 in your system. Software that needs to provide performance will have to make use of all the other lanes, as opposed to Revit stubbornly staying in the slow lane\u201d. We wonder if Autodesk has any plans for a major reworking of the Revit code or, which seems more likely, it may bypass updating the software architecture of the desktop version of Revit and can get better model performance by leveraging a new system architecture in \u2018the cloud\u2019?\nWith 19, Graphisoft is certainly seeking to have ArchiCAD\u2019s performance on big models compared to Revit\u2019s. At the launch event Graphisoft\u2019s ArchiCAD\/Revit comparisons were very much at the forefront. Graphisoft showed its own benchmark results finding the insignificant\u2019 improvement from Revit\u2019s current multi-threaded capabilities and went on to state that \u201816GB is the minimum RAM requirement for Revit, while 16GB is the maximum you will need in ArchiCAD\u2019.\nGraphisoft\u2019s development team feels it does a better job with 3D graphics acceleration, multi core utilisation, memory efficiency and now predictive processing.\nFor Mac users, ArchiCAD is a much more simple choice as there\u2019s only two BIM modellers on the market that run on OS X \u2013 ArchiCAD and Vectorworks. But ArchiCAD 19 is the only multi core BIM tool that could justify the purchase of a lovely Mac Pro and make use of the cores on its Xeon processor.\nPrice: From \u00a31,699 Website: graphisoft.com\nFrom multi-threading to multi core\nMulti-threading is a program\u2019s ability to break itself down to multiple concurrent threads that can be executed separately by a computer.\nA multiprocessor (or multi-core) computer can run two or more of the threads at a time, which means that the program \u201cruns faster\u201d on a multiprocessor machine than on a single-processor machine.\n\u201cTraditional\u201d single-thread applications cannot make use of two or more processors, therefore they don\u2019t run faster on multiprocessor machines.\nMulti core processors. All Modern Intel desktop processors have multiple cores. Depending on the product there can be 2, 4, 6, 12, even 18 cores per processor.\nIn a dual processor system, which has 4 cores per processor, there would actually be 8 physical cores available.\nIn an ideal world, four cores would provide linear scaling of four times the processing capacity of one but most applications don\u2019t see this kind of benefit.\nHyper-threading is a very specific technology from Intel, which aims to improve the parallelisation of tasks for software developers. Some applications use Hyper-threading, many do not.\nHyper-threading turns each physical core into a virtual dual core system. So a dual core processor that supports Hyper-threading actually has four virtual cores.\nIn our example of a dual processor system, each with four cores (eight in total), the Operating System would actually see 16 virtual cores.\nHyper-threading is slower than actually having completely separate physical cores, as there are some operations which cause \u2018collisions\u2019 of system resources, which can negate the performance advantages and actually make it slower.\nSoftware has to be written to get the balance right. When done well, speed benefits of around 15% can be achieved vs no Hyper-threading.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-xavier-de-kestelier-hassell\/","title":"Video: NXT BLD 2019 \u2013 Xavier De Kestelier, HASSELL","date":1565049600000,"text":"Extra-Terrestrial Architecture \u2013 NXT BLD London, June 2019\nThe AEC industry often looks at the automotive and aerospace industries on how to develop its own design technology and methodologies. As an industry, we often feel like the poor, not so technologically developed cousin of the more developed and engineer heavy industries. Xavier De Kestelier from HASSELL opposes this idea, and will show how a design focussed approach can unlock new potential in a field that is traditionally engineer driven. The HASSELL design proposal for the 3D Printed Habitat for NASA\u2019s Centennial Challenge is a perfect example of this. HASSELL sought perspectives from outside the traditional aerospace industry to explore how a human habitat could be designed and delivered on Mars using autonomous 3D printing technologies. HASSELL partnered with Eckersley O\u2019Callaghan to design the external shell, which could be constructed entirely by autonomous robots using Mars\u2019 natural regolith..\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/be-inspired-netherlands\/","title":"BE Inspired Netherlands","date":1291507200000,"text":"Bentley Systems\u2019 BE Inspired event started off as a temporary replacement to the company\u2019s main user conference when the banking crisis hit. Now the award event appears to be a standard fixture in the calendar and this year Martyn Day was an awards judge.\nLooking back, it is amazing that the banking crisis was a few years ago. It seems to have been nothing but trouble for the building and construction industry since. With travel bans introduced by most of their customers, many CAD companies scaled back their user events.\nBentley Systems had to cancel its own big end-user jamboree and instead opted to run the annual awards ceremony, together with a select number of customers in Charlotte, North Carolina. The intimacy of a small group provided a great opportunity to talk to the company\u2019s leaders, as well as their more innovative customers. Bentley Systems subsequently deemed the new format a success and now runs two events, one for users and one on \u2018thought leadership\u2019 for press, analysts and infrastructure customers who entered the company\u2019s annual awards.\nThis year\u2019s event was in Amsterdam and I was invited to be on the judging panel for the Architecture and Structure categories of the awards. While the sceptical among you may think that these awards go to customers that spend the most, I can confirm that it is totally independent and a panel of non-Bentley affiliated industry folks get to look through hundreds of entries (documents, videos, images) and sweat it on numerous conference calls and in face-to-face meetings.\nAs we weeded our way through the 164 entries it got harder to favour one over another. There were debates, sometimes heated, frequently with passion about the various merits of who, why, what and how. Our Bentley \u2018minders\u2019 helped out where they could but strictly stayed out of any decision-making (even when we begged them for help to break split decisions). It was a difficult but extremely worthwhile process as we got to see some very good real-world uses of 3D CAD technology.\nKeynotes\nBentley loves the word \u2018infrastructure\u2019, and it is guaranteed to be in any presentation by the company \u2014 but what a few years the global infrastructure market has endured.\nBentley Systems CEO, Greg Bentley, gave a keynote that highlighted how things were slowly getting better, from company revenues to product usage. Bentley has a very advanced server architecture that means Bentley knows when and how often its software is used. In a number of graphs we could see the drop in hours spent designing as the global banking crisis hit. Alongside was almost two year\u2019s of data showing very slow, gradual increasing use in the United States; but there was variation across other continents, with Asia doing the best and perhaps Europe still suffering the most.\nBentley is back in growth mode after a banking-crisis dip. Greg Bentley presented a few graphs that would indicate that the company is bouncing back quicker than its long-time rival Autodesk. This could probably be due to Bentley\u2019s clients benefitting from the additional infrastructure spend from government stimulus money. A segment of the market in which Autodesk has less penetration.\nBentley has compiled and released for the first time the Bentley Infrastructure 500 Top Owners ranking to help people appreciate and explore the magnitude of investment in infrastructure and the potential to continually increase the return on that investment.\nThe infrastructure value represented is over $13 trillion which, to provide perspective, is close to the United States\u2019 annual GDP and equal to the combined annual GDPs of China, Japan, and Germany.\nData sources for the Bentley Infrastructure 500 included published financial statements and third-party research. The rankings, which Bentley will update annually, make it possible to readily compare investment levels across types of infrastructure, regions of the world, and public and private organisations.\nThe top five countries, in terms of the infrastructure value of the Top Owners headquartered in each, are:\n1. United States \u2014 166 Top Owners, $4,241 billion\n2. Japan \u2014 58 Top Owners, $1,349 billion\n3. China \u2014 29 Top Owners, $870 billion\n4. France \u2014 22 Top Owners, $747 billion\n5. United Kingdom \u2014 23 Top Owners, $617 billion\nThe document makes for interesting reading and I have no idea how it was compiled as it would seem an impossible task. Facts like the biggest manufacturer owner is Toyota Motor at $72 billion and the biggest bank owner is Royal Bank of Scotland at $25 billion \u2014 so now we all know what we have bailed out and is now publicly-owned. Read it here.\nBentley senior vice-president Bhupinder Singh gave an update on the benefits of a number of technologies that were released last year, namely i-model and the integrated structural model. There was an interesting demonstration of forthcoming 3D document dimensioning and a lot of talk about supporting the latest tablet PCs and the iPad, in fact there was an iPad app on show in the break-out coffee area, together with some cool 3D visualisation technologies.\nDuring one of the socials I had a great conversation with Mr Singh of the merits of cloud computing and the opportunities it raised for Bentley. The company already has an extensive client-server architecture of its users and there appears to be lots of opportunity for the company to offer online services in addition to its traditional desktop solutions \u2014 although do not expect to see Bentley develop for anything too far from Microsoft, except perhaps the iPad.\nProjects\nThis year\u2019s entries brought an embarrassment of riches, with everyone from signature architects like Zaha Hadid and a horrendously complex steelwork created by Arups, to innovative water catching buildings inspired by bugs to a mundane airport landing refurbishment. All had their own story and valid reasons why they should be highlighted. I have picked a selection here that pushed my and my fellow judge\u2019s buttons.\nThe Namibian Hydrological Centre of Excellence\nNot a winner but certainly highly recommended and innovative. Atlas industries demonstrated the design for a fictional hydrological centre that could be fabricated in a desert\/sea location, such as the skeleton coast in Namibia. Africa has a water shortage and this design, inspired by a beetle, which sits on top of the sand dunes to catch the morning fog on its hind legs, really imitates nature. A fog catching net provides a place for water to condense and be trapped, where it is stored and purified. The design also provided a visitor centre and lecture rooms and utilised self shading to limit artificial cooling. We all hope that someone decides to build one of these and try it out.\nMarina Bay Sands\nWinner Innovation in Structural Engineering \u2014 The Moshe Safdie design features three cascading hotel towers topped with the 1.2 hectare Sands SkyPark cantilever platform. Also on the site, are crystal pavilions and a museum inspired by the shape of a lotus flower.\nThe SkyPark cantilever, 200 metres up, is the longest of its kind in the world and required an immense amount of steelwork (7,000 tonnes). Arups designed the structure to withstand typhoon strength winds and the vibration caused by people movement. The sheer amount of structural work required impressed the judges, let alone the unique design requirements, as well as all the editing during the iterative design process. Think what you will of the building\u2019s aesthetics, the structural model was deemed a thing of great beauty.\n41 Cooper Square\nMorphosis architects \u2014 Not a winner, but the most hotly contested of the projects submitted. 41 Cooper Square, is the new academic building for The Cooper Union, a highly respected New York \u2018free\u2019 university in America which has been running for 150 years. Internationally renowned architects, Morphosis had to produce a modern building to tight budgetary constraints and the end result is really stunning. The young team used all the latest technologies and the presentation displayed an extensive understanding of Generative Components.\nThere was a little confusion as to which part of the project submitted for the competition, as the documentation related to the building but the event presentation focussed on the \u2018Gigamesh\u2019 atrium that was designed with GC. It consists of an undulating lattice that reaches the full height of the building and involved the scaffolding and moulding contractors to work together to hand craft and suspend a giant mesh.\nDuring the design the mesh structure was constantly questioned due to budget and was finally built out of some contingency money that had been saved. The passion to include this in the design was obvious and the amount of trial and error required to build the intense mesh required a lot of learning on the job. The use of GC in the definition of the design was exemplary, especially as there were so many changes. However, we didn\u2019t hear enough about the use of GC in the building design and there were questions raised as to whether this was art or architecture. Still Morphosis is a fantastic practice, with obviously very bright designers.\nCrystal Towers\nOne of two winning entries from Henning Larsen Architects, The Crystal Towers is a commission from Saudi Binladen Group. Within a new financial district in Saudi Arabia\u2019s capital city, Riyadh, the two 18- and 26-storey Crystal Towers are located between the \u2018Financial Plaza\u2019 and the \u2018Wadi\u2019, a pedestrian thoroughfare.\nRecessed, scaled, crystalline openings were calculated to minimise solar heat gain and associated cooling requirements, while optimising views to the surrounding plaza and landscape. The project is designed to achieve LEED Certification upon completion. The team demonstrated an interesting use of Generative Components to design the stone cladding of this highly insulated and unusual building.\nVillas in the sky\nWinner Generative Components innovation. Having already one another award the judges were concerned about giving the same team another award but each category had to be judged on its merits and in this design and presentation we were left in no doubt that Generative Components was used in a very compelling way to created a very complex facade that went through many changes in the design process.\nThe 34-storey tower has shifted upper plates to create a jagged look. The angled panels at the top are used to reduce the strong sunlight and energy consumption. GC was used to automate much of this unusual fa\u00e7ade.\nThe Stone Towers business park\nWinner of the Innovation in Campuses award, the Stone Towers business park is designed to harness the power of the sun. Zaha Hadid\u2019s 180,000 m2 Stone Towers project had the typically strong forms that we have come to expect from her studio. The project combines world-class office facilities with retail stores, food and parking facilities, and a five-star hotel.\nThe towers comprise 550,000 sq m above ground and another 600,000 sq m below ground. Inspired by the patterns and textures of ancient Egyptian stonework, the tower fa\u00e7ades will be covered with recesses and protrusions that capture the effects of light and shadow. In this an incredibly dense mixed-use development, the judges couldn\u2019t help but be blown away.\nConclusion\nThis year\u2019s BE Inspired event lacked the big product launch news of past events and I missed the usually highly engaging Keith Bentley technology keynote, but there was a wealth of great customer projects. One of the winners, London\u2019s CrossRail, gave a fantastic presentation of the huge effort that is going on in the UK capital, in one of the biggest projects currently in the world. It is a huge endeavour to enable those troublesome bankers to commute to work quicker.\nIf you are a MicroStation customer but have not thought about entering the competition, I would strongly urge you to do so. If you do, pay attention to the detail and documents you submit. Tell the story of your project and how you used all the technology at your disposal to innovate and win against the odds.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/news-dassault-systemes-helps-build-virtual-singapore\/","title":"NEWS: Dassault Syst\u00e8mes helps build Virtual Singapore","date":1435795200000,"text":"3D digital platform to be used to run simulations using realistic large-scale scenarios of Singapore\nDassault Syst\u00e8mes is working with the National Research Foundation (NRF) in Singapore, to develop Virtual Singapore, a dynamic, 3D digital model of Singapore that will employ data analytics and simulate modelling capabilities for testing concepts and services, planning, researching technologies and generating community collaboration.\nWith images and data collected from various public agencies, including geometric, geospatial and topology, as well as legacy and real-time data such as demographics, movement or climate, Virtual Singapore users will be able to create rich visual models and realistic large-scale simulations of Singapore.\nPotential uses of Virtual Singapore include:\n\u2022 Virtual experimentation, for example visualisation of 3G\/4G network coverage areas.\n\u2022 Virtual test-bedding, for example modelling and simulating crowd dispersion to establish evacuation procedures during an emergency.\n\u2022 Planning and decision-making, for example to analyse transport flows and pedestrian movement platforms.\n\u2022 Research and development of new technologies or capabilities.\nVirtual Singapore is based on Dassault Syst\u00e8mes\u2019 3DEXPERIENCity, a scalable, single unified hub that can address architecture, infrastructure, planning, resources and inhabitants through virtualisation, simulation and collaboration capabilities.\nVirtual Singapore was launched in December 2014 as part of Singapore\u2019s Smart Nation drive. The Virtual Singapore platform is expected to be completed by 2018.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-synchro-developing-mobile-app-codenamed-zephyr\/","title":"NEWS: Synchro developing mobile app, codenamed Zephyr","date":1432684800000,"text":"App will be fully integrated with Synchro Pro to bring 4D scheduling and project monitoring to the field\n4D scheduling and construction project management software specialist Synchro is developing a mobile app, codenamed \u2018Zephyr\u2019. The app, which is fully integrated with Synchro Pro, the company\u2019s 4D visual project management software, aims to bring 4D scheduling and project monitoring to the field.\nSynchro says it is also keeping track of new technologies such as computer vision technology, drones, and point clouds and is already working with Redpoint on integrating micro location services into its software.\nThe key methodology that \u2018Zephyr\u2019 employs for mobile reporting is to report job status from the field at the task level and more detailed status reporting at the level of resources.\nAccording to Synchro, Zephyr is designed to take advantage of the range of services available on mobile platforms: fast 3D geometry, navigation, location services, email, voice and text notes, photography and video, weather reports, and live connection.\nData reportable from Zephyr will be governed by the user\u2019s role and permission in Synchro Pro, and will fully synchronise a focused 4D view of the project between Synchro Pro and Zephyr its mobile-filed extension.\nThe developers say a Pro mobile server has been built and tested with Zephyr. The mobile server provides a wireless connection point between a Synchro Pro (and therefore a Synchro Pro Workgroup) and Zephyr.\nSynchro is currently testing Zephyr is on the iPad, but the company intends to release on other platforms as well.\nAlong with mobile apps, Synchro says there is a lot of excitement in the industry about micro location services, computer vision technology, drones, and point clouds, to help capture job status in the field.\nThe company says that, as these technologies mature and attain reasonable price and performance, it will incorporate them. The company has already integrated micro location services into the \u201cZephyr Alpha 3\u201d build, which has been accomplished through a partnership with Redpoint Positioning. Redpoint offers real-time location services, currently accurate to 20cm., through its proprietary WiFi\/Bluetooth network.\nSynchro says that, with a Redpoint network installed on your job site, you can walk the site with Zephyr and see where you are, inside the model of your project.\nThe software is currently able to connect to any nearby Redpoint network simply by typing its IP address into Zephyr\u2019s Setting sheet. The entire network\u2019s geometry is displayed inside the 3D model. Synchro explains that, as you walk the site, your position is displayed within the model on your iPad. The company plans to use this technology to align Zephyr\u2019s view of the site to the actual site, to the degree supported.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/tci-software-releases-maptools-pro-v5\/","title":"TCI Software releases MapTools Pro v5","date":1017187200000,"text":"TCI Software has announced the release of MapTools Pro v5, a suite of over 100 automated mapping tools for use with AutoCADr R14 \u2013 2002, including a new One-Pick Automation feature. Now in its 10th year, MapTools is used by mapmakers in all 50 states and 43 foreign countries. The complete toolkit has undergone the most intensive revision in its 10 year history, with more than 65 new tools and tool improvements.\nMapTools encourages the use and reuse of automated operations by making their creation, storage and recall quick and easy. With one pick, a predefined operation can carry out dozens or hundreds of editing tasks, and that same instruction set can be applied to any number of drawings totally automatically.\nInstruction sets are created and managed using MapTools Commander and can contain any combination of 18 MapTools automated functions and freeform autolisp commands. With one pick, a ready to run autolisp routine is created and stored in a special MapTools Commander Library. These automation functions can then be applied to a drawing with a simple selection from the library or applied to any group of drawings unattended.\nMapTools automated functions include three Parametric Editing Tools (PET). These Search and Replace processors can accomplish virtually any entity modification job and are also used to assemble selection sets for the other automated tools. Together, the three PET functions can be configured in over 2 billion possible combinations. Wild-carding and ranging expand search flexibility, and most replace operations can perform absolute or relative changes. PET allows the creation of very fine tuned automated Search and Replace operations, which can be named and added to the MapTools Commander Automation Library for one pick execution or application to groups of drawings unattended.\nTen automated linework processors can be applied using MapTools Commander:\n- Join \u2013 with line following\n- Clean \u2013 now performs Extend, Trim, Snap and Cluster\n- NURBS fit \u2013 a polyline spline fit that hits every vertex\n- Curvefit \u2013 uses Fuzzy Logic to reduce drawing size up to 10X\n- Clip \u2013 can find and erase or relayer linework inside the clip boundary\n- Close \u2013 creates closed polylines\n- Vertex Interpolate \u2013 generalize, densify or normalize (equal spacing)\n- Label \u2013 use Text, Blocks or Elevation based Labels\n- Scallop \u2013 with programmable bulge\nAutomated Rubbersheeting involves the creation of a master calibration point set (stored rubbersheet definitions for a large area). Using MapTools Commander, groups of drawings can then be rubbersheeted in batch, using the stored calibration data.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-city-of-helsinki-streams-huge-city-dataset-to-mobile-ar-and-vr\/","title":"NEWS: City of Helsinki streams huge city dataset to mobile AR and VR","date":1530835200000,"text":"Reality mesh model automatically optimised by Umbra technology so it can be viewed on phones, tablets, headsets and web browsers\nThe City of Helsinki, Finland is using Umbra technology to stream a massive 3D model of the entire city to web browsers and mobile devices so it can be viewed in real-time.\nThe reality mesh model of Helsinki is based on aerial photographs of the city taken in the summer of 2015. Helsinki generated an enormous point cloud, representing a 50 square kilometre area of the city, then processed that data into a 700 GB texture-mapped 3D mesh.\nThe city of Helsinki\u2019s goal was to make this open dataset available for anyone to use, but the sheer size and complexity of the 3D data presented an insurmountable challenge. At that point the city spoke with Umbra about using its automated optimisation on the dataset to make it easily deliverable. Umbra\u2019s fully-automated cloud platform optimised the mesh and now the entire dataset can be streamed to AR or VR-capable mobile platforms such as phones, tablets, headsets and even web browsers.\nThe Helsinki city model\u2019s buildings were placed at the right elevation by transferring the buildings\u2019 footprints from the city plan\u2019s base map to the digital terrain model. The buildings were then converted into 3D models by combining the terrain model with a surface model that included the shapes of the buildings\u2019 walls and roofs making a digital twin of the city.\n\u201cThis was really a new and unique project collaboration between Umbra and the City of Helsinki \u2013 combining game engines and the city\u2019s own technology into the next-generation of city modelling,\u201d said Jarmo Suomisto, Project Manager at Helsingin City of Helsinki \u2013 3D City Information Model. \u201cThis collaboration brings together so many stakeholders and technical standards, such as the open data policy and manufacturing technology, as well as the public and private sector. Through this cooperation, this whole project has been made possible and Helsinki appreciates Umbra\u2019s willingness to develop new technologies that truly push the envelope.\u201d\nBased on a patented approach to 3D streaming, Umbra\u2019s solution enables photo-realistic, multi-billion polygon models to be streamed and manipulated in real-time on mobile, untethered devices including phones, tablets, and VR and AR headsets. Umbra also makes it possible to integrate these highly-optimised models directly into commonly used 3D development environments like Unity or Unreal.\n\u201cUmbra\u2019s new cloud-based optimisation and delivery solution operates similar to Google Earth, but has the potential to render and stream significantly higher resolution 3D content, even down to sub-millimetre accuracy depending on the source scan,\u201d said Shawn Adamek, Chief Strategy Officer at Umbra. \u201cIn addition, we are democratising city-scale 3D scanning, giving cities and developers the opportunity to take ownership of their own datasets, rather than using and licensing existing mapping data.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/video-nxt-bld-2018-london-conference-andrei-jipa-eth-zurich\/","title":"Video: NXT BLD 2018 London conference - Andrei Jipa, ETH Zurich","date":1530144000000,"text":"Smart Concrete \u2013 NXT BLD London, June 2018\nAndrei Jipa is a PhD researcher at ETH Zurich, an exceptional STEM University which is trailblazing digital building technologies in AEC.\nJipa\u2019s talk centred on smart concrete and he really blew the doors off optimising 3D printed concrete for detailed and precision work. Jipa has a process for 3D printing very thin plastic shells to create formwork. These are then filled with thin layers of high performance concrete, creating 15mm thin tubes of concrete and designs with very fine detail not normally seen in concrete.\nAs a test, his team entered a boat competition and made a concrete skeleton tubular frame for a 4 metre-long kayak which looked mind-blowing. Another project produced an immensely complex force-optimised concrete ceiling, which had tessellated surfaces for enhanced acoustic performance. Essential viewing that will totally change your mind-set on\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/develop3d-live-event-preview\/","title":"DEVELOP3D Live event preview","date":1520208000000,"text":"DEVELOP3D Live (Warwick, March 20) offers the perfect opportunity to see the very latest CAD, VR, 3D printing and visualisation technologies and learn from those truly pushing the boundaries of design. Tickets are free so claim yours now\nBIM conferences play an essential role in the advancement of the construction industry, but for AEC firms really looking to push the boundaries of design technology, there is much to learn from the fast paced automotive, product design and manufacturing sectors.\nDEVELOP3D LIVE (March 20, Warwick) offers the perfect opportunity to do just that. The one-day event, organised by AEC Magazine\u2019s sister publication DEVELOP3D, will showcase the very latest in design, engineering, manufacturing and visualisation technology.\nThere\u2019ll be a huge exhibition with 60+ exhibitors and a four-stream conference covering everything from Artificial Intelligence (AI), generative design and future CAD tools, to 3D printing and VR\/AR\/MR. There\u2019ll also be a chance to see the very latest in design viz technology including some exciting new tools from the Chaos Group, the developer of leading architectural rendering software V-Ray.\nThe future of automotive\nThe keynote speakers this year come from the automotive sector, but these are no ordinary automakers.\nMike (Mouse) McCoy, a film director, producer and motorcycle racer, is now CEO of Hackrod, a firm which specialises in custom vehicles and aims to produce the first generatively designed and metal 3D printed car. In fact, Hackrod is currently building a huge print bed for metal 3D printing with simultaneous machining.\nMcCoy believes strongly in the democratisation of car design and manufacturing and sees a future where AI and new 3D printing techniques will liberate customers from what big auto produces for the masses.\nDavid Moseley of California-based Lucid Motors will go deep into simulation and explore the role of multi-physics simulation in the development of a brand new all-electric vehicle, the Lucid Air.\nThis luxury sedan, which has a target market date for 2018, will come in a v ariety of configurations. It has a 400-mile range from its on-board 100kWh battery. It\u2019s also no slouch, with acceleration from 0-60 mph in 2.5 seconds.\nDesign viz and VR\nOne of the big focuses for DEVELOP3D Live 2018 is a conference stream dedicated entirely to design visualisation and Virtual Reality (VR).\nThere will be rendering experts and customers from Chaos Group (V-Ray) sharing details of exciting new technologies and optimised design viz workflows.\nHP\u2019s Sean Young will introduce Mars Home Planet, a global mission to unite architects and engineers to design an urban area for 1 million people on Mars and bring it to life in VR.\nRichard Seale, lead automotive designer at Seymourpowell, will give a live demonstration of \u2018Reality Works\u2019 \u2013 a new VR sketch tool that allows designers to sketch in 3D at full scale to create data and streamline collaboration between both designer and modeller.\nAMD\u2019s Jamie Gwilliam will explore the hardware requirements for the best VR\/AR experience, explain how CAD and VR\/AR graphics cards differ and discuss the differences between CPU and GPU rendering.\nEscape Technology\u2019s Lee Danskin will examine how cloud workflows can be used in visual effects in all industries including architecture.\nThere will also be a big focus on rendring materials. Swatchbook\u2019s Yazan Malkosh will present a new cloud platform designed to revolutionise the exploration, visualisation and sharing of materials. Meanwhile, Allegorithmic\u2019s Pierre Maheut will introduce Substance Designer, a nodebased material authoring tool, to create procedural and\/or scanned Physically Based Rendering (PBR) materials.\nExhibition\nDEVELOP3D Live features a huge exhibition with over 60 stands. There will be many new-to-market products on show and plenty to touch and see, including the very latest in VR Ready desktop and mobile workstations.\nSee develop3dlive.com\/exhibitors for a full run down of who will be there.\nConference\nDEVELOP3D Live boasts four concurrent conference streams featuring a whole range of designers, engineers and industry specialists. In addition to the dedicated stream for design viz and VR, topics include product design, engineering, 3D printing and fabrication. See develop3dlive.com\/conference-program-2018\/ for the full programme.\nClaim your free ticket\nDEVELOP3D Live March 20, Warwick\ndevelop3dlive.com\/register-now\nWarwick Arts Centre, CV4 7AL\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/simulation\/planning-the-office-space-of-the-future\/","title":"Planning the office space of the future","date":1595376000000,"text":"As organisations prepare for a safe return to the office after the Covid-19 lockdown, a new data driven space planning tool is there to help, as James Ward, associate director of architecture at Arup explains\nAlthough we do not yet know the full extent of the coronavi rus\u2019 long term effects, the global pandemic has alreadyhad a highly disruptive impact on business and the workplace. For commercial property owners, developers and tenants alike, the crisis has caused a high degree of market uncertainty. The pandemic has accelerated a range of pre-existing trends in the commercial property sector, including those regarding health and wellbeing, activity-based working, flexibility and the drive for better space utilisation. Sustainability, smart buildings and the digital workplace are an integral part of this transformation, reshaping what will be the office space of the future.\nAs the crisis appears to abate in the UK and organisations attempt to resume normal working practices, safety is of paramount concern. Employers and employees alike will have many questions, the most important being, \u201cIs our office safe to work in\u201d?\nThere is little certainty around transmission mechanisms for the virus but one principle that seems to be widely accepted is that maintaining physical distance between people is a good thing. Indeed, addressing the UK science and technology select committee, the UK government\u2019s chief scientific adviser, Patrick Vallance, stressed that distancing measures must continue to be upheld at this stage of the virus. This inevitably raises questions, not only around how best to occupy desks in the office, but also how people\u2019s movements around the office can be managed in line with government guidelines.\nAs governments around the world encourage people to return to workplaces and city centres to mitigate the economic repercussions of the pandemic, questions around the occupation of particular offices are going to become increasingly pertinent. Moreover, certain sectors of the world economy, such as the performing arts and hospitality, are still effectively in lockdown and need assistance in managing their transition back to viability.\nWhilst it is impossible to entirely eliminate the risk of transmission, organisations will have an obligation to reduce risk to the lowest reasonably practicable level. Consequently, for organisations, an increasingly pertinent question is what the optimum number of people who can safely occupy an existing space is. As many businesses and organisations are returning in limited numbers, today these questions can be addressed with pragmatic approaches developed by their in-house FM teams. However, in planning to ramp up tomorrow, particularly as government guidance evolves, the situation is likely to become more complex and risks potentially higher.\nSpace Explorer\nData driven tools such as Arup\u2019s Space Explorer hold the key to surmounting these key challenges. The service can be used to reshape office space, laboratories, retail spaces or any other venue where people congregate. Bringing together space planning functionality with intelligent modelling and simulation of people\u2019s movements, Arup is using the software to help multinational organisations around the world with their return to the office.\nSpace Explorer allows architects to replicate an existing layout quickly and test this against several scenarios that can optimise the use of desk space, or re-plan them to maximise occupation. Layout adjustments are optimised quickly using generative techniques which can establish the maximum occupation possible within government regulations. The tools also allow the location of business units and teams on the floor to be tested. The teams\u2019 locations can be optimised using iterative mathematical optimisation against an adjacent model developed alongside the client. This way, potential issues of physical dayto-day iterations between teams can be mitigated.\nOnce the layout has been established, Space Explorer can seamlessly harness the power of Arup\u2019s MassMotion pedestrian modelling software, exporting data directly into the tool in order to set up simulations that model people\u2019s movements within the space.\nMassMotion has been developed in-house and, based on pioneering research into the science of human movement, the software provides technical analysis of the flow of people through physical spaces. The latest version of MassMotion comes with additional, experimental behavioural modes, that enable testing of physical distancing scenarios in a dynamic condition.\nMassMotion simulations are run with a number of input parameters agreed with the client, such as the frequency of trips to the washrooms, tea points and printers, but also with factors such as background rates of inter-floor travel and stair usage.\nThe simulations map proximity where breaches of social distancing rules are unavoidable, for example along circulation routes and in lobbies. Whilst not all of these can be mitigated, often improvements can be made by reconfiguring furniture layout and introducing one-way systems.\nSince the simulation parameters are identical in each iteration, it becomes clear which interventions make improvements and which are detrimental when compared against the existing layout.\nMost significantly, the software highlights workspaces that have disproportionately high exposure. Whilst these are often adjacent to aisles, it can sometimes be difficult to spot issues. A blanket approach of avoiding aisle seats entirely can often be unnecessary and result in a significant loss of capacity.\nTypically, these workspaces can be removed, or the furniture in that area locally re-planned to mitigate the risk more effectively. The seamless integration of Space Explorer and MassMotion allows this to happen very quickly, permitting multiple layouts to be tested and for different options to be presented to the client.\nSuch analysis can help the senior management of organisations engage with their colleagues around the measures taken to ensure their workplace is as safe as it can possibly be. In conjunction with this, other interventions, such as modifications to air condition systems, increased cleaning regimes, and touch free devices, can help build confidence of staff to return to their offices.\nTo test the impact of reducing social distancing rules, we have run some simulations on a typical floor of Arup\u2019s own London office. At 100% capacity, our simulation found that employees spent 21 minutes on average in close contact with another person across a seven-and-a-half-hour day. At 30% capacity, typical with 2m social distancing, they spent just two minutes. Whilst Arup is not making assumptions about the health risks of the decreased exposure, the differences nonetheless are significant.\nUltimately, institutions worldwide that are still paying for office space will face a commercial imperative to densify their occupation. Clearly, considerably more people will be working from home from now on and some companies can, and have already, reduced their required office space. Space Explorer can provide valuable insights as organisations attempt to chart a safe path back to normal operations.\nWorkplace safety obviously has many other elements beyond physical distancing, such as the use of masks for example, and the implications of the virus outbreak continue to develop the more that scientists learn. For now, Space Explorer can help clients explore what\u2019s possible, safe, optimal and practical and help with the rebuilding of confidence with staff. It is expected that the tool and techniques developed during the pandemic will have enduring uses too, giving clients insight into how their staff may interact and help model the office space of the future.\n\u25a0 arup.com\/expertise\/services\/digital\/space-explorer\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-nctech-automates-hdr-point-cloud-colourisation\/","title":"NEWS: NCTech automates HDR point cloud colourisation","date":1424995200000,"text":"ColourCloud software to \u2018transform\u2019 laser scans with fast point cloud colourisation overlay\nNCTech, the creator of the iSTAR 360\u00ba panoramic digital imaging device, reckons its new ColourCloud automatic HDR point cloud colourisation software can significantly reduce the time architects and surveyors spend on site. The company says HDR images are captured at each scan position in just 20 seconds while batch processing reduces post-capture scan colourisation to a fraction of the time normally taken.\nThe software is designed to offer architects and surveyors \u2018speedy, pixel-perfect overlay\u2019 to support iSTAR files and e57 industry standard laser scan outputs from leading scanner manufacturers including Trimble, Leica Geosystems and Faro.\nNCTech says the ColourCloud process provides fully automatic colourisation which speeds up workflow rates, requires no stitching, resolves parallax issues and is precision calibrated to sub-pixel accuracy, resulting in full colour HDR overlay from any laser scan to within +\/-2 pixels.\n\u201cFrom our research, we know Architecture and Surveying professionals want the ability to speed up workflow rate where laser scanning and imaging is concerned,\u201d explains Neil Tocher, Chief Technical Officer of NCTech . \u201cColourCloud is designed to use a single open file format to integrate into the existing output from software across all manufacturers scanning devices and is a significant improvement on existing processes which can take a professional considerable time to colourise a single 3D scan from a scanner.\n\u201cThis means that hundreds of millions of 3D scan data points can be colourised from an iSTAR image in seconds. We are redefining the colourisation process with total automation in a fraction of the time it currently takes.\u201d\nAs a comparison, NCTech highlights an example in which the iSTAR camera captured a 50 million pixel image within 20 seconds; the same resolution scan (without colour) would take around 20-30 minutes.\nA high dynamic range capture of up to nine exposures takes just over two minutes with iSTAR, says NCtech, adding that this is a fully automatic process, done at the push of a button.\nNCTech will be holding a webinar on the 10th March to mark the launch of its new ColourCloud automated point-cloud colourisation software.\nMore details here.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/what-s-the-big-deal-about-bim-anyway\/","title":"What\u2019s the big deal about BIM, anyway?","date":1449705600000,"text":"Trimble\u2019s John Bacus shares his thoughts on BIM\u2019s dirty little secret, and why complex and expensive BIM tools are still basically being used for drafting\nIf you\u2019re anything like me, for the better part of a decade you\u2019ve been inundated with good news about how Building Information Modelling (BIM) is ushering in a new golden age of design and construction efficiency.\nCertainly there have been significant projects built using the principles of BIM that could never have been built any other way. Frank Gehry\u2019s Beekman Tower project in Manhattan is a particularly notable example. Through careful analysis and clever design optimisation, this beautiful building was also completed on time and on budget.\nAs my friend Ragnar Wessman (the \u2018Father of Tekla Structures\u2019) recently pointed out over a pint of Finnish lager, our industry still feels compelled to define BIM anew every time we talk about it\u2026 so we might as well begin there.\nBack in the early 1990s, I found it pretty tough to get a job. The economy wasn\u2019t great and as a young punk fresh out of architecture school, I was probably more of a liability than an asset to any firm that might hire me. The one thing I had going for me was that I knew about \u2018computers\u2019, particularly those made by Apple and, most especially, about doing computer graphics with them. And so, through what in hindsight seems an unusual turn of events, I found myself with a job offer from a prestigious design firm in Berlin.\nAmong the many things I love about the Germanic mindset is the belief that if there is a correct way to do something, then there really isn\u2019t any reason why you would want to do it any other way. While 2D CAD systems were pretty well integrated into most architectural practices at the time, the tool of choice in Berlin was something better. Rather than doing the mental gymnastics necessary to project 3D building designs into 2D orthographic drawings, it was possible to make a fully detailed 3D model of the project and then let the computer make your drawings for you. To that end, we used ArchiCAD, running on the hottest Macintosh computers we could find.\nArchiCAD was great, something different from the rest of the 2D CAD systems of the day. We could build complete \u2018Virtual Buildings\u2019 with it: 3D models with building objects in them that were more than just simple line drawings that architects had been creating by hand for a thousand years or more.\nVirtual building, which seems indistinguishable from what we think of as \u2018BIM\u2019 today, promised that you could work through the entire design of your building to completely constructible levels of detail in the computer. All the troubling and difficult problems that the design team might encounter in the field could be found and resolved virtually before the first shovel broke ground on the jobsite. It is now common knowledge that decisions made during the design phase of a building are much more cost effective than decisions made after the concrete has started curing. Additionally, decisions made during design are typically better, more thorough and carefully considered. Design improves with simulation, analysis and iteration. BIM permits that when effectively implemented. Nobody likes to discover design problems on site when there is no time to consider alternate solutions.\nBut there\u2019s a bit of a dirty secret hiding in the rhetoric. Time and again as I\u2019ve talked with friends and colleagues across the AEC industry, I find the same basic patterns. The complex and expensive BIM tools they have licensed for their teams to use are still basically being used for drafting. Flip those great BIM projects into a 3D view and you\u2019ll find a hodgepodge of geometry that was really just put in place to make the drafting look good. In the spirit of full transparency, I was guilty of doing exactly the same thing all those years ago in Berlin.\nIf you think of drawings as being the deliverable that gets the building built, then this all makes terrific sense. Why waste billable time building a detailed model that doesn\u2019t contribute to getting your story across any better than a solid set of drawings? Which, by the way, is what an architect\u2019s contract says they are obligated to deliver. Anything more just seems like a lot of wasted effort.\nIf all you really want from BIM is a way to automate drafting, then there are dozens of tools on the market that can convert polygons in a model into lines in a drawing. Even SketchUp, which we designed to be the simplest tool we could imagine would be useful to an architect, can pull that off without too much fuss. But this is where we have to start thinking differently. There really is a better way, and it doesn\u2019t have to be all that complicated, either.\nThe real benefits to BIM come from simulating a project to sufficient levels of detail as to remove uncertainty from the construction process. We should be able to know exactly how the project is going to look, how it will go together and how it will perform. We should also know how much it will cost and how long it will take to build. And experience now shows that we can simulate all of those things to a high degree of certainty in the computer before construction begins.\nThat simulation can and should happen throughout the design process, starting right from the first conceptual sketches. There\u2019s no reason to wait until the design has grown into a highly detailed state before testing it for constructability, affordability and performance. In fact, the best and most impactful design decisions are almost always made during the conceptual phase. That\u2019s when the design is still plastic enough to support big moves. Later in the process, everything is so locked down that you\u2019re often doing little more than rationalising your earlier decisions.\nIf you accept that pre-construction simulation is where the real benefits of BIM exist, then there\u2019s a really simple story to be told. Because then, at its heart, BIM is a methodology; not a tool. 3D modelling (detailed or conceptual), energy analysis, drafting automation and parametric object definitions all have their place in a BIM process. But BIM is defined essentially by none of them. You can do very effective constructible simulations of a building using many different applications. Some people do so using only a spreadsheet.\nIf there is one defining characteristic of tools that support BIM processes, I think it is their ability to carry non-graphical attribute data on objects in the model. In other words, if you make a model that displays a set of columns, those columns should both look like columns (graphically) and know that they are semantically of a \u2018column\u2019 type. Typically, this means that they carry classifications and attributes, like type (Corinthian), height, weight, manufacturer, material, fire rating, etc., arrayed in some pre-arranged schema like IFC.\nModels with attributes can be used for analysis (cost, schedule, energy use, code compliance, etc.) that support detailed design decision making, ultimately allowing the building to reach a fully constructible level of detail before construction begins. This analysis can begin at the first moment of semantic awareness. A simple polygon drawn on the ground plane can be tagged as \u2018is:parking\u2019 and immediately enter into conceptual cost analysis. As that polygon is refined over time, the conceptual cost estimate can also improve. But having that information early and right at your fingertips is absolutely invaluable.\nSo maybe there\u2019s a case for a really simple BIM that, in the end, provides the greatest possible benefit to our industry. Forget all the fluff around this central point: BIM contains building objects that have properties and attributes beyond their visual appearance. That\u2019s it. They don\u2019t have to be fixed in a particular schema, they don\u2019t have to have parametric properties, and they don\u2019t have to be anything beyond the best possible representation of the design at any given moment in its development. Loose when the design is conceptual; increasingly refined as the design iterates over time.\nNot coincidently this is how Gehry Partners uses Digital Project, the AEC industry-specific implementation of Catia that Gehry Technologies built and has used on so many truly groundbreaking projects. It is also pretty much the same system that we call \u2018information modelling\u2019 in SketchUp, allowing any schema for object classification and any dictionary of attributes to define objects in the most appropriate semantic for any given project.\nThe trouble with many of the big BIM tools that apparently dominate the popular BIM discourse today is that they try to do too much and, in doing so, they overly and prematurely constrain the design process. Their advanced parametric tool boxes are full of wall tools, slab tools, window tools, spiral ramp tools, etc. They excel at making building information models, as long as your building is made of parts they have already imagined you would need to draw.\nTools that don\u2019t impose such rigid schema on the design are much more conducive to making great design decisions. You should be able to impose your own schema on the design to serve your project\u2019s particular requirements. Want to reimagine what it means to enclose space in your building using something between a wall and a slab? With a properly unconstrained system, all you have to do is draw it the way you imagine building it.\nWhen I asked Ragnar to define the future of BIM over beers that night in Helsinki, he said that the future of BIM was to disappear; meaning that it would eventually be replaced simply by, \u201cthe way we make buildings\u201d. I think he\u2019s exactly right. In our zeal as technologists, my CAD industry peers and I are always looking to define something groundbreaking, revolutionary and new for the markets we serve. Maybe this time what we really need to acknowledge is that the revolution, with all its high-minded rhetoric, is largely behind us.\nJohn Bacus is product management director of SketchUp, Trimble Buildings, where he is responsible primarily for the growing SketchUp family of products.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-dan-harper-cityscape\/","title":"Video: NXT BLD London conference \u2013 Dan Harper, Cityscape","date":1502150400000,"text":"Virtual Reality (VR) beyond the hype \u2013 NXT BLD London, June 2017\nVR is here and the buzz has been substantial. But is it something that you should be using or is it just a gimmick? How do I use it and, perhaps most importantly, why should I be using it? However, the question that supercedes all others, says Dan, is how can VR and real-time technologies generate value within a design business?\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Paul Nichols, Skanska\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Arthur Mamou-Mani, Mamou-Mani\nConstructing (and deconstructing) buildings with cable robots\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/au2015-day-one\/","title":"Live from Autodesk University 2015 - Day One","date":1448928000000,"text":"Join us for AEC Magazine\u2019s live coverage from Autodesk University in Las Vegas.\nWelcome to our live blog from the opening day of Autodesk University 2015, the software giant\u2019s user event for its vast portfolio of design tools.\nThere are more nationalities milling the halls than your average airport international arrivals lounge \u2013 and the scale of the event is greater than anything else I\u2019ve ever attended, which is a reflection of the software being covered here: product design, engineering, architecture, construction, plus a smidgen of stuff for the visual effects markets \u2013 it\u2019s busy.\nWe\u2019ll be bringing you all the action below the line, so stick with us throughout the day.\n10.20 \u2013 The cavernous general session is about to begin \u2013 the theme seems to be \u2018giant green warehouse rave\u2019 complete with live DJ, blazing light show, and a mini centre podium, onto which we expect to be seeing Autodesk CEO Carl Bass throwing shapes and waving glowsticks in a short while.\n10.30 \u2013 So, Storm Troopers have just escorted on to the stage\u2026\n10.32 \u2013 700 user-based sessions going on over the course of the event. 700. That\u2019s pretty epic, should you need some training.\n10.36 \u2013 More raving\u2026\n10.40 \u2013 Carl Bass is here with his keynote talk \u2013 \u2018focusing on what\u2019s really important, and the really talented people I\u2019ve met\u2019\n10.42 \u2013 \u2018Innovation\u2026 it\u2019s a combination of pragmatic improvements and combining the with new things, and the often unsexy work of packaging it up\u2019\n10:44 \u2013 Bass is back from visiting the new 3 million sq ft Apple HQ in construction in Cupertino. \u2018An awesome experience.\u2019 Construction of the building is using prefab concrete panels: \u2018The engineers were talking about them like they were aircraft parts. Building and manufacturing are converging.\u2019\n10.49 \u2013 Bass, in his natural habitat of his workshop, started looking for a makeshift, automated IoT solution for fixing his knackered CNC machine from the sound it was making. There a professional solution that works in the same way \u2013 by collecting, analysing and taking the appropriate action on their own.\n10.52 \u2013 The future of work \u2013 Bass doesn\u2019t think jobs will disappear in the engineering sector, but getting the right people trained is going to be a big problem. Having visited Facebook\u2019s offices, he suggests that they have snack machines and social areas (bars!) might not be the best way to get the best workers doing meaningful work.\nAutodesk\u2019s interns have been doing some ridiculous projects this summer \u2013 something suggests that they\u2019re getting the right balance. \u2018Creating a business and a mission that makes the right people want to work for us\u2019.\n11.00 \u2013 Andy McAfee from MIT is now onstage \u2013 asking the question \u2018what are the most important historic points in human history?\u2019 At a dinner party, this is going to have everyone debating. I don\u2019t get invited to many dinner parties\u2026 sad times\u2026\nHe\u2019s right when he says \u2018technology is the key\u2019 over everything else.\n\u2018We continue to underestimate good old fashioned innovation\u2026 when wood becomes really expensive because we\u2019ve used it all, we turn to other materials\u2026 Innovating our way out of problems\u2019\nIt\u2019s not all good \u2013 as we still continue to burn through natural resources \u2013 but with dematerialisation, we\u2019re getting better. Computerisation is helping us use fewer resources (we\u2019ve already passed peak material use). Interesting stuff.\n\u2018Around the world we\u2019re giving more land back to nature every year.\u2019\nWe\u2019ve two big challenges still to face \u2013 stop cooking the planet, and how wages need to rise \u2013 \u2018I\u2019m not a communist\u2019\n11.16 \u2013 CTO Jeff Kowalski is on stage, wearing a lovely wine coloured jumper and telling us how we\u2019re moving into an Augmented Age \u2013 and it\u2019s going to radically alter the way we think, work and exist.\nWe\u2019re moving from passive to generative design \u2013 \u2018computers can go off and explore the entire data set and return to us with ideas we\u2019d never have thought of\u2019\nAutodesk has been working with Airbus \u2013 busy developing the technology techniques for its 2050 concept.\nA world first look at the new \u2018Bionic Partition\u2019, an onboard wall, that weighs less than half the standard weight, saving 25kg for each one installed, and is even stronger than the old model.\nNew levels of product to be designed \u2013 intuitive and empathic \u2013 instead of us learning a CAD tool, Kowalski expects the future to be CAD tools that learn us.\nCurrently Autodesk are working on getting a robot in their lab to understand simple voice commands and gestures to complete manual tasks \u2013 no CAD or CAM, but he\u2019s drilling shapes in walls .\n11.29 \u2013 Dr Hugh Herr from MIT \u2013 \u2018I\u2019m in the transport industry: I design legs for a living\u2019. He lost his biological legs in 1982 from frostbite when mountaineering, so designed some new ones. Top lad!\nHe now climbs at a better standard than he did previously because of his bionic limbs.\nDeveloping new mechanical and electrical interfaces that means one day we\u2019re all have bionic technology inside us.\nThe future human will \u2018sculpt\u2019 their body \u2013 designing their very human body. \u2018The designer will design themselves\u2019.\n\u2018Take technology away from me and I\u2019m crippled. With it, I\u2019m free\u2019 \u2013 incredible stuff.\n11.38 \u2013 Jeff\u2019s back on stage, looking at getting smart technology working smarter as part of a \u2018nervous system\u2019 of technology. \u2018As designers we can learn from web designers \u2013 an entire site is regimented down to the pixel\u2019\nAutodesk have been working with the Bandito Brothers (life-size Hot Wheels tracks) to design a car with a nervous system \u2013 it\u2019s the future! \u2018Technology isn\u2019t going to replace us, it\u2019s going to help us design better things!\u2019\nAnd with that, it\u2019s lunchtime! Back in a flash folks!\nThe afternoon session is underway with a little less bombast than the morning, but there should be interesting stories galore as we look at some of the most innovative projects using Autodesk technology.\n15.00 \u2013 Dennin Oosterman from ReDeTec3D is showing off his Protocycler \u2013 a 3D Printing filament recycler.\n15.25 \u2013 The entirely electric city might be the next big shift in humanity, says BIM specialist Michael Thydell. A flexible infrastructure doesn\u2019t come without it\u2019s problems, and adding sensors to all aspects of buildings to collect realtime data might be the answer.\nUsing as much data as possible means designers \u2013 in this case architects \u2013 can create better products.\n15.36 \u2013 John Jacobs, who ran on stage and shouted his bit so fast I didn\u2019t hear a word of what he said, has now ran off stage and a video is playing about some architects designing a concert hall. I think he\u2019s from construction firm JE Dunn, and it\u2019s about allowing collaborative working with Revit. The video is full of bad acting, although the key message is nice.\nJJ is back on stage, explaining how they can cut months off schedules through getting their teams to work together better, and how costings estimates can be much closer to the end figure right at the start of the project. They use a tool called LENS to provide a costing from whatever details are available at the start \u2013 even napkin sketches \u2013 and apparently get within five per cent of the final costs.\n15.55 \u2013 More architectural content \u2013 this time from engineers, Dewberry, and Downtown inc who are looking at population growth and global warming (think rising water levels, floods, and sustainable cities of the future).\nDowntown Inc are a Washington DC company, and they\u2019re rather proud that they\u2019re building a very sustainable future for the city, today. Although to be truly sustainable it needs to be smart, so they\u2019re looking into using as much data as possible and analysing it in realtime. They\u2019re using Infoworks 360 to develop a rapid energy model on a district model \u2013 helping inform energy plans and grid modernisation.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-stefana-parascho-gramazio-kohler-research\/","title":"Video: NXT BLD 2018 London conference - Stefana Parascho, Gramazio Kohler Research","date":1530144000000,"text":"Cooperative robotics in architecture \u2013 NXT BLD London, June 2018\nStefana Parascho is a PhD research at Gramazio Kohler Research, an institution within ETH Zurich University. Parascho specialises in multi-robotic assembly processes. ETH has a factory floor with a gantry with two inverted robot arms, giving a build volume of 45 x 70 metres. Parascho talked about student projects, using the robot arms to place bricks in complex structures, which the robots disassemble at the end. Another involved welding wires together to build a complex form. The resources at ETH Zurich are really quite astounding and looks well worth a visit..\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-subscription\/","title":"Autodesk Subscription","date":1444953600000,"text":"As Autodesk accelerates towards a subscription-only model for its CAD tools and services, Martyn Day analyses what it all means\nAutodesk, the volume leader in the CAD market, is now well on its way to transforming its business model from selling boxes of software to supplying design tools to customers solely on subscription.\nAutodesk has decided to accelerate its phased plan to stop selling perpetual licenses of software. It had announced that as of January 31, 2016, it would no longer be possible to buy perpetual licenses of individual products (Revit, AutoCAD, Inventor etc.). Now, from July 2016, Autodesk will no longer sell perpetual licenses of its bundled application \u2018Suites\u2019 (Autodesk Design Suite, AutoCAD Design Suite, Building Design Suite, etc.). New customers will have to sign up for a time-based subscription contact for the applications or suites they require.\nThis accelerated move will mean that by mid-2016, Autodesk will have an almost entirely subscription-based business model, following slowly in the footsteps of Adobe, the industry\u2019s poster child for subscription transition, which went harder and faster than any other company and upset many customers in the process.\nThe reason given for the acceleration by Jeff Wright, vice-president, strategy and marketing at Autodesk, was that the company realised that having a foot in both camps \u2014 perpetual licence sales and subscription only \u2014 was a lot harder to transition than originally envisaged. There was a recognition that the company needs to be \u201cin one camp\u201d.\nTo smooth this transition, Autodesk will offer a range of simplified subscription plans for individuals, teams and enterprises, Mr Wright said. Customers will be able to purchase individual or shared subscriptions with the option of single user licensing or shared network licensing. Those customers that purchase or purchased a perpetual license of Autodesk Design & Creation Suites to July 31, 2016, will continue to own those licenses, and customers on existing maintenance contracts will continue to receive updates and new releases for as long as the maintenance is kept.\nWhy subscription?\nIn the 1980s and 1990s Autodesk used to release updates to its only product, AutoCAD, every two to three years and went through extended periods of marketing and selling upgrades to the next release. Customers tended to upgrade their software every three to five years and there was not a maintenance option.\nFor many businesses this was painful. A lot of energy and money was spent to get the installed base to move along with new developments. Now that Autodesk has many products in its portfolio, covering multiple vertical markets, and is moving from big yearly releases to streamed updates, serving a subscription-base. This represents a major transformation and has potential business benefits to Autodesk:\nPredictable recurring revenue vs burst is good for the markets, smoothing out quarter-to-quarter financial results.\nBy going increasingly direct there is additional margin for Autodesk.\nSubscription management software makes piracy more difficult.\nLess monolithic development of apps, more dynamic updates.\nBigger potential to offer services for higher subscription levels.\nLowers the barrier to entry for new customers.\nPotentially means less income now but greater income per customer over time.\nSubscription\u2019s bedfellow of choice is \u2018the cloud\u2019 and Autodesk is also in the midst of architecting a potential formidable backbone with its \u2018360\u2019 product portfolio, which connects and merges desktop, mobile and cloud-services. Autodesk Subscription plans include various levels of cloud-based storage and extended features, such as fast rendering or analysis. Autodesk has gone from offering a DOS-based 2D drafting tool to building 3D design ecosystems, spanning multiple platforms.\nPros and cons\nSubscription\u2019s most noticeable benefit is the lack of a \u2018joiner\u2019s fee\u2019. To buy a perpetually owned seat of AutoCAD costs several thousand pounds and is something that needs to be budgeted for. To get a seat of AutoCAD on subscription costs \u00a3185 a month, no up front fee. Seats can be increased or dropped quickly, so should the market go into recession, overheads can be cut. Subscribers will also have access to the latest release with a global-use license and there are cloud benefits too.\nFor accountants, subscription payments are operational expenditure, as opposed to buying the software which is capital expenditure with a depreciating asset over years. Subscription payments are therefore tax deductible within the same financial year. Although here, it is arguable if Autodesk products are a depreciating asset, given the efforts the company made historically to stop users reselling their perpetual licenses.\nAutodesk also highlights that subscription comes with e-learning and telephone \u2018technical support\u2019, but with exceptionally mature and complex products this is remote. It certainly will not be like having local help from a reseller or hands-on training. That kind of support will cost, probably a lot more than the product subscription.\nWhile there are undoubted benefits, some customers see a move to subscription as a loss of freedom. In the past the customer could evaluate upgrades to see if they offered value to their business and would pay to upgrade or opt to miss out. Autodesk\u2019s move to predictable yearly releases increased the frequency of this evaluation but also the upgrade pricing was concocted to penalise those that fell behind, making subscription of perpetual seats the lowest cost of ownership.\nThen there were the dreaded DWG format changes and the \u2018obits\u2019 \u2014 when a version of a product over three releases old (so, three years\u2019 old) were retired and customers were given the option to upgrade or lose that capability. Now with the subscription-only model less than a year away, it is very much a take it or leave it, term-based access and for those not pushing the boundaries or using the cloud, mobile or collaboration, the added benefits seem, at the moment, completely hypothetical.\nAs Autodesk moved into subscription, it had noticeable teething problems getting the \u2018portion size\u2019 for annual fee. Suites on subscription compounded the issue further. Levels of user contentment with updates varies within each vertical market, but there have been many negative comments on the pace of development of Revit in return for the subscription monies paid. For example, firms may have the Building Design Suite, but most designers use Revit for over 90% of the time. The enhancements, however, are spread across the multiple products in the Suite.\nThe situation is much more clear cut when looking at new and quickly expanding products like Autodesk Fusion 360, a 3D CAD\/CAM tool designed for multiple OS, mobile and cloud from the get-go. It has only ever been sold as a subscription product and is only $25 a month. Its aim is to torpedo the current Windows\/desktop-based modelling tools and is priced to penetrate as a companion seat that eventually usurps the likes of SolidWorks, a popular product design and engineering-focused CAD tool.\nCustomers of 3D mechanical design toolAutodesk Inventor get Fusion as part of the subscription and so Autodesk is playing the hunter, as opposed to the farmer, with its own mature desktop products, which have yet to become truly cloud applications.\nChannel model\nAutodesk\u2019s move to subscription opens up questions about how, in the future, the company envisages its relationship with its Value Added Distributors (VADs) and Value Added Resellers (VARs). The distribution layer is clearly being replaced with web fulfillment and for a long time, Autodesk has been weaning VARs off relying on revenue from box and upgrade sales. Changes to Autodesk\u2019s engagement with its existing channel has always been a delicate issue, due to its symbiotic nature.\nSo far, the proportion of maintenance\/ subscription revenue paid to VARs has helped resellers survive through some very tough economic times. But with Autodesk selling increasingly direct in enterprise accounts and now moving new business to subscription-only accounts, how will the VARs be rewarded?\nMany have moved to offering training and consultancy, but that brings its own challenges. Autodesk\u2019s products are a lot more complex than hours of e-learning will ever be able to convey.\nDespite the low monthly price, products like Fusion include high-end CAM capabilities, which requires a lot of expertise and training. The cost of training and consultancy for this would be prohibitive.\nI suspect that over time many small resellers will cease to exist and only those with large installed bases will get enough out of customer services to survive.\nConclusion\nThe move to the cloud and to subscription is commonly seen as extinction level event. Those who had a big presence on the Windows desktop are not necessarily going to be the winners on future mixed devices and mixed operating systems. Autodesk is ahead of its competitors in this regard and it is now compressing its timescales further.\nAutodesk Fusion 360 serves as an indication of the cloud-architecture expected for all products in the vertical markets. Running over the Internet, on a cloud server, applications will be almost impossible to pirate.\nWith distribution and VAR margin removed or reduced considerably, Autodesk would also keep a bigger share of the subscription payments. It makes good business sense.\nThe key issue will be to ensure customers believe the subs fees provide value for money, instead of feeling they are merely renting the software. Autodesk needs to ensure customers use a range of the new tools and services to warrant the cost.\nFrom up in the cloud, Autodesk will still have to nurture customer and partner relationships, as well as improve development and velocity of its core platform products.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/bim-born-in-the-usa\/","title":"BIM \u2013 born in the USA","date":1516752000000,"text":"In the second instalment of a series of articles examining the level of BIM adoption globally, Rebecca De Cicco of Digital Node and Women in BIM turns her attention to the USA\nIn my previous article, I outlined how Australia is implementing BIM, the level of support BIM gets from government, and what challenges the country is facing, particularly when it comes to skills.\nIn this article, I turn my attention to the USA, a nation that faces similar challenges to Australia and the UK, even though its BIM implementation plan began back in the early 2000s and continues to evolve.\nTrying to determine what\u2019s happening in the US in terms of BIM adoption and implementation is difficult, given the size of the country and its complex structure of federal and state government bodies, each with their own frameworks.\nIt was in 2003 that the General Services Administration (GSA), through its Public Buildings Service (PBS), established the National 3D-4D-BIM Program. It promoted a policy mandating BIM adoption for all PBS projects and outlined its intention to partner with BIM vendors, other federal agencies, professional associations, open standard organisations, and academic and research institutions.\nPushed by industry\nWhile the GSA is pushing the value and benefits of digital technology and BIM for projects, there hasn\u2019t been an overall government push like we\u2019ve seen in the UK. In the US, it comes more from industry. While the efforts the GSA has made are extremely worthwhile, they are not measured, so there\u2019s no way to know how industry is utilising the processes.\nWhen formulating its own government response to BIM, the UK looked to the US and its National 3D-4D-BIM Program. Since then, the UK has taken further steps to make BIM \u2018business as usual\u2019 and has a framework for maturity with UK BIM Level 2.\nIn the US, however, it\u2019s quite difficult to embed a consistent methodology against a backdrop of political division and fragmentation. In the US, advanced users and organisations don\u2019t like being pushed too forcefully on a process-driven approach. They see each project as an individually distinct challenge, driven by different client needs and requirements.\nTake standards, for example: the development and use of standards like the PAS 1192 suite doesn\u2019t exist in the US. There are national CAD standards and a national BIM standard, but these are quite complex and, as a result, not driven across the industry consistently.\nRebecca De Cicco is the director and founder of Digital Node, a BIM-based consultancy working with clients all over the world to educate, manage and support the implementation of a clearly defined process, underpinned by technology.\nAs a small consultancy, Digital Node is connected to many experts across the globe, so we reached out to our global community to validate our thoughts regarding BIM implementation in the US.\nA fellow gunslinger, Aaron Maller, director of Parallax Team, is a good friend and close ally when it comes to BIM and pushing industry in a technology-focused way.\nHe told us: \u201cAs with many countries, the extent of the proliferation of BIM on AEC projects in the US depends widely on the prowess of the practitioners you ask. Many are using BIM, and leveraging the values of it in tangible ways. What isn\u2019t happening in the US is a country-wide debate, a club-style academic exercise over BIM standards, mandates or requirements, where everyone tries to agree on one \u2018hypothetical\u2019 set of standards by which projects will happen.\u201d\nAccording to Maller, \u201cCompanies have standards and some project owners have standards.\u201d These get fleshed out and melded as a project kicks off and, as a result, the projects are more than capable of succeeding.\nBut, he points out, many parts of the world currently debate standards and workflows, \u201cas if only a national or global standard can mean success, but it simply isn\u2019t true.\u201d\nThe US has developed a National Standard before (for CAD), he points out, and though many companies used it, it certainly was not the majority. Even so, many companies were successful at CAD, comfortably ignoring the national CAD standards.\n\u201cThe mileage the US gets out of its BIM projects \u2013 like the standards used \u2013 varies widely, depending on how seriously both the clients involved and the practitioners take BIM, on any given project as well as their knowledge and understanding of BIM,\u201d says Maller.\n\u201cOne notable thing I will say about BIM in the US is that I don\u2019t have to look far to find real examples of projects that are done well, all the way down to the details, well beyond the lustre of marketing material.\u201d\nThe hugely important industry-driven group, the BIM Forum (which is the US chapter of buildingSMART International), adds Maller, is an extremely active community and has developed specifications for use by the AEC industry. \u201cThe LOD specification document, although historically heavily geometry-driven, is being used all around the world, even more so than tools such as the NBS BIM Toolkit, or the LOD Specification in the UK,\u201d he says.\nThe BIM Forum\u2019s mission \u2018to promote and support the use of open BIM standards throughout the industry in a long-term effort to drive towards fully digitized information exchanges\u2019 is one to be applauded, he says, \u201ceven though they don\u2019t push for an actual framework for adoption.\u201d They also support industry as heavily as they can, being a not-for-profit and voluntary group of people offering their time for free.\nThe challenge of the skills gap\nThe US has always been a tech-savvy country with strong digital technology skills, making it a world leader in the digitisation of many industries and fields. While the skills shortages we see in the UK regarding BIM are similar for the US, the focus on education and digitisation in the latter is much more advanced. For example, universities such as the Massachusetts Institute of Technology are working hard to push innovation and digitisation.\nAs I\u2019ve already suggested, however, the process-driven aspects of BIM are of less concern to US organisations. Take, for example, the hugely successful coworking specialist WeWork: it is pushing the ability to design, construct and manage assets all over the world without a structured process like the UK BIM Level 2 approach. For WeWork, it\u2019s more about ensuring that the end client ultimately saves money and time during all stages of a project.\nOn the whole the open culture of the US should assist in the drive towards greater collaboration. Most people I am connected with in the US and Canada are open in their levels of communication and happy to share. This open culture, as experienced at events such as Autodesk University (AU), helps attendees in coming together to learn, to share and to promote their companies and processes. For me, this openness is what makes such events so valuable.\nThat kind of openness is much-needed at a time when digital innovation is moving at a rapid pace. Advancements in Augmented Reality, Virtual Reality and Artificial Intelligence are likely to change the face of construction as we know it. We should be using collaboration to prepare for the challenges ahead. The UK could perhaps learn an important lesson from our American cousins in that regard.\nRead Rebecca\u2019s other articles in this series:\n\u2018BIM, the Chinese way\u2019\nBIM adoption in the world\u2019s most populous nation.\n\u2018BIM, the Kiwi Way\u2019\nThe growth of BIM in New Zealand\n\u2018BIM in Australia \u2013 are we there yet?\u2019\nThe level of BIM adoption in Australia\n\u2018BIM in Europe \u2013 a bright future?\u2018\nHow Europe is grasping the opportunities available to deliver a harmonious digital construction industry\nNXT BLD 2018\nRebecca addressed NXT BLD 2018, presenting her unique global perspective on BIM adoption, specifically with implementation of Smart City projects. See her presentation here.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/preview-rhino-inside\/","title":"Preview: Rhino Inside","date":1569801600000,"text":"Rhino 3D with Grasshopper occupies a special place in the AEC industry. Now, to complete its global domination, McNeel will soon make the software able to work in pretty much any Windows application, including Unity, AutoCAD and even Revit\nWhile most firms make a big song and dance about their releases, McNeel chooses a more discreet path. Over the years it has developed a large and loyal community, covering many vertical markets, from jewellery design to ship building, together with an ever-expanding ecosystem of add-on applications.\nThe company originally released the Rhinoceros 3D modeller in 1994. And, together with the Grasshopper 3D scripting visual tools, has become endemic in the AEC industry for conceptual and geometric definition. It is used by nearly all the leading architectural practices (Zaha Hadid Architects, Foster + Partners, Gensler, HOK, Heatherwicks etc.) as a design platform. It typically drives design derivation before BIM tools get anywhere near the geometry.\nThe closest BIM integration with Rhino has been Graphisoft\u2019s ongoing development to forge strong links with its flagship BIM tool, ArchiCAD, enabling Rhino \/ Grasshopper to drive its GDL-based BIM components. Since 2016 this integration has proved a winning combination for many firms that wanted closer collaboration between generative forms and component-based BIM models.\nFor Rhino 7, which is currently available as a work-in-progress (WIP) release, McNeel has made changes to the software which enables it to run as a plug in inside Windows-based host applications, enabling seamless access to the API and model data.\nThis new capability is called Rhino Inside. In typical McNeel style, the technology was quietly uploaded to GitHub to be tested and to see what the community thought of it. Through requests from testers, the feature set grew and it\u2019s really only been this summer that McNeel folks have been openly presenting it in public.\nIt means that Rhino \/ Grasshopper can now be used inside Revit as a direct plug-in. Prior to this, it involved a whole load of shenanigans to link the two. Many architectural firms define the geometry of a project in Rhino and will wait until the absolute last minute before committing it to Revit, as Revit was never really designed to be a real-time generative display engine.\nWith the new plug-in, geometry can be modelled in Rhino and used to drive live geometry creation in Revit. It can also be round tripped, with geometry sucked out of Revit into Rhino.\nMcNeel has developed its own set of Grasshopper \/ Revit components and selections (picked by as category, family, level, or by name), enabling them to be referenced within the Grasshopper environment and then the equivalent called from within the Revit database when driving Revit. This means Grasshopper scripts can be used to create native models using actual Revit components like Dynamo can and not just generate though IFC or imported geometry.\nRevit Inside is a work in progress, but is already generating a lot of excitement iamong its target user base. It opens up a whole new level of integration between the McNeel ecosystem of applications and Revit data, which comes from the most common BIM tool in the US and UK. Many architectural firms will be adopting this over their current workarounds for bi-directional data flow and it offers an alternative to existing and limiting file translations.\nWith Rhino\/Grasshopper now capable of directly driving both ArchiCAD and Revit, and the BricsCAD \u2018up-start\u2019 soon to announce Rhino\/Grasshopper support as well, one could start to ask the question if BIM really has much of a footprint in the conceptual design phase of a project, as it is mainly relegated to the detail and documentation phases within innovative architectural practices.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/openroads-designer-drawings\/","title":"OpenRoads Designer: drawings","date":1495670400000,"text":"The humble drawing finally comes of age with Bentley introducing automated drawing production in its new-generation civil design tool, writes Greg Corke\nBentley has a bold vision for civil engineering and construction. Its \u2018constructioneering\u2019 partnership with Topcon presents an entirely digital future, where engineering models drive autonomous construction vehicles and reality is captured in real time for QA and as-built asset records.\nIt\u2019s a compelling narrative, but in an industry that is often slow to embrace new technologies and processes, the humble 2D drawing continues to be the essential deliverable in civil engineering projects.\nThis has not been lost on Bentley\u2019s civil design software development team, which has dedicated considerable resources to automating drawing production in OpenRoads Designer Connect Edition, the successor to Bentley\u2019s civil engineering stalwarts: InRoads, Geopak, MX, and PowerCivil.\nLaying the foundations\nOpenRoads first appeared in 2013 as a common modelling technology for InRoads, Geopak and MX, acting as a stepping stone to a unified civil engineering detail design tool. It delivered intelligent 3D design tools with parametric constraints and dynamic civil cells that adapted to the design as it changed.\nAt the time, standards-based 2D deliverables were still handled by the individual applications. This remained the case until the launch of OpenRoads Designer Connect Edition last month.\nThe big difference with this new-generation software tool is the level of automation that has been built into drawing production. Previously, while drawings were derived from 3D models, they still required a fair amount of manual finishing and management.\n\u201c[OpenRoads Designer] is more interactive, in that a user can set up different ways of looking at the same information dependent upon the phase within the project that they\u2019re at,\u201d explains Steve Cockerell, industry marketing director, road and rail, Bentley Systems. \u201cYou might have a very simple view with a minimal amount of geometry showing whilst you\u2019re designing, but a country or a project requirement might dictate that you need a very distinct set of information export for all of the intersection points \u2014 so it\u2019s literally just picking a style for that particular phase for the same information to be drawn or exposed.\u201d\nCockerell\u2019s use of the word \u2018exposed\u2019 is important here as, in OpenRoads Designer, the drawing is inextricably linked to the 3D model. \u201cIt\u2019s looking back to the geometry and extracting that information and presenting it in the style, as opposed to the more manual process that we\u2019ve gone through before where you\u2019d have to annotate your plan and other items. It was semi-automatic.\u201d\nIn OpenRoads Designer, the new documentation centre can deal with different changes in scale in different areas within a single sheet and manage that for you.\n\u201cIt gives us complete flexibility to stitch drawing sets together, to federate them with a set of named boundaries that control the drawing set in a \u2018motif\u2019 file,\u201d adds Bentley\u2019s Ian Rosam, director, product management, civil design. \u201cAnd we can utilise that motif file to produce the drawings for different series and requirements.\u201d\nImportantly, any changes made to the model automatically ripple through to the drawings. Edits can also be made directly in the sheets and OpenRoads Designer updates the source geometry accordingly.\nThis ripple effect extends to annotations \u2014 so long as the user snaps them directly onto geometry and they are not left floating in space.\nDrawing title blocks can also be automatically populated, as Cockerell explains, \u201cI know it\u2019s a simple thing, but every drawing has got to know what it is and be referenced back to a plan set. Then, if the design changes, you\u2019ve got to go back and change every set of plans that are affected by that, so it [OpenRoads Designer] allows those design changes to automatically ripple through to the drawing production end of things.\u201d\nConclusion\nWith OpenRoads Designer Connect Edition, Bentley has developed a fully automated drawing production system. This means that once the drawing sheets are set up, engineers can concentrate on the model, without having to worry about the knock-on effect that last-minute design changes might have on documentation. In essence, modelling and detailing can become much more fluent.\nWorking in this way, the time savings alone could be huge. However, by making a drawing a 100% by-product of a model, trust in the model will likely grow, encouraging its use as a key deliverable. And this could open doors to \u2018constructioneering\u2019 workflows, where the design model is digitally connected to construction and beyond.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/structural-engineering\/review-tekla-structures-2018\/","title":"Review: Tekla Structures 2018","date":1526342400000,"text":"Trimble brings its structural BIM tool up to date with new workflows for complex modelling and design automation as well as support for point clouds. By Greg Corke\nTekla Structures has changed beyond all recognition from the days of XSteel, the steel detailing software from which it evolved. The structural BIM tool may still have a core customer base of steel fabricators, but it has expanded its reach significantly over the years and now covers design all the way to construction, for steel, reinforced concrete and pre-fabricated concrete projects.\nThere\u2019s one core product, but different licences give access to the many different task-specific features. There\u2019s even a licence catering to firms working on smaller projects, that limits the number of steel parts or reinforcement bars in a model.\nFor the 2018 release, Trimble looks to have taken inspiration from architectural BIM tools, adding support for point clouds, a live link to the algorithmic modelling tool Grasshopper, as well as better quality graphics, and improved usability through an enhanced user interface. The new release is also faster than Tekla Structures 2017i, which Trimble admits was slower in some areas than the release before.\nBut Trimble has not forgotten its bread and butter that is steel fabrication. There are a number of new enhancements to make drawing production more efficient. We\u2019ll look at all of these new features and more in this review.\nGrasshopper-Tekla Live Link\nOne of the most exciting new developments is the new Grasshopper-Tekla Live Link. Grasshopper extends the popular NURBS modelling tool Rhino with algorithmic modelling functionality.\nThe software is very powerful, very popular and very affordable and is used by architects around the world for anything from generative design to robotic control. But it\u2019s also very graphical, so generally much easier to work with, compared to other algorithmic modelling tools.\nThrough the new live link, Grasshopper can drive geometry creation inside Tekla Structures. From a structural engineer\u2019s perspective, it could be used to realise an architect\u2019s vision for a hi-tech building, rather than having to model everything from scratch in Tekla Structures. But it can also be used to quickly generate a structure that\u2019s either exceptionally intricate, such as a double curved roof or complex concrete shell, or repetitive based on a parametrically driven template.\nHere, an example, could be something as simple as a portal frame with a mezzanine floor. The fabricator would simply create a Grasshopper script for a certain type of structure then punch in the number of bays, height of the eaves and height of the apex and the design is automatically generated inside Tekla Structures. Importantly, if you then change any of the parameters in Grasshopper, the design instantly updates in Tekla Structures.\nTekla Structures does have its own API for performing tasks like this, but Grasshopper has the benefit of being very visual and, importantly, used everywhere. There\u2019s a big pool of architects and engineers graduating from university with the necessary skills.\nThe Grasshopper-Tekla Live Link is not part of Tekla Structures 2018 per se \u2013 it\u2019s an extension available in the Tekla Warehouse \u2013 but it only works with this latest 2018 release. By offering extensions like this, Trimble is able to develop new tools and make them available to customers much quicker. They can then be refined, following user feedback, and often brought into the next full release of the software.\nUser Interface\nTekla introduced a new ribbon user interface in 2016 and has been refining it ever since. One of the biggest changes in 2018 is the new property pane, which makes it much easier and quicker to handle and modify object properties in Tekla Structures.\nWith previous versions, selecting multiple objects brought up several cascading dialogue boxes, each containing the individual properties of specific objects. Now everything is handled in the property pane which allows you to quickly see which properties are common, which properties are not, and then modify them accordingly, all in one go.\nThere\u2019s also a new online side pane that helps new users search and access online help, learning materials, and forum posts. It can also use notifications to alert users to new features and service pack releases.\nAnother UI change is the base points and work plane handler, which makes it much easier to control which work plane is currently active in the model. It could be used for carrying out specific modelling tasks, or to align your model to the co-ordinate system of imported data.\nThe final two UI changes we\u2019ll cover are very simple, but effective. The Undo history command allows you to undo or redo several commands in one go. This is great for getting good visibility when correcting mistakes, as the model history clearly lists which specific commands will be undone. You can also place bookmarks as you go to make it easy to undo big chunks of work in one click.\nThe select previous tool allows you to re-select objects that you have accidentally deselected, which can be a real pain if you\u2019ve spent ages selecting multiple objects only to lose your selection accidentally. Of course, for larger selections, users would probably use filtering.\nVisual fidelity\nTekla Structures now has a brand-new graphics engine based on DirectX that is primarily designed to improve the visual fidelity of the model in the viewport. It does this through the use of subtle shading and ambient occlusion (an effect that renders shadows more realistically) to give more depth to models.\nAnti-aliasing reduces the stepped effect on diagonal lines and flicker as the model rotates on screen. The new engine also highlights geometry much better when transparent and renders reinforcing bars with edge lines so they stand out.\nUsers will need a DirectX 11 compatible graphics card (Tekla recommends Nvidia GeForce) but can still fall back on the old OpenGL engine if they have an older GPU.\nPoint clouds\nTekla Structures has now joined the likes of Revit, MicroStation, ArchiCAD and other AEC software tools by supporting point clouds natively. Users can import a variety of different formats including ASCII, E57, LAS, LAZ, PTS, PTX, Potree and Trimble scan format (.tzf). Import can be quite processor intensive but runs on a separate thread to the core Tekla Structures application, so it shouldn\u2019t slow your workstation down.\nThe functionality is pretty simple, and it is really just a viewing tool; there are no dedicated point cloud processing tools for registration, raster clean up or automatic object recognition. But there are valuable use cases.\nThe main one is to provide as-built context for new 3D models. Users can snap to points on the point cloud and then model Tekla Structures objects around it. This could be a useful tool for an engineer or fabricator who wants to add a new structure to an existing building, such as a mezzanine floor. Previously it would have required a traditional survey or the use of third party software to turn the point cloud into a vector model. Now the point cloud can be imported directly and used in much the same way one would a scanned raster drawing in a 2D CAD system.\nThe second use case is for tracking construction progress, by comparing the as-built point cloud to the fabrication model. This is a visual checking process only, not an automated workflow like the one offered in Verity, the dedicated point cloud verification tool recently acquired by Topcon. However, by structuring the verification workflow missing objects should be able to be identified quite easily.\nTekla also offers \u2018Point Cloud Manager\u2019, which can be downloaded from the Tekla Warehouse. This standalone application uses clip planes to trim models to size, which can then be exported. It could be useful when a survey company hands you a giant point cloud of an entire site, terrain, trees and all, but you are only interested in a very specific part of the model. It saves you importing massive, multi Gigabyte point clouds into Tekla Structures which could slow down the system unnecessarily.\nTrimble Connect\nTo enhance team collaboration, the new release is more tightly integrated with Trimble\u2019s multi-disciplinary collaboration environment, Trimble Connect. The cloud-based tool evolved from Gehry Technologies GTeam which Trimble acquired several years ago.\nTekla Structures models can be linked to a Trimble Connect project in the cloud using the Trimble Connector. Users can check for clashes against project models from other disciplines and applications like Autodesk Revit. This can be done from within Trimble Connect or from within Tekla Structures.\nAny comments that are added to the project in Trimble Connect can automatically appear inside the model inside Tekla Structures. It\u2019s a great way of getting live feedback from site inspections, for example, using an iPad. ToDo notes can also be synchronised between Tekla Structures and Trimble Connect and vice versa.\nThis optimised workflow is a far cry from the file-based workflow championed by Tekla Structures and Tekla BIMsight where any issues found inside Tekla BIMsight had to be manually located inside Tekla Structures. Now, the whole design review process can be very seamless and very efficient.\nThe Trimble Connect integration isn\u2019t just limited to 3D models; you can also collaborate on drawings in a similar way. In addition, documents such as PDFs or CNC files can be attached to any object within a model.\nAnalysis links\nThere have been several improvements to how Tekla Structures works alongside design and analysis software. Much of these centre on improving the way the software generates the analysis model, reducing the need for manual editing.\nIn previous versions, users may have found that the analytical nodes of beams, columns or braces weren\u2019t connected once imported into their analysis software. This would result in time-consuming manual editing to create a fully connected analytical model. Now Tekla Structures automatically connects all the analytical wires to common nodes, resulting in full connectivity when exporting to the Tekla Structural Designer analysis software.\nBuilding on this, Trimble has improved the round-trip workflow between the two applications to help keep everything in sync. Previously, the physical and analytical models were exported from Tekla Structures and interpreted by Tekla Structural Designer, but often small modelling errors crept in. Now the analytical model is repaired and improved before it is exported with the physical model to Tekla Structural Designer.\nOnce inside Tekla Structural Designer, design changes can be quickly effected, and the updated model can then be exported and synced back into Tekla Structures.\nModelling\nIt is now easier to work with bent plates. Previously if you wanted to edit a bent plate, say to change the angle or adjust the plate depth, you\u2019d have to explode it, edit the geometry and then recombine everything. Now you can just drag handles to make changes.\nThere\u2019s also a new tool for modelling spiral beams and more accurate welds, including compound welds and polygon welds.\nTools for concrete contractors to help streamline the detailed modelling of construable formwork were added to the Tekla Warehouse a couple of years ago. These originally focused on walls, and specific tools for clamps, braces, ties and wailers have all been updated in 2018. However, the big new addition is a specific formwork placing tool for slabs which handles panels, girders, and shuttering props etc.\nDrawing content manager\nDespite there being such a big focus on 3D models, Trimble hasn\u2019t forgotten its \u2018bread and butter\u2019 deliverable \u2013 the 2D drawing. Tekla Structures now has a Drawing Content Manager which is specially designed to help users check and edit marks, notes and model objects in drawings.\nLike the property pane, the Drawing Content Manager sits on the right-hand side of the UI. To check drawings, simply click on individual parts as listed in the Drawing Content Manager, then instantly see them or their part marks highlighted in the drawing.\nParts can be easily sorted in the list by part type, mark quantity, position, class or profile or you can search for something specific. Welds, bolts and reinforcement can also be viewed in a similar way according to their own attributes.\nIf part marks are missing, or not needed in the drawing, they can be added, removed or edited at the click of a button, as can associative notes. You can also work the other way around \u2013 select parts in the drawing and see them listed in the Drawing Content Manager. It\u2019s a simple feature but it\u2019s a lot more efficient than manual checking with printed drawings.\nTo bring more clarity to drawings, automatically, Tekla Structures now has a new mark placing algorithm that arranges marks and associative notes more clearly, avoiding crossing leader lines. Marks can also be dragged and dropped, highlighted and then automatically aligned at the click of a button.\nAll of these drawing features are probably more useful to engineers, or those who do lots of bespoke projects, rather than a fabricator who does portal frames day in day out and already has finely tuned templates.\nPrecast detailing and fabrication\nOver the past few years Tekla Structures has been expanding into pre-cast concrete. The new release includes a range of tools designed to automate repetitive detailing tasks. This includes an interactive tool for double walls that can insert reinforcement (mesh and additional reinforcement), braced girders, lifters and propping inserts all in one go. It\u2019s parametrically driven so can automatically adjust to the dimensions of the wall.\nBricks can now be added automatically as well. Previously courses had to be placed manually, on a single panel, then copied across.\nPre-cast capabilities now go beyond detailing and a new 3D-model based truck-load planning application called Stacker can help plan how pre-cast units are loaded and stacked on the back of a truck and taken to site.\nConclusion\nThis is an important new release from Trimble which helps bring Tekla Structures up to speed with its architecturally- focused peers.\nThe point cloud functionality in particular offers new opportunities for engineers and fabricators working on renovation projects. And with low cost, simple to use laser scanners, like the Leica BLK360 (reviewed here), point cloud data acquisition could even be brought in house.\nThere has also been significant work done on interoperability. The Grasshopper link gives users access to powerful algorithmic modelling tools for quickly generating complex forms. And there are huge opportunities for design automation and a pool of skilled graduates to help firms take advantage.\nAnalysis also gets some welcome attention by helping keep physical and analysis model properly connected. And the powerful link to Trimble Connect has the potential to transform collaboration across multi-disciplinary teams.\nAdmittedly, many of the new tools highlighted in this review may not appeal to veterans, who use the software on autopilot for repetitive work. But, for a new generation of engineers, fabricators and contractors, it\u2019s important to stay current as these types of features are now to be expected in modern BIM tools.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-robot-advancesteel-link-available-in-autodesk-labs\/","title":"NEWS: Robot-AdvanceSteel link available in Autodesk Labs","date":1438819200000,"text":"Links provides import, export, and synchronize capabilities between steel detailing and structural analysis software\nAutodesk is experimenting with a data exchange utility that enables engineers and designers to share data between steel detailing software Autodesk Advance Steel 2016 and Autodesk Robot Structural Analysis Professional 2016.\nThe Robot-AdvanceSteel Link technology preview offers import, export, and synchronize functionality.\nUsing Robot-AdvanceSteel Link, models created in Robot, can be exported in an .smlx format. The .smlx file can be then imported in Advance Steel to continue the workflow with structure detailing capabilities and creation of the fabrication documents.\nUsers can also start their work in Advance Steel. Using Robot-AdvanceSteel Link, models can be exported and then imported in Robot in order to perform structural analysis and code checking. Results can be transferred through the .smlx file to Advance Steel and used there for code checking of connections. Users can also track changes in the same model in the different applications using the synchronization feature.\nUsing Robot-AdvanceSteel Link, models created in Robot, can be exported in an .smlx format. The .smlx file can be then imported in Advance Steel to continue the workflow with structure detailing capabilities and creation of the fabrication documents. Users can also start their work in Advance Steel. Using Robot-AdvanceSteel Link, models can be exported and then imported in Robot in order to perform structural analysis and code checking. Results can be transferred through the .smlx file to Advance Steel and used there for code checking of connections. Users can also track changes in the same model in the different applications using the synchronization feature.\nThe Robot-AdvanceSteel Link technology preview is available in Autodesk Labs as a free download.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-newtecnic-to-deliver-facade-for-new-central-bank-of-iraq\/","title":"NEWS: Newtecnic to deliver fa\u00e7ade for New Central Bank of Iraq","date":1530835200000,"text":"Advanced engineering design firm to engineer, coordinate and oversee construction of the building\u2019s fa\u00e7ades\nInternational building engineering firm, Newtecnic is devising new algorithms to help design and engineer the structure and fa\u00e7ade of the new Zaha Hadid designed Central Bank of Iraq (CBI). The company will also use 3D printing to prototype components and fixings to ensure they perfectly fit the as-built main structure.\nStanding on a 200m x 100m (656 x 328 ft) podium, the 172m (562 ft) tall building, which sits on the shore of the river Tigris in Baghdad, signals a new phase in the development of both the city and the Iraqi economy and is a symbol of financial stability and security.\nNewtecnic CEO Andrew Watts said, \u201cThe CBI tower presents complexity because it increases in width as it rises. Designing and creating digital geometry and physical formwork for the twisting fa\u00e7ade profiles is challenging as is providing user comfort by controlling light levels and solar gain in the climate of Baghdad. The grass roofed podium, enclosing offices and entrances, is also complex since the concrete-based form comprises the aesthetic with no auxiliary decoration.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/digital-twin\/public-beta-for-autodesk-tandem-digital-twin-platform\/","title":"Public beta for Autodesk Tandem digital twin platform","date":1613952000000,"text":"First shown at Autodesk University last year, Autodesk Tandem is Autodesk\u2019s cloud-based digital twin platform for bridge, facility, building, or any other structure\nAutodesk has announced the availability of the public beta of Autodesk Tandem, its digital twin solution to enable ready-to-go operations to help owners get the most out of their investment.\nAutodesk Tandem is a digital twin platform that allows a building project to start digital and stay digital, from design to build to operations, and transforms rich data into business intelligence.\nAutodesk Tandem harnesses the building information modelling (BIM) data created throughout the project lifecycle essential to creating a true digital twin of the asset.\nAt the project\u2019s completion, the project team can deliver a comprehensive digital handover to owners of easily accessible and insightful data, accelerating operational readiness and empowering better business decisions.\nDisorganized data means loss\nWithout a digital twin at handover, mounds of disorganised data in varying formats end up buried in electronic folders, rendering useless a treasure trove of valuable data insight. For an owner, that\u2019s lost opportunity and money that cannot be recovered.\nAutodesk Tandem aims to unlock the treasure by freeing, organising, and standardising data from design and construction, creating a simple and intuitive digital replica of all the components, systems, and spaces in a facility.\nMarin Pastar Global Technology Leader of Vertical Information Modelling at Jacobs, a leading design, construction, and consulting firm, was a contributor to the development of Autodesk Tandem beta .\nWith approximately 80 percent of a facility\u2019s total lifecycle cost realized during operations, Pastar says AEC professionals have an enormous opportunity to add value by helping owners reduce costs through tools like Digital Twins.\n\u201cOur ability to affect and reduce total cost of ownership drops drastically the farther we progress from design to construction to completion,\u201d said Pastar.\n\u201cIf we consider what an owner will need at handover from the start, we can ensure proper management of assets based on their business goals and processes. We have an obligation to maximize our clients\u2019 investment.\u201d\nConnecting systems, reducing risk, saving money\nChief Technologist Brian Melton of Black & Veatch, a globally renowned engineering and construction company, is collaborating closely with firm on the direction of Autodesk Tandem\u2019s development.\n\u201cWe see Autodesk Tandem interfacing with data systems like Computerized Maintenance and Management Systems (CMMS) and supervisory control and data acquisition (SCADA) systems. This provides owners and operators access to all information though a connected experience that allows them to proactively reduce risk and increase resiliency,\u201d said Melton. \u201cOwners can start asking: Where are all the assets that currently have open maintenance work orders? What are the chances of this asset failing in the next five years? Is safety equipment near work areas? More connected systems and predictive insights that are visually aggregated using modern BIM\/design data will be part of the handover package of the future.\u201d\nMelton sees Autodesk Tandem and digital twins as part of the broader digital transformation, where every system becomes more and more connected as data is generated. In that future, systems learn from one another, share insights, and optimize performance in real-time.\nThis data-connected world requires cooperation and openness from the technology community, and Autodesk says it is committed to doing its part.\nLast year, Autodesk became a Founding Member of the Digital Twin Consortium, a US-based organisation collaborating on digital twin best practices and standards, and a member of the Open Design Alliance, a non-profit technology consortium that provides support and access to design file formats.\nAEC Magazine thoughts on Autodesk Tandem\nAutodesk, as the volume BIM leader is in a strong position to commoditise the digital twin market. With an inexorable drive to provide its solutions and services via the cloud, the core framework for digital twins seem to be yet another possible byproduct from Autodesk\u2019s BIM,Civil modelling and digital twin services.\nIn its current guise, Autodesk Tandem is initially targeted at repurposing customer\u2019s existing design data vs incorporating the as-built condition.\nLaser scans and real-time sensor feedback support are in the pipeline but not yet included. This puts Autodesk behind its competitors, such as Bentley Systems, but for the whole segment, it\u2019s early days and we are sure Autodesk is providing the necessary resources and development to the team to rapidly expand the capabilities.\nAutodesk\u2019s sheer number of BIM customers will help drive exploration of this new business models for Architects and Construction firms. It will be interesting to see if digital twin development can make sense for small projects, other than the known key market of process plant, universities and firms with huge property portfolios.\nWith the drive to deliver digital twins, Autodesk recognises that data needs to flow between all constituent applications and is claiming a focus on being more of an industry open player with regards to Autodesk Tandem.\nIt has signed up to the ODA to access their IFC toolkit (which also, by the way, saves Autodesk development dollars on its own IFC efforts). It will be interesting to see if competitive digital twin developers get access to Autodesk\u2019s cloud APIs and Forge to access user\u2019s Revit data.\nAutodesk Tandem is on an open beta program today and you can sign up here\nAEC Magazine has been examining the Digital Twin market over the last two years and have a number of articles on the topic:\nDeveloped in Newcastle, Space has a powerful digital twin platform, read more here\nRead more on how Bentley Systems\u2019 CTO, Keith Bentley sees the evolution of this space, here and here","source":"aecmag.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/17\/skana-robotics-helps-fleets-of-underwater-robots-communicate-with-each-other\/","title":"Skana Robotics helps fleets of underwater robots communicate with each other | TechCrunch","date":1765929600000,"text":"Underwater autonomous vessels and robots could play a substantial role in defense operations, but submersibles have historically had trouble communicating across large distances unless they rose to the surface. But coming up to transmit poses the very obvious risk of being exposed.\nSkana Robotics thinks it\u2019s made a breakthrough with underwater communications using AI \u2014 but not the large language models the industry touts today.\nTel Aviv-based Skana has developed a new capability for its fleet management software system, SeaSphere, that allows groups of vessels to communicate with each other underwater across long distances using AI.\nThe system allows vessels to share data and react to what they hear from other robots. This, Skana says, gives individual units the ability to autonomously adapt to the information they receive and change their course or task while still working toward the same general mission as the fleet. The startup says its software can also be used to secure underwater infrastructure and supply chains.\n\u201cCommunication between vessels is one of the main challenges during the deployment of multi-domain, multi-vessel operations,\u201d Idan Levy, the co-founder and CEO of Skana Robotics, told TechCrunch. \u201cThe problem that we tackle is how you can deploy hundreds of unmanned vessels in an operation, share data, communicate on the surface level and under the water.\u201d\nTeddy Lazebnik, an AI scientist and professor at the University of Haifa in Israel, led the research to develop this new capability. Lazebnik told TechCrunch that to build this decision-making algorithm, they couldn\u2019t turn to the latest AI technology, but had to use AI algorithms that are a bit older and more mathematically driven.\n\u201cThe new algorithms have two properties: they are more powerful, but as a result, are less predictable,\u201d Lazebnik said. \u201cHypothetically, you\u2019re paying in the performance or the \u201cwow effect\u201d of the of this algorithm, but the older ones, you gain explainability, predictability and actually generality.\u201d\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nSkana Robotics was founded in 2024 and exited stealth mode earlier this year. The company is currently focused on selling to governments and companies in Europe, as maritime threat levels increase due to the war between Russia and Ukraine.\nLevy said the company is in talks for a sizable government contract that it hopes to close by the end of the year. In 2026, Skana hopes to release the commercial version of its product and start proving its tech out in the wild.\n\u201cWe want to show we can use this in scale,\u201d Lazebnik said. \u201cWe argue that our software can handle complex maneuvers, etc. We want to show it. We claim we know how to manage an operation. We want admirals from EU and in EU countries to actually check this argument and see by themselves that we actually get results.\u201d","source":"techcrunch.com"}
{"url":"https:\/\/aecmag.com\/news\/scan-to-bim-a-reality-check\/","title":"Scan-to-BIM: a reality check","date":1342742400000,"text":"Building Information Modelling (BIM) is mainly focused on newbuild projects, but refurbishment and renovation requires a different approach. Using 3D Laser scanning, there is a growing interest in \u2018Scan-to-BIM\u2019. Martyn Day caught up with Steve Bury a leading UK practitioner of point cloud data and Autodesk Revit.\nThe concept of Scan-to-BIM sounds like something out of science-fiction. Take one building, laser scan millions of points in minutes and produce a \u2018point cloud\u2019. Next, import that into your BIM tool, which automatically recognises doors, walls, windows, ceilings, and spaces and you get an intelligent 3D BIM model out the other end. Unfortunately, it is science fiction; it is just not yet possible.\nWith a growing refurbishment market, Scan-to-BIM is getting a lot of coverage in the industry and now, with nearly every major CAD software developer supporting point cloud data within their base systems, the hype has never been greater.\nAs one of the UK\u2019s emergent surveying experts offering a Scan-to-BIM service, Steve Bury of Bury Associates in Worcester, has a much more realistic view of what is possible.\nFormed in 1997 Bury Associates has been supplying Revit models from survey data since 2006. AEC Magazine spoke to Mr Bury to get a sense of where the scan-to-BIM market has developed and where it is now.\nLaser-scan workflow\nMartyn Day With laser scanners and point cloud data now featuring heavily in your Scan-to-BIM service, how has your surveying workflow changed over the years?\nSteve Bury We were using standard data collection methods \u2014 total stations with AutoCAD \u2014 and then taking these DWG files into Revit and building our BIM models from there. Now we also use laser scanners, which measure millions of points per second; measuring millions of points across whole building surfaces, as opposed to a total station that is just measuring precise specified points on key features.\nLaser scanners are like a blunderbuss they just get everything, while a total station measures what you want \u2014 the corners of the room, the height of the room, the position of the door etc.\nWhile sometimes it\u2019s quicker to use the old methods, rather than the laser scanner, the great advantage of scanning is its speed on site. You collect absolutely everything very quickly. While there will be blindspots as laser scanners work on line-of-sight you get most things. But we still use a tape measure and all measurements have to tie it into a survey control.\nWe survey with a GPS system; it\u2019s accurate to within a centimetre or two and we use that to get a datum, which links back to Ordnance Survey data and we put that on an Ordnance Survey grid.\nHere, we tend to knock off the first three digits of the reference because Revit doesn\u2019t like these huge co-ordinates. Then we travel through the building taking measurements, referring back to the grid system, so we build an accurate survey of the building. Then we bring the laser scanners in and we tie into those control points.\nWe effectively create layers of scan data built on an accurate grid system. When you laser scan a job, you take a number of scans and these add together within that accurate grid system.\nWe also put up reference targets throughout the building. It means you can freely move around with the laser scanner to capture point clouds and bring them all together back at base on the computer and the software automatically recognises the targets, spheres and chequer boards we have put up.\nWhile this process is automated you still need the human eye to make sure everything is set up okay. If there\u2019s an error then the whole model can be out because they reference one another, so we check every scan prior to adding it to the master survey co-ordinate database.\nFor this we use the Faro registration and viewing software program called Scene, which comes with the Faro laser scanners. We also use PoinTools as an add on to AutoCAD and Rhino especially, for editing the point cloud and removing the noise. This can be ghosting from cars going past or anything else which moves within the laser scan.\nSpecialist point cloud tools\nMD It is interesting that you continue to use specialist third party point cloud software, despite AutoCAD and Revit now having point cloud capability. What do you think about the relatively new Autodesk point cloud engine in AutoCAD?\nJB Well, It\u2019s as good as the one that\u2019s in Revit [laughter]. They have got a long way to go. Autodesk is buying up new technology to improve its offerings but the software we use, PoinTools, sadly, has been bought by Bentley Systems and perhaps that\u2019s a real problem, as for a long time we were waiting for the PoinTools interface with Revit. I guess that isn\u2019t going to happen now because Bentley and Autodesk are competitors. I\u2019m looking forward to seeing what Autodesk does with Alice labs, another point cloud company which they acquired recently.\nData deliverables\nMD In terms of deliverables, you offer everything from standard 2D CAD drawings to full Revit BIM models. With a growing fascination with laser scanning, and the price of laser scanners dropping considerably, are architects now asking for just the point-cloud data?\nSB In my opinion it\u2019s a waste of time and there\u2019s a real lack of understanding about point clouds. I blame Autodesk and perhaps the resellers for minimising the reality of the huge amount of effort it takes to import point cloud and convert back to something useful.\nWhen we go on site and do a laser scan we could easily generate over 6GB of data. On some of our larger projects we\u2019ve taken over 300 scans, which ended up being over 20GB of data. Nobody can handle that \u2014 the data sets are just so incredibly large.\nWhile it may look spectacular, because it\u2019s the real world in dense points, it\u2019s actually really dumb data. To manipulate it and actually do something useful with it is a completely different issue. It\u2019s a skill to take that huge amount of information and produce what the client actually wants, which could be a 2D drawing or a 3D BIM model.\nWe\u2019re getting people asking for the point cloud data because they don\u2019t understand what it involves and then they\u2019ve seen a video on the web and think their CAD system can import it.\nScan-to-BIM automation\nMD There\u2019s a perception in the industry that Scan-to-BIM is an automated process. What would you say in response to this?\nSB If the marketing is to be believed it\u2019s automatic! This is a real problem for setting expectation. People expect miracles from surveying with a laser scanning. To be honest, getting the real world into a BIM system was an industry afterthought.\nWhen Revit was designed, or any other BIM system, no one gave any serious consideration as to how you would bring in laser scan data. BIM systems are highly intolerant, they don\u2019t like staircases going through walls or any mistakes that can be made on site surveying. There is an assumption that Scan-to-BIM has come to the forefront and is an automated process when it\u2019s absolutely not!\nWhile there are some applications on the market, which offer some interpretation of the cloud, basically finding vertical faces and defining them as walls, we have tried most of them and have come to the conclusion that they all have a long way to go.\nIt\u2019s a bit like voice recognition software on your phone. When I first got my latest phone I tried to send a text saying to a friend I was approaching Kidderminster, what he got was a text saying \u2018I\u2019ve got dementia!\u2019\nIt\u2019s the same with automatic recognition software. If the building is perfectly rectangular with flat walls and no objects in the way, it\u2019s fantastic. But if you\u2019ve got a real building with real things going on in it, they don\u2019t know what to make of what they find in the point cloud. Revit does really like buildings with straight lines and nice clean walls, but try going to an old stone walled building in the Cotswolds and it\u2019s totally impractical.\nModelling standards\nMD When it comes to delivering a Revit BIM model, every individual company seems to operate to their own standards. Lines, circles and arcs in 2D drawings are generic but the intelligent components in Revit need defined families and consistency. What are your experiences of this?\nSB From the work I\u2019ve seen the standards vary greatly, especially on the level of detail, to some very rudimentary modelling. That may very well be fit for purpose but the problem is there are no standards yet, standards of quality.\nI have two major projects on my desk at the moment; there were no specifications whatsoever when it comes to the standards of the BIM model they want, just \u2018a Revit model\u2019. We know we will do a proper job but we may be pricing against somebody who is going to build a very rudimentary Revit model.\nWe think there\u2019s a lot of work to be done to define good standards and procedures for getting survey data into Revit. We have developed in-house standards and are aiming to promote the specification that we\u2019ve written by sending it out to architects and other people using Revit in the industry as no one else is doing it.\nProprietary formats\nMD I have been to a number of scanning events and heard first hand from firms that have invested heavily in working out a Scan-to-BIM methodology. One of the problems with using many bits of hardware and software has been the explosion of proprietary file formats and some have even employed full-time \u2018data wranglers\u2019 to create paroles of data in the right format for each project participant. Do you have similar issues?\nSB Scan-to-BIM is a minefield at the moment. We\u2019re helping an American firm scan and model a major building in San Francisco. We are getting data off them in all sorts of formats, so we\u2019re constantly having to jig things around and convert data. While it\u2019s all possible, it just adds time.\nThe few of us that are producing surveys in a BIM format are doing it in Autodesk\u2019s Revit. We don\u2019t sell ourselves as so much a BIM company, more that we can deliver in Revit. However we get requests from firms using other BIM tools. We bid on one job in which the architect was using ArchiCAD. As they couldn\u2019t find anyone using that system they were looking for a surveyor to model the building in Revit and then use IFCs to export it back into ArchiCAD.\nWe don\u2019t know how that project eventually ended up but all I can say is that we\u2019re doing a job for that client now and they just wanted a 2D drawing as a deliverable. We have a long way to go before IFCs are the DXF of the industry, we are a very long way off. I\u2019m not even sure if it\u2019s the right direction. It causes me great concern and I can see all sorts of problems in the future if we don\u2019t get common standards.\nScanning in vogue\nMD There\u2019s a huge \u2018wow factor\u2019 and marketing behind scanning technology at the moment. How has this impacted your data deliverables?\nSB All the time now, people are dictating to us how we do surveys saying that they want a laser scan survey, even if we think it\u2019s not appropriate. In the 30 years I\u2019ve been doing this job, until today, people never came up and said to me \u2018I want you to do a survey in this way, using this methodology or technology\u2019. The deliverable and what they wanted was always an accurate set of drawings. They now may want a Revit model but now I\u2019m being told to do this I have to laser scan the job.\nWe did the whole of Manchester Central library. We chose not to scan it; we used conventional survey instruments and our own data collection methods that we developed, and from that we created a 3D BIM model.\nYou can define a wall with a start and end point, a height and a thickness but now with clients specifying laser scanning, we are being forced to measure 10 million points across that wall! It\u2019s too much information and information that will never get used.\nThere are ways you can take that laser data and produce useful things; 3D panoramics, like Google Street view, [for example]. [This would be] far more useful to the architect than the raw point cloud. You could see how many plug sockets, fittings, how many light sockets etc.\nIf you want the raw point cloud, of course we will supply it, but it should not be considered to be THE deliverable. We\u2019ve had a number of people say we\u2019ll do it ourselves. They only end up doing that once because unless you\u2019re handling that type of data (point clouds) every day, you just can\u2019t get competent at doing it.\nYou can\u2019t just use Revit to edit point clouds; you have to use other pieces of software in order to extract the information.\nOne of the fantastic things about scanning is that it captures everything, so with historical buildings if it should burn down or suffer damage, if it were to be laser scanned, there would be an accurate record of what was there before, carvings or mouldings etc.\nPhotographs only go so far, but if you had a laser scan of it, you would have everything you need to reconstruct that building. You could even use the data to create moulds to make replica interior decorations. The cost of purely scanning and holding that data isn\u2019t that high.\nComplete BIM\nMD One of the core concepts of BIM is that architectural, structural and service data is captured and used as a collaborative platform. With so much work going in at the start to build a BIM model at the survey stage, how are these other elements incorporated in Bury Associate\u2019s work?\nSB We never have much knowledge of what will be done to the building. We\u2019re there at the start of the project, we measure up the building and [produce] the surveyed information, and that\u2019s it.\nI think that\u2019s going to change and particularly when it comes down to M&E and steelwork, we should all be working together at the start to produce that initial BIM model, where they can add that expert knowledge about supporting walls, steel walls, struggle elements etc. That has not happened. It\u2019s something that is in the plan for our business in the future we will offer a service, which is more forensic, looking at asbestos, looking at drainage.\nAt the moment we are also investigating radio-controlled helicopters with cameras on to inspect roofs. This will help us complete the BIM model, as with standard scanners on the ground we can\u2019t see those details. We\u2019ve been trying Autodesk 123D catch, which has been a bit hit and miss but from what we\u2019ve seen of it, it has great potential. We hear you can get really quite good precision out of it but have not achieved that yet.\nThe cost\nMD With the extra amount of work involved in laser scanning, point cloud editing and then constructing a Revit BIM model, how much extra does it cost to get a BIM deliverable over a traditional set of 2D survey drawings?\nSB It obviously depends on the size of the building and its complexity. I\u2019d say as a ballpark, it\u2019s 30%-50% more to have a 3D BIM model. This sometimes causes misunderstandings with customers getting quotes, as they think that we are collecting the same information to create surveys to produce 2D drawing deliverables as to create a 3D model.\nThis is a general problem with customers understanding of BIM; the costs are front loaded towards the start of the design process. We\u2019re still creating 2D CAD drawings to supplement the BIM model because BIM in the whole refurbishment and renovation market is still in its infancy. Thinking about it, it\u2019s still new for newbuilds as well!","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-evolution\/","title":"Autodesk Evolution part I","date":1385856000000,"text":"On a visit to Autodesk\u2019s headquarters in downtown San Francisco, we quizzed the company\u2019s AEC development team on near and long-term development plans and got some candid answers.\nAt a recent investor day in San Francisco Autodesk announced it was ending the ability for customers to upgrade their software without being on Subscription.\nThis was described by Andrew Anagnost, Autodesk senior vice-president, industry strategy & marketing, as being an expected move having \u201cconditioned\u201d customers to the fact that Autodesk Subscription was the most cost-effective way of owning Autodesk software, together with getting access to additional services. Undoubtedly this will come as a shock to many who, despite the financial incentives of Subscription, have decided to upgrade only when they actually wanted to.\nAutodesk\u2019s introduction of time-limited Rental offerings has in some way replaced the need to go through the old upgrade process \u2014 the latest version is always available for download and at a lower cost than updating an old release.\nFrom the figures I have seen, perpetually owned software on Subscription (full cost of software plus yearly cost) versus yearly rental takes about three years of ownership to benefit buying a full license over renting.\nThis option also alters the accounting for engineering tools, capital expenditure versus operational expenditure and may depend on how and when you want to write off your software against future tax liabilities.\nAdmittedly it does make long term decisions of \u2018buy to own\u2019 or \u2018rent\u2019 a bit of a complex equation and who knows what the future holds? Autodesk is keen to point out that unlike Adobe, previous and future full license acquisitions of Autodesk software are still perpetually owned \u2014 albeit non-upgradable if not on subscription.\nUpgrades and business model\nAutodesk\u2019s business model has been a constant transition. When AutoCAD was all the company offered, it could be two or three years before a new release came out This moved to yearly releases with Subscription being the cheapest way to stay up to date, with corresponding escalating financial costs to update for those that stayed behind.\nIn this model the \u2018sweeper\u2019 was introduced; which forced those who preferred a longer cycle to upgrade or face buying a whole new copy if they wanted to protect their investment.\nIn the past four years Autodesk has been steadily moving away from its reliance from this three year upgrade cycle revenue to annual Subscription.\nTo push Subscriptions more, Autodesk produced \u2018Suites\u2019, offering even more software for a relatively small yearly fee.\nThe final act is the complete removal of \u2018Upgrade\u2019 as an option. I suspect this decision will be met with lots of wailing and gnashing of teeth by customers who feel \u2018herded\u2019 by Autodesk\u2019s business model but now with \u2018rental\u2019 there are other aspects to Autodesk\u2019s offerings which help mitigate this decision.\nFor Autodesk there is also distinct danger in this move, in that should many customers move to rent their software, they will have much greater freedom to move elsewhere, with less of a conscious historical and ongoing investment to maintain.\nThe latest release is always there for rent when required and there is no big \u2018joining fee\u2019 of buying the software. It will also help keep Autodesk\u2019s development teams focused to deliver useable new functionality and services, which from release to release can be a little hit and miss and only deliver small portions.\nAutodesk will be more vocal on these changes in the coming months, and I am sure the devil will be in the detail. For now, with the news that upgrades will be no more as of 2014, every firm should be weighing up the tactical benefit of using Autodesk Rental licenses.\nAutodesk 360\nAutodesk has been the key industry pontificator on the benefits of the cloud. The company\u2019s mantra for the last three years has been that the cloud will bring \u2018Infinite\u2019 computing to the masses. Instead of requiring expensive desktops, cloud-based applications can perform hard calculations involved in rendering, building analysis and simulation in the blink of an eye.\nHowever the reality is that, with the exception of rendering, online analysis is not being used by many. Infinite computing is a vision that is some way off.\nAutodesk\u2019s 360 platform is the source of all this hype. Autodesk 360 is a lot more than just a collaboration portal or a place to get renderings done. It is Autodesk\u2019s own operating system, its very own global network with which to interface with its customers and as it slowly develops will be the backbone of all the company\u2019s products and services for desktop, mobile and cloud.\nIn the past Autodesk software was exclusively available from dealers through distributors. It arrived on disks then memory sticks. Access codes could take an age to arrive at your dealer from Autodesk\u2019s offices in Switzerland and the whole process was very resource intensive. Soon all you will need to do is to log into Autodesk 360, where every user has an account and every subscription comes with management tools, to allow CAD managers to allocate CAD resources and capabilities through a single portal. Account holders could opt to rent more software or buy more cloud credits to run renders and analysis calculations using Autodesk\u2019s powerful computers.\nAutodesk 360 is a place where all the drawings and models can be stored, distributed, accessed and viewed via desktop or mobile. Most Autodesk applications will eventually be available on whatever system you want: tablet, desktop, through a browser, on a Mac. I am told Autodesk already has Revit \u2018running on\u2019 an iPad thanks to years of development.\nAutodesk has also developed an API for Autodesk 360 so its developer partners can provide additional functionality, should they want to use the processing power of the cloud, or even sit within Autodesk\u2019s own servers, which I presume would also be bought through the 360 platform.\nThis is very similar to Autodesk\u2019s original business model, which had AutoCAD as the platform with third party developers writing additional functionality through its API. However, Autodesk 360 is so much more for both Autodesk and its customers.\nFor Autodesk it is the ultimate customer relationship management (CRM) system and direct product fulfillment. For customers it is a one-stop shop for Autodesk products, third-party products, cloud processing services, rental, CAD management administration and document distribution.\nOne wonders where this would leave dealers. The company is making all the right noises to assure dealers that the channel is still an intrinsic part of its \u2018go to\u2019 market; but I feel the role of the dealer is set to change more dramatically over the next three years than it has at any other stage in the last twenty, with more services and less box revenue.\nAdd the end of upgrades to the universal access that Autodesk 360 offers and the Autodesk offering is rapidly becoming like joining a club.\nIn the past you subscribed to the next release of AutoCAD; soon it will only be about a range of services, of which CAD software is just one of many things you will be able to get for your yearly fee. Autodesk 360 is an extremely significant development and will impact every line of how Autodesk does business and how designers and engineers do business with Autodesk.\nAutoCAD 360\nNot to be confused with Autodesk 360, AutoCAD 360 is the cloud version of everyone\u2019s favourite AutoCAD.\nDo not worry AutoCAD is still going to be available on your desktop and for a very long time will be the version that has everything in it.\nAutoCAD 360 is an in-process development, which will offer AutoCAD functionality through a web browser using HTML5. For now the functionality is perhaps just a tad better than AutoSketch once was but the team will eventually add in all the functionality that makes sense.\nSo you can have AutoCAD locally on your desktop, or access it through a browser on any machine.\nAs AutoCAD 360 is cloud based and your data can be stored in the cloud, you can even rock up at one of your clients, log in through the web and launch AutoCAD 360 with all your drawings.\nAutoCAD 360 replaces the short lived AutoCAD WS which was Flash based. It will be interesting to see how many other Autodesk apps become available as web-alternatives. I suspect that most will follow suit.\nAEC development\nSo, where is Autodesk going with its AEC development? This year\u2019s Revit update probably saw the least exciting release of new functionality. With the focus on Suites, Autodesk seemed to forget that within these bundles there was always one key product by which users defined the majority of their day to day job.\nWith its first release in 2000, as code goes, Revit is actually getting quite long in the tooth and is due to be re-written or updated to a next generation product. The software clearly struggles with large models, does not have a contemporary graphics engine (unlike Bentley AECOsim), and fails to make use of modern architectures such as multi core processors \u2014 something which Graphisoft\u2019s ArchiCAD does.\nI have been led to believe that Revit was being rewritten but Autodesk was, company-wide, investing heavily in getting its software cloud-ready. Some of that infinite computing would go down well in the Revit community, complex BIM models need grunt.\nAnthony Hauck, product line manager for Revit\/Building Design, explained some of the planned changes with Revit. Mr Hauck agreed that performance was an issue and with each release the team was always looking to make some performance increase to give users more headroom.\nUnfortunately, with each increase, it either does not take long for users to eat that up by adding more detail, or users were already pushing beyond the boundries.\nWhen it came to multi-core processors the team had looked at the problem, but by splitting up the code to run across multiple cores, they found reassembling it all on the other side gave little, if any, benefit to the performance.\nFor now Revit can make use of multiple cores when rendering but it seems unlikely that the desktop version will be multi-core any time soon. The solution Mr Hauck said the team is looking at is to defer and localise Revit updates when it refreshes the model to relieve the processor.\nGraphics was certainly something that the company was actively looking into and admitted to working with the graphics card community to update Revit\u2019s pipeline to gain the benefit of today\u2019s fantastic GPUs. This could significantly help interaction and manipulation of large models.\nWhile Autodesk is committed to both the desktop and the cloud, the latter is the target destination for next generation platforms. One only has to look at Autodesk Fusion to see the cloud solution the company\u2019s manufacturing division came up with. AEC is also on the same track but at the moment less visibly.\nSenior vice-president, platform solutions and emerging business, Amar Hanspal told me about the until now unknown \u2018Project Skyscraper\u2019, which is looking to use the cloud, or perhaps local servers, to offload some of the heavy lifting and segregate the Revit database from the front-end.\nThis would mean that the applications would be light and model data can be streamed to clients, sending only what is needed, based on what the user is doing. This makes a lot of sense as what mainly kills Revit\u2019s performance is the omnipresent nature of all the data, all of the time.\nAs Autodesk moves to offering BIM to the complete building lifecycle, that amount of detail is only expected to increase.\nSo, for those wondering if Revit was in maintenance mode while the next generation cloud version was in development can rest easy that the team is still actively working on enhancing the desktop version. What seems likely is that the product will evolve to enable a cloud or server hosted dataset, which intelligently provides the right geometry and information to teams of designers.\nAutodesk Pier 9\nWith Autodesk growing and now preferring to be based at One Market in San Francisco, the company has been expanding to take over more floors of the main building together with investing in other spaces and facilities.\nPier 9, one of the historical piers jutting out into San Francisco bay, is just a ten minute walk from One Market.\nHere, Autodesk has created what can only be described as the \u2018ultimate man cave\u2019 for its employees. With cost estimates between $5 million and $7 million, Autodesk has created a modern space filled with the latest digital fabrication tools: 3D printers, bio\/nano lab, laser and water cutters, CNC machines, a woodworking and metalworking shop, an electronics workshop, a commercial test kitchen, and an industrial sewing centre as well as smaller specialty project areas.\nThis is very much along the lines of Techshop or the emerging hackspaces \u2014 providing a place for employees to learn new skills and test out the company\u2019s software.\nPier 9 is also the office space for Autodesk\u2019s \u2018Instructables\u2019 website team, who edit and create thousands of \u2018How to\u2019 posts, from cooking recipes to robots and rockets.\nWalking around the workshops it is clear to see that Autodesk has invested heavily in some very big machines, which cover typical CNC devices that their customers may also use, as well as the high-end five and six axis machines.\nIt is a giant lab for Autodesk employees and they are already using it to refine their applications and help hardware manufacturers. A case in point was the giant metal-cutting water jet, which will only run off AutoCAD DXF files but it did not like Autodesk\u2019s DXF output!\nAt this facility, Autodesk also has temporary \u2018artists in residence\u2019, who get access to all this fantastic hardware to work on agreed projects and push the design and manufacturing technologies, however weird they may be, such as a drum kit linked to servers and driven by an Arduino device, or a fire-breathing \u2018Zoltan\u2019 mechanical fortune teller.\nCivil and infrastructure\nFor the last two years Autodesk has largely ignored promotion of AutoCAD Civil3D concentrating instead on Autodesk Infrastructure Modeller, which then tuned into InfraWorks and is now called InfraWorks 360 Pro. As if the name changes are not confusing enough, Rich Humphrey, senior director for Autodesk InfraWorks 360, reassured that Civil3D is still the go to product for detailed design, documentation for Civil infrastructure projects but Autodesk admits that the product is now pretty mature.\nThe excitement in the company is now based around InfraWorks 360 Pro because it wants to drive BIM for Infrastructure and a new platform that pulls together disparate datasets required to meet the needs of lifecycle assets. When documentation is required, Civil3D still needs to generate the detailed drawings but over time the data model will probably merge.\nMr Humphrey explained that InfraWorks 360 is a new codebase that came from the company\u2019s experiences in creating LandXplorer, Map, MapGuide and Civil3D. Customers were asking for a database driven unified data model for infrastructure models in one platform, as opposed to point solutions.\nThe innovation is not just at the back-end. The front end is almost game-like, making it easy to use. Simply sketching a line will generate a rules-based 3D model of a highway. The company thinks it is a game changer.\nFor now Autodesk has 30-40 big companies that are using the software and the product has yet to be properly positioned.\nConclusion\nBefore now the \u2018Infinite computing\u2019 mantra seemed pretty meaningless. But having seen just how much development Autodesk is doing, fundamentally changing the way the products and the company will interface with customers and the new options that this opens up to the way customers can work, I see that it is not only meaningless but totally underestimates the change that is coming to the way we all use software and computing.\nThe computing industry is changing and Windows \u2014 in fact all operating systems \u2014 are being replaced with cloud platforms, which can deliver what you need, where you want it, with a powerful back end.\nWhile all this work is going on there will be serious challenges in streaming this functionality to firms with low bandwidth connections, or many users.\nThis is why product mixes of the future will be whatever makes sense \u2014 desktop, mobile or HTML5.\nAutodesk is clearly looking to offer a wide breadth of applications to suit all needs, so, there\u2019s no need to worry. It is not going to take away your beloved desktop application anytime soon; in fact the number of tools and possibilities will just increase.\nWith the Autodesk 360 backbone and Subscription and Rental priced to please customers, Autodesk\u2019s change in business model starts to make more sense. The value proposition changes from how much AutoCAD \u2018improvement\u2019 do I get for my yearly fee to what services and level of access to tools do I get from Autodesk for the cost?\nComing up\u2026\nCLICK HERE to read the second part of this article where we look at Autodesk\u2019s vision for \u2018Big Data\u2019, Reality capture and Building Analysis \u2014 areas where it is making major advances with products such as Recap Pro and Project Memento.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/smart-city-innovators\/","title":"Smart City innovators","date":1542931200000,"text":"In the second of a series of three articles, Rebecca De Cicco details the importance of supporting innovative companies in developing technologies for smart cities\nNew technologies are increasingly dominating and changing how we live. Our love for technology has seen companies like Apple, Microsoft, Amazon and Facebook become the largest on the planet. Behemoths like Alphabet (Google\u2019s parent company) are looking to Smart City technologies with, for example, its Sidewalk Labs venture that aims to improve urban life. But while these giants are servicing many of our desires, it\u2019s the smaller tech companies that are developing the tools to create and drive the Smart City.\nWith so many developments and ideas on the radar like sensor technologies and autonomous electric vehicles, it is the SMEs that have the ability to bring these ideas to market. The challenge, however, lies in supporting their businesses financially and testing the concepts.\nProgrammes such as Innovate UK and the Future Cities Catapult are there to support these innovative companies to create the solutions to our urban challenges. By providing support through demonstrator programmes, grant funding competitions, network-building activities, strategy, and Future Cities Missions, SMEs can seize improved access to knowledge and markets not just in the UK, but globally.\nCollaboration is a crucial part of the aim to grow UK companies and by providing them with the environments to test prototypes in real urban settings along with the networks of leading academics, designers and architects, SMEs in the UK could prove to develop the foundations for our Smart Cities across the world. Collaboration should also include local authorities; after all, this is where the monitoring of a city\u2019s infrastructure, services and environment could have an immediate impact on citizens. Take, for instance, the development of applications for smart metering for parking and utility management which all need the willingness of the authority for successful implementation.\nSmart City technology companies\nThe use of BIM within infrastructure has a significant bearing on Smart Cities, and as I mentioned in my previous article, when BIM is utilised fully, \u2018smart\u2019 information could be translated to an asset owner or city and reused downstream. Ensuring these systems are connected to other Smart City applications will offer the data required for future infrastructure, planning and maintenance.\nAs a small technology-based SME our focus is not only on BIM and its technologies for the built environment, rather offering insight to government and institutional bodies on what the future of a city landscape will be to develop policies and principles around it. We focus on helping these policymakers understand that our current working methods just won\u2019t cut it in the future and the time to start planning is now before we run the risk of running behind.\nRebecca De Cicco is the director and founder of Digital Node, a BIM-based consultancy working with clients all over the world to educate, manage and support the implementation of a clearly defined process, underpinned by technology.\nCities across the world are currently being supported by subject matter experts in this field to help their strategies and make recommendations toward the services we\u2019ll need to procure, test and review prior to implementing policy.\nDigital Node was one of several SMEs that travelled to Melbourne for the Future Cities Mission to Australia. We were fortunate to be exposed to many companies that are selling services to the market in Australia, and all of whom are leading the future city challenge in London and globally. Here is a selection.\nWestfield Technology Group has developed and manufactured a fully autonomous POD that can run on private roads\/ cycle paths. It also produces the GTM vehicle (M1 vehicle), and (in conjunction with Johnstons Sweepers) makes an autonomous road sweeper that can detect foreign objects. The company also enables platooning of vehicles (a virtual train) to reduce congestion and supports trials, deployment and safety-case work with their team.\nLooking to \u2018smart\u2019 parking, the app from JustPark aims to make parking easier by enabling over 1.5 million drivers to find parking in seconds \u2013 with real-time and predictive information on availability, restrictions and price. Drivers reserve and pay for a guaranteed space via the company\u2019s apps helping property owners to manage their parking assets more effectively \u2013 from office car parks to on-street bays, multi-storeys to private driveways.\nGrid Smarter Cities has an ecosystem of solutions to connect communities and people with transport, parking, goods and services. The company is involved in telematics, disability access, virtual kerbside management, skip distribution and waste management solutions. In its own words, Grid envisages the city of the near future as \u201cdigital, connected and convenient\u201d, aiming to revolutionise urban mobility while reducing pollution.\nThe technologies supporting the smart city agenda will continue to evolve. We will see an explosion in the coming years of a multitude of start-ups, which will shape how we use, grow and navigate our cities. Future generations will not accept analogue, and manual ways of interacting with services and objects and the way in which this will evolve will not only be inclusive of the more forward-thinking cities, but commonplace in the majority of our developed cities around the world.\nThe small, agile SME will take its place as an authority. These small companies, fuelled by innovative and generally Generation X employees, will govern and drive the industry to challenge itself, while the technologies that support these challenges will evolve further than our minds can imagine.\nClick here to read the first article in this series, \u2018Supporting the smart city agenda\u2019, in which Rebecca looks at what the UK is achieving in this burgeoning sector and how BIM will need to play its part.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/students-from-st-thomas-more-school-win-2007-national-engineers-week-future-city-competition\/","title":"Students from St. Thomas More School win 2007 National Engineers Week Future City competition","date":1172361600000,"text":"Washington, USA, 24 February 2007 \u2013 A city of the future \u2013 \u201cMwinda\u201d \u2013 engineered by students from St. Thomas More School in Baton Rouge has won the 2007 National Engineers Week Future City Competition.\nTeams from 35 middle schools nationwide, winners of regional competitions in January, participated in the Future City National Finals, February 19-22 at the Hyatt Regency Capitol Hill in Washington, D.C. The Future City National Finals was hosted by Bentley Systems, a leading engineering software company, and chair of the competition\u2019s Leadership Council.\nBentley has provided the first prize for the St. Thomas More School team \u2013 a trip to US Space Camp in Huntsville, Alabama. All regional winning teams received an all-expense-paid trip to Washington for the National Finals.\nThe students \u2013 Jake Bowers (12), Emily Ponti (14), and Krisha Sherburne (12) \u2013 teamed up with their teacher Shirley Newman, and volunteer engineer mentor Guy Macarios. St. Thomas More was the winner of the Louisiana regional competition held on January 26 in Alexandria.\nThe St. Thomas More team\u2019s Future City \u2013 Mwinda, meaning light in Lingala, a dialect of the Republic of Congo \u2013 is a new city in an old land. The mission of Mwinda is to ensure opportunity for a good life by providing power, housing, water, food, and transportation.\nLocated on the Fimi River near the Equator, Mwinda is in an area necessary to this mission: access to water, proximity to Lake Mai-Ndombe, settlement both on savannah and peripheral forest lands, and availability of rich mineral deposits.\nWith innovative technology and modern engineering practices, Mwinda has developed an integrated, interdependent industrial design. Using principally renewable energy resources, the city produces excess electricity which it sells to other cities and countries of what has been called the Dark Continent.\nIn one application, PAFC fuel cells (PAFCs) are powered with hydrogen from phyto-hydrogen generators, genetically enhanced algal cultures which produce hydrogen as a byproduct, and with solar collector hydrogen generators.\nA second system, TseTse (mythological African goddess of lightning), uses massive lightning containment capacitors, composed of dielectric glass and conductive metals from waste. Third, the Candu Reactor, powered by raw uranium mined robotically, operates in the industrial zone.\n\u2013 Second place to Nevada Christian Home School\nSecond place went to Nevada Christian Home School from Sparks, Nevada for their Future City \u201cALTA (Alpine Living Tahoe Adventure)\u201d. The Nevada Christian Home School team, which won the Northern Nevada competition held on January 20 in Reno, is comprised of students Cameron Etchart, Ethan Foster, and Ari Henry, teacher Tracy Henry and engineer mentor Pete Etchart.\n\u2013 Third place: Helen Keller Middle School\nHelen Keller Middle School in Royal Oak, Michigan, from the Michigan competition held January 17 in Novi, took third place honors for their Future City, \u201cMirai.\u201d The team is comprised of Mackenzie MacDormott, Sarah Lewan and Mairi Mundy-Dowd, teacher Donna Tarsavage and engineer mentor Jennifer Partlan.\nFuture City National Finals teams this year represented public, parochial, private and home schools and were comprised of 55 girls and 50 boys. As varied as the regional winners may be, they all have one thing in common: a program that challenges them to explore engineering, science, math, the arts and writing and, at the same time, discover abilities they never knew they had.\nThe students created detailed \u2013 often fantastic \u2013 cities of tomorrow that give intriguing insights to how young minds envision their future. At the same time, their bold designs and innovative concepts provide a refreshingly optimistic appreciation of how our nation can realistically deal with the many challenges facing its cities, including environmental disasters, crime, urban decay and urban sprawl.\n\u201cOne of the greatest challenges for those of us with engineering at the core of our businesses is securing a talented and diverse workforce for the future,\u201d said Bentley Systems CEO Greg Bentley.\n\u201cOur company has joined many others in supporting the National Engineers Week Future City Competition, which captures the attention of students when their choice of courses could have otherwise foreclosed engineering as a pursuit. The combination of engineer-mentors, hands-on learning, and teamwork engages students\u2019 imaginations and interest in engineering.\u201d\nJohn Hofmeister, President of Shell Oil Company, which provides funding to nine regional competitions in addition to the National Finals, says the forward thinking the competition generates benefits the entire profession.\n\u201cShell encourages achievement in technology and engineering,\u201d he notes, \u201cso Future City fits perfectly with our strategy to support promising students as they pursue innovative projects with an underlying emphasis on math and science, extremely important skills for many occupations at Shell. And as the number of graduates in engineering and geosciences diminishes, it\u2019s ever more important to encourage students to build these skills at an early age.\u201d\n\u2013 About Future City competition\nFuture City, celebrating its 15th anniversary, asks middle school students to create cities of the future, first on computer and then in large tabletop models. Working in teams with a teacher and volunteer engineer mentor, students create their cities using the SimCity 3000 videogame donated to all participating schools by Electronic Arts, Inc. of Redwood City, California.\nThey write a city abstract and an essay on using engineering to solve an important social need \u2013 this year\u2019s theme is using fuel cells to power a city of the future. Then they present and defend their cities before engineer judges at the competition. Some 30,000 students from more than 1,000 schools participated in 2006-07.\nFuture City is sponsored in part by Engineers Week, February 18-24, a consortium of more than 100 engineering societies and major corporations, co-chaired in 2007 by the Society of Manufacturing Engineers (SME) and Tyco Electronics Corporation.\nShell Oil Company is a major contributor to the Future City National Finals and a primary funder of nine regional competitions. The 2007 Essay sponsor is The Institute of Electrical and Electronics Engineers (IEEE-USA).","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/trimble-and-nikon-sign-definitive-agreement-to-form-joint-venture-for-survey\/","title":"Trimble and Nikon sign definitive agreement to form joint venture for survey","date":1049068800000,"text":"Trimble and Nikon Corporation announced that they have reached a definitive agreement to form a 50-50 joint venture in Japan, Nikon-Trimble Co., Ltd., to address the survey instruments market. Financially, Nikon will contribute \u00a51.2 billion (approximately US$10 million) in cash, while Trimble will contribute \u00a5500 million (approximately US$4.2 million) in cash and \u00a5700 million (approximately US$5.8 million) in its common stock. Nikon-Trimble Co., Ltd. will purchase assets from Nikon Geotecs Co., Ltd. and Trimble Japan KK and is expected to begin operations during the second quarter of 2003.\nUnder terms of the agreement, Trimble will acquire a 50 percent ownership stake in\nNikon-Trimble Co., Ltd., which will assume the operations of Nikon Geotecs Co., Ltd. in\nJapan. The new entity will focus on the design and manufacture of surveying instruments including mechanical total stations and related products. In Japan, the joint venture will distribute Nikon\u2019s survey products as well as Trimble\u2019s survey products including Global Positioning System (GPS) and robotic total stations. Outside of Japan, Trimble will become the exclusive distributor of Nikon survey and construction products. Trimble expects the joint venture to enhance its market position in survey instruments through geographic expansion and market penetration. The Nikon instruments will broaden Trimble\u2019s survey and construction product portfolio and enable the Company to better access emerging markets in Russia, Eastern Europe, India and China. It will provide Trimble with the ability to sell its GPS and robotic technology to existing Nikon customers around the world. Additionally, the new company is expected to improve Trimble\u2019s market position in Japan, which remains a major market for survey instruments.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/opinion\/making-light-work-part-i\/","title":"Making light work: part I","date":1165449600000,"text":"There are a whole host of approaches to lighting indoor spaces, which all have relative strengths and weaknesses. In the first part of a four-part comparison of these interior lighting methods, Darren Brooker walks us through lighting in conjunction with radiosity rendering.\nThere are many approaches to lighting an indoor space in CG and if the results are aesthetically pleasing then the means by which the output was produced is largely irrelevant. However, given that the differences in the capabilities of the different approaches are reflected in differing render times; knowing your way around these different methods can be invaluable in a production environment.\n\u201d The process of modelling light propagation involves dividing the original surfaces into a mesh. The radiosity algorithm then calculates the amount of light distributed from each mesh element to every other mesh element. \u201c\nWithin this series of articles, we\u00ddll look at a range of approaches to lighting, starting here with radiosity rendering, which can produce stunningly realistic results. However, the flipside of this realism is the time that these renders require to calculate the radiosity solution. Furthermore, if you have moving objects within your scene, this makes this technique fairly unusable, as the radiosity solution is constantly changing and therefore needs to be calculated for every frame. There is a workaround for using HDR maps to relight the moving elements within this type of scenario, but you\u00ddll have to wait until next month for this, when we move onto global illumination with mental ray and HDR lighting.\nPositives:\n1. Solution only needs calculating once for multiple cameras.\n2. Simple to set up, once the workflow is understood.\n3. Can provide beautifully realistic warm and soft results.\nNegatives:\n1. Does not address all global illumination elements (caustics are only dealt with by mental ray).\n2. Solution needs recalculating if objects moving in scene.\n3. Slow to calculate.\nTips & Tricks\n\u2013 Always check your system units.\n\u2013 Use photometric lights to represent real-world fixtures.\n\u2013 Use the Exposure Control to account for high dynamic ranges.\n\u2013 Turn on Reflectance & Transmittance Information in\nPreferences.\n\u2013 Design your materials carefully, in line with real-world materials.\n\u2013 Work with white materials initially to define your render settings.\n\u2013 Introduce curved objects to help define your render settings.\nRadiosity has its roots in thermal engineering research, before its methods for simulating radiative heat transfer between surfaces was reworked by computer graphics researchers, twenty years later, in the early 1980s. This process of modelling light propagation involves dividing the original surfaces into a mesh of small elements. The radiosity algorithm calculates the amount of light distributed from each mesh element to every other mesh element. Nonetheless, radiosity does not in fact address all global illumination effects, but as it excels at rendering diffuse inter-reflections and raytracing excels at rendering specular reflections, the two complement each other perfectly. After the radiosity solution has been calculated, a two-dimensional view of it is rendered, with raytracing adding the effects that it is particularly suited to providing: raytraced shadows, and materials that feature raytraced reflections and refractions. The final render thus combines both of these techniques in an image that appears more realistic than either technique alone could manage.\nIn order to understand the workflow for working with radiosity in 3ds Max, it makes sense to take a look at how radiosity is processed by the application, which is a three-stage process. Open the 01Radiosity.max file and within the Advanced Lighting tab of the Render Scene dialog, select the Radiosity plug-in from the drop-down. Near the top of the options appearing in this panel, you\u00ddll now see the Initial Quality setting, set to 85%. This is the first stage of the refinement process that involves the distribution of diffuse lighting in the scene.\nFigure 1 \u2014 After the initial run, \u2018Exposure Control\u2019 makes the initial exposure looks correct, but there are still noise and geometry issues.\nFigure 2 \u2014 In the Refine Iterations stage light is \u2018regathered\u2019 at every surface element and there is a definite improvement in quality, though some geometry issues are still apparent.\nFigure 3 \u2014 Final production quality rendering is achieved in the third, Pixel Regathering, stage.\nBetween each iteration, the radiosity engine measures the amount of noise that was computed between surfaces, with the contribution to the scene\u00dds average brightness decreasing logarithmically between iterations. An Initial Quality setting of 100% would provide a 100% accurate energy distribution, but this would not be 100% in terms of the visual quality of the solution. A setting of 80 \u2013 85% is usually sufficient for good results, so leave this setting as it is and hit Render. You should get a really blown-out result, which is because you need to use the Exposure Control to compensate for the dynamic range of radiosity rendering. Hit the Setup button further down the same rollout and choose Logarithmic Exposure Control.\nIf you render now, you\u00ddll see that the exposure looks correct, but that there are noise and geometry issues, as you can see in figure 01. This is dealt with by the second refinement stage \u00b1 Refine Iterations. Due to the random nature of the sampling during the previous Initial Quality stage, some of the smaller surfaces or mesh elements in the scene might miss being hit by a sufficient amount of rays. These small surfaces remain dark, and result in the appearance of dark spots. To alleviate these artefacts, the Refine Iterations stage \u00d9regathers light\u00dd at every surface element. Change the Refine Iterations (All Objects) setting to 2 and set the Indirect and Direct Light Filtering to 3 and 2 respectively. You should see that your render now takes next to no time to complete and there\u00dds a definite improvement in quality, though some geometry issues are still apparent, as you can see in figure 02.\nTo deal with these topology-related artefacts, which often appear as shadows or light leaks, a third refinement stage, known as Pixel Regathering, occurs at the time of image rendering. This involves a final regather process for each pixel of the image, which can add a fair amount of time to the rendering of a final image. To see this, turn on Render Direct Illumination in the Rendering Parameters rollout and render again. You\u00ddll see the difference this makes to your render time, but also your final rendering. There are still artefacts, in the form of light leaks, but you should see that this solution is almost there. If you want to look at the final render settings for this scene, open 01RadiosityFinished.max, where you can look through these rollouts to see what else is required to get to a final production quality rendering standard, as shown in figure 03.\nNow that you understand what determines the quality (and render times) of a radiosity render, you should open the 01RadiosityExample.max file, which is all set up to render at final render quality, just like our main image. If you look at the render settings for this scene, you will see that they are identical to the final quality settings for the previous scene file. However, this scene is more complex and this is the scene that we will be relighting each month using different methods in order to compare render times, output quality and any particular strengths and weaknesses.\nThings to note are that the interior lighting uses photometric lights, which use a pre-defined photometric web that is a 3D representation of the light distribution of a light source. Many lighting manufacturers provide web files that model their products, so particularly if you are visualising a space that is to feature specific light fittings, this is something you should take advantage of.\n\u201d Many lighting manufacturers provide web files that model their products, so particularly if you are visualising a space that is to feature specific light fittings, this is something you should take advantage of. \u201c\nA related thing is to ensure your scene\u00dds units are correctly defined. As radiosity works with physical lighting, so the lighting simulations obey its physical laws. You can imagine that a three-metre high space will look quite different to a three kilometre high space when lit by the same light fittings. The Units Setup dialog should be used to ensure that this is being dealt with correctly. The System Unit Setup is the most important component of this dialog, as the Display Unit is just a tool that lets you customise how units are displayed in the UI. Setting up units correctly is particularly important if you are importing geometry from another application.\nVisit www.stinkypops.co.uk\/aecPt01.html to download a zipped archive of all the scene files and textures that you will need to complete this tutorial. Unzip them to your chosen location on your hard drive.\nAnother thing to ensure is that you are designing your materials in line with their real-world equivalents. The best way to do this is to make sure that you have the Reflectance & Transmittance Information turned on in the Preferences. This allows you to see how reflective your materials are within the Material Editor. For example, a white paint material should only have a reflectance of about 80%, so by turning on this feature, you can ensure that your materials behave as their real-world equivalents would.\nAll of these tips will equip you well for rendering with both radiosity and mental ray, as well as with HDR lighting, so these fundamentals are good to remember early on. Next month we\u00ddll continue with global illumination, this time using 3ds Max\u00dds second renderer \u00b1 mental ray \u00b1 and we\u00ddll start to take a look at rendering out floating-point HDR maps in order to relight additional scene elements.\nTo read the second part of this tutorial, click here\nFor the third part of this tutorial, click here\nDarren Brooker is a BAFTA award-winning lighting artist who has worked at many top UK studios. He works for Autodesk as a product specialist. His book, Essential CG Lighting Techniques with 3ds max, is published by Focal Press","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/mcneel-flying-circus\/","title":"McNeel","date":1214870400000,"text":"While many big CAD vendors wielding hefty corporate marketing budgets have come and gone in the past 20 years, small developer Robert McNeel & Associates has managed to carve out its own niche with Rhino and AccuRender. Martyn Day recently visited the company\u00dds Seattle HQ.\nHaving written about CAD and design technology for the best part of twenty years, you can\u00ddt help but get very familiar with the inner workings of the CAD developers, how development happens, marketing is oriented, and how the route to market and sales are achieved.\nThere are a few basic models and principles that seem to permeate through 99% of the industry. But then there is Robert McNeel and Associates, a company that appears to swim against this tide like an Alaskan Salmon dodging all the hungry bears to the spawning grounds. The consensus from competitors is that it shouldn\u00ddt exist the way it is run, but somehow it does and very successfully.\nI\u00ddve known Bob McNeel for the best part of my journalistic career and you don\u00ddt have to look much further to discover the reason for McNeel & Associate\u00dds different take on the market. Bob McNeel is a trained accountant who fell into what turned out to be a very successful AutoCAD dealership in Seattle back in the \u00d9good old days\u00dd of early AutoCAD.\nMcNeel hit my radar when his company started to develop AutoCAD add-ons and I reviewed AccuRender, its photorealistic \u00d9inside AutoCAD\u00dd renderer when it was launched in the UK market.\nA few years later McNeel began to develop a 3D NURBS (Non-Uniform Rational B-Spline) modelling package named Rhino. For years it was in development and the betas were all given away for free. I remember talking with Bob McNeel on what the give-away business model was about.\nAt that time nobody gave software away and people were telling him that he was insane to do it. Bob was happy to get the software out there for people to find uses for it and let that drive development. After three years Rhino eventually went commercial for a very reasonable price and, against all odds, the people who had been using it for free started to buy it. Today, McNeel claims to have 250,000 Rhino users worldwide, covering diverse industries such as marine, apparel, product design, jewellery design, analysis, CAM and architecture.\nMcNeel & Associates is now out of the AutoCAD dealership game. Autodesk moved to limit dealers selling out of fixed distance from their offices, which reduced their potential to grow, unless they opened multiple offices. Then the move to AutoCAD verticals arrived, with ever increasing targets, and with the Rhino software business growing the AutoCAD\/ Autodesk relationship terminated. However, old and new clients still request AutoCAD training on old versions of AutoCAD.\nSince Rhino was launched, McNeel & Associates has built a whole ecosystem around Rhino and its rendering products. In many ways it\u00dds like Autodesk in the early days, with the vendor working on the core product and the third-party developers plugging into the application and adding functionality to tailor it to a specific vertical. There are already well over a thousand developers working with Rhino and extending its functionality around the world. McNeel has also expanded its range of products, adding other animals, such as Flamingo, a ray trace and radiosity rendering engine for Rhino, and Penguin for a conceptual sketch look and feel, as well as stylised renderings.\nThen there is Bongo for animations and Brazil, a Splutterfish rendering add-on for Rhino. For those with multiple licenses, there is a product called \u00d9Zoo\u00dd to manage your software.\nMcNeel and Associates continues its commitment to broad free beta software and within weeks of a release being formalised, the next generation of technologies under development usually get shipped as a beta. With Version 4 out, version 5 is available on the company\u00dds Labs page.\nAccuRender\nFor those of us that are long in the tooth, and pre-dating McNeel\u00dds fascination with animals, AccuRender was probably the first scan-line rendering application we used in AutoCAD. It had fractal trees, textures galore and quick results. Even though Autodesk competed aggressively against it, it ended up owning its own OEM license when it acquired Revit, as AccuRender was built-in as the default renderer.\nIn fact, Autodesk has only just replaced AccuRender in Revit with mental ray. That actually seemed to nark Bob McNeel as Autodesk failed to update the rendering engine for years, despite their license agreement allowing them to include the latest versions.\nWhile the name might be old, the technology is not and has always been updated. McNeel and Associates has expanded its reach too under the guise of AccuRender nXt \u00b1 the next generation. Scheduled to work with AutoCAD 2007- 2009, AutoCAD 64-bit, SketchUp, Revit, Rhino and IntelliCAD, the next AccuRender is in development and beta participation is free.\n{mospagebreak}\nThere is a brand new engine, which runs four to five times faster, is capable of working on larger models with support for multi-cores and handles threads a lot better than previous releases. The new algorithms anti-aliased image quality is looking very good already and renders are continually refined until the user stops the process. You can expect better tone reproduction and materials, fewer artifacts and a new User Interface. Daylight renders benefit from a new HDR algorithm, with better indoor day lighting too. Indirect lighting without using Radiosity is also possible.\nNew DNA\nThe big news is that there is a new animal about to emerge: Grasshopper. Again a technology for Rhino, Grasshopper was a development preciously named Explicit History and targeted Generative design. For those familiar with Bentley\u00dds Generative Components, Grasshopper is along those lines but without the programming. Bob McNeel calls it Visual Basic, without the Basic.\nThere has been a lot of interest in Generative design systems in the architectural world. Dr Robert Aish, while at Bentley, came up with Generative Components and now Autodesk has hired him he has a similar task to perform using Autodesk\u00dds technology (we presume built-on either Maya or 3ds Max).\nGenerative systems so far all require expertise in both geometry and programming, to write the applications that either derive the form or give the software boundaries and geometric relationships to allow designers to interact with the framework of a design.\nWith Bentley already there and Autodesk in hot pursuit I was pretty amazed to see an early demonstration of Grasshopper in Rhino last year. The output appeared to be similar to GC but the way the scripts are generated was amazingly \u00d9plug and play\u00dd. You don\u00ddt need to know how to program to create a parametric model using Grasshopper as there is a new window for creating what amounts to a bank of geometry \u00d9effects peddles\u00dd, as you would with a guitar. You take the input, which may be a line and you literally plug it into an effect, which can be any kind of mathematical transform, and which can subsequently be plugged into other mathematical effects peddles, all driving the creation of a script (in the background) that drives Rhino\u00dds geometry engine. It\u00dds all drag and drop and plug and play.\nThe system uses some specific terms, namely Parameters, Components and Connectors. A parameter stores data; a Component contains an action; and a Connector connects \u00d9Ins to Outs\u00dd or \u00d9Outs to Ins\u00dd.\nThe Components can be selected from a range of mathematical functions (Tan, Cos, Pi, Divide, Sine, Log etc.) or entered by the user with X, Y and Z. Variables can be assigned to sliders allowing you to play with the important underlying geometry. One other interesting way of driving the system is the ability to draw curves and plug those into the effects boxes. So while you might not know how to fully express an equation, you might be able to draw the output or input that you want. I have not seen anything like this in GC.\nWhat you end up building is a series of wired up boxes which drive the geometry creation. This could be merely to create a form or to make a dynamic environment \u00b1 perhaps a planar wall that deforms when placed in proximity to another piece of geometry. All the time the Rhino viewport displays the results of this plug and play visual scripting, giving constant feedback.\nThe great news is that, as this is McNeel & Associates, Grasshopper is free and is considered part of the core program. With the sheer number of Rhino users in architectural firms and its position as a mainstay in Universities, expect to see a lot of users adopting Grasshopper for Generative design.\nBentley recently introduced a $250 downloadable version of Generative Components to spread the word (which actually includes a full copy of MicroStation, but is only functional in GC mode). It seems that the fight has been joined at the \u00d9grassroots\u00dd level of adoption. Perhaps this is the reason behind the naming? Although Bob has mentioned that in the 70s TV series there was a character referred to as Grasshopper \u00b1 the loan Kung Fu monk who always sorted out problems \u2013 and I can see how Grasshopper could be seen as Kung Fu for architectural geometry!\nWhile many industry people thought Bob McNeel was mad to give away Rhino for so long, you\u00ddd be hard pushed now to find a CAD software company that doesn\u00ddt give early versions away on labs to measure reaction. Autodesk\u00dds path to volume is to trial the software on its labs site and then launch and sell rev 2 of the product.\nEven now, the rapid appearance of the Rhino version 5 beta, just as Rhino 4 ships is unheard of. It\u00dds based on really enthusiastic users always wanting more and happy to play with forthcoming technology. However, again, to be doing this without a subscription model in place would have the big CEOs weeping into their cornflakes with all that missed opportunity to ratchet up the income dollars.\nWhen I arrived in the offices of Robert McNeel & Associates in Seattle, Bob was on the phone, telling the caller that Rhino probably wasn\u00ddt the best option for what they were looking for and that he (or she) should probably look for a specific vertical application for the industry that they were in as it might do more. Throughout my two days with Bob and the development team I was trying to figure out why this laid back approach to developing and selling CAD tools worked. In the most capitalist nation on the planet McNeel is an anathema to corporate culture. There is almost a total lack of greed and it\u00dds incredibly refreshing.\nWhile CAD vendors spent millions in marketing, McNeel & Associates doesn\u00ddt, instead preferring their customers to be the product\u00dds \u00d9marketeers\u00dd, spreading the good word by mouth and if that\u00dds a slow process then so be it. The McNeel ethos is to listen to the customers, help customers, because if the customers are happy, the products do well. It\u00dds not about milking customers.\nTo date, McNeel & Associates has done well to stay below most of the big CAD vendor\u00dds radar but with increased market coverage and new products that directly compete, like Grasshopper, the company is bound to become a competitive target. Can these vendors compete against the mature, low-cost products McNeel creates or establish such a community of happy users to drive development? Somehow, I don\u00ddt think Bob has sleepless nights in Seattle over it.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/reality-capture-modelling\/site-intelligence-drone-construction\/","title":"Site intelligence","date":1590537600000,"text":"By using drones to capture images at critical stages of the construction process, one US firm has gained unprecedented visibility into its projects\nJohn Andres is the third generation of his family to work at Andres Construction, a Dallas-based firm that manages major commercial, institutional, and developer construction projects across Texas.\nFor many years the company contracted with a helicopter company to capture aerial images of its construction sites. While the images captured from the helicopter were attractive marketing assets, they didn\u2019t deliver valuable construction site information. The airborne photographer didn\u2019t capture the images from the same angle at regular frequencies, which meant that the images didn\u2019t provide full construction site visibility or reliably measure change over time. Plus, they had to be manually organised and processed \u2013 a time-consuming exercise that didn\u2019t deliver additional business insights.\nJohn, a drone enthusiast, knew that drones would give Andres Construction much-needed visibility into its construction sites. With high-resolution 360 photos, John and his colleagues could zoom in to precision data captures and gain detailed site awareness. Multi-angle image series would allow them to see the full perimeter view of their sites, monitor progress, and ensure safety and compliance. High precision orthomosaics would give them a birds-eye-view tool to remotely measure distance, area, volumes, and improve site planning. But before John could create a drone program, he had to convince company leadership\u2014his father, uncle, and grandfather\u2014that it was a good idea.\nAndres conducted a test project where site images were captured with both a helicopter and a drone. The drone used JobSight, a software tool that uses flight automation to capture images from the same angle, at the same frequency and deliver them to users as three data types: 360 photos, multi-angle image series, and orthomosaic images.\nThe Andres Construction team found that JobSight offered unprecedented visibility into its construction projects. It gave them up-to-date information on their flight schedules and allowed them to control the timing of their data capture \u2014 a major benefit given that construction sites are active throughout the day and project timelines often change. Images could be captured at critical stages during the construction process, including prior to pouring concrete. By capturing pre-pour data, Andres Construction could maintain site visibility and look beneath the surface even after concrete was poured. The software allowed them to monitor progress, check on available materials, and screen for safety concerns from their central office, without having to leave their desks.\nNow, Andres Construction works with a Drone Service Provider (DSP) and uses JobSight to capture and process thousands of images across its entire portfolio of construction projects and deliver progress updates to customers.\nOn a recent build, the Andres team was preparing to drill piers, the load-bearing supports that hold up a building. When John conducted a routine review of the construction site\u2019s JobSight captures, something didn\u2019t look quite right. By using JobSight\u2019s data types to look at the site from multiple angles, John determined that the piers were laid out to be drilled in the wrong place. John and his team overlaid their drawings onto a JobSight orthomosaic image of the site and determined the correct placement for the piers, remedying what could have been a costly and time-consuming mistake.\n20\/20 vision\nJohn Andres starts his workday at the JobSight dashboard, where he monitors project progress and ensures that all projects are safe and on schedule. He gets a notification every time JobSight captures new images. Those images are sorted by date in the JobSight viewer, so John can see what happens on the company\u2019s construction sites week to week.\n\u201cIt\u2019s impossible for us to go to every project site every day when we\u2019re in the main office,\u201d John said. \u201cAt the same time, we need to track production rates, ensure that material is on-site, and be able to see the work that got done a week ago, or several months ago. JobSight allows us to do that.\u201d\nFor Andres Construction, JobSight has been a powerful way to track all their projects, offer their clients a quality assurance value-add, and create a comprehensive system of record.\n\u201cEvery week we capture the same images from the same angles,\u201d John Andres said. \u201cWith JobSight, we can peel back the layers on a building, see that progression, and know that things were done right along the way.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/new-automated-solution-streamlines-government-services-for-citizens\/","title":"New automated solution streamlines government services for citizens","date":1092096000000,"text":"Accela, the provider of government enterprise management software solutions, has announced that New Orleans, Louisiana has launched the company\u2019s Web based software solution, Accela Automation, as the new enterprise application for the city\u2019s\nDepartment of Safety and Permits and Planning Department.\nThe new system provides a centralized approach to permitting and land management, as well as planning, licensing, and enforcement activities. The solution allows City employees to input data to a centralized database and access the most recent information about a particular application or permit at any time. Regardless of their role in the department, the flexible user interface allows every employee to access the information they need to complete their jobs.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/review-trimble-mepdesigner-for-sketchup\/","title":"Trimble MEPdesigner for SketchUp","date":1426550400000,"text":"Shaun Bryant takes a look at SketchUp plug-in MEPdesigner (V1), and finds a low-cost, easy to learn option for 3D design that is already competing well with other MEP application offerings.\nTrimble Navigation was founded in 1978 and is one of the most well-known names in construction and surveying, with a focus on surveying in construction, agriculture, mapping and GIS, as well as providing many mobile solutions for workers in the field. It employs approximately 7,600 employees in 33 countries and has an annual revenue of around $2.5 billion.\nRecent acquisitions include SketchUp in 2012 and Gehry Technologies in 2014, along with many others, making Trimble a world-wide player in the CAD marketplace. It has improved SketchUp and is making it accessible to many mobile field teams that need a simple, and inexpensive, 3D CAD solution.\nMEPdesigner is a SketchUp plug-in aimed at the mechanical, electrical and piping disciplines. I had the opportunity to talk with Paul Goldsmith (segment manager) and Douglas Elliott (product manager) from Trimble about MEPdesigner (Version 1), how the new plug-in works and what plans they have for it in the MEP marketplace.\nInitially, Trimble looked at SketchUp and decided to follow the 80\/20 rule. As a guide, the top 20 percent of MEP users have Autodesk Revit MEP. Trimble wanted a product that allowed the other 80 percent tro have a 3D MEP design tool. MEPdesigner fits nicely into that other 80 percent segment, providing 3D design capability.\nIn an ambitious six-month development period, Trimble created V1 of MEPdesigner, primarily aimed at electrical contractors and engineers who already use Trimble\u2019s SketchUp for conceptual engineering and design. Even if designers are not using SketchUp already, the interface is easy to learn and use.\nTrimble\u2019s MEPdesigner workflow allows for conceptual 3D design to be ported into Trimble Connect for online design development and clash detection in existing 3D models. This allows design information to be taken from the 3D model to the field.\nIt also provides SketchUp users with a quick, efficient tool for early conceptual MEP design, showing placement of equipment in a 3D environment, which allows for quick clash detection and good design intent communication with both the design team and team members onsite.\nWhen it comes to project management, MEPdesigner enables easy communication of the 3D model between the owner, engineer, contractor and the field team and provides communication of design intent of power, low voltage and speciality MEP systems. Collision detection is the name of the day which, in turn, can lead to effective pre-fabrication opportunities and enhance the safety record on the construction site.\nAnother major benefit of MEPdesigner is the ease in which daily work packages can be conveyed to the appropriate design teams. Using Trimble Connect via an internet browser, all 3D design can be highlighted and clash detected easily.\nAs discussed with the Trimble team, SketchUp and MEPdesigner provides the opportunity to make all MEP team members 3D experts. MEPdesigner requires SketchUp Pro to operate but this, in turn, gives access to the SketchUp 3D Warehouse and many manufacturers\u2019 content also available in SketchUp, thus providing an incredible amount of content for any designer; beginner, intermediate or expert.\nMEPdesigner allows MEP design to go mobile. I have already mentioned interoperability with Trimble Connect. Any MEP design can be viewed in the SketchUp Mobile Viewer, on tablets and other mobile device. Field points can be added in MEPdesigner for the teams out in the field, which can then be utilised by the Trimble Field Points software with Robotic Station, providing accurate GPS point location for MEP data on any site.\nAs designs are created in MEPdesigner, the Level of Detail (LoD) becomes important. Trimble is aiming for LoD of 350 (LoD 350) with all visual representation of 3D designs. This is the required standard.\nAs part of the Trimble eco-system, MEPdesigner is distributed via the Extension Warehouse, along with numerous Trimble AutoCAD apps, such as 2015Trimble Connect (acquired from Gehry Technologies).\nI was very impressed with the section tool in Trimble Connect. In a 3D view, this allowed for what has to be some of the easiest section manipulation I have seen in a 3D model. It is slick, fast and works.\nAs I mentioned previously, Version 1 of MEPdesigner is aimed at electrical designers and contractors, with the majority of 3D parts coming from another Trimble spin-off, TradeService. This powers all 3D data in MEPdesigner.\nWhile these parts are primarily electrical, there will be much more focus on piping and ducting in later versions with the 3D Warehouse providing the majority of 3D components in SketchUp itself.\nOne of SketchUp\u2019s biggest successes is the ability to convert 2D drawings to 3D models. In a step-by-step process, SketchUp and MEPdesigner can develop a 3D MEP model quickly and effectively.\nStep 1\nUsing Trimble Connect, a 2D reference model can be brought in to SketchUp. A 3D model can then be generated from the 2D reference drawing, using 3D components in SketchUp.\nTypical components would be walls, doors and windows. Service openings can then be generated as well.\nAs Trimble Connect is a free SketchUp Extension, the reference model link is seamless.\nStep 2\nUsing MEPdesigner, the 3D model would be populated with 3D MEP components, which would then be connected with MEPdesigner by way of electrical conduit, as V1 of MEPdesigner is primarily electrical. There is no ability to bend conduit or use flex conduit in V1 as its main content revolves around larger rigid electrical components. However, custom components can be developed from the basic components provided, such as a breaker panel with specific connections.\nMEPdesigner is different to most SketchUp plug-ins by way of its top level menu and toolbar that you use to bring up the design palette and tools. This interface is used to add parts and route conduit. Once these parts are in place, part grips can be enabled in SketchUp in the usual way and conduit and cable trays come in with real world lengths with spacers and couplings.\nOnce created, all MEPdesigner models can be loaded into Tekla BIMsight, where collision detections can tagged and assigned to field teams. These tagged collisions can then be brought back into MEPdesigner, reviewed, re-designed accordingly and then signed off by the field teams.\nUsing SketchUp Layout, views from the 3D model can be sent out to paper or electronic drawings. Any points can then be sent out to field teams using the Trimble Robotic Station.\nConclusion\nSketchUp Pro with the MEPdesigner plug-in provides a great 3D MEP solution for under $1,000. However, it does have a long way to go to compete with some of its competitors, such as Autodesk Revit MEP with its flexible conduit and layout testing tools.\nBut bearing in mind that MEPdesigner is only at Version 1, I am sure the development team at Trimble will be working hard to make improvements and to develop much more content.\nAs stated by the Trimble team, V1 of MEPdesigner is aimed at the electrical marketplace setting up larger electrical installations. With piping and ducting to follow, I can see how the MEPdesigner SketchUp plug-in will go from strength to strength, competing well with other MEP application offerings.\nSketchUp has improved dramatically since the acquisition by Trimble, and I can see that MEPdesigner will be treated with the same rigid development and marketing, thus positioning it in the marketplace as the low-cost, easy-to-learn solution, which is exactly where Trimble wants to be.\nOverall, I can see MEPdesigner being a highly functional plug-in for SketchUp with a good future ahead of it, as long as Trimble maintains the product development and component development it has put in to SketchUp so far.\nAbout the author\nShaun Bryant is an Autodesk Certified Professional with twenty-six years total industry experience using AutoCAD and Revit.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/au2015-interview-with-autodesk-ceo-carl-bass\/","title":"Autodesk University 2015 - Press Q&A with Autodesk top brass","date":1448928000000,"text":"The low down on artificial intelligence, the cloud, the changing role of resellers, the blurring of building and manufacturing and more\nAs is traditional practice at AU, Autodesk held an afternoon session featuring key executives to give an overview of the mainstage keynotes and to be on hand for the international press to ask questions.\nThis year the Q&A sessions featured: Andrew Anagnost, senior VP of industry, strategy and marketing, chief technology officer Jeff Kowalski, and president and CEO Carl Bass.\nThe session opened with Anagnost picking out the customer stories an underlying messages from Kowalski and Bass\u2019 Keynotes, highlighting the work that Autodesk has been doing with Airbus on computer generated solutions to engineering problems.\nThe big message was that this is a fundamentally different way for design and looking forward, where we can all start to work with computers to solve problems, as opposed to merely document.\nAnagnost is a key player at Autodesk, where he has been overseeing the company\u2019s move to a cloud and subscription-based model, a business model that Autodesk will truly \u2018flick the switch\u2019 on next year, as from July 2016, products will only be available on Autodesk Subscription.\nFrom talking with dealers at the event, many were hastily trying to position themselves to survive on Autodesk\u2019s lower-margin business model. The two things they could agree on was that they needed to sell more \u2018services\u2019 and that there will be fewer Autodesk resellers in the future.\nAnagnost refused to come up with a number when asked how many Value Added Resellers (VARs) would be lost through this business model change, but agreed there would be consolidation.\nOn the fate of resellers, he replied, \u201cThere are already partners out there that have moved over 60 per cent of their new run rate to our new business model. What they are asking from us is for more stuff like Forge, they want development platforms and they want a way to connect processes and make things work and deploy these new types of solutions in their customers.\nThey are itching to do this, these are our best partners. These are the partners we are going to invest in and build our ecosystem.\n\u201cYou are right, in the new world where we are moving towards subscription for everything, some partners who made their business from just turning the crank, answering phone calls\u2026 I think it\u2019s going to be tough for them, that\u2019s the reality. We told them over and over. Our best partners are already figuring out what they need, they are asking us for it and we are delivering it.\n\u201cThe more capable our partners get with the Forge platform, the more business they are going to have.\u201c\nNext year there will be a seismic shift in the Autodesk business model, altering how much you pay, how you pay and where you get software.\nFor those that rely on the local reseller channel this could impact any level of \u2018free\u2019 help on offer, access to on-site demonstrations, training and potentially a change in reseller.\nAnagnost seems to be blurring the lines on what Autodesk wants from its future channel, more developer than product sales.\nCEO and CTO\nThe main act for this Q&A session was the appearance of Kowalski and Bass side by side. As usual the the questions were varied and ranged from the aims of the new Dreamcatcher technology to sales issues in China and Russia. On the subject of sustainability, the pair were asked on the the usage of Autodesk\u2019s sustainability analysis tools and if it\u2019s an important part of Autodesk\u2019s sales. CEO Bass, replied: \u201cI wouldn\u2019t look as our investment in Sustainability as one which is based on a return, financially speaking. It\u2019s one of the areas where we think there are big problems in the world and there are a number of things that we can contribute.\n\u201cThis can range from energy analysis of building to great examples such as \u2018lightweighting\u2019 parts with Dreamcatcher. There is an enormous positive long term effect in deploying this technology. This helps our customers, which is good business.\u201d\nKowalski added: \u201cWe have given customers tools so they can understand the impact of designs, assess performance, operational cost and optimise all of that before hey commit to a final design.\n\u201cWe are now looking to take back the actual \u2018in use\u2019 operation data back into the model, so they can understand the discrepancies and \u2018debug\u2019 the design.\u201d\nBoth Kowalski and Bass were looking forward to hearing what was going to come out from the Paris COP21 summit. They were probably very happy with the result.\nOn the development of Autodesk\u2019s 3D print platform Spark, and hardware, Ember, Kowalski commented: \u201c There are three components to Spark \u2013 the API, the reference printer the Ember and an investment fund to boot up the ecosystem.\n\u201cWe were successful in transferring the conversation around 3D printing from \u2018trinket\u2019 making to actual additive industrial manufacturing. The printer we developed is actually capable of producing real production parts, it\u2019s very high resolution and open sourced.\n\u201cThrough the fund we got to see nearly everything that was about to happen in the market and have meaningful impact.\u201d\nBass added: \u201cAs a side note here, the difference between Autodesk the software giant and Autodesk the hardware start-up, the other day we had one of those hardware start-up ecstatic nightmares, that take place simultaneously, when someone came to use and asked for 1,000 ember printers. We had an incredible desire to say yes but had no capacity to produce them!\n\u201cIt\u2019s giving us a different lens into how some of our customers live. Just to add to Jeff\u2019s point about trinkets, we have moved from talking trinkets to Airbus flying a 3D printed part.\u201d\nOn the topic of Autodesk\u2019s acquisition strategy, Bass explained: \u201cWe tend to do 10-15 acquisitions a year, and I put them in 3 buckets: small, medium and large.\n\u201cThe smallest category is a team and technology, people who are passionate, educated, love the thing they are doing and know a lot about it. The second one are folks that have a product, or almost have a product and the third is a company, that has multiple products, established and mature. Many of our acquisitions fall into the first two buckets.\n\u201cWhen we have a desire to get into a market and the best way is to acquiring or \u2018acquihire\u2019 a team or buy a product in development to shorten our time to market. It\u2019s fairly rare for us to actually buy a company\u201d\nWhen he was asked what the Autodesk will look like in 2020, Bass answered: \u201cOur products will become indistinguishable. We will have a range of cloud-based products and services that people will put together in ways that makes sense for them, they will access and reconstruct that platform in their own way.\n\u201cThe desktop \u2018thing\u2019 will now take place in the cloud.\n\u201cLook at Fusion 360 and PLM 360, they have front ends for input and almost all the computation happens on the other side of the wire. That\u2019s the cloud-based world of products and services that all software companies will be in.\u201d\nOn the move to the cloud, Bass thinks that mobility is becoming mainstream, an example being people increasingly not wanting to travel with their laptops. \u201cWith the advent of products like the MS Surface Pro and App\ni.e. iPad Pro, it\u2019s becoming increasingly more obvious that tablets are taking over, even for people that use design software\u201d.\nHe added that the super computer we have in our pocket is going to play an increasing role in cloud services.\nFrom other answers we gleamed:\nWe heard that Autodesk hires young people to look at Autodesk solutions and interfaces with fresh eyes and then compare them to competitors products.\nBass admitted to owning a drone and has a similar outlook to them as 3D printers and trinkets. While the consumer market for drone is interesting, Bass thinks the big benefits are going to come from industrial uses. On the giant Apple campus being build in California, morning meetings are based on UAV video from the day before.\nBuilding and Manufacturing are starting to blur, with modelling being used increasingly in the AEC space and digital fabrication becoming better understood and deployed onsite. AEC is becoming a manufacturing process and building are becoming assemblies.\nOn technology, Bass admits that he gets 98 per cent excited by most new technology but there\u2019s always two per cent that \u2018creeps him out\u2019, should that be drones, biotechnology, 3D printing. \u2018Bad people do bad things\u2019 with any technology, he said.\nKowalski admitted that a few years ago they had taken a 3D printed gun to a conference in the UK and waved it around on stage to show technology can be used or abused.\nGlobally, a few questions opened up the challenge Autodesk faces around the world. Having been developing software in China for ten years and with many customers it\u2019s \u2018certainly not the easiest place to do business\u2019. Meanwhile in Russia, the government recently banned state-owned firms from buying foreign-made software, to which Bass responded that all Government procurement is: \u201cincredibly short sighted and about the most stupid thing that anyone can do in the IT realm, should that be for security concerns or to support the domestic software industry.\u201d\nWhat is Autodesk Forge?\nAt Autodesk University, Autodesk launched a new development platform (PaaS \u2013 Platform as a Service) and $100 million fund to assist the next generation of third party developers.\nForge runs on Amazon Web Services and on Autodesk\u2019s own server centres, which are currently in development.\nThis cloud platform is a single place where Autodesk\u2019s products and services are available for access by Third Party developers through APIs and SDKs.\nOver the past five years Autodesk has been rewriting or creating new web-based platforms, such as 3D rendering, model collaboration and DWG read\/write as componentized \u2018web services\u2019, which all of its applications can call on when functionality is needed. Now developers will be able to hook into Autodesk\u2019s PaaS and integrate or build new applications to focus on specific markets.\nThe big wedge of money is there to attract start-ups and pay for development work in key areas that Autodesk is targeting.\nThe focus, we are told, will mainly be in the manufacturing segment, where Autodesk is hell bent on sprinkling Fusion amongst its competitor\u2019s customers.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/intergraph-stockholders-approve-acquisition-by-hellman-friedman-llc-and-texas-pacific-group\/","title":"Intergraph stockholders approve acquisition by Hellman & Friedman LLC and Texas Pacific Group","date":1164067200000,"text":"Huntsville, Alabama, USA, 20 November 2006 \u2013 Intergraph Corporation, a leading provider of spatial information management (SIM) software, today announced that the stockholders of the Company voted to adopt the merger agreement providing for the acquisition of Intergraph Corporation by an investor group led by Hellman & Friedman LLC and Texas Pacific Group at a special meeting of the stockholders held today at Intergraph\u2019s executive offices in Madison, Alabama.\nBased on the preliminary tally of shares voted, approximately 99% of the shares of Intergraph common stock present and voting at the special meeting (in person or by proxy) voted in favor of the proposed merger agreement. The number of shares that voted to adopt the merger agreement represents approximately 73% of the total number of shares of Intergraph common stock outstanding and entitled to vote as of October 11, 2006, the record date for the special meeting.\nThe proposed merger was announced on August 31, 2006 and is expected to close by the end of November 2006, subject to the satisfaction or waiver of the conditions set forth in the merger agreement. Under the terms of the merger agreement, Intergraph stockholders will receive $44.00 per share in cash, without interest, for each share of Intergraph common stock held.\n\u2013 About Intergraph\nIntergraph Corporation is a provider of spatial information management (SIM) software. Security organizations, businesses and governments in more than 60 countries rely on the company\u2019s spatial technology and services to make better and faster operational decisions.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-clearedge3d-announces-edgewise-5-0\/","title":"NEWS: ClearEdge3D announces EdgeWise 5.0","date":1427846400000,"text":"New software includes enhanced pipe extraction, improved Revit integration and a Plant3D plug-in\nEdgeWise 5.0, the latest release of the software from ClearEdge 3D that can create as built BIM models from point clouds, features automated extraction of gridded steel and concrete structure and improved Revit integration that enables import of out-of-tolerance and non-standard as-built elbows.\nThe software uses what is described as advanced feature extraction and object recognition technologies to automate the 3D modeling of as-built structural members, MEP, process plants and buildings from point clouds. EdgeWise BIM Suite, a BIM-specific version of the software with tight Revit integration, can automatically identify and extract walls, windows, doors, pipes and other features from point clouds and export them as Revit family objects.\n\u201cThe new algorithms in EdgeWise 5.0, especially the automated modeling algorithms for structure, are a major progression for the industry,\u201d said Chris Scotton, ClearEdge3D\u2019s President & CEO. \u201cOur team of computational geometry experts has spent the last year rewriting and greatly improving our core technology. Users can now automatically extract gridded structure from a point cloud which will be a massive time-saver for anyone modeling as-built steel and concrete.\u201d\nAdded Kevin Williams, ClearEdge3D\u2019s founder and Chief Scientist, \u201cWe developed a whole new set of algorithms to dramatically improve our automated pipe extraction. The entire QA step of the EdgeWise workflow can be dramatically shortened due to the accuracy these new algorithms deliver.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/interview-joe-croser-bentley\/","title":"Interview: Joe Croser, Bentley","date":1160092800000,"text":"Martyn Day caught up with Joe Croser, Bentley\u00dds Global Marketing Director, Platform Products, to talk about MicroStation\u00dds new 3D interface, the improved 3D graphics pipeline and support for Microsoft\u00dds forthcoming Operating System, Vista.\nLooking at MicroStation V8 and V8 XM, there seems to be two distinct categories of work being done; the architecture\/database and then the GUI graphics and front end. How much of V8 is new and how much of the old pre-V8 MicroStation is still in there? Was the whole V8 program a multi-year, managed rewriting of MicroStation? Is it complete now?\nJoe Croser: Great start Martyn! You have hit the nail on the head. Our V8 Generation of products has indeed been a multi-year project, the latest development being the V8 XM editions of MicroStation and ProjectWise, which were released earlier this year.\nWhy has it taken a generation of V8 to make the changes? Software technology and product development is a constantly moving window, and so the developmental challenges we face as a company are shaped by our industry, the needs of our users and by our users\u00dd readiness to embrace change. At Bentley, we provide users with a comprehensive portfolio of software to design, build, map and operate the world\u00dds infrastructure, and many of our users rely on a number of different products from the portfolio to get their work done.\nHence, one of the greatest challenges faced by our development teams is the continued provision of forward and backward compatibility, across different versions and across different applications. Some software developers factor out this obvious need to maintain compatibility in favour of new features, but that has never been our practice. Infrastructure is unlike many other industries (including manufacturing) in that a project can run for years, perhaps even decades throughout its lifecycle, so bi-directional compatibility is a fundamental requirement for all large projects and organisations.\nThis is one of the reasons why we have adopted a common platform for application development, enabling us to innovate in MicroStation and derive wholesale benefit from that innovation in every other vertical application. Indeed, the V8 Generation has enjoyed substantial innovations that have driven many changes to the file architecture and to the user interface and graphics engine.\nWhen we released the first V8 Generation of MicroStation in 2001, we changed the DGN file format for the first time in its 17 year history, and we have not changed it again since. This one change enabled us to dramatically raise our headroom and create space for new innovations across the V8 Generation of applications, and the list of innovations is long. The innovations have included Design History, Digital Security, PDF creation and referencing, SharePoint integration and the greatest innovation of all, interoperability. Providing direct, in-place editing of DWG \u00b1 without translation \u00b1 and the new EC Framework; an interoperability layer connects a world of infrastructure applications.\n\u201d One of the reasons we have adopted a common platform for application development is to enable us to innovate in MicroStation and derive wholesale benefit from that innovation in every other vertical application. \u201c\nAdvertisementAdvertisement\nIn the V8 XM Edition of MicroStation, we see the new user interface and experience the power behind the new DirectX graphics engine. It is therefore hard to say exactly how much of the MicroStation V8 XM Edition consists of pre-V8 code. We have changed enough thus far to entirely change the look and feel of MicroStation and yet we have retained enough of the old code to maintain forwards and backwards compatibility for all versions of MicroStation \u00b1 not just the last three.\nCould you explain a little about how MicroStation acts as the platform for the Bentley vertical products and how, as a division, you would select what features need to be added to each release to co-coordinate with these products. If seems like you must be being pulled in many different directions!!\nJC: I could speak all day about how MicroStation acts as a platform to our comprehensive portfolio of vertical applications and the benefits it delivers to our users through unparalleled interoperability, but I will try to keep it brief. Essentially, to serve common needs across different applications and industries by developing in a single platform, we are able to concentrate our innovations at an operating-system level for widespread benefit. Think of MicroStation as our operating system for the vertical applications, much like Windows is to Microsoft Office. In MicroStation, we build in many common functions that dramatically and positively affect interoperability in the verticals.\nAt a platform level, we have recently introduced what we call the Engineering Component (EC) Framework, which enables applications to share data without loss. This is the framework that we are using in our Civil development team to enable InRoads, Geopak Roads and MX to all create, manipulate and edit the same data using different applications. Previously, they all stored their data separately and shared it with some degree of loss. So why, you may ask, are we doing this?\nVertical products are rapidly gaining favour across the infrastructure industry in the face of a global skills shortage. Users are striving to increase their productivity and quality by going vertical and adopting point-solutions in order to process an increased workload, with the same size team. At Bentley we recognised this industry trend more than 10 years ago. That was when we laid the foundations for our comprehensive portfolio and started executing on our acquisition strategy. In the last five years alone, we have made about 20 strategic acquisitions that have enabled us to extend our portfolio considerably.\nAnother good industry example is the plant industry, which is exploring every available avenue to deal with its volumetric increase of work. The plant industry went vertical years ago, so more recently it has been managing its increased number of projects by spreading the workload across distributed enterprises using collaboration technologies like ProjectWise \u00b1 a scalable system for collaboration that enables distributed enterprises and related organisations to successfully deliver infrastructure projects.\nOf course this is all background to your question. We used to resource development by first designing the platform before passing it to the vertical development teams to build their applications on top. This approach led to much duplicated development work being done by each vertical and resulted in too many different tools being created to essentially do the same thing. This also confused users of one application and confounded those trying to switch between a number of applications during design.\n\u201d Clearly DirectX is the right technology to implement. DirectX integrates with all modern graphics cards and provides far superior vector and raster video speeds when compared to OpenGL. \u201c\nHence today we approach the challenge differently. We now have vertical development teams working inside the platform team, which dramatically increases positive communication between all groups within platform and between the platform and the vertical teams. More development work is now being executed once in platform for the benefit of all four verticals and their applications. The result is increased innovation in MicroStation and an increase of innovations by MicroStation in the vertical applications. User productivity is being driven through the roof with the same number of people on each team.\nHow hard have the verticals been pushing for increased 3D performance?\nJC: 3D is everywhere now and I don\u00ddt think there is an organisation out there that would not want to benefit from increased 3D performance. We recognised the value of 3D early in the development of MicroStation and we led the charge to improve it. 3D has always been a Bentley strength, and in MicroStation, we remain ahead of the field in modelling capabilities with general solids, parametric solids, surfaces, NURB surfaces and the new mesh modelling tools. Of course, there is also the new technology that I know you have eagerly followed through its development, Generative Components. GC is entirely new and an incredibly powerful technology the like of which is not available outside of MicroStation and the vertical applications that contain MicroStation innovations.\nBut I don\u00ddt think that the benefits reaped by the verticals are necessarily held hostage by 3D performance. The vertical teams constantly drive the platform team to develop new technologies that enable them to create more user-friendly products that interoperate seamlessly. And again I understand why few professionals use the same tools for concept, design, detailing, analysis and production. So, I think the value inherent in using vertical applications is realised through a greater versatility as well as better 3D technology.\nCertainly, 3D was the big visual change for users moving from CAD drafting to BIM, and great value is associated with using a single model to coordinate designs and extract many drawings There are also higher returns to be had from simply sharing data without loss. With Bentley solutions you are able to move your architectural model to a structural environment without loss and by being able to round-trip your structural design model and your structural analysis model while modifying and enriching the data and associated drawings.\nAnother great benefit is the presence of a common and familiar interface across different applications, so that when you move between different vertical point solutions, you remain in an environment that is familiar and configured with the same tools.\nFrom talking with Keith Bentley, even he couldn\u00ddt wait to get the front end of MicroStation \u00d9into the 21st Century\u00dd. What kind of technologies has Bentley selected to use in the new graphics part of the CAD system and what kind of advantages will end-users most obviously see?\nJC: You know it\u00dds funny how we all become accustomed to what we see every day, it becomes the norm and anything different is at first disparaged as being abnormal, sometimes even weird. I remember having conversations with you about MicroStation\u00dds interface being \u00fdold and 3D unfriendly\u00af and I remember the conversation you refer to here with Keith and I remember thinking \u00fd\u00cdMartyn is wrong, MicroStation is a great 3D tool. How can it be 3D unfriendly?\u00af\nSo now I fast forward to September 2006 and I look at the new MicroStation V8 XM interface and I think \u00fd\u00cdthis is such a nice place to spend some time, this feels like a really cool place to work, this is a great interface for 3D!\u00af \u00cdand I get used to it. Now the V8 XM Edition interface is the norm and anything that went before just doesn\u00ddt cut it as well. Now I can see that you might have had a point \u00b1 but it\u00dds the last time I\u00ddll admit that to you! And knowing that with V8 XM we slingshot MicroStation into the 21st century makes it easier to say too.\n\u201d We have paid considerable attention to the way a gamer moves around their immersive gaming world and we have emulated the same ease of movement around 3D models with some new tools in the MicroStation V8 XM Edition. \u201c\nSo what of the new graphics technology in play? Well it is all built around the DirectX technologies from Microsoft. Simply put, DirectX is a Windows technology that enables higher performance in graphics on your PC. At the core of DirectX are its application programming interfaces, or APIs. The APIs act as a kind of bridge for the hardware and the software to communicate. The DirectX APIs give multimedia applications access to the advanced features of high-performance hardware such as 3D graphics acceleration chips. They control low-level functions, including 2D graphics acceleration; support for input devices such as joysticks, keyboards, and mice. Because of DirectX, what you experience with your computer is better 3D graphics.\nWhen I talk about it being a Windows technology I make that reference with two things in mind. First it is a Microsoft technology for the Windows platform and second (and because of this) it is optimised for Windowed applications making use of many concurrent windows or view ports. This specific optimisation means that graphics hungry applications like CAD, BIM and 3D modelling products that frequently have users working through a number of open windows in the application, can process data much faster \u00b1 even for the hidden windows. This DirectX, multi-windowed performance is in stark contrast to older technologies such as Open GL which was only ever optimised for applications working in one active window or view port at one time. An example of old, un-optimised technologies that continue to use Open GL would be Revit.\nThere\u00dds always a risk in changing the interface for users, and here we acknowledge that Bentley has tried to minimise the impacts of technology upgrades on users, having only had one major file change in the product\u00dds history. While there may be a classic interface for \u00d9old\u00dd users to work with, is this 100% of the old GUI (i.e. how are new features supported in the old GUI). Are there migration tools provided with SELECT, and to really get the most out of the new functionality, will users need to migrate to the new style interface? (Am guessing there\u00dds a productivity argument to be made here?).\nJC: I hear you and we understand that changes to software can place the productivity of our users at risk which is precisely why we work as hard as we do to make change a positive thing for all of them. With the V8 XM Edition we ran extensive usability sessions on all aspects of the development which necessarily included at all times the interface. Hence it was tested, tweaked and retested over much iteration to ensure that it was not only easy to use but that it worked as designed.\nI don\u00ddt think that the \u00fdclassic\u00af interface as you put it is a thing for \u00fdold\u00af users to work with thereafter. It is a way of smoothing the transition so that the UI changes made available to the users in the V8 XM can be adopted at the users own pace. The classic interface is also beneficial for organizations that have customized their MicroStation UI \u00b1 they need a route to embrace the new edition without being forced to make wholesale changes to the way they work immediately.\nOf course SELECT users benefit through the many routes to technical assistance and online learning opportunities. All new and renewed SELECT agreements now benefit from two online eLearning courses for MicroStation V8 XM as well as the many technical documents that are available in the online SELECT knowledge base. Furthermore, subscribers to the new online Bentley LEARN Server http:\/\/www.bentley.com\/learn can take advantage of a number of multi-media hints and tips videos.\nBentley LEARN is a subscription companion to Bentley SELECT that gives organizations unlimited access to OnDemand eLearning from the Bentley Institute. Bentley LEARN removes the common barriers to learning such as time, topics, and cost so that organisations can get the training they need to be more productive and to remain competitive.\nCould you highlight the biggest changes in end-user experience with the new GUI?\nJC: I think the biggest changes in end user experience result from the new combination of tools and workflow in an integrated user interface. The patent-pending \u00fdTask\u00af technology is a key innovation in the V8 XM Edition along side \u00fdElement Templates\u00af, a new standards-based innovation for working consistently to create consistent work.\nMicroStation Tasks enable administrators and users to match their workflow to their tools for dramatic productivity gains. A typical series of tasks (with a small \u00fdt\u00af) done in the previous version might have included 20 clicks of a mouse to select the correct attributes, select the right tool from a toolbox before placing the data and then arranging it with the correct draw order etc. With a combination of Tasks and Element Templates the same process could require a fraction of the clicks. And when tied into the new patent-pending Keyboard Position Mapping the click rate is reduced by another order of magnitude.\nThe net benefit is higher productivity with more consistent work done in accordance with project standards. And of course, because these are innovations by MicroStation they are all made available for the vertical applications. Hence the speed of task completion in a BIM application based on MicroStation V8 XM Edition is also greatly enhanced.\nA new interface and improved 3D performance are obviously welcome. Does Bentley think that perhaps MicroStation\u00dds old interface somewhat impaired customers move to 3D? There\u00dds obviously a massive cultural element to this, how does 3D usage differ in the vertical areas in which Bentley operates? Which is the fastest growing adopter of 3D?\nJC: I don\u00ddt think we have experienced any problems regarding users delaying a move to 3D \u00b1 indeed our users have in many cases been advocates of 3D for years. While I understand that many AutoCAD users have a lot of catching up to do MicroStation has for a long time been the 3D choice of architectural practices like the Richard Rogers Partnership and Foster and Partners. While working for RRP in the mid 90s I modelled the Greenwich Millennium Dome in 3D using MicroStation SE and the whole project was designed, modelled and documented using a mixture of 2D and 3D with MicroStation. Furthermore, representatives from Foster and Partners have frequently given presentations detailing how they were able to use MicroStation to model buildings in 3D, in order to derive construction documentation that would have been impossible to conceive and realise any other way.\nBut you are right in that there is a cultural shift required by any organisation that is moving from an entirely 2D workflow to a partial 3D\/2D workflow. This is another prime example of why the Bentley approach is right. Because our MicroStation platform provides the applications in our comprehensive portfolio with many innovations so the transition from a traditional CAD drafting workflow to BIM for example is easier due to interface and environment similarities. In fact it is fair to say that if you use MicroStation or PowerDraft you are already on the ramp to BIM.\nThe benefits of an improved graphics interface and graphics pipeline appear to benefit 3D users. How will 2D customers see the benefits? Do you have any metrics on speed improvements? What does PowerDraft V8 XM gain on the graphics side?\nJC: MicroStation PowerDraft is as its name suggests a derivative of MicroStation, as are Bentley Redline and Bentley View. All are hewn from the same code and all include many of the same innovations that first surface in MicroStation. In fact to be fair, MicroStation PowerDraft is possibly the biggest winner of the V8 XM Edition platform derivatives as it gains all of the new graphics subsystem and user interface updates as well as some of the other new features like Link Sets and PDF reference attachments. Remember also that MicroStation PowerDraft is not just a 2D drafting product \u00b1 it too is a platform for the \u00fdPower series\u00af which includes PowerCivil, PowerSurvey, PowerMap, PowerSite and PowerRebar. As such the MicroStation PowerDraft platform makes excellent use of its new architecture.\nOn the subject of graphics improvements, the key thing about the DirectX technologies is that it enables higher performance in graphics on your PC \u00b1 regardless of the number of Ds! Remember, DirectX also controls low-level functions, including 2D graphics acceleration, so the user wins regardless. In the V8 XM Editions all platforms and applications benefit from measurably faster graphics by 250%+ which has opened the door to the inclusion of gradient fills and element transparency while still accelerating performance.\nWhat\u00dds next? MicroStation creates 2D and solids 3D geometry in a fairly traditional way. There have been a number of innovative applications like Sketchup and even now AutoCAD has learnt a thing or two in this interesting conceptual design area. While Bentley is certainly active in the high-end of geometry creation this with Generative components, but what is Bentley\u00dds opinion of the \u00d9easy-to-use\u00dd conceptual modelling tools, and is it working on similar applications?\nJC: To answer this question we come back to my earlier point made about the market trend towards specialised point solutions. Users are going vertical to increase their productivity and conceptual modelling tools are just one type of vertical application that an architect or engineer will turn to at a given point in their process. Users rightly demand the best tools for any given task and some of our users do indeed use Sketchup for conceptual modelling. Which is why we created our read\/write interface between MicroStation and the Sketchup file format.\nUsers of MicroStation V8 XM Edition and associated applications can now make direct use of the Google 3D Warehouse to share 3D models using the Sketchup file format. Again proving that the greatest innovation by MicroStation is interoperability.\nLooking forward, there\u00dds a lot of buzz about the Microsoft Vista operating system, due out sometime next year. Was XM developed with Vista as the target system? Or was XP the target platform of choice? When people buy new hardware next year it will be difficult to find machines that don\u00ddt come with Vista pre-installed. When will XM be available for Vista? Do you anticipate there be further graphics speed benefits in users moving XM to Vista?\nJC: The MicroStation V8 XM Edition was designed to run on Windows 2000, and Windows XP. However, in parallel we have been testing its performance on Windows Vista and the early reports are excellent. As a Microsoft Gold ISV we work very closely with their development teams for many different projects \u00b1 one recent and very successful example is ProjectWise StartPoint, our entry-level collaboration tool based on Microsoft SharePoint.\nOf course the Vista development is another great example of our joint development efforts and we aim to release a SELECT update early next year with Vista Logo compliance. Based upon current testing and continued optimisation we expect the graphics performance to continue to improve. We are very pleased.\nMicrosoft\u00dds drive behind DirectX, at the cost of OpenGL, has caused a lot of gossip in the graphics world. While OpenGL will see some sort of support in Vista, Direct X is seen by many as the way forward. You too have chosen to support DirectX. What were the arguments that persuaded Bentley to take this route?\nJC: Clearly DirectX is the right technology to implement. DirectX integrates with all modern graphics cards and provides far superior vector and raster video speeds when compared to OpenGL. You only have to look to the gaming industry to see that OpenGL has a limited future. It really is a no-contest. I don\u00ddt think that there is a gaming machine on the planet that is worth having that runs OpenGL.\nIn a philosophical mindset, CAD used to drive a huge graphics market with expensive 2D and then 3D graphics cards and to an extent still does. This now seems to be handed on to the games market, which is pushing the enveloped of 3D worlds and performance. How much attention does Bentley\u00dds graphics development team look at the state-of-the-art in games to feed this through into MicroStation development. We are already seeing Bentley offer collaboration with virtual worlds like Google Earth, what kinds of graphics capabilities (outside the CAD area) are of interest now and could you highlight a few possible \u00d9futures\u00dd for MicroStation\u00dds 3D capabilities?\nJC: Wow. In development here we have people in one of two camps regarding their love for computer games. In the first camp we have developers that would happily play games all evening and then in the second camp we have developers that would happily play games all night! Joking aside, we have paid considerable attention to the way a gamer moves around their immersive gaming world and we have emulated the same ease of movement around 3D models with some new tools in the MicroStation V8 XM Edition. These tools have been implemented to make the transition from the immersive gaming world to the virtual BIM world simple. As far as futures I do not want to share too much with you right now, there is a lot to take in already. Perhaps in another month or two I will have some news for you of further innovations by MicroStation. Let\u00dds reconnect then!","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/collaboration\/asite-report-explores-opportunities-of-digital-engineering\/","title":"Asite report explores opportunities of digital engineering","date":1597622400000,"text":"Report examines digital engineering and provides recommendations for global advancement\nAdvanced building materials, smart buildings, BIM, digital twins, and modern methods of construction are amongst the more transformative technologies set to shape the construction industry, according to a new research report from Asite.\nThe report entitled \u2018Digital Engineering: Optimizing Construction\u2019s Digital Future\u2019, examines digital engineering from a number of different standpoints, including the benefits of digital engineering across the project lifecycle, innovative projects including that are leading the way, the technologies set to be the most transformative, and global government initiatives to push the adoption of digital engineering techniques.\n\u201cTo help enhance digital adoption and ensure the future prosperity of our industry, this report provides recommendations for how we can optimize our use of digital engineering processes and create a more integrated and collaborative industry,\u201d says Nathan Doughty, Asite CEO. \u201cAs our industry continues to recover and rebuild in the wake of COVID-19, now is the right time to take the opportunity to build a digital ecosystem that works for everyone.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-advanced-solutions-to-support-lifecycle-bim\/","title":"NEWS: Advanced Solutions to support \u2018Lifecycle BIM\u2019","date":1416960000000,"text":"US firm brings in partners to provide a \u2018Comprehensive Solution\u2019 for Facilities Management\nAdvanced Solutions is looking to help firms streamline the flow of information and provide better data to building owners and operators for improved Facilities Management.\nThe US-based Autodesk Platinum Partner has partnered with Codebook International, Brockwell IT Consulting, and Ecodomus to deliver what it calls Lifecycle BIM \u2013 moving BIM beyond the planning and design phase of a project, with the help of GIS, data management, and facility management software.\n\u201cAdvanced Solutions prides itself on our innovative software solutions and high quality consulting services,\u201d says Michael Golway, Advanced Solutions President and CEO. \u201cLifecycle BIM offers a key value proposition and service to owners, general contractors and all consultants in a project.\u201d\nAndy Hamer, CodeBook International CEO, added, \u201cThrough Advanced Solutions, CodeBook will now be expanding its professional collaboration on key projects across the United States.\u201dThe CodeBook Data Management application manages building information from project inception through to facility management after project completion. Working with industry standard CAD and BIM tools, CodeBook links to models, shares data, and gives rapid, detailed reports and validations throughout the design and construction phases of a project. On completion, the building owner takes possession of a detailed, room-by-room, information database linked to the 3D building model.\nBrockwell IT Consulting specialises in geospatial asset management and design systems for utilities, providing solutions for infrastructure design, planning and management, as well as geospatial design. President and CTO of Brockwell IT Consulting, Stephen Brockwell stated, \u201cTheir [Advanced Solutions] Lifecycle BIM approach provides customers with seamless integration of all the necessary data inside the building and the critical infrastructure outside it. Brockwell IT Consulting aims to improve the ability to serve our clients and theirs.\u201d\nAnother partner in the Lifecycle BIM cache, Ecodomus software will provide a 3D view of facilities in what is described as an easy-to-use format for facility managers that links BIM with real-time facility operations data acquired via meters & sensors (Building Automation Systems, BAS) and facility management (FM) software.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/the-connected-future\/","title":"The connected future","date":1537488000000,"text":"George Broadbent and Dr. Xifan Jeff Chen of global AECO technology consultancy Microdesk explore the potential for connected operations in asset management, harnessing the power of BIM, digital twins, IoT and deep learning\nAsset Management essentially is a cost-effective, systematic process of managing assets. It is however, this promise of cost management that is creating market pressure for owners and facility operators to implement an Internet of Things (IoT)- based solution. Connected Operations, which is designed to improve the operational efficiency of physical assets, serves as a critical component to complete the root definition of Asset Management.\nConnected Operations is developing from a compelling concept used in manufacturing to an AECO industry-wide process. The benefits of Connected Operations implementation include enhanced interoperability efficiency, faster implementation speed, and optimised operation safety. It is triggering the development of several up and downstream sectors, like risk management, supply chain management, and manufacturing services. Connected Operations is stimulating the development and integration of state-of-art technologies such as IoT, Artificial Intelligence, cloud computing, Machine\/Deep Learning, big data, and Augmented Reality into the Asset Management process.\nConnected operations advances\nThe rapid growth of Connected Operations is a result of both internal and external factors. The AECO industry has been craving new technologies and solutions to increase operational efficiency and enhance asset performance. IoT-led data-driven technologies have sprung up to enable real-time visibility for organisations to overcome these challenges. Cost reduction is another critical motivator that is pushing Connected Operations forward.\nThe introduction of IoT plays a significant role in redefining the term \u201cconnected\u201d in this process. More controllable operational reliability ranks at the top of the capabilities of IoT-enabled Connected Asset Management. Although it is still mission impossible to fully predict operational reliability due to any number of uncertainties, industry as a whole is getting one step closer to utilising the power of IoT-driven data analysis.\nSensor-equipped connected assets automatically log machine performance and operation status. This data is recorded and analysed via a Computerised Maintenance Management System (CMMS) in combination with cloud-based AI systems to make predictions on system performance. When this is compared to the traditional fixed maintenance schedule (i.e. calendar based), IoT-enabled preventive maintenance can lower operations and maintenance (O&M) costs significantly, while taking maximum advantages of human resources. IoT \u201cweaponises\u201d the existing Connected Operations with a state-of-art data-driven core, to diminish the boundary between tangible assets and their intangible digital constructs. In addition to scheduling preventive maintenance, data collected from connected assets can be used to perform a failure analysis targeting the root cause of unplanned downtime to improve performance in the future. Big data, the idea that the world is replete with more information than ever, is now gradually reshaping existing asset management from passive to a proactive and more sustainable mode.\nData leads to major changes\nThe world is converging due to the trends of merging of technologies withinand between-industries that have been changing the way people work, communicate, and interact with the physical world. Data-driven Connected Operations reveals enormous possibilities to collaborate with other state-of-art technologies. The Artificial Neural Network (ANN) based deterioration model has been widely used in failure analysis, while emergent Artificial Intelligence (AI) became a stronger challenger because it offers more accurate schedule predictive and proactive maintenance of assets. With the availability of big data from connected assets, machine learning can be utilised to detect patterns hidden in collected data to produce actionable insights to increase the accuracy of prediction and benefit decision making.\nLooking into the upstream industry, Building Information Modelling (BIM), instead of being described as a threedimensional model, is a pure data presentation of built assets. The nature of datadriven decisions makes BIM and Connected Operations highly compatible and complementary. While owners and operators were struggling to find a solution to reduce interoperability inefficiency, the BIM and CMMS integrated process emerged from obscurity as an organic workflow. They carry data over through planning, design, construction, commissioning, and into the operations and maintenance stages.\nThe National Institute of Standards and Technology (NIST) examined US capital facilities and determined that organisations spend $0.23 per square foot annually due to interoperability inefficiency. A well-established as-built BIM model or digital twin that captures as asset information post construction and maintains data consistency and integrity could fundamentally reduce this cost. A BIM and CMMS integrated implementation could follow up to minimise the remaining expenses of interoperability inefficiency.\nAligning technical challenges\nAlthough IoT and other Connected Operations related strategies and technologies carry tremendous potential to reshape how asset management is utilised for facilities, it is not free from challenges. All stakeholders involved in the Connected Operations process must work together to understand the methodology to maximise the efficacy of the solution. This requires coordination between facility operations, IT and business leaders. The relatively steep learning curve determines the degree of implementation and further affects the system performance.\nFrom another point of view, this emerging topic draws attention to the complexity of maintaining the consistency, integrity, and interoperability of data collected from all sources. It became even more complicated when dealing with roll-in and rollout of new data formats and old data formats, as well as several different data formats. Potential scenarios include having the ability to merge BIM data with existing two-dimensional records in CMMS or trying to relate data inputs from an upstream BIM as-constructed model with collected IoT data in a bidirectional way.\nFrom a financial perspective, Connected Operations may lead to relatively high capital costs for system implementation, sensor and monitor installation, as-built model and data collection, and labour training. Despite the Life Cycle Benefit\/Cost Analysis of Connected Operations exhibits promising a productive Benefit\/Cost Ratio (BCR), ROI and Payback Period, it is still a key factor keeping clients from understanding the system at the very beginning. Besides, policy, execution, adaptability, and other external influences that also indirectly positively or negatively impact system implementation, it is becoming an accepted method to implement Connected Operations from the Enterprise Level. Once applied, IoT-enabled Connected Enterprise Asset Management (EAM) provides a robust environment for achieving exceptional value from assets.\nTechnology of the Future\nConnected Operations is paving a highway to the future with the data of today as the roadbed. The advent of IoTenabled Connected Operations is driven by the industry pressures for operational efficiency enhancement, and the graduation of IoT technology from academia to the industry.\nIoT-enabled Connected Operations does everything that traditional solutions do while adding intelligence to make existing workflows adaptive and proactive through real-time alerts, short response times, and predictive maintenance. It also dramatically expands the scope of traditional asset management to embrace state-of-art technologies inside and outside the AECO field.\nLooking ahead, the future development of AI-based cognitive computing and Machine\/Deep Learning offers the real promise of more accurate schedule predictive and proactive maintenance of assets. The scope of Connected Operations has been expanded to embrace the data flow from design stage to supply chain. As the rapid deployment of BIM in the AECO industry continues, the integration of BIM and CMMS will be better connected by a unified code and standardised constructionasset data flow. This will further accelerate data exposure and benefit data usage in the asset management stage.\nThe Enterprise Asset Management sector has been described as \u201cthe single, biggest controllable expenditure on the planet.\u201d IoT-enabled, data-driven Connected Operations is fundamentally changing the way that traditional asset management has been utilised for generations. With this new vision, Asset Management, the systematic process of developing, operating, maintaining, upgrading, and disposing of assets costeffectively, has been pushed to the forefront of the trend, and will most likely hold that position for a very long time.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/bim-by-acca\/","title":"BIM by ACCA","date":1590537600000,"text":"It\u2019s not every day you find a new BIM developer, especially one that has a mature and capable software suite addressing nearly all aspects of digital building design and construction. Welcome to the world of Italy-based ACCA\nACCA\u2019s cloudbased CDE \u2018usBIM. platform\u2019 can collate project information in a spatially referenced master model based on IFCWe all know the main play ers in BIM and digital construction \u2013 Autodesk, Nemetschek, Trimble andBentley Systems. While we have always hoped that there would be new start-ups and fresh approaches within the authoring tool market, it came as a shock to be introduced to a firm, based in Italy, that had, for a number of decades, been developing professional applications for the European market.\nBased in Bagnoli Irpino, Campania, ACCA is about an hour\u2019s drive inland from Naples airport. Situated within the Picentini Mountains, the company\u2019s huge solar and wind powered headquarter building (built in 2017) is a living testament to its own design software and looks out onto the idyllic mountain-encircled vista, across the valley.\nThe headquarters houses all development, sales and marketing, together with training and an auditorium. There is also a \u2018living\u2019 museum for digital design tools of the past, which the company had used, including some very early 8086 computers, Apple Macs, a ZX Spectrum, some \u2018luggable\u2019 laptops and my favourite, which was a CalComp Summasketch II digitiser, which I used many, many moons ago with AutoCAD Release 10.\nACCA was founded in 1989 headed up by Guido Cianciulli, as a private business which wrote its own MS-DOS, Windows and Mac OS applications for Bill of Quantities and Cost Estimating. The first commercial product, called PriMus, went on to dominate the Italian market. Based on this success, ACCA went on to develop analysis tools for building models in 1996, health and safety certification, structural and engineering calculators. In 2010 it finally launched its own BIM tool and Revit competitor, called Edificius, which integrates with ACCA\u2019s other construction products.\nAdherence to open standards is an underlying development philosophy, with IFC being a key feature within all products. The company is an active member of BuildingSmart, winning accolades for the quality of its IFC output. ACCA is proud that it has the highest number of IFC certified BIM licences on the market today.\nIn many respects my visit to ACCA reminded me of my first trips to see Graphisoft and its founder G\u00e1bor Boj\u00e1r. ACCA was also an early pioneer of identifying the benefits of computing to the building industry in the 1980s. Both firms have spent decades building software companies. And because of that, have proud heritages in changing the industry. ACCA and Graphisoft also share strong beliefs in IFC and the OpenBIM movement at key industry fundamentals.\nSoftware development\nA casual browsing of ACCA\u2019s website highlights just how wide the span of software development has been: BIM modelling, standalone BIM viewing, clash detection, landscape and terrain modelling, MEP, structural, concrete, steel, masonry, CDE tools, photovoltaic design, thermal bridge analysis, cost estimating, quantity take off, construction planning and time sequencing, rendering (photorealistic and real time) and VR. ACCA even has a tool for scaffolding design. The company actually offers over 90 applications.\nPrimarily the firm has concentrated on the Italian and Spanish markets which is is probably why ACCA is not so well known in Northern Europe, despite having English versions of its Windowsbased applications. However, it\u2019s clear that the company has been developing the right tools, and it has certainly thrived in a highly competitive market, especially against the international reach and dominance of firms like of Autodesk with Revit.\nBIM\nBefore even discussing the capabilities of the company\u2019s core BIM tool, the first thing to note is the cost. A full seat is just \u20ac599 a year, only a tad more than a yearly subscription for AutoCAD LT. And all that for something that is on a par with Revit in terms of functionality. This price point is obviously a key advantage for anyone looking to get into BIM or to change modeller.\nNow, I will admit that, for the uninitiated, the naming conventions of ACCA do take some getting used to. Its core BIM tool is called Edificius. In general the company has a lot of brands with \u2018us\u2019 in the title, but more on this later. The software is straightforward and uses the Windows ribbon interface.\nFirst set up a project and then simply start drawing in plan or in 3D with intelligent walls, doors, windows, with all the multi-representational variations that you can accept or define yourself.\nThere are plenty of intelligent cursorbased tools to ensure modelling and geometry is aligned and dimensioned correctly. This thing is really fast too, either in modelling or bringing through a 2D drawing and quickly turning it into intelligent components.\nYou get real time plan, sections, elevations and isometric views, to create all the construction documents you need and, as you would expect, they are all linked for editing and automatic updates.\nEdificius is a truly multidisciplinary tool with steelwork and MEP, as well as landscape all in the same environment. The other multidisciplinary applications (Land, MEP, etc.) all share the same base platform and interface.\nAt this price point, it\u2019s a bit of a gem and has been industry tested on residential all the way up to train stations.\nThe software also has a built-in photorealistic renderer and can produce solar studies and high quality stills for clients. The development team has recently upgraded the rendering engine to utilise AMD\u2019s Radeon ProRender technology, which is a physically-based materials renderer which makes the most of underlying GPU, CPU and memory performance using OpenCL. The quality of the interior and exterior renders are superb. And, according to the ACCA development team, images are delivered an order of magnitude quicker than the previous rendering technology, especially on larger scenes.\nI was impressed specifically with the quality of the interior images and the included firms photomatch software is a simple way to place the proposed building in the context of a site photograph.\nIf that\u2019s not enough, this also expands out to VR, which is being built into the Edificius stack, enabling all designs to be VR ready. The company has developed its own interface and construction-ready VR system for designers and clients to navigate through models.\nCommon Data Environments\nOne of the new areas of development for ACCA is the growing need for easy-touse and fast Common Data Environments (CDE) for collaboration. ACCA believes there\u2019s a lot of potential here to use its IFC knowledge and modelcentric approach to connect teams throughout the lifecycle of a building.\nCalled the \u2018usBIM.platform\u2019, the cloudbased application is capable of collating all the project information in a spatially referenced master model based on the IFC open standard.\nThe customisable system is designed to display huge models with a real-time display on desktop or mobile. It includes a built-in gate management capability for handling and issuing Work In Progress (WIP), Shared, Published and Archived data in accordance with ISO 19650. The system also supports data and documents related to the project which can be structured and linked to the model.\nusBIM.platform also integrates with ACCA\u2019s PriMus software which is used for managing construction timelines. In fact, this CDE could be used throughout the design and construction phases and then handed on to the owner after completion for asset management or Digital Twin usage.\nBased on the technology there is a free IFC viewing application, called usBIM. viewer+. It actually does a fair bit more than just view, it can federate multiple models, edit, modify and convert to other formats.\nConclusion\nHaving written about CAD and BIM for decades one just doesn\u2019t expect to find a \u2018new company\u2019 that has been around for almost as long \u2013 and one that has thrived, served on thousands of projects and developed a broad set of advanced building design and construction tools.\nFrom both the scenery, the software and the pricing, it was a bit like stumbling on an Italian BIM Shangri-La.\nIt\u2019s impossible to do the many developments justice in the space allocation of an article. It\u2019s certainly worth some time looking at Edificius and the usBIM.platform, as well as downloading the free IFC viewer. When this whole pandemic is over, I am looking forward to a return visit.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/groupbc-to-provide-cde-for-canada-water-masterplan\/","title":"GroupBC to provide CDE for Canada Water Masterplan","date":1581379200000,"text":"BC Enterprise+ will provide \u2018360-degree view of the entire project\u2019s assets\u2019 pre, during and post construction\nReal estate investment company, British Land, will be partnering with asset information management provider GroupBC and digital transformation expert Concentre Consulting as part of plans to deliver a \u00a33bn new town centre at Canada Water in London.\nThe 53-acre project, which is anticipated to take around 15 years to deliver, will see GroupBC implement its BC Enterprise+ common data environment (CDE) solution. According to GroupBC, this will enable a Digital Twin of the entire Canada Water Masterplan as well as using enriched 3D models and mapping information. The solution\u2019s visual map interface will situate all built assets into its surrounding environment, with the aim of providing a holistic, thorough view of the entire project.\nThe masterplan will deliver around two million sq. ft. of workspace to accommodate around 20,000 jobs, one million sq. ft. of retail, leisure, entertainment, education and community space and around 3,000 new homes, of which 35% will be affordable. Wellbeing will also be embedded into plan, with large green areas, a new town square and a high street.\n\u201cThis solution will offer British Land unlimited user licenses, enabling users to have a 360-degree view of the entire project\u2019s assets pre, during and post construction, assuring quality and streamlining execution,\u201d said Sanjeev Shah, Partner and Strategy and International Development Director at GroupBC. \u201cWe are delighted to be working with British Land, on what will be a momentous project that will truly make a difference to this area of south-east London.\u201d\nAccording to GroupBC, the CDE is aligned with BIM standards PAS 1992 and ISO 19650 and will enforce document naming conventions to assure consistency of information. Audit trails will also be retained to track the \u2018golden thread\u2019 of accountability to help assure quality building design and performance, and information accessibility. On handover, the system will allow British Land to monitor and maintain ongoing operations of all built assets across the Canada Water scheme.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/sponsored-content\/global-cities-innovate-with-3d-modeling\/","title":"Global Cities Innovate with 3D Modelling","date":1570579200000,"text":"Leaders use realistic cityscapes to make decisions, collaborate, and improve quality of life [sponsored content]\nLeaders use realistic cityscapes to make decisions, collaborate, and improve quality of life [sponsored content]\nWhen smart cities need to make planning, infrastructure, and services decisions, they increasingly turn to virtual cityscapes to model scenarios and understand impacts. London, Helsinki, Boston, and Singapore are among those leading the way.\n3D city models provide realistic renderings of existing buildings (inside and out), transportation infrastructure, and green space as well as proposed projects. With 3D models, it\u2019s easier to see how a new building, solar panel installation, or bridge would impact an area. Once a plan is proposed and integrated into the model, it becomes an effective tool for communication with partners and the public. Beyond planning, these models are invaluable for situational awareness and emergency response.\nGeographic information system (GIS) technology has been the primary means for developing these realistic models. Unlike physical models or two-dimensional drawings, governments can easily modify 3D cityscapes to reflect new buildings, new materials, and even changing landscapes.\nIn Boston, the Boston Planning & Development Agency uses its 3D model for flood modelling, line-of sight evaluations, and shadow studies that guide development near the historic Boston Common.\nWith a law in Massachusetts restricting the amount of time any building can cast shadows on Boston Common, the city needs detailed analytics on how proposed development would affect the nation\u2019s oldest park. Replacing its wooden physical model with a 3D digital model, gave planners access to details they could not visualise before. They can clearly see how new zoning and development could lead to increased shadows on the park at various times of the year when sun angle and daylight hours vary.\nCity leaders quickly realised how having up-to-date, detailed images of Boston\u2019s buildings and assets significantly benefited collaboration among stakeholders including city staff and developers. It also put data at the center of critical decision-making and negotiations. Now they want to extend the benefits to others. \u201cWe want to make the smart 3D model user friendly and widely available to whomever for whatever,\u201d said Corey Zehngebot, senior architect and urban designer at BPDA. \u201cThe sky\u2019s the limit in terms of what we might be able to use it for in the future.\u201d\nCollaboration Between Cities\nThe trend toward 3D modelling is gaining traction across the globe. In February, the cities of London and Helsinki announced they are working together in a \u2018City to City Digital Declaration,\u2019 sharing best practices on 3D city modelling among other tech-related initiatives.\n\u201cThis Digital Declaration sets out a formal framework for co-operation between Helsinki and London to develop our respective in-house expertise and links with the tech sector to use city data to improve the lives of our citizens,\u201d said London\u2019s Chief Digital Officer, Theo Blackwell. Helsinki\u2019s Chief Digital Officer, Mikko Rusama, added, \u201cHelsinki\u2019s vision is to be the most functional city in the world that makes the best use of digitalization.\u201d\nAs the leaders of these two tech-savvy cities are keenly aware, the visual nature of maps simplifies communication about challenges, existing situations, and possibilities. 3D models give smart cities an effective way to collaborate internally, with the public, and with other governments.\nCreating a Digital Twin\nMoving farther east, the global powerhouse city of Singapore is also making significant investments in 3D city models for broad use in-house and with citizens. Singapore has invested millions of dollars in its 3D model that will be used by government agencies and the greater public. The Prime Minister\u2019s Office, National Research Foundation, Singapore Land Authority, and Government Technology Agency of Singapore are all backing the project. The 3D model will provide an effective way to test ideas, plan, and address local challenges associated with growth and other pressing issues.\nThe information in Singapore\u2019s model goes well beyond structural data. Singapore will include vast amounts of terrain, demographic, and climate data combined with real-time sensor data. Visualizing building, the natural environment, and the movement of people will help the government determine where to invest in new infrastructure and effectively deliver services. It will benefit everything from planning to disaster response.\nThough these cities are investing millions in their efforts, creating 3D models is within reach for many cities now that the cost of capturing imagery is shrinking with the widespread availability of drones and other forms of rapid imagery capture inside and outside of buildings. The 3D city models combined with comprehensive strategies on how to use them\u2014as we see with London, Helsinki, Boston, and Singapore\u2014will help leaders move smart growth plans forward and enhance quality of life for citizens.\nTo get an inside look at projects already happening in New York City and around the world go to: go.esri.com\/seeing\/aecmag\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/construction\/bentley-systems-acquires-civil-construction-specialist-e7\/","title":"Bentley Systems acquires civil construction specialist E7","date":1614643200000,"text":"Acquisition will build on capabilities of Synchro to enable 4D construction digital twins\nBentley Systems has acquired E7, an Australia-based specialist in construction delivery software for heavy civil projects.\nAccording to the company, the acquisition will add capabilities to its project delivery solution that helps civil contractors meet the schedule and budget requirements of infrastructure investment programs.\nSpecifically, it will extend the capabilities of Bentley\u2019s digital construction management software, Synchro, to create what Bentley describes as a \u2018comprehensive 4D construction digital twin solution\u2019.\n\u201cVisibility into field resource utilisation is key to project profitability,\u201d said Dustin Parkman, vice president, project delivery, Bentley Systems. \u201cThe addition of E7 to Bentley\u2019s Synchro digital construction delivery solution complements our industry-leading 4D construction modelling with field-based resource management tools that are essential for heavy civil contractors.\n\u201cThis new combination makes possible a truly comprehensive digital construction delivery solution for heavy civil contractors everywhere.\u201d\nE7 is designed to help firms better utilise resources on site through mobile and web interfaces that transform manual, site-based tasks into digital workflows.\nIt includes capabilities specifically designed for heavy civil construction, including daily diaries, unplanned event tracking, timesheets, dockets, daily costs, and quantity progress measurement. which enable supervisors to produce daily progress measurement reports.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-topcon-extends-reach-of-total-stations\/","title":"NEWS: Topcon extends reach of total stations","date":1488844800000,"text":"Topcon GT Series \u2018seamlessly connects\u2019 with Autodesk BIM 360 Layout software\nTopcon\u2019s GT series of robotic total stations can now be used with Autodesk BIM 360 Layout software to help operators connect coordinated models to the field layout process.\n\u201cThe GT series is the fastest robotic motor total station in the market with turning speeds of up to 180 degrees per second \u2014 and now the system is even more powerful with the added BIM 360 Layout seamless functionality,\u201d said Ray Kerwin, director of global surveying products.\nThe integration marks the latest in a series of collaborations between Autodesk and Topcon of total station solutions for contractors including the LN-100 Layout Navigator and the DS-200i imaging station.\nThe BIM 360 Layout app is available on the Apple App Store.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/beyond-revit-autodesk-seeks-to-reinvent-collaborative-bim\/","title":"What comes after Revit? Autodesk aims to reinvent collaborative BIM","date":1559520000000,"text":"In 2016, Autodesk announced Project Quantum, described as a platform technology for \u201cevolving the way BIM works, in the era of the cloud, by providing a common data environment\u201d. The project then went dark but now it\u2019s back and called Project Plasma\nAt Autodesk University in 2016, the then senior vice president of products at Autodesk, Amar Hanspal, gave the AEC keynote and slightly opened the kimono on a new technology the company was developing to tackle crippling issues the industry faces because of its long history of working in a federated way \u2013 replicating poor collaboration between firms in the digital realm, leaving some of the benefits of BIM on the table.\nBefore this keynoter there was considerable debate internally at Autodesk, as to if Quantum should have been exposed so early in its development. However, the backdrop to this was that many mature Revit customers were asking: what was next for Revit and where was development going?\nAutodesk\u2019s best Revit customers were concerned at the lack of updates to the core application and the move to \u2018Suites\u2019, and subscription had seemingly dissipated development to incremental updates across a wide number of applications, when the heart of most firms\u2019 BIM efforts centred around Revit and collaborative workflows.\nRevit is over 20 years old. While it has seen significant re-engineering, the core element is still limited to running on a single CPU core, the database quickly swells in size and detail and suffers from an old graphics pipeline that is hard to accelerate in an increasingly GPU rich world.\nAs Autodesk has moved to the cloud in a major way, the desktop applications really need to store the data on the cloud, in BIM 360, to benefit from Autodesk\u2019s increasing array of cloud services, such as document management, analysis, collaboration, as well as a growing number of cloud-based applications from third-party developers.\nAutodesk\u2019s products also have a long history of not being able to share data as well as you would expect coming from the same company. And, looking ahead to a world of digital fabrication, there are fundamental problems with BIM tools optimised to produce co-ordinated, symbolic 2D drawings, as opposed to driving CNC machines and robots, which require 1 to 1 modelling.\nQuantum, as it was pitched, was an elegant solution to many of these challenges in moving forward to the next generation of BIM. You had to admire the decision to basically relook at the whole industry, how it works (or doesn\u2019t work) and to realise that yet another monolithic application is really not going to map to the current federated industry workflows that silo data, are eminently file-based and ultimately damage the flow of data.\nAutodesk had decided to look at a data-centric approach that could include current workflows, alleviate some of the pain by providing headroom to Revit, connect teams and address the growing use of multiple applications in designers\u2019 tool sets.\nDigital Fabrication is coming to AEC and not just at the high-end. Factories are being built everywhere to prepare to modularise, pre-fabricate and utilise automated digital fabrication methods. BIM data, at 1:100 or 1:50 cannot drive this, not without being remodelled in a Mechanical Computer Aided Design (MCAD) application such as Inventor or Solidworks.\nBy adding high levels of detail to BIM models, databases swell-up and they quickly become unmanageable. Again, with Quantum, Autodesk\u2019s solution introduced a novel approach, where BIM models would \u2018hand-off\u2019 at set interface points components that needed to be manufactured by better adapted CAD systems. This meant that different professionals in the workflow could all have different versions of the same model, but they were connected by a common platform. What\u2019s more, live geometry could be pumped around the system in real time for teams to see the model in various levels of detail.\nThis was indeed a brave new world and it seemingly was a very slick way of introducing a change in the way BIM would touch every player in a federated AEC project. Unfortunately, company politics intervened, Hanspal left Autodesk after the company\u2019s board chose Andrew Anaganost as new CEO, and news on Quantum went dark.\nRoll forward to Autodesk University 2018 and in one of those corridor conversations that happen by happenstance, AEC Magazine learned that Quantum had indeed survived but had actually been a victim of its own success. The technology was deemed to be so useful that the company decided to take a broader view of its potential to all products and verticals (e.g. AEC, manufacturing) and so paused to take in more internal stakeholders into its development as a platform technology. The net result is Project Plasma.\nAwe inspiring\nEarlier this year, AEC Magazine had the opportunity to talk with Autodesk\u2019s chief software architect, Jim Awe, about the name change and the company\u2019s vision for Plasma and its capabilities.\nAwe explained, \u201cWhat happened was, the idea of quantum, and doing automated workflows in a trusted way gained momentum. As we talked to customers, they definitely supported that idea and that was crucial to moving project collaboration forward. Then when we started to talk to other people around the company, we found that the manufacturing group were facing the exact same problem.\n\u201cAutodesk has traditionally been a design-based company and the goal of most of the products like Revit were to produce construction documents and then \u2018throw them over the wall\u2019 and have someone figure out how to make it. It turns out, manufacturing had the same issue, that they design everything upfront and then had to figure out which tools on the factory floor were going to make different parts of the assembly. They did the same thing and would break up the model, divide it up between multiple people who would figure out how to make their part in the process.\n\u201cThis is a strategic shift. It\u2019s not good enough to just design something, you have to be able to make it and in order to do that, you need a workflow that goes throughout the project life-cycle. This technology [Plasma] should be in the platform and so it became a much bigger, more elaborate, effort. We are taking our time to get it right because it\u2019s so important.\u201d\nWe asked Awe, how the platform worked, \u201cThe best analogy which describes what we\u2019re trying to do here, is if you look at what Apple did with iOS. Apple has a platform, where you can build an App and plug into well-known services of the iOS system and then construct a mobile workflow. The App hands off the GPS location to maps, and photos are integrated into other workflows, all on your mobile device. We need to integrate enough of the pieces so that when you try and move data from, say Revit to a fabrication phase, you are moving the appropriate amount of data and the person on the other end knows where to find it and absorb it into their workflow.\n\u201cCarrying on with the Apple iOS analogy, we will definitely build some of our own applications plugged into this and we fully expect some of our Forge partners to hook up into Plamsa in lots of interesting ways.\n\u201cAll the professional and IP boundaries are maintained with the data that is flowing between collaborators; it\u2019s tracked, it\u2019s scoped, so you\u2019re not just sending the entire model across. In most instances it\u2019s more likely to be a subset of it.\u201d\nAutodesk is calling this data exchange a form of a \u2018data contract\u2019, so users decide what limited data they want to share with each project participant and the system tracks and approves the exchange, as it goes through a series of \u2018gates\u2019. This means control is maintained by each of the originators in a project, sharing data which can\u2019t be edited, so while the structural engineer can see the architectural elements, she can\u2019t change components that the architect is responsible for and vice versa.\nData Contracts and Escrow\nWhile Quantum went through some changes in the quiet period, Autodesk has come back with more detail of the mechanics and concepts of how it works. In many respects, it\u2019s a combination of common data environment (some Autodeskers call this a Unified Data Environment) with transactional intelligence that is perhaps akin to digital currencies or banking systems.\nThe two core concepts in Plasma, are the Data Contract, and Escrow. A Data Contract could be defined as packaging up data for the curtain wall fabricator, such as the gridlines and some other relevant information about the curtain wall, but not sending the rest of the model. Escrow is the neutral place through which you pass all your data, tracking all the exchanges.\nThe other piece of the puzzle are the necessary plug-ins to different applications. These plug-ins know how to push and pull data from the applications as defined by user defined Data Contracts. For instance, Revit will have a plug-in to pull data out of Revit and to receive data back from other project participants.\nAutodesk will create these for all of its own relevant applications, but it will also make some for the most used non-Autodesk products and will provide toolkits for any developer to enable Plasma transactions.\nThis is very similar to how financial systems work and we wondered if Autodesk was looking at using blockchain in this ecosystem? Awe replied, \u201cWe have discussed blockchain, and the Escrow part of the process could definitely use blockchain, but we have not decided to do that just yet. It may be overkill for what we\u2019re trying to achieve with Plasma.\n\u201cThere are new emerging technologies that may do what blockchain does in a more straightforward and lightweight way, such as Amazon, which just announced what they call the Ledger Database, which basically behaves like Blockchain. We are currently exploring how to secure the legal part of the system.\n\u201cIf you look at the progression of how were trying to deliver this, the theory is the legal part. The implementation, so far, has focused on the data interoperability piece, which is what and how do you get the data automated in and out of the application. How do you construct a workflow that will spin up all the right compute nodes and send the proper notifications? The mechanism is now in place but we have not figured out the legal side as to how these transactions don\u2019t get tampered with. We also have to work out what legal exposure Autodesk has in providing this service!\n\u201cThis is more than just technology, it\u2019s Autodesk looking at who is going to be responsible for that Escrow. There are a couple of options; Autodesk could decide to take responsibility, or maybe create a spin-off entity, or we could put the onus on the owner, or concede it to something like blockchain and have a technology solution, where nobody needs to be responsible as a technology is taking care of it. But for right now the mechanisms we have in place will work for any of those scenarios, but we haven\u2019t decided yet how to administer it. That can come a little bit later as for now, we need to make sure that in our tests, the data flows in a reliable, automated way.\u201d\nAutodesk is currently trialing Plasma with a lot of internal prototypes, mostly with Revit and other internal Autodesk products, such as Civil3D, Inventor and Fusion. Perhaps, oddly the development team is especially excited with the potential of links to Excel, where they are finding a lot of potential workflows because users can do workflow authoring, as opposed to workflows authored by developers.\nMost workflows for Plasma will require programs, either applications developed by Autodesk, or third-party developers, but end users can develop their own tools in products such as Excel and Dynamo which can process extracted data and export it out via a Data Contract or vice versa. Autodesk has developed a number of internal examples where they prototyped a little bit of logic that lives outside of Revit.\nQuality issues\nWith data being submitted by federated users, the issue of quality management and standards across the system might be a concern. We asked Awe how quality checking of model data could be achieved . Awe responded, \u201cThat\u2019s where the contract helps, because now you\u2019re asking for specific data to adhere to that contract. We tested the contract system on a simple problem, wall framing boundaries, the framing for interior walls. The customer we were working with said they sometimes receive Revit designs which might have a single wall component, rising up seven storeys! The architect obviously thought that was the easiest way of modelling it for his version of the truth, but it makes no sense for those making interior walls from Revit models.\n\u201cIn a wall framing boundary-based contract, it would state you are not allowed to produce a seven-storey wall, you have to deliver it chunked-up in an expected way so that the structural application that is receiving the data can further process it into manufacturable panels.\n\u201cWe call these checking mechanisms validators, where any unexpected data coming through the gate will raise a flag. Another example would be, if I\u2019m working on fabricating curtain wall panels, I wouldn\u2019t expect those panels to be below ground, so maybe there is a rule that checks that those incoming panels don\u2019t have a negative elevation and that they match the floor-to-floor heights of the building.\n\u201cWe think this is going to be similar to how we build software development pipelines, basically running regression tests and other processes when design changes are submitted, and eliminating human error that is introduced when people have to manually process or re-model that data. Once it is initially set up, it would all be automated and the validators and other processes kick in each time updated design data passes through a gate.\n\u201cInitially we will seed template libraries with contracts for our own workflows, such as between Revit and Civil 3D. But, there will always be unique workflows where the customer has to do it. Initially we expect there will be enterprising, tech savvy customers who are doing this and we hope that they will contribute some of their contracts to the community, and over time hopefully the community will promote them to be de facto standards. Also, third-party developers are likely to come up with workflows of their own.\u201d\nAsynchronous vs Synchronous\nPlasma can work in two ways, asynchronous workflows and synchronous workflows. One is on user demand, the other is truly dynamic.\nThe asynchronous methodology is more akin to current workflows, just without all the horrendous data wrangling. With the plasma architecture connecting applications in workflows via plug-ins, these apps actually know absolutely nothing about each other, they only know how to read the Data Contracts through the Escrow system. Designers are editing their models independently of each other until they decide to push it through the Escrow service. Designers get notified when there is a change, and the user simply opens the gate to allow that change to come into their work environment. Users have complete control to open that gate or not.\nIn a synchronous workflow, the opposite is true, and the gate is kept open and the designer sees live updates in their workspace from project participants who have contracted to share work parcels. In demos, this seems like an amazing capability, but it could be too dynamic for its own good! Humans will probably need to embrace a different work methodology for real time collaborative workspaces. The good news is that users can flip at will between asynchronous and synchronous work states. Awe agrees, \u201cMy guess is that most workflows that cross an app boundary or cross a discipline boundary, will choose an asynchronous workflow. And users will only react when you reach certain milestones.\u201d\nWhile users don\u2019t have to give all of their geometry in the process, it should be noted that once geometry is shared in Escrow, there is a permanent record of it and you can\u2019t take it back.\nSo, it seems that Plasma is transactional or live but offers incredible fluidity to the design process with increased granularisation and a control mechanism. In fact, Plasma is a living breathing view of a model\u2019s development. It\u2019s possible to tag the Data Contract with a status such as \u2018work in progress\u2019 or a Milestone but also allow designers to exchange data more frequently. If a user hits Undo in Revit, it will only Undo the work they have done in their model, but users can roll back to a previous version of the Data Contract. Users can see what receiving versions of the Data Contract had on their model. It\u2019s possible to play through all of the decisions in the lifecycle of a design and even go back to a previous version of the Data Contract and start modelling from there or replay through all the data that came through those gates.\nAccording to Awe, Branching and Merging are built into the underlying database technology, \u201cWe do use it in the Escrow system and the contract definition,\u201d he says. \u201cHowever, if users want to have that same technology in a core application like Revit, then that\u2019s a different story. Revit wasn\u2019t built that way. Revit could start making use of that facility in the database and we\u2019ve done some experiments, but we haven\u2019t decided if we are going to go back and re-engineer those applications.\u201d\nSpeeding up Revit?\nIn our initial discussion, one of Quantum\u2019s aims was to take the load off Revit. As it has become Plasma and become a platform technology there seems to have been less emphasis placed on this. Awe explained, \u201cAll things are possible! The way we\u2019ve specifically approached this, is that if we have to stitch together a workflow across the project ecosystem, you have to be able to include all tools that already exist, without major modifications.\n\u201cWe have enabled AutoCAD with a simple ARX plug-in, so it too can participate, but those other apps which we have built from scratch on this platform have lots of additional new capabilities. Revit too can still participate without any changes. The theory is that every application out there that\u2019s available today has to be able to connect to the workflow. As you adopt more and more of these new data platform features, they get richer and richer, but it\u2019s not required. It\u2019s up to us and up to customer demand as to how much Revit evolves, but it doesn\u2019t have to evolve at all to have just bare minimum participation.\n\u201cRight now, it\u2019s a much bigger challenge to decouple workflows from Revit, because Revit is already doing that design coordination between structure and the rest of the building. But if you play the theory out, ten years down the road, ideally Revit would also be able to decouple every system in the building and say I don\u2019t need to model the entire thing myself, I just need to coordinate with someone else who\u2019s modelling that piece.\u201cYou could basically divide Revit up into more specialised modellers for the specific systems but still coordinate between the different disciplines. But right now, Revit is taking on that responsibility and doing all the coordination itself. Plasma enables the single version of the truth. However, it is essentially distributed. Each app, each persona in the ecosystem is able to maintain the model that makes the most sense to them and then communicates the part that needs to have a collective sense of the overall model for collective co-ordination.\u201d\nRevit development\nWe asked Awe how Plasma impacted current Revit development. He responded, \u201cThe Revit team are being very aggressive in trying to stay up-to-date with everything that we are doing in the data platform.\u201d\nIt seems the Revit team is continuing to work on a number of experiments with regards to data granularity within the database but as it stands there is a lot of business data trapped inside of the Revit database, as it was designed to be a multi-discipline repository. The question appears to be, how much data would it be okay to remove from Revit and how much is required to remain because it needs to be there to fulfil the business logic?\nAwe says Revit is going to continue to evolve and the company is committed to keeping Revit fresh and \u2018architecturally sound\u2019 as Plasma moves forward. Awe commented that the Revit team is learning a lot from the AutoCAD team, which is the most aggressively re-architected platform within Autodesk. He said, \u201cThey are constantly figuring out what they need to do next and not afraid to update the technology.\u201d\nReading between the lines, Revit\u2019s demise is a long way off, if ever. Stage one will be connecting it to the Plasma workflow. At that point, the need for Revit to be the sole point of coordination lessens, as the data, the version of the truth, becomes distributed and supports varying levels of detail within various applications, even stored if multiple formats.\nIt\u2019s also clear that having a single product that can edit components supplied by all disciplines, as Revit does now, comes with some risks. Plasma will be all about designers with different roles maintaining their data in whatever system suits them, while submitting controlled work packets to the federated team, probably asynchronously.\nWhile Autodesk has talked mainly about the benefits of plugging in different Autodesk products, like Inventor, and imagines plugging into common, competitive tools like McNeel Rhino, there appears to be no reason why Plasma couldn\u2019t work just as well between different Revit users. And this is exciting, as it potentially unifies an industry that has struggled with data wrangling ever since the dawn of BIM.\nReplacing Revit\nWe certainly got the impression that perhaps, long-term, Revit\u2019s replacement was more likely to be actually multiple discipline-specific applications, which would be native to the Plasma way of working and these could be cloud, mobile or desktop based. We are so conditioned to defining ourselves or our jobs by the modelling software we use. In a data-centric approach, the authoring tool is no longer the star, it\u2019s the dynamism of the data and the intelligence that\u2019s embedded in the system. Nobody describes themselves by the web browser they currently use, it\u2019s just the Internet.\nWith this data centric approach, it will be much quicker and easier for developers to create small applications which perform discrete tasks within the many disciplines, these could be analysis applications, smart sensor data readouts or fa\u00e7ade analysis tools. In the past, developers would have had to create tools to plug into applications (like Revit) to access the data which only it could load but Plasma and Autodesk\u2019s new development environment Forge cut out the middle man.\nForge developers have been buzzing about using Revit.IO, which is a new component held in the cloud , available for application developers. It enables Revit functionality to be applied to Revit models on services such as BIM 360. We asked if this was a special cloud version of Revit, Awe explained. \u201cIt\u2019s really just a headless User Interface-less Revit that runs on a server. So it\u2019s not a new version of Revit, but it does allow Forge developers to load Revit models and access Revit functions for cloud-based workflows, which is very useful.\u201d\nIf you are wondering how this collaboration platform sits alongside Autodesk\u2019s BIM 360 platform, Awe explained, \u201cBIM 360 is currently acting as the data platform for a subset of the project lifecycle. But as we get more ambitious about Design-to-Make workflows, they will start to encompass more activities than are appropriate for BIM 360 to surface directly. BIM 360 will likely get \u2018jacked up\u2019 a little bit and the platform is going to get richer underneath. This will allow users to bounce data off Plasma, some of which will show up in BIM 360 and interact with BIM 360 but it doesn\u2019t mean you go to BIM 360 to do everything, it just means BIM 360 is a large window into project data and workflow\u2019.\nTypical customers\nWith an advanced collaborative workflow, and perhaps a requirement for broader technical knowledge and programming resources, we wondered who the typical Plasma customer would be. Awe explained, \u201cWe don\u2019t think this technology is limited to the high-end architects like Zaha Hadid Architects and Foster + Partners.\n\u201cWhen buildings are difficult to define and construct, sure, this technology will help, but we\u2019ve seen many simple examples of collaboration between designer and fabricator where huge improvements were made in efficiency and quality because they had a feedback loop. That doesn\u2019t usually happen in a typical \u2018paper waterfall workflow\u2019. This means you are going to have people in the supply chain who have a much clearer view of the designs early on. And can contribute much more within the process. It\u2019s going to be down to the cleverness of the firms to embrace new technology.\nTimeframe\nWhile Quantum was first aired in 2016, in 2019 the development seems to be progressing but there is a lot to do. Awe said that Autodesk is not committing to any delivery dates and with a new VP of Cloud Platform, Sam Ramji, there appears to have been a re-evaluation of what the company had developed so far, the systems architecture and what will and won\u2019t be delivered.\nAutodesk wants to avoid pre-announcing early technology and return to being more conservative. In fact, internally Autodesk has qualms about letting Project names slip into the open, Awe said that Autotodesk refers to this style of data-centric workflow across the project lifecycle as a \u2018Plasma Workflow\u2019 and hence the project name, but in time we have been told it will likely appear as just a capability of the Forge Data Platform in the future. For the time being, the company has done a lot of internal prototypes with Plasma Workflow and it has a number of construction firms who are willing to try it out on simple workflows, like wall framing and layout, experimenting with ways to remove the paper from the process.\nIn terms of performance optimisation, it\u2019s also very early days. We asked what kind of performance recent tests had given for multiple users. Awe explained, \u201cSo far with all our tests, the size of the data is small. Pushing out to individual contracts is not that big a problem, the real test will be when you want to aggregate all the data together, from across the system.\nFor example, if you wanted to do something like clash detection or viewing the project in its entirety, every participating system would have to provide a display mesh, and some application would need to be optimised to load and work with a much heavier set of data.\nConclusion\nOur initial hopes of Quantum being the next generation of Revit in some ways have drowned in the sea of automated collaboration. However, we now realise that the whole concept of applications, especially desktop ones is somewhat moot in the world of cloud-based workflows. Not unlike the end of The Avengers: Infinity War, we see desktop apps and workflows turning to dust in slow motion.\nProject Plasma is a total rethink of data flows in a cloud-enabled world. It\u2019s Autodesk recognising that one application cannot be infinitely expanded to solve all upstream and downstream problems in digital design workflows \u2013 an outlook that in its previous life, Autodesk tried to drive every nail with an AutoCAD shaped hammer.\nPlasma retains the elegance of both catering to current workflows and toolsets, while providing collaboration through a common data environment, which is highly user controlled and flexible in its usage. However ultimately monolithic applications that try to be everything to everyone and attempt to hold the co-ordination in a single database on a desktop will only continue to silo project data and never solve the collaboration issue.\nWe get the feeling that Plasma is still years off from meeting its prime objectives, but some elements of collaboration and exchange will be available earlier than the complete system. Over time, and when it makes sense, new applications will appear for performing discrete and industry-specific functions, removing the need for Revit to handle coordination and to concentrate on authoring. It\u2019s also great to hear Autodesk is looking to remove the need for drawings when design to digital fabrication makes sense.\nIn software terms we commonly talk about the next generation; with Quantum \/ Plasma being developed from the data level upwards, it would make more sense to look at Autodesk developing an environment for a new species.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/enerquest-systems-announces-new-color-lidar-point-classification-technology\/","title":"EnerQuest systems announces new color LIDAR point classification technology","date":1032307200000,"text":"EnerQuest Systems LLC has developed a technology, named SILC (Spectral Imagery LIDAR Composite), for the automatic classification of LIDAR points. Multispectral pixels are now associated with individual XYZ values to discriminate between roads, buildings, trees, water and other features.\nThe data are derived from EnerQuest\u2019s RAMST sensors. \u201cThe precise calibration between the imaging sensors and the LIDAR unit allow for this unique solution\u201d .\nThe Laser points are projected through collinearity equations onto their proper position on the frame array. Each laser point is then attributed with the appropriate multispectral value. This technology leverages the spectral clustering algorithms used for decades in remote sensing and combines it with the spatial attributes of high-resolution LIDAR data.\nLIDAR point classification will have profound implications in the GIS and LIDAR industries. It allows for comprehensive analysis and individual feature identification and classification at a level of accuracy and detail previously unavailable. The data will now provide a rich environment for rapid visualization, analysis and utilization, in support of GIS and geospatial applications.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/structural-engineering\/agacad-looks-to-boost-revit-s-rc-detailing-capabilities\/","title":"AGACAD looks to boost Revit\u2019s RC detailing capabilities","date":1588118400000,"text":"Lithuanian software developer adds wall and beam reinforcement capabilities to its Precast Concrete suite\nAGACAD is looking to boost Revit\u2019s precast concrete detailing capabilities with new tools that can generate detailed reinforcement for concrete walls and beams with all shop drawings and production files.\nThe new Wall Reinforcement and Beam Reinforcement software with automated rebar-modelling functionality are part of AGACAD\u2019s Precast Concrete suite of Revit add-ons. Features include rules-based creation, insertion and modification, real-time updates, saveable templates and built-in best practices.\n\u201cWe\u2019re committed to helping our customers deliver BIM-based services in a more productive way, providing the automation and customization needed for factories and on-site work too,\u201d says Donatas Aksomitas, CEO of AGACAD.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/future-reality-from-contextcapture-to-ai\/","title":"Future reality \u2013 from streamable, scalable meshes to deep learning","date":1512000000000,"text":"Greg Corke reports on the latest developments in reality modelling at Bentley Systems, including streamable, scalable reality meshes and automatic mesh classification using deep learning.\nReality modelling at Bentley Systems has come a long way in a very short time. It was only a few years ago that the company was talking up point clouds as being the new fundamental data type, just like 2D vectors, 3D solids and 2D raster. Now, everything is about the reality mesh and using it to capture site conditions, as-built projects or even entire cities.\nThe beauty of the reality mesh is that you don\u2019t need sophisticated surveying equipment. For a small project, simply take one hundred or so simple photographs from the ground and the air using a drone and you can have an engineering- ready dataset in no time at all.\nReality meshes are everywhere and crop up in virtually every conversation you have with a Bentley exec, whether it\u2019s about road and rail or buildings and design viz. There\u2019s a genuine excitement about the potential of the technology, but equally, the delivery mechanism, which is enabled through the company\u2019s Scalable Mesh technology, encapsulated in the 3SM format.\nScalable meshes allow you to stream multi-resolution meshes on demand, automatically, to desktop or mobile devices. With no theoretical limit on size, they can change the way designers, contractors and visualisers work with incredibly large reality modelling datasets.\n3SM files can be exported from the new ContextCapture Connect Edition software. Data can be streamed on demand from ProjectWise ContextShare at a resolution appropriate to the scale at which the project is being viewed. Low-resolution data would be shown when viewing a road corridor in its entirety, but more detail would be automatically streamed in as you zoom into the model.\nIn a plant, for example, you could even read the small print on a safety notice, providing the data has been captured with a high-res camera. This could be extremely useful for inspection or asset management with the creation of a digital twin.\nBentley describes 3SM as a more complete solution that solves many of the problems of other formats commonly used to provide context for engineering projects. For example, DTM and TIN are nowhere near as scalable, while STM and 3MX are not considered \u2018engineering- ready\u2019, so they can\u2019t be used to calculate accurate quantities.\n3SM also has the benefit of being able to handle different data types and a scalable mesh file can contain terrain data from many sources such as DTM, point clouds, raster elevation and more.\nReality capture\nThe data for reality meshes can be acquired from many different sources. On a road project, for example, photographs could be captured with an Unmanned Aircraft System (UAS). For smaller scale projects, an Unmanned Aerial Vehicle (UAV), commonly known as a drone, might be equipped with multiple data capture devices, including an SLR camera, a laser scanner or a thermographic imaging device. All of this data can be fed into ContextCapture to create the mesh.\nFor interiors, Bentley recently partnered with GeoSlam to streamline the workflow between ContextCapture and GeoSlam\u2019s handheld devices that capture laser scans and photos in real time as you walk through a building. This is a great alternative to static laser scanners that can be time-consuming to set up in each room, while taking photos of walls with no textures, or reflective surfaces can cause challenges for photogrammetry.\nHowever, as Bentley\u2019s Francois Valois admits, GeoSlam\u2019s technology is not suited to all workflows. \u201cIt won\u2019t necessarily give you engineering grade data, but it will definitely give you speed and it is perfect for asset management.\u201d\nData quality\nOne of the key challenges with ContextCapture is ensuring that the quality and coverage of the photographs you feed into the software will create a good quality model. With drones, particularly when working in confined spaces, the challenges are even greater, so Bentley has partnered with Drone Harmony, a company that offers an Android app to help easily plan out flight paths.\nThe software automatically gives the right overlap between photos and can fly around complex buildings, maintaining a constant distance, which is important, as it will define the resolution of the mesh.\nBentley has also developed a 3D resolution tool in ContextCapture that allows users to easily assess the quality of the mesh data. In simple terms, areas of the mesh that are coloured \u2018green\u2019 are good, meaning the data was derived from the most photos or was close to the laser scanner, whereas things that are coloured red are \u2018bad\u2019. If you are using the reality mesh to make engineering decisions, it can help you judge what the data can be used for.\nIn ContextCapture Update 8, out in 2018, it will be possible to use photos only for texturing the mesh and not for creating the 3D model, which is useful when using photos and laser scans combined. A smart algorithm will decide whether the photos or the laser scan is better, but the user will still be able to override.\nEngineering and construction\nReality meshes can be used for design, engineering, construction, maintenance, inspection, visualisation and more. You can accurately measure coordinates, distances, areas and volumes with ease.\nBentley is also developing task-specific tools, such as one that can analyse volume differences between two reality meshes (or a reality mesh and a design model) and view the cut or fill quantities.\nPractical applications include tracking construction progress, quality control or even to check the accuracy of contractor billing. Bentley also plans to extend this type of verification technology to building design and BIM models.\nMesh production\nThe creation of mesh data from photographs using ContextCapture takes lot of compute power. Bentley is well aware of this and dedicates significant development resources to optimise the engine and reduce processing time. ContextCapture Update 7 is out later this year, and Bentley execs reckon it is more than 30% faster than Update 5, with some projects offering a 50% increase.\nThe processing is done using Nvidia CUDA GPUs (not CPUs) and can be spread across multiple GPUs and multiple machines.\nHowever, for firms that don\u2019t want to invest in powerful GPU hardware, Bentley also offers the ContextCapture Cloud processing service, which allows organisations to process images and point clouds and create a number of deliverables including reality meshes, orthophotos, and digital surface models.\nThe service is scalable on demand, meaning more engines can be applied to urgent jobs in order to get results back quicker. Simply define quality and speed and the cloud processing service does the rest.\nJob submission can be done through the ContextCapture Console client, a desktop client that lets you connect to the cloud and upload your photos.\nGeoreferencing can be done by importing ground control points as a text file, then manually picking the points in the photos. ContextCapture also now offers a QR code framework to capture ground control points automatically. Simply place printed QR codes at known locations on site, prior to capturing the images, and the software will automatically find them in the scene.\nBentley also offers a ContextCapture Mobile app that lets you create 3D models from images taken with your phone or tablet. As mobile compute power is limited, all of the processing is done in the cloud. Simply upload your photos, then once the mesh has been processed it can be displayed on your device. This application is more appropriate for creating meshes of smaller objects and is well suited to inspection, or even to assess cracks.\nMesh distribution\nThe cloud isn\u2019t only good for brute-force processing; it\u2019s also great for managing and sharing reality modelling data.\nThe new ProjectWise ContextShare cloud service can stream reality mesh data to desktop tools like MicroStation or Descartes, mobile tools like Navigator or even to a simple web browser. It means everyone instantly has access to the latest data and there\u2019s no need to distribute giant files. Moving forward, ProjectWise ContextShare will also be able to handle point clouds and images.\nIn the future, Bentley will more tightly link reality modelling data to the iModel Hub, a new technology than maintains a \u2018timeline of changes\u2019 or, if you prefer, an accountable record of who did what, and when.\nDeep learning\nAutomation is the future and Bentley is currently exploring different ways of using Artificial Intelligence (AI) to get more out of reality mesh data.\nAt YII in Singapore last month, Bentley Fellow Zheng Yi Wu shared details of two R&D projects that use the AI technique deep learning. He explained that, in the last two years, computers have become better at recognising images than humans, based on an error rate of 5%, and this has led to big leaps in AI development.\nThe first project is designed to automatically detect and quantify cracks on infrastructure projects. Following image acquisition, the process involves post processing, feature extraction, edge detection and then quantification of the crack \u2013 not just, here\u2019s a crack, but also how long and how wide that crack is. Zheng showed the crack detection tool working on several use cases including pavements, buildings and bridges. By automating the detection process and automating the drone survey, this technology should make it much quicker and easier to prioritise maintenance \u2013 and, of course, to identify issues before it is too late.\nThe second project is exploring the automatic classification of reality mesh data. Currently in MicroStation or ContextCapture Editor, you are able select an area within the mesh and classify it as a specific type of object. Zheng Yi Wu\u2019s team has been using deep learning to train a system to automatically recognise objects \u2013 starting with trees and roads \u2013 and then learn how to apply those classifications to the rest of the reality mesh.\nConclusion\nIt feels like there is an unstoppable momentum behind reality modelling at the moment. The technology was credited in almost twice as many nominated projects at the Bentley Be Inspired awards this year, compared to 2016, and 21 of the 55 finalists used reality modelling to base their engineering work on digital context, said Greg Bentley in his keynote.\nThe beauty of ContextCapture is that it isn\u2019t fussy about where it gets its source data \u2013 reality meshes can be crafted from point clouds as well as photographs. However, it\u2019s the ease with which photographs can be captured by drones \u2013 and then re-captured on a weekly, if not daily, basis \u2013 that makes the technology so compelling. And then when it comes to distribution, with colossal scalable meshes able to be streamed on demand into its design, asset management and visualisation tools, Bentley is leading the charge.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/visualisation\/review-v-ray-for-rhino-3-6\/","title":"Review: V-Ray for Rhino 3.6","date":1537747200000,"text":"Al Dean takes a look at the latest release of V-Ray for Rhino3D. With this product, has the Chaos Group created the ultimate photorealistic renderer for McNeel\u2019s 3D modelling Swiss Army knife?\nRhino is hugely popular among architects for its advanced surface modelling capabilities. It can create incredible forms from scratch and import and export a huge array of data types. It\u2019s also very good value for money \u2013 the list price is 995 Euros \u2013 no subscription, no maintenance costs \u2013 and this includes the powerful Grasshopper visual scripting tool.\nThe final reason for its popularity is, I suspect, the huge wealth of third-party add-ons that are available. One of the most popular classes of add-on is rendering and visualisation.\nWhile Rhino has its own built-in rendering tools and its own add-ons (Flamingo and more recently, McNeel has built in the open source Cycles renderer), there are a range of third-party vendors looking to add their own special brand of magic.\nThis month, we\u2019re going to take an indepth look at the latest update to one of these \u2014 namely, V-Ray for Rhino \u2014 which costs \u00a3220 per year and is developed by the Bulgarian team at Chaos Group.\nIf you\u2019ve not come across V-Ray before, it\u2019s essentially a singular physically-based rendering system that has been developed for integration in a wide spread of 3D modelling and visualisation systems. It\u2019s currently available for 3ds max, Revit, Blender, SketchUp, Modo, Maya, Cinema 4D, Form-Z, and of course, Rhino. So let\u2019s dig into what it does, how it does it and what you can produce with it. If you\u2019ve used Rhino with plug-ins before, you\u2019ll know that installation is pretty straightforward \u2014 open up the Rhi file and it\u2019ll sort itself out. Then place all your files where you need them.\nIt\u2019s then a case of configuring V-ray how you want it \u2014 either running as default or, if you have shared resources for textures and materials (V-Ray makes sharing standardised materials very easy), then you\u2019ll need to point the various folders to where they need to go.\nGetting started\nGetting up and running with V-Ray for Rhino is pretty straightforward. You\u2019ll find a new menu is added to the pulldown and a small toolbar gives you access to the most common Rhino integrated commands, including light creation and depth of field focus control. It\u2019ll also bring up the V-Ray dialog window.\nIf you\u2019ve used V-Ray for Rhino before, you\u2019ll immediately notice that this portion of the interface has been dramatically reworked. The whole thing looks and feels much easier to use, and considering that the workflow is split across two key dialogs (the operations inside Rhino and the V-Ray dialog), it all holds together nicely.\nAs a last point, it\u2019s worth noting that if you need to switch the renderer in Rhino, you\u2019ll find V-Ray for Rhino under the Render menu (look for Current Renderer). You\u2019ll also need to switch your renderer viewport to use the V-Ray Interactive option from the view shading controls.\nSo, shall we explore the set-up and rendering process? Assuming that you\u2019ve got your geometry render ready to go, then the best place to begin is with adding materials to that model.\nMaterial creation\nMaterial creation is possibly the most time-consuming aspect of using V-Ray in the context of Rhino. V-Ray has special requirements when it comes to material definitions. So, while you can take any existing Rhino materials you have in place and convert them to V-Ray materials (using the included utility), the chances are that you\u2019re going to end up defining them from scratch in the majority of instances.\nThe good news is that there is a wide range of options, controls and parameters to let you do that, and there are also a number of shortcuts.\nFirst, the system is supplied with a library of materials which you can use as a starting point, most of which are suited to AEC workflows.\nSecond, the web is your friend and there\u2019s a wide range of downloadable materials for V-Ray (both free and paid) \u2013 look out for the vismat format. These can simply be opened up in V-Ray and tweaked or used directly.\nThere\u2019s also a newly added method of importing from a standard file format called MDL, which does much the same thing, but may give you another source of starter materials. The other option is to use Chaos\u2019 VRScans library. In essence, VRScans are physically measured materials that Chaos makes available for download. If you\u2019re looking for those 100% accurate materials and references, this is an excellent place to start (though editability is limited).\nIt is worth noting that the VRScans library is a separate add-on, costing \u00a3260 per year for access to the 600 or more real world, captured materials. That charge comes every year. If you stop paying, the materials are watermarked.\nBuild your own materials\nIf none of these options work, then you\u2019ve got a full set of tools to dive in and build your own materials from scratch. Of course, you don\u2019t want to do this for every project, so V-Ray makes your materials a lot more portable \u2013 both between an individual user\u2019s projects and between members of a design group. This means that one person who\u2019s got all the mad materials skills on your team could generate all of its material definitions, enabling the team to standardise on them.\nRather than clogging up your project with a huge library every time, the goal here is to only add those materials you might need to your V-Ray project. That keeps things nice, tidy and manageable, and is particularly useful if you are working on group projects, where others might need to jump in and continue work.\nIf there\u2019s a downside to V-Ray, it\u2019s not to do with Chaos Group\u2019s work, but rather some of the c lunky texture mapping controls inside Rhino. You still need to use these to control your material and how it\u2019s applied and the more complex your material, the more complex this can get \u2013 for example, if you\u2019re running multi-layer materials or using decals heavily.\nLighting set-up\nThere are two methods of working with V-Ray inside Rhino \u2013 using the existing tools in Rhino then converting the assets (lights, materials and so on) or creating them from scratch.\nDuring our tests, we found that it was much quicker and easier to define your V-Ray assets from scratch, using the V-Ray native tools. This is particularly true when it comes to lights.\nPerhaps your starting point might be adding in a HDR image as the source of reflections, lighting and background to your scene. To do this with V-Ray\u2019s tools is pretty simple, but they\u2019re a little hidden away \u2014 you do it using the adaptive sphere light tool.\nThis allows you to add in a marker using either a sphere or as a point, which makes it very hard to spot in the model views. Strangely, the position you place this in makes no difference, so I\u2019m not too sure why they\u2019ve even bothered with the graphical widget.\nYou then connect this widget up to your chosen HDR image and boom, the lights go on, your shadows start to resolve, and your image immediately starts to look better.\nOf course, alongside this, you\u2019ve also got a range of more traditional CG lighting assets \u2014 point, spot and area lights \u2014 and there are IES lights, too. If you\u2019ve not come across IES lighting definition, then you can use manufacturer-provided datasets to build a very accurate representation of a light.\nThe combination of HDR environment lighting and manually created, tweaked and positioned lights means that you have a full set of lighting set-up description tools available at your disposal. Interestingly, there are some tools available if your scenes feature multiples of the same light that you\u2019d like to control easily. Using the new \u2018instancing\u2019 option, it\u2019s now possible to have multiple lights in your scene, all controlled from one light in the V-Ray dialogs. This will come in useful in many instances.\nIn our test projects, for instance, we used a single light set-up to get the style we wanted, then instanced it across everywhere that a light was needed in the light clusters.\nThis gave us the control we needed, from a single light and I\u2019m sure it\u2019ll prove particularly useful for those working in the architectural field.\nThe V-Ray dialog and frame buffer\nThese two components are the focalpoint for everything that V-Ray does. While Rhino provides the interaction with the geometry, it\u2019s from the V-Ray dialog that materials are defined and applied, lights created and controlled, and the scene defined.\nIt\u2019s also where you\u2019ll find some of the more advanced options for the system. Some of these involve automation of Rhino to achieve specific rendering activities, but also to expose some of the more powerful aspects of V-Ray.\nFor example, while it\u2019s possible to create an infinite plane using Rhino, V-Ray adds in a command to do this explicitly and once the ground plane is in place, you can then add in the materials you want.\nThis is also where you define the specifics of your output, both in terms of resolution, but also those ray tracing controls that can make or break a project. While V-Ray for Rhino features an interactive rendering window, which is perfect for previewing your work, as you adjust and tweak your lights, materials and model, the chances are that you\u2019ll need higher resolution output.\nDuring our tests, this was something that immediately escaped me, but eventually I tracked it down as an option hidden away in the V-Ray dialog \u2014 proving, as ever, that it\u2019s worth spending time working through the tutorials and help system.\nHere, under the \u2018save image\u2019 option, you\u2019ll find a toggle for both image resolution and for image saving (which allows you to add a location).\nIt\u2019s also worth noting that there\u2019s a curious workflow in terms of how you preview your resultant imagery. In the first instance, you can have a viewport in Rhino displayed as a progressive V-Ray preview. This is perfect for dialling in materials, lighting set-up and so on.\nAn alternative is to use the V-Ray buffer window. Once switched to interactive mode, this links to your chosen Rhino viewport, so geometry selections are easier during material set-up and you can take advantage of a denoiser that can strip some time out of your final renders.\nTogether, the interactive viewport, with Rhino running as per normal and four shaded views, and the V-Ray interactive frame buffer running alongside it, make for an ideal combination.\nThen, when you want your final image, you kick off a non-interactive render to your required resolution \u2013 or indeed, add it to a batch for processing later.\nConclusion\nV-Ray has achieved legendary status among its many users and integrations into 3ds max, Maya and other standalone visualisation systems have long been on offer, are widely accepted and have significantly matured.\nThe integration with Rhino differs in the sense that Rhino is not a system intended for the viz professional, but instead for designers, whether they\u2019re working in product design or architectural design.\nThe good news is that, in its last few releases, Chaos Group has put in a great deal of effort into improving the user experience. Gone are the more esoteric icons and operations that could confuse new users. These have been replaced with a clearly set-out strip of operations in a toolbar, with more advanced options neatly packaged up in the V-Ray dialog.\nAs a result, what we\u2019ve ended up with is a set of tools tha t let you work with the design tool of your choosing (in this instance, Rhino) in order to create stunning imagery that might have previously required you to learn and use an entirely different set of tools.\nWhile for visualisation professionals, this option might not replace your 3ds max plus V-Ray set-up, it will certainly let you share assets between the two systems, both in terms of materials and scene data.\nHaving a high-end rendering engine inside Rhino certainly makes a great deal of sense. After all, if you\u2019re already using Rhino heavily in your work, you\u2019ll be used to navigating some of its eccentricities and have probably established your own methods to achieve your goals, which work well for you.\nAnd, on the whole, V-Ray extends Rhino\u2019s well-respected usefulness significantly. The combination of these two products, in fact, is pretty damn impressive.\n\u25a0 \u00a3220 per year\nRhino renderers: what other options are out there?\nRhino has its own built in renderer but, as anyone that\u2019s tried it will know, it has its limitations. McNeel\u2019s decision to include the ray tracing Cycles engine from Blender came with Rhino 6, but still the system isn\u2019t up to much. So what else is out there?\nRadeon ProRender\nSupplier: AMD Price: Free\nAMD has been developing its free Radeon ProRender add-on for a number of systems for some time now. Computation can be done on both CPU and GPU, but the GPU is the favoured option. Interestingly, the product isn\u2019t restricted to AMD GPUs and will gain benefit from compute on Nvidia boards as well, although you\u2019ll get the best performance from one of AMD\u2019s cards. Be careful, however, if you also have V-ray installed, as there\u2019s current conflict between the two products.\nOctaneRender\nSupplier: Otoy Price: $589\nOctane Render is a real-time renderer that Otoy bought out quite some time ago. It was a pioneer in the GPU computation of rendered scenes, so has gained a reputation as being incredibly quick, particularly when using consumer-level GPUs. At present, this is a more costly beast, but we\u2019re hearing talk of licence changes coming with the Octane 4 release. These will presumably see a shift to a subscription- based deal, which could prove more attractive to new customers.\n\u25a0 otoy.com\nMaxwell\nSupplier: Next Limit Price: \u20ac695\nMaxwell has been a well-respected rendering solution for a good decade or more and it has found a home in quite a number of organisations, particularly those focused on industrial design. Alongside its integrations into SolidWorks, the Rhino variant works nicely with all of the con trols you\u2019d expect. It is worth noting that Maxwell only works with Rhino 5. Additionally, unlike almost every other Rhino renderer, Maxwell supports the Mac version of Rhino.\nTheaRender\nSupplier: Altair Price: $450\nWe hadn\u2019t heard of this one, until we did a little digging. We were subsequently surprised to find out it was developed by an outfit call Solid Iris, but is owned by simulation software developer, Altair Engineering. (the acquisition of Solid Iris by Altair dates back to 2016.) TheaRender for Rhino looks to support Rhino 5, includes all of the bells and whistles you would expect (HDR image support, physically based materials) and it takes advantage of your GPU for computation.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/oasys-enhances-pedestrian-simulation-tool-massmotion\/","title":"Oasys enhances pedestrian simulation tool MassMotion","date":1581379200000,"text":"Software used in planning of airports, train stations and stadia, and better understanding user experience within other spaces\nPedestrian simulation software Oasys MassMotion has been updated with several new features including new elevator analysis options, conditional wait spaces in process chains and a range of usability enhancements. A Simple Chinese interface and user support has also been released this month.\nInitially developed for Arup engineers who needed to understand the impact of crowding on major infrastructure projects, Oasys MassMotion is now used on a variety of projects, from planning designs of major airports, train stations and stadia through to understanding user experience within art galleries, theme parks, and public events.\nThe software works by associating imported or created geometry as key elements that virtual pedestrians can understand: such as walkable surfaces, doorways, or stairs. The origins, destinations, and any intermediate itineraries of these pedestrians are then set by the user, and the simulation is run. Movement algorithms and network assessments determine the most logical route for agents to move, with dynamic reactions to crowding and emergent conditions. The aim is to reduce the time taken to model different scenarios and allow for alternative building layouts, populations, or operations to be tested rapidly. From understanding evacuation times to helping design construction phasing, MassMotion can be used throughout the design and operation stages of a project.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/prime-minister-announces-mission-to-moon\/","title":"Prime Minister announces mission to moon","date":1061596800000,"text":"The Prime Minister of India announced the Chandrayan-1 mission, on August 15, 2003 during his Independence Day address to the nation. This represents India\u2019s foray into a planetary exploration era in the coming decades. Today, India is confident of undertaking a complex space mission because of its indigenously developed launch vehicle and spacecraft capabilities. This mission will provide a unique opportunity for frontier scientific research. Chandrayan-1 is expected to be the forerunner of more ambitious planetary missions in the years to come, including landing robots on the moon and visits by Indian spacecraft to other planets in the solar system.\nThe Chandrayan-1 mission envisages placing a 525-kg satellite in a polar orbit 100-km above the moon. The satellite will be launched using a modified version of India\u2019s indigenous Polar Satellite Launch Vehicle (PSLV). The spacecraft will initially be launched into Geo-synchronous Transfer Orbit, and subsequently manoeuvred into its final lunar orbit using its own propulsion system. The main objectives of Chandrayan-1 include obtaining imagery of the moon\u2019s surface using high-resolution remote sensing instruments in the visible, near infrared, low and high-energy X-ray regions. Furthermore, considering the interest expressed by the international scientific community, a provision has also been made to accommodate instruments from other countries. The spacecraft is expected to be ready for launch by 2008.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-4d-construction-modelling-helping-transform-fc-barcelona-stadium\/","title":"4D construction modelling helping transform FC Barcelona stadium","date":1531958400000,"text":"Synchro, Microsoft Azure and HoloLens provide immersive construction management solution for Espai Bar\u00e7a sports complex\nSynchro, the 4D construction modeling solution recently acquired by Bentley Systems, is being used as part of a mixed reality construction management solution for FC Barcelona\u2019s major renovation to the Espai Bar\u00e7a sports arena, which when completed will accommodate more than 100,000 fans.\nIn conjunction with time-lapse monitoring footage of the construction site provided by EarthCam, Bentley\u2019s Synchro will help the stadium project team to visualise and compare construction progress with the 4D construction model.\nGreg Demchak, technical architect for Bentley, said, \u201cThe scope of this project demands that we look for new solutions, and for this, we turned to Microsoft. Using Azure infrastructure, HoloLens technology, and Cognitive Services, multiple users can interact with a holographic representation of the stadium data at any moment in the construction process. We are also leveraging machine learning and on-site images to digitally track actual progress with Synchro.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/graphisoft-archicad-15-p1\/","title":"Graphisoft ArchiCAD 15 p1","date":1311638400000,"text":"Building Information Modelling (BIM) developers have mainly been concerned with providing tools to complete new build projects. In these challenging economic times of make do and mend, features for renovation and retrofit are the \u2018new new\u2019. By Martyn Day.\nA walk through central London would indicate that there is still quite a bit of construction work going on. Pretty much from anywhere in the heart of the city, Renzo Piano\u2019s Shard is visible as the core takes shape dominating the capital\u2019s skyline. The reality is that these prestigious projects still have momentum but the normal bread and butter architectural projects have become a mix of new and renovation.\nOne of the first victims of the global economic downturn was the then Labour government\u2019s \u2018Schools of the future\u2019 programme, killing 715 new schools, work for which many smaller practices were involved. The result of this change in policy is that there will have to be more renovation of existing building stock to meet the public\u2019s needs, as well as meet stringent future energy efficiency targets.\nI write this article on a day that sees the energy firms increase their winter prices by a whopping 19 percent for gas and 10 percent for electricity. Energy consumption will become a critical factor in the owning and operating of any building, and old, \u2018inefficient\u2019 buildings are clearly going to become liabilities. In Germany alone, the government wants to reduce energy consumption by 80 percent by 2050, which means that over 12 million homes will need to be updated.\nPerhaps it is well timed that the latest version of Graphisoft\u2019s BIM modeller, ArchiCAD 15 comes with a suite of new capabilities specifically targeted at improving the workflow of renovation and retrofit projects. This comes amid a general trend from Building Information Modelling (BIM) software developers to deal with as-built structures in a much more inclusive manner to meet the change in project mix of their customers. The renovation market in the UK is expected to be worth \u00a346 billion in 2012. In 2009 you would have been hard pressed to find any serious renovation features in the popular BIM tools, now it\u2019s deemed a competitive necessity.\nWhile, as usual, there are a plethora of new features in ArchiCAD 15, with updates and enhancements, we have split this review into two; renovation this month and then the nuts and bolts general functions in the September \/ October edition of AEC Magazine.\nArchiCAD Renovation\nSo how can BIM best support renovation workflows? In Graphisoft\u2019s view of the process, the first task is to capture the as-built conditions and this is usually done via a survey. This survey feeds into the BIM system and a model is produced. The next step is to indicate which parts of the building need to be demolished with some automation in identifying elements\/breaking up existing elements and auto filling of holes, as well as some way of identifying new structure versus existing structure.\nThe system would be accurate to produce the correct quantity calculations for the new elements and as the majority of energy-saving retrofit jobs will include pipework, Mechanical, Electrical Plumbing (MEP) elements will be essential. New energy calculations will need to be run on the upgraded structure and then clearly handled in producing demolition and construction documentation.\nSurvey\nCapturing the existing structure can be carried out in a number of ways. Traditional surveying may provide a set of drawings which need to be imported and then modelled up, which while easy isn\u2019t probably the best use of time. There are a number of add-on products for ArchiCAD that can assist in the automation of digitally capturing this data (Orthograph, Flexijet and ArchiMap).\nGraphisoft has not yet included support for point-cloud data, which is another emerging technology for a \u2018Scan to BIM\u2019 solution, but this can be integrated using other point-cloud specialist tools, such as PoinTools (www.pointools.com).\nElements\nBIM models consist of elements. After a model of an as-built structure is completed, the next step is to indicate how the updated building will be configured. To enable this Graphisoft has added some new element-level renovation statuses for existing, new, and to-be-demolished structures (and includes a nice hammer icon), providing quick feedback on the status of any element through a renovation project.\nFrom the new renovation palette, users access the element selection tool, renovation filters (to display the project at various stages). Clicking through elements in the model will provide instant feedback as to the renovation status (existing, new, to-be-demolished).\nSimply click on elements and select demolish. Then create new walls, doors, windows and other elements that will replace the demolished sections, while remembering to tag the new geometry as \u2018new\u2019 in the renovation palette. As the new elements are tagged, ArchiCAD can work out exact quantities of materials.\nTo get a clearer view of the status, toggle the renovate filter and \u2018existing\u2019 versus \u2018new\u2019 and \u2018to-be- demolished\u2019 are clearly highlighted in different colours. The filters can be user-defined to hide dimensions, have solid fills, remove confusing intersections and hide zones of areas that are eliminated.\nIn practice this is exceptionally easy and made very clear with the colour filters. The added benefit here is that all the various statuses are kept in a single file, removing duplication of files and effort.\nMEP\nWith increasing demands on the engineered components within buildings, Graphisoft has enhanced the ability for the architectural version of ArchiCAD to import MEP IFCs and integrate MEP elements within the system.\nThere is a specific version of ArchiCAD for MEP called MEP modeller for building services engineers. Graphisoft has even written plug-ins for AutoCAD MEP and Revit MEP to ease data translation straight out of Autodesk\u2019s popular products into ArchiCAD.\nWith version 15, architects can visualise and fully understand the 3D MEP systems created by engineering teams before demolition plans are formed. ArchiCAD can also perform clash detection with a single click, finding and highlighting clashes.\nEnergy\nArchiCAD has its very own energy analysis module called EcoDesigner. It performs dynamic energy evaluation throughout the design process, providing: building envelope performance, energy consumption, monthly energy balance and a building\u2019s carbon footprint.\nIdeally any energy retrofit will be tested throughout the design process to optimise the design. Material properties can be allocated to construction or entered directly through \u2018U\u2019 value.\nNew in this release, the support for \u2018passive\u2019 building performance, more cooling and ventilation options for MEP elements, solar collectors, the ability to calculate on multiple buildings, export to Excel and an export to Strusoft\u2019s high-end VIP energy solver.\nConclusion\nIt is easy to document the existing, demolition and planned stages of the project using the filtered views. By creating layout folders from these views, the views are automatically placed in drawings which can then be published. By plotting PDF for instance, the three plans can be saved together and quickly viewed, with the different filters proving colour feedback as to what is happening in the three key stages.\nArchiCAD comes with pre-configured renovation plan types together with a selection of popular European drafting styles that follow local CAD standards and documentation conventions.\nIn this release Graphisoft has not just added some renovation capabilities to ArchiCAD but has delivered a comprehensive vision of workflow for documenting and modelling what can be a very complex process. It\u2019s easy to use, intuitive and flexible for those that want to generate their own standards.\nI would like to see support for point-cloud at some point as laser scanning can offer considerable benefits in capturing as-built conditions in 3D. The technology to intelligently convert this data in BIM components has not been solved yet but there will come a time when laser- based surveys will replace the traditional surveying methodology of pen and paper or disto and data logger.\nIn the September\/October edition of AEC Magazine we continue our journey through ArchiCAD 15, with a focus on performance, modelling and documentation improvements.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/high-speed-delivery-willmott-dixon\/","title":"High-speed delivery","date":1605571200000,"text":"Willmott Dixon deploys cloud-based document and information management solution to help deliver the National College for Advanced Transport and Infrastructure defect-free and on-time\nThe National College for Advanced Transport and Infrastructure (NCATI), the new name for the National College for High Speed Rail (NCHSR), will play a key role in helping provide the higher-level skills needed to transform the UK rail and transport infrastructure network. Willmott Dixon, the privately-owned contracting and interior fit-out group, was awarded the delivery.\nThe project duration was 15 months and was valued at \u00a321.2m, comprising both extensive teaching and workshop space. The complexity of the project required a system that brought all the various groups together to work from a common data environment, allowing Willmott Dixon to surface and address challenges early and often.\nInitially, NCHSR was not a BIM level 2 requirement from the stakeholders. However, prior to the project starting, Willmott Dixon made the decision to deliver all new projects to BIM Level 2 after seeing the benefits on previous projects.\nThe collaboration conundrum\nOne of the largest challenges Willmott Dixon faced in completing the project was the interoperability between software platforms \u2014 between the architects, structural engineers, varying internal standards and several software solutions, working off the same set of data was a near-impossibility.\nOnce the supply chain extended to the inclusion of fabrication software, the issue became even more unmanageable, resulting in more time spent resolving than actually federating the models. Due to interoperability issues, the company couldn\u2019t create a data file so the simplest method for resolving this issue was for the architects to produce a coordinate setting out drawings for the project team to base the model from.\nTime and costs were increasing pressure on the administrative burden that arose from using more traditional paperbased processes. The company was looking for a system that would automate transfer of accurate, complete and unambiguous information that could increase productivity, reduce cost and help ensure a smooth handover.\nMaster view\nTo operate from a single system, Willmott Dixon implemented Viewpoint For Projects across its whole construction process. VFP is a cloud-based document and information management solution which enables customers to share, control and collaborate on project documents with dispersed projects teams.\n\u201cAcross the business, we all use VFP,\u201d says senior design coordinator James Henderson. \u201cWe use it for pre-construction, so before a job becomes live we look at concept design, all the way through to construction.\n\u201cWhen we\u2019re delivering a project onsite with our delivery team, VFP is a common theme throughout the whole business and it also allows us to work in a common data environment particularly when we\u2019re delivering projects and BIM Levels 1 and 2.\u201d\nStraight from the field\nThe NCHSR project also utilised the Viewpoint tablet-based system Field View, which integrated with the common data environment VFP in order to provide the latest project information in real-time from anywhere.\nThe Willmott Dixon supply chain and consultants also utilised these systems, particularly the use of tablets onsite to manage Field View data as opposed to leaving the work front to refer to emails or hard copies of project information.\nThe design consultants shared every revision of project information to VFP so that remote access to the latest project information was provided to the wider team, allowing continuous integration of project information.\nChanging hands By digitally transforming its processes, Willmott Dixon created benefits to endusers following handover, which included eliminating work for the estates department by being able to operate the building efficiently immediately after handover.\nGaining access to real-time information meant all stakeholders in the project could make quick, informed decisions on operation and maintenance expenditure based on actual asset performance and status. Costs were reduced from automating processes that were previously done on paper and, as all construction data was available, refurbishments could be made at a lower cost and in less time.\nAs Willmott Dixon BIM information manager Ben Jowett notes, \u201cOne of the key benefits for us, particularly using VFP, was the control of access of information \u2014 we work on a lot of law and order projects and digital information security is very important, particularly with BIM \u2014 there are a lot of security standards surrounding the protection of digital information. I think the access controls were flexible enough to allow us to cater them to what a project needed whilst also having a standardised approach to it as well.\u201d\nReal-time records Henderson adds, \u201cThe main benefit of using the software in terms of ROI for risk mitigation would be the use of site diaries. We\u2019re able to record what is going on in real time so if there are any issues we can then go back to event as if it was yesterday to find out what had and had not occurred so we can move along more proactively.\n\u201cUsing it like myself, from a design point of view, it\u2019s the revision control \u2014 so the software only ever shows the latest revision so there is no chance of working to the wrong information.\u201d\nThe integration of VFP and Field View allowed the NCHSR Project team to remotely manage a number of construction processes much more time-efficiently such as: health and safety permits, quality delivery inspections and site diaries. One significant benefit the team realised by using these systems was more time spent onsite versus utilising time on the administrative burden that more traditional paper-based processes require.\nFixing snags along the way\n\u201cWith Field View, snags are broken down into two parts. First, we try and deal with all snags while work is taking place onsite to make the project run a lot smoother \u2014 so we start a quality delivery system, so as we\u2019re going through the works as they get signed off as the task is being created. In doing that, we created 523 tasks,\u201d continues Henderson.\n\u201cThen, when we get to the end stage of the job, when we\u2019re trying to get to the finishes, we start another snagging process. In total, there were 293 snags identified using Field View, which allowed us to get the \u2018real-time effect,\u2019 where we work a lot quicker. The biggest benefit is that we are snag-free two days ahead of handover, so we can hand over a BIM Level 2 project, snagfree, to the client.\u201d\nStaying on-site\n\u201cThis process would have originally required site teams to go back to the office and fill out the necessary paperwork,\u201d says Jowett. \u201cInstead, what Field View allows our delivery teams to do is actually stay on site whilst carrying out the tasks they need to, which keeps the project running smoothly. The last time we surveyed our people at Willmott Dixon on Field View they believed they were saving up to five hours per person per week. Spread out across all of our projects that\u2019s quite a significant saving in time.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-plangrid-raises-18-million-in-funding\/","title":"NEWS: PlanGrid raises $18 million in funding","date":1431648000000,"text":"Investment to help fuel growth of cloud-based construction document collaboration platform\nPlanGrid, whose cloud-based platform is used to store millions of construction documents for viewing and collaborating on tablet or smartphone, has raised $18 million in series A financing to help fuel the platform\u2019s growth.\nPlanGrid is a cloud-based construction document collaboration platform that allows plans and markups to be shared with everyone on a construction project, no matter where they are. It lets contractors and architects collaborate from their desktop or mobile devices across all their project plans, specs, photos, RFIs and punchlists.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nNEWS: Radeon ProRender for Cinema 4D + Blender\nGraitec releases Opentree 2023\nNEWS: Scan introduces workstation rental program\nTeradici PCoIP to offer remote access for Mac\nGenerative design to help address risks of COVID-19\nREVIEW: AMD FirePro W4300 GPU\nAllplan updates BIM solution for bridge construction\nPresenZ for V-Ray\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-looking-beyond-design\/","title":"Autodesk looking beyond design","date":1577059200000,"text":"Greg Corke reports from Autodesk University where construction took centre stage as Autodesk re-invents itself as the end-to-end AEC company.\nOver the past 18 months Autodesk has been on a shopping spree like no other in its 37-year history. With the acquisition of big-ticket software firms like PlanGrid, BuildingConnected and Assemble Systems, the company is betting big on construction and cloud-based collaboration. It now has new capabilities in mobile construction management, bid and risk management, as well as takeoff, estimating and scheduling. However, the area in which Autodesk is looking to dominate already has many players all trying to do the same thing, such as Trimble and ProCore.\nAs a company, Autodesk looks very different to the laser focused design software firm it used to be. It now has a comprehensive portfolio of technologies for architects, engineers, contractors and owners that touch the entire construction process from concept all the way through to operations.\nOf course, when any software developer acquires so many disparate technologies, it\u2019s always going to have a huge job on its hands to deliver an integrated solution while keeping its new customers happy. At Autodesk University in Vegas last month it took the first steps on the long road to delivering a cohesive technology stack.\nThe big news was the official unveiling of Autodesk Construction Cloud, which essentially brings together Assemble, BuildingConnected, PlanGrid and BIM 360 (Docs, Layout, Ops, etc.) under one umbrella. While the majority of these tools are focused on contractors, and were developed mainly for US-based processes, Jim Lynch, vice president and general manager of Autodesk Construction Solutions made the point that Construction Cloud isn\u2019t just about construction. It actually connects the entire project workflow, he said, which is why it\u2019s also important to architects and engineers.\nAs the Autodesk mantra goes, it\u2019s \u2018design, plan, build, operate\u2019. In practical terms, it\u2019s about making critical data available to anyone who needs it along that path.\n\u201cYou can start in your authoring tools, collaborate across the team during design, push plans and models to pre-construction for procurement, coordination, and quantity takeoff. And once you\u2019re ready, directly push that data to your teams in the field,\u201d said Lynch.\nOf course, for this to happen, data needs to flow seamlessly, and Lynch believes connectivity is the most important part of Construction Cloud. \u201cThere are a lot of great technologies in the market in the construction technology sector that deliver meaningful productivity gains in a specific area, but it\u2019s really when you thread these things together it creates workflows that deliver the real value.\u201d\nOn a technical level, Autodesk has started to create connections between its key technologies. During the AEC keynote, Allison Scott, Autodesk\u2019s director, head of construction thought leadership & customer marketing, who previously worked for Skanska, ran through some of the recent developments.\nWith Navisworks Publisher for Assemble, she explained how teams can now bring coordinated models from design review software Navisworks into Assemble so they can be used for estimating, scheduling or other downstream workflows. Importantly, all data is accessible onsite through the Assemble mobile app.\nAutodesk is also aiming for tighter connection between the \u2018plan and build handoff\u2019 with a new BuildingConnected to PlanGrid workflow, as Scott explained, drawing from her own experiences in industry, \u201cModels, plans, estimates and documents are often manually uploaded into project management tools \u2013 I remember this. This is a time-consuming process that can result in missing files and data errors. So now, with a simple push of a button, you can automatically send preconstruction files from BuildingConnected to PlanGrid.\u201d\nScott explained that companies are already benefitting from this new workflow. For example, global consulting, design and construction services firm CRB can now \u2018seamlessly transfer complex data from the design and planning phase into the hands of the workers on the site.\u2019 According to Scott, the company estimates this integration will save them at least two weeks of work on every project and help them improve communication with their subcontractors.\nBut it\u2019s not just cloud to cloud. PlanGrid BIM is designed to allow users to access BIM data from Revit, in either 2D or 3D, directly within PlanGrid on mobile devices, the idea being that construction teams can get quicker access to models and metadata to get a better understanding of design intent.\nAutodesk has also been linking up its core tools, connecting Civil 3D to BIM 360 Design. Autodesk AI development manager Racel Williams made the point that, when combined with the existing Revit Cloud Worksharing capabilities, this will benefit firms who design airports, railway stations, and other complex projects with both vertical and horizontal structures.\nAutodesk also has the added challenge that there is some overlap between its products. PlanGrid and BIM 360, for example, are both used during the build phase for project management and collaboration on site, something that Scott acknowledged in her presentation, \u201cWe\u2019re being extremely thoughtful and understanding the capabilities that you know and love in each of these solutions, and we\u2019re focusing on strengthening their ties,\u201d she said, adding that the new PlanGrid Connect integration platform has deepened the connection between office and site, while still allowing users to work in either BIM 360 or PlanGrid.\n\u201cNow, you can automatically sync documents, drawings, issues, tasks, and more between BIM 360 and PlanGrid,\u201d she said. \u201cInformation can be created and shared from BIM 360 and directly pushed to PlanGrid users in the field and PlanGrid users can now take the custom field reports they\u2019ve created and automatically save them back to BIM 360.\u201d\nThe integration between these two tools is not just for site work. It has also been extended to operations, to help better support building maintenance.\n\u201cBIM 360 Ops users can easily manage work order tickets created from PlanGrid,\u201d said Scott. \u201cSo, as a building manager is walking a facility, they can create an issue on the as-built plans using PlanGrid on their mobile device, and a ticket is automatically created in BIM 360 Ops for tracking and resolution.\u201d\nIn a heavily fragmented industry, Autodesk has a very ambitious vision for Construction Cloud. In order to deliver on this vision, it not only needs to nail the issue of data mobility through optimised workflows, but it faces a huge challenge in terms of education. With the breadth and depth of its newly acquired technologies, it will take time and investment to get the most out of the software. And, in many cases, firms will need to undergo huge cultural change to move away from long established paper-based workflows.\nAutodesk also acknowledges the challenge of working with firms with proven digital processes and the importance of taking small steps, as Scott explained during the AEC press conference. \u201cWe have customers that are really using Assemble in a way that\u2019s very effective for them. We can\u2019t change that. But how do we extend and improve upon that? How do we extend and improve upon the way that teams are using PlanGrid? How do we extend and improve upon BuildingConnected and BIM 360, and then begin to connect the ways they talk to each other, so that we\u2019re not disrupting the way that they work?\n\u201cWe\u2019re just making it easier for them to their day jobs, and also in some cases uncovering new ways of working, they didn\u2019t know possible.\u201d\nIndustrialised construction\nIn this magazine we\u2019ve written a lot about the growing trend for offsite, modular or industrialised construction and the lack of tools fit for purpose. According to Lynch, Autodesk Construction Cloud will become the foundation for the convergence of construction and manufacturing, enabling the seamless flow of data from design, to fabrication, and ultimately out to the job site. Autodesk Inventor, the company\u2019s primary manufacturing software tool, looks set to play a key role, with the company working on a connection with Revit.\nThere are many challenges ahead and Autodesk is working on answers to three in particular: how does a design team approach a design when they know how it will be constructed or fabricated, how do you get the level of detail required for the shop floor and how do you manage the installation or assembly? Autodesk has technologies that can handle all of these, but they\u2019re not yet connected.\nModular Construction was one of the key themes for the opening keynote with Andrew Anagnost highlighting one of the most ambitious modular projects currently in progress, the AC Marriott New York Hotel in Manhattan. At 26 storeys when finished it will be the world\u2019s tallest modular hotel, comprising hundreds of individual steel-framed modular rooms. Each room gets fully fitted out in a factory in Poland, including all finishes, furniture and equipment, before being taken to site and stacked much like shipping containers.\nNew York City construction company Skystone is leading the project. The company used Revit to develop a library of parts that can be mixed and matched to spec using ProjectFrog\u2019s KitConnect service, which is built on Autodesk Forge.\nSkystone uses KitConnect to publish component libraries to the cloud, from where they can be assembled and configured into a single central model, as Autodesk CEO Andrew Anagnost explained, \u201cSo if a component like a window is swapped out, the Bill of Materials for each module remains consistent and accurate, because the changes propagate to the Revit model, and to any other tool referencing it, like BIM 360, which Skystone is using to share 3D documentation across all phases of the project.\u201d\nSkystone has also recently started using Assemble. At the moment it\u2019s just to identify when materials have arrived at the facility and to mark off when modules are complete, but there are plans to use many other aspects of the software in the future.\nIn the factory, robots are used to weld the steel frames, but the rest is done by hand. The biggest benefit to this offsite approach is that work can be done in parallel, so the building\u2019s concrete foundations and core can be constructed on site while the modules are being manufactured in Poland. This means a rapidly accelerated, more predicable construction process and one that creates less disruption and noise on site.\nLooking to the future, because of the technology stack that Skystone is using, and the rich information held within the models, the company would be able to give a quote with detailed pricing for a similar hotel with different room configurations in three days or less.\nGenerative design \/ design automation\nFew would deny that there is huge potential for generative design in architecture and engineering, especially when it comes to optimising designs for multiple, often competing criteria, like costs and performance.\nAutodesk has offered the technology in various forms for nearly ten years now, but it\u2019s never gone mainstream in its customer base. One of the reasons for this is strong competition from Grasshopper, the Rhino-based visual programming tool, but also because generative design software is often not that easy to use, as Williams explained, \u201cMany generative design tools out there can be really hard to leverage because they typically require you to know how to code or understand special terminology, or how a genetic algorithm works.\u201d\nAutodesk is now looking to change this and make generative design more accessible, and not just for those who know how to code. It\u2019s working on a beta technology called Project Refinery, which is integrated into Revit.\nDuring the AEC keynote, Williams gave a demonstration showing how a designer could perform a massing study to explore the allocation of retail versus office space \u2013 the key aim being to minimise cost and maximise rentable area. The software presented the options in a design grid, complete with 3D thumbnails that can be individually rotated, zoomed and panned. To dial into the details, the inputs and outputs were displayed in a Parallel Coordinates graph. Project Refinery looks like a very interesting technology and one that we will be taking a closer look at next month.\nGenerative design also featured in the opening keynote with Anagnost focusing on how Airbus is using the technology to optimise a new factory for installing engines. Anagnost explained that Airbus turned to generative design because of the challenging plot of land on its Hamburg campus, which was triangular and smaller than it would have liked.\nOn a building like this, form always follows function, so Airbus identified ten different sets of variables to optimise for, covering the social, environmental, financial and operational aspects of the design, including the flow of workers and parts throughout the factory. The study generated hundreds of options, which Airbus then narrowed down to two designs, as Anagnost explained. \u201cBoth have better flow scores than the existing standards, so either factory functions more efficiently, the engines are installed more quickly, the logistic flows are more efficient, and so are the workers because they take fewer steps each day\u2026 Both options are less costly, so now they just need to choose the one that suits them best.\u201d\nAutodesk is also working on new tools to automate tedious, time consuming tasks, something that chimes with Williams, a former architect who spent the early days of her career doing \u2018boring activities\u2019 like creating door schedules. In her own words she was basically just following standards.\nVisual programming tool Dynamo has been available for Revit for some time, but Autodesk is now looking to make task automation much easier with the introduction of Dynamo Player. According to Williams, it allows anyone to run a script to automate tasks even if they don\u2019t know how to code or use Dynamo.\nWilliams showed how the technology could be used to quickly generate clash free structural reinforcements for window and door openings. \u201cA task that used to take ten minutes to do manually in Revit now only takes ten seconds using Dynamo player,\u201d she said.\nDynamo player is also being used by Build Change, a non-profit organisation that strengthens buildings in emerging nations to help protect them from earthquakes and typhoons. Almost all houses are different, and each requires its own set of designs and drawings which, in the past, were all done manually.\nBy embracing new technology, the company has now built an automated design to construction workflow which uses Revit and Dynamo scripts to automate the retrofit design for light types of buildings. The team codes in Dynamo, creating structural engineering rules, and then Revit is used to produce a 3D design, Bill of Materials and the construction drawings. \u201cWhat once took our team of three people four to five days, now takes one person three hours. That\u2019s 97% less time,\u201d said Elizabeth Hausler, founder and CEO, Build Change.\nThis is a hugely impressive saving and one that will completely transform the way its skilled architects and engineers are used, but Build Change is not stopping there. It\u2019s now looking to save even more time by using Artificial Intelligence (AI) to change how homes are assessed.\nRather than sending out engineers and architects to every house, home owners can upload photos to the Build Change cloud and these are then used to quickly and automatically assess homes, as Hausler explained, \u201cIt looks at things like the continuous length of solid walls and proximity of walls to the corners, the length of wall between openings. We train the machine for image capture and all that data then tells us if it\u2019s possible to retrofit the home.\u201d\nBuild Change has the ambitious goal of strengthening 10 million homes in the next 10 years but needs three million dollars to fund the program. On stage Anagnost revealed to Hausler that Autodesk would be donating half a million dollars to help Build Change reach its goal. On a day where heavily scripted presentations were the norm, seeing the genuine surprise and emotion on the face of Hausler when Anagnost broke the news was a truly wonderful moment.\nIt\u2019s not just buildings that are benefitting from Dynamo. The visual programming tool is now available for Civil 3D as well and can be used to automate a range of repetitive tasks such as the design of guardrails for road projects, signals on a rail track or disabled ramps on a site.\nWilliams explained how global design and consultancy firm Arcadis hired an intern to write a script to design the overhead wiring on a railway track, \u201cWhen they changed the track layout, the script would automatically update the overhead wiring along with it,\u201d she said, adding that the results were so impressive and saved them so much time that they even included wind deflection in the same script to ensure none of the poles would hit the nearby infrastructure in bad weather.\nDigital twins\nIt\u2019s been widely reported that 70% of the total cost of ownership for a building is during the operation phase. And with the promise of cost and energy savings, it\u2019s no surprise that building owners and operators are increasingly looking to digital twins. According to Nicolas Mangon, VP AEC strategy and marketing, there are 30 billion Internet of Things (IoT) connected devices in buildings and this is growing.\nDigital twins as a technology is being pushed hard by many AEC software developers, especially Autodesk competitor Bentley Systems. Autodesk hasn\u2019t really jumped on the digital twin bandwagon yet, but as the AEC industry grapples with definitions, it had a strong and clear message for AU. According to Mangon, you get your digital twin by combining BIM plus IoT plus Artificial Intelligence (AI) and it can then help building owners and operators better manage and understand the performance of the built environment.\nIn the AEC keynote, Mangon focused on a hospital project which was fitted with smart doors, complete with sensors that can monitor smoke, temperature, humidity, particles, air pressure and more. In one possible scenario the door didn\u2019t shut, or was blocked, so triggered an alert in facilities management software BIM 360 Ops, producing a ticket for a technician, who is then sent to the exact location using an indoor map based on the Revit model.\nAs the technician is reviewing the ticket, AI indicates that there\u2019s an overdue maintenance on one of the exhaust fans that might result in a possible contamination. Dashboards can then be pulled up to show how the fan has been performing over time and if other fans are meeting requirements. Armed with this information the owner can take a proactive approach to maintenance in this and other hospitals.\nThe model-centric software used in this demonstration was not a product, rather a technology based on Autodesk Forge that can visualise and aggregate all kinds of IoT data. It was later revealed to be Project Dasher, which actually dates back to 2016, but should come to market in 2020.\nMangon stressed the importance of using AI in Digital Twins to look at large amounts of data from other sources including occupant data, social media, weather conditions, HR systems and many more. \u201c[Digital twins] will provide owners and operators with insight like never before. Our buildings will be more efficient, they will produce less C02 and they will be more resilient for the future,\u201d he said.\nWhere GIS meets BIM\nIt\u2019s been two years since Autodesk announced its partnership with Esri to build a bridge between GIS and BIM. The aim is for users of Esri software to be able to access, update and use BIM data in a spatial context throughout the lifecycle of an asset. Conversely, architects and engineers will get better access to GIS data from within Autodesk\u2019s design and construction tools.\n\u201cThe more we talk, the more we realise that our customers are demanding that engineering and design workflows aren\u2019t just in isolation, they have to have context, environmental context, social context,\u201d said Esri CEO Jack Dangermond.\nTo date, the two companies have connected up ArcGIS (Esri\u2019s mapping and analytics platform) to InfraWorks, Map 3D and Civil 3D, so infrastructure projects designed in those Autodesk products can be published directly back to ArcGIS. But the real prize is through the cloud and work is underway to connect ArcGIS data to models in BIM 360. But it\u2019s not just about bringing in contextual GIS data, it\u2019s also about being able to access Esri services.\nIn the AEC press conference, Mangon explained how some of the major AEC companies are using both Autodesk and Esri software but admitted that Autodesk and Esri could engage with these firms better. \u201cAs we go and talk to customers, sometimes we go separately, and we need to build that connective tissue, reinforcing the partnership,\u201d he said.\nFrom BIM to real time 3D\nAutodesk\u2019s partnership with Unity was the big news from AU 2018. One year later and we now see the first real fruits of this relationship with the launch of Unity Reflect. The real-time 3D tool, which can create live links to multiple Revit models and also bring across the BIM metadata, is available now for an annual subscription of $690. Support for Navisworks, ArchiCAD, Rhino and SketchUp is also in the pipeline.\nAutodesk confirmed that there will be a future integration with Autodesk BIM 360 that will allow users to flag design issues in VR and have them show up automatically in BIM 360\u2019s issue registry so they can be resolved later on. It\u2019s this kind of round-trip workflow that\u2019s lacking in most AEC-focused VR tools at the moment, even though BIM 360 integration is everywhere.\nUnity has big plans for the software, including new workflows for on-site Quality Assurance & Control and support for higher quality visuals through its High-Definition Rendering Pipeline.\nIt also has several use case workflows under consideration, including a two-way live link, drone scanning, smart cities (IoT) and construction sequencing.\nIt\u2019s refreshing to see a company share its roadmap and potential future plans so openly, but one can\u2019t help but wonder if Unity will start to tread on the toes of the company that gave it the keys to Revit and a level of integration enjoyed by no other.\nTo learn more about Unity in AEC and Unity Reflect, read AEC Magazine\u2019s in-depth article.\nHyperloop One\nWhen it comes to customer stories, few can compete with the engineering wonder that is Virgin Hyperloop One. During the AEC keynote, CTO Josh Gievel gave an insight into the futuristic transport technology that shoots electromagnetically levitated passenger and cargo pods through a depressurised tube.\nBy eliminating the impact of air-drag and friction, less energy is used making it cheaper to operate. And, of course, pods can travel at exceptionally high speeds. The ultimate aim is to reach 670mph, as fast as an aeroplane and three times as fast as a high-speed train. To date, 240mph has been achieved on a 500m test track that was built in Las Vegas in a mere six months, but the company plans to take it to commercial operation in just a few years.\nOf course, technology is playing a key role in making this a reality and the design team is using a raft of tools to design both the linear infrastructure and the vehicle design simultaneously. These include Civil 3D, InfraWorks, Revit and Inventor. Virgin Hyperloop One is also adopting BIM 360.\nConclusion\nWith its recent big-ticket acquisitions and launch of Construction Cloud, Autodesk is arming itself for the next big battleground in AEC. It has huge ambitions for construction but now faces the challenge of making Construction Cloud bigger than the sum of its parts, while fending off competition from the likes of ProCore and Trimble. Autodesk\u2019s trump card might be its connection with its design authoring tools, but with the move towards digital fabrication it will also face competition from firms with more knowledge of direct manufacture within AEC, such as Dassault Syst\u00e8mes Solidworks and Catia. (You can read more about this topic in this AEC Magazine article).\nThe huge investment and pursuit of cloud services has come at some cost to the development of its more mature products, especially in the discipline of architecture. Those with Revit would have noticed the depth of yearly additions suffer. Also, the next generation BIM and collaborative technology Project Plasma (formerly Quantum) has been stretched out as more of a multi-year project.\nHowever, Autodesk has a renewed focus on design automation and generative design. Making it more accessible with Project Refinery and Dynamo Player could help realise the potential of what has been slow burners in terms of technology adoption. Digital twins, currently one of the buzzwords in AEC and a big focal point for Bentley Systems, is also on the agenda, and it will be interesting to see what Autodesk delivers in 2020.\nBut Autodesk has also set some boundaries. With its close relationship with Unity and the plethora of VR collaboration tools that now plug into BIM 360, it seems more than happy to let third parties handle the gamification and expansion of BIM into real time visualisation, virtual reality and mixed reality, while it manages the data from the cloud.\nAs Autodesk continues to branch out from design, the next few years are going to be critical as it tries to reposition itself as a company that takes every bit of architecture, engineering and construction as seriously as the others.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/hp-skyroom-10\/","title":"HP SkyRoom 1.0","date":1258934400000,"text":"Video conferencing and real-time collaboration for 3D CAD is the name of the game with HP\u2019s long awaited SkyRoom. It is an exciting debut, but Greg Corke is already calling out for Version 2.0.\nA few years back \u2018collaboration\u2019 was THE marketing buzzword. Joe collaborated with Jane and Jane collaborated with Jack \u2014 all using the latest collaborative design software. Today, while the hype has settled down and CAD software companies have new marketing check boxes to tick, one may argue that the \u2018c\u2019 word has never been more relevant.\nRestrictions on business travel \u2014 for economic or environmental reasons \u2014 mean that the ability to collaborate effectively with geographically dispersed colleagues is becoming increasingly important. While video conferencing and online collaboration tools have been around for over a decade, with its new SkyRoom technology HP thinks it has something special on its hands.\nSkyRoom provides a collaborative design environment where users can share desktops and talk face to face with extended design teams using standard PC webcams and microphones. While mainstream tools like Microsoft Meeting and GoToMeeting already offer this type of functionality, HP believes it has the edge when it comes to working with interactive 3D datasets. In order to maintain the smooth appearance of 3D models when they are manipulated on screen, a huge amount of data needs to be sent and it needs to be sent quickly and processed efficiently.\nRemote Graphics software\nSending vast amounts of information through the ether to different locations is no trivial task and this is where HP\u2019s Remote Graphics Software (RGS) comes into play. Originally designed to send data back from Mars, the technology uses highly efficient compression algorithms to stream pixel data.\nStarting off on the presenter\u2019s machine SkyRoom works by monitoring and updating only changes in screen appearance \u2014 not the entire display \u2014 then compresses the information before sending it to the participants, where it is decompressed and updated. In this way network traffic is said to be greatly reduced.\nHP is focusing the technology on local networks but the reach of SkyRoom can also be extended beyond the firewall using Virtual Private Networks (VPNs) and work on expanding this to any TCP\/IP connection is already under way.\nObviously bandwidth is an issue and at 300-400k\/sec (up and down), the minimum requirements for SkyRoom are not trivial. As the compression algorithm is multi-threaded, multi core PCs are also required. However, powerful 3D graphics cards are not mandatory in every machine. As SkyRoom only streams 2D pixel data and not 3D vector data, it is only the host workstation that needs a 3D CAD-capable graphics card. The participant\u2019s machines just need a 2D graphics card capable of playing HD video.\nSkyRoom\u2019s use of 2D pixel data also has added security benefits. Because intelligent vector-based CAD geometry is never shared, confidential data will not \u2018accidentally\u2019 fall into the wrong hands.\nSetting up a collaborative session is incredibly easy to do. The presenter can invite up to three team members from a \u2018buddy\u2019 list and once they accept and face-to-face contact has been established the host \u2018ring fences\u2019 an area on his or her desktop to share. This could be a simple document or the modelling window in a 3D application. The view inside this \u2018bounding box\u2019 is then displayed on each participant\u2019s screen and constantly updated in real time. In demonstrations we have seen over local networks there is no discernible lag and both the 3D model and streamed video are incredibly smooth. However, these were canned demos and we have yet to see this technology in action over VPN.\nOne of the most attractive things about SkyRoom is that it comes free with every new HP workstation. However, SkyRoom is not limited to HP hardware and is compatible with most dual core Windows machines regardless of brand. In this case it is available for purchase for \u00a3100 per seat.\nLimitations\nWhile SkyRoom\u2019s beauty is certainly in its simplicity, there are some significant limitations when it comes to collaborating effectively. Participants can not take control of the host\u2019s keyboard and mouse, and there are no markup tools available. These omissions are certain to make it hard for participants to discuss specific parts or design elements, and feedback is limited to verbal instructions. In addition, as the software can only be used externally using VPN, careful planning will be required to make sure all design, engineering and supply chain participants can be hooked up to the system.\nConclusion\nEver since HP launched its Remote Graphics Software (RGS) it has been crying out for someone to get their claws into it and turn it into a marketable tool for collaborative design. This has now happened with HP SkyRoom, where up to four participants can collaborate on a design and experience the benefits of a face-to-face meeting, albeit a virtual face-to-face meeting.\nWhile this initial release shows much promise it is still a way off from being the finished product. Face-to-face discussions are certainly an important element of collaboration, but SkyRoom lacks those all-important tools required to get every participant directly involved with the 3D geometry, though we are told this will be addressed in future releases.\nDespite its limitations, SkyRoom is still an interesting technology \u2014 and becomes even more so when you consider it comes free with every new HP workstation.\nAt a time where users are looking to get more out of their technology, this is a certainly a major value add for HP in the increasingly competitive workstation sector.","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/eads-space-wins-contract-to-define-mars-sample-return-missions\/","title":"EADS Space wins contract to define Mars sample return missions","date":1076457600000,"text":"EADS Space has been awarded a EUR600k Study by ESA to carry out the first definition of a European Mars Sample Return (MSR) mission. The study will benefit from the combined resources of EADS Astrium and EADS Space Transportation.\nWhile EADS Astrium will define the overall mission and the spacecraft, EADS Space Transportation will be responsible for defining re-entry systems and a \u2018Mars Ascent Vehicle\u2019 \u2013 a small rocket to carry the precious sample up through the Martian atmosphere. European astronauts may land on Mars one day, but getting them there and safely returning them to Earth will involve many steps and numerous technical challenges in propulsion, structures, computers and software. It will require sophisticated spacecraft to escape from Earth\u2019s orbit; fly to Mars, survive atmospheric entry and landing; operate on the surface; take-off; return to Earth and then finally get the crew back on terra firma.\nLong before this can be accomplished some key technologies must be demonstrated. The best way to do this is to fly a robotic mission with a scaled-down version of the eventual manned mission. This is exactly the goal of Mars Sample Return, the second flagship mission of the European Space Agency\u2019s Aurora planetary exploration initiative and one of the most eagerly awaited future space missions for the planetary scientists. Because Martian winds have transported dust across the planet\u2019s surface over millions of years, the MSR sample could include particles from many different sources, representing a wide variety of rock types and ages, like grains of sand on a beach.\nEach granule could offer completely different insights into the rich geologic past of the Red Planet. Scientists could now \u201clook at the sample as if each grain were a rock,\u201d said Professor Colin Pillinger of the Open University. This would build on the decades of research already carried out on lunar rock samples. EADS Space has used its unique heritage in building launch vehicles, planetary spacecraft and re-entry systems, combined with a deep understanding of the science goals to win the ESA mission study.\nESA\u2019s Aurora Project Manager Bruno Gardini said \u201cThe Mars Sample Return mission is one of the most challenging missions ever considered by ESA. Not only does it include many new technologies and four or five different spacecraft, but it is also a mission of tremendous scientific importance and the first robotic mission with a similar profile to a possible human expedition to Mars.\u201d Business Manager for new space science projects at EADS Astrium, Dr Dave Parker said: \u201cMars Sample Return is the holy grail for planetary scientists.\nIt has been studied in the US for some years, but now Europe will tackle the technical and cost problems with renewed vigour.\u201d\nMarie-Claire Perkinson, Senior Systems Engineer at EADS Astrium, Stevenage, leading the study said. \u201cMars Sample Return has so many technical challenges that we need an outstanding group of experts. Our industrial team includes EADS Space in France; Galileo Avionica in Italy, Sener in Spain and Utopia Consultancies in Germany.\u201d\nOnce ESA raises the appropriate support and funds for the implementation of the MSR mission, launch could be as early as 2011. EADS Astrium is wholly owned by EADS SPACE. In 2002 EADS SPACE had a turnover of EUR2.2 billion and 12,300 employees in France, Germany, the United Kingdom and Spain.","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/pci-geomatics-upgrades-geomatica-software\/","title":"PCI Geomatics upgrades Geomatica Software","date":1191369600000,"text":"Richmond Hill, Canada, October 1, 2007: PCI Geomatics, developer of image-centric software and solutions for the geospatial industry, announced the release of Geomatica 10.1.1. This release features notable enhancements to sensor support as well as additional functionality within the software.\nSupported in Geomatica 10.1.1 is Kompsat-2, the high-resolution satellite belonging to the Korean Aerospace Research Institute (KARI). KOMPSAT-2 acquires imagery in black and white (Pan) at a resolution of 1 m, and in colour (MS) across 4 bands in the visible (red, green, blue) and near-infrared at a resolution of 4 m. Support for ALOS data products has also been extended to include ERSDAC PALSAR level 1.5 long version, level 4.1 and level 4.2 data.\nOther satellites fully supported in this release include:\nIn addition, PCI Geomatics\u2019 Generic Database (GDB) Technology has been extended, allowing for the exporting of raster and vector layers into KML format with direct loading of layers into Google Earth (version 4.0 or greater). The GDB Technology will also support LASF version 1.0 for both reading and writing. A new RMOVERLAP algorithm has been added to the Modeler and EASI environments which simplify cutlines generated by OrthoEngine. By removing the overlap between cutline polygons, cutlines are easier to edit and are more intuitive. The GRNEHN algorithm is now included in the pan sharpening workflow, which produces a more natural dataset and is a better match with actual photos, as it improves the green band in the output image.\n\u201cBy focusing this update on sensor support, we are providing our customers with the most up-to-date tools to incorporate data from the latest high resolution satellites into their workflows,\u201d said Peter Hazlett, Product Manager for Geomatica.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/digital-twins-for-a-sustainable-built-environment\/","title":"Digital twins for a sustainable built environment","date":1558483200000,"text":"As the world strives to meet climate change targets, the role of digital twins is set to become increasingly important, writes Don McLean, CEO & founder, IES\nThe digital twin \u2013 a digital reproduction of a physical entity, linked by sensors \u2013 will become increasingly important as we attempt to improve the energy efficiency of our building stock in order to meet climate change targets. The UK Government\u2019s Industrial Strategy Clean Growth Grand Challenge (tinyurl.com\/UKgreener) recognises that the UK is already playing a leading role in providing the technologies, innovations, goods and services for a Clean Growth future, but within the construction industry we can do more.\nOur buildings have changed significantly over recent years, but the way we design, handover and operate them unfortunately has not. The building industry therefore needs to catch up with other industries in their use of digital technology and data.\nThe good news is this is beginning to happen. There are many pioneering projects, both within the UK and globally, that are pushing the boundaries in the use of digital twin technology to create better performing, more sustainable buildings and communities.\nFor example, at IES we have used our latest environmental digital twin technology \u2013 the Intelligent Communities Lifecycle (ICL) \u2013 to discover 12% energy savings for an existing net-zero energy building in the tropics. That same technology has also been used to create a Virtual Campus Energy Model to support the Dundalk institute of Technology\u2019s Net Zero Energy Campus aspiration, providing analysis and understanding of the dynamics within their distributed energy network.\nUndoubtedly, digital twin technology and other technological advances have the potential to revolutionise how we manage, interact and operate the buildings in our local communities, campuses and cities. However, this technology isn\u2019t yet being utilised to its full potential.\nBuilding simulation software has been around for decades and architects, engineers, construction companies and city planners have long used building information modelling software to help them design, construct and operate buildings.\nHowever, with the addition of real-time sensors, big data and cloud computing, it\u2019s now possible to create digital twins of entire communities and simulate how things will look and interact in a huge range of different scenarios. For example: \u201cWhat opportunity is there to use renewables?\u201d and \u201cWhat savings could be achieved through a community-wide solution, such as a district heating system?\u201d\nThroughout development of the ICL, we\u2019ve been involved in many ground-breaking pilot projects and rigorous testing to bring the technology to the construction industry. Some of the most innovative pilot projects are Project SCENe in Nottingham, UK and the Nanyang Technological University (NTU) EcoCampus in Singapore.\nProject SCENe: Trent Basin\nTrent Basin is a low-energy community located within Nottingham Waterside. The development is supported by the Energy Research Accelerator and the Innovate UK funded Project SCENe, a research project led by the University of Nottingham and ATKearney. It is home to a ground-breaking energy project, where energy is being stored on site in the largest community energy battery in Europe.\nIES used digital twin technology to create an interactive platform that enables the Trent Basin community to visualise its energy data in real-time. The platform provides information on renewable energy generation and storage, alongside energy consumption data, and general information about the homes.\n\\The aim of the 3D Community Interaction Model was to provide a visual tool that promotes public engagement in the community energy scheme and communicates the results of this low energy housing development. It integrates real-time data of the energy used, generated and stored at the Trent Basin, allows residents to compare household-level data with the community average and see how much energy the project is producing and selling to the grid.\nThe project makes use of cutting-edge smart home and Internet of Things technologies to better understand and predict energy use and behaviour. This provides residents with information they need to make informed decisions and to help optimise the operation of the community energy scheme.\nWhether using the online platform or the 147-Inch Touch Screen, residents can move virtually around the real-life site, see how much energy is being generated, assess the charging state of the battery in real-time and compare this with the other real-time data available such as the weather.\nThe aim is to make energy easy and compelling to understand, to help citizens realise its potential as an essential component for wellbeing and resilience. The touch screen is part of a suite of methods to support this, including voice-activated helpers, bespoke and mainstream social media platforms, a customised smart metering app compatible on any smart device, and community-based activities.\nNTU EcoCampus, Singapore\nIES delivered a 3D masterplanning and visualisation model, virtual testing and building performance optimisation for Nanyang Technological University (NTU)\u2019s flagship EcoCampus. Delivered in two phases, the project used digital twin technology to provide high-level visualisation and analysis of testbed energy reduction technologies on site, before delving into detailed simulation and calibrated modelling of 21 campus buildings.\nNTU firstly wanted to understand, at campus level, which testbed solutions were performing the best, and identify the optimum scale and location for their deployment.\nThe EcoCampus initiative covers the entire NTU 200-hectare campus and adjoining 50-hectare JTC Corporation CleanTech Business Park. There are over 200 buildings on site with a 1.1million m\u00b2 floor area.\nPhase 1 of the project concentrated on creating a masterplanning model of the EcoCampus, complete with energy signatures for each building on the campus.\nThe model was accurate to 91% for total energy consumption and 97% for chiller energy consumption. A corresponding online cloud based Campus Information Model for communication and engagement with campus staff and students was also created and connected to the masterplanning model for automatic updates.\nThe masterplanning model was used as a baseline to simulate and analyse testbed technologies ranging from improved thermal performance of the building envelope, to lighting sensors, chiller optimisation and smart plugs that turn equipment off out of hours. These measures combined reduced energy consumption across the campus by 10%, saving $3.9M and 8.2kt of carbon.\nIn Phase 2, the implementation stage, the best solutions from the Phase 1 \u2018testbed\u2019 were chosen and applied.\nUsing real operational data from utilities and NTU\u2019s Building Management Systems (BMS), IES identified opportunities to achieve optimal performance in existing buildings across the campus, using its innovative Ci2 (Collect, Investigate, Compare, Invest) process.\nDuring the \u2018Collect\u2019 and \u2018Investigate\u2019 stage, building information was gathered and operational data imported into IES\u2019 operational data management and analysis tool, iSCAN, to investigate issues\/faults across a selection of 21 buildings on the NTU campus.\nVirtual models were created in the IES Virtual Environment (VE) for each of the buildings and calibrated using the operational data. These models established an accurate baseline for the existing buildings in operation, enabling IES to \u2018Compare\u2019 and determine potential savings for a range of technologies in the \u2018Invest\u2019 stage.\nThe results demonstrated that the technologies simulated could achieve 31% average energy savings and a total cost saving of approximately $4.7million.\nConclusion\nProject examples like these prove that digital twin technology is already making a significant impact in reducing the built environment\u2019s carbon footprint, and these numbers are continually rising as more and more building professionals understand the social, environmental and economic benefits of using this technology and integrated performance analysis tools. We should be proud of what we\u2019ve achieved so far. However, with recent climate reports issuing stark warnings on the urgency to act on climate change (ipcc.ch\/sr15), the call to action is clear. Let\u2019s evolve in our use of digital technologies and make a significant impact.\nWhat is a digital twin?\nThe digital twin \u2013 a digital reproduction of a physical entity, linked by sensors \u2013 is a new, exciting tech development that is growing in momentum, not only in construction, but across all industries.\nIn the built environment, Digital twins act as a live digital model of a physical asset and can function as essential problem-solvers, providing decision support information needed to improve asset performance, influence future building design and ultimately reduce risk.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/reality-capture-modelling\/leica-blk2go\/","title":"Leica BLK2GO","date":1591833600000,"text":"As demand for more portable, rapid scanning solutions increases, Leica has unleashed its first, handheld SLAM device, the BLK2GO, together with a new cloud-based visualisation and collaboration platform, HxDR. Martyn Day reports\nThe laser scanning market tends to move glacially. It\u2019s an old and mature technology, and revolution is not in its nature. Therehave been some glimpses of excitement over the years \u2014 when Faro brought out a \u00a320k scanner that could fit in an aeroplane carry on, or when the major CAD software developers built point cloud engines into their core BIM tools, but critical mass has never been achieved.\nThen in 2016 Leica did something atypical \u2013 it brought out the ridiculously gorgeous \u00a315k BLK360. The compact laser scanner was ideal for scanning interiors and the device was tied into the equally innovative Autodesk Recap on an iPad, but just as we thought this might be a real catalyst for change, no other firms joined in at that price point. Laser scanning continued to stubbornly stay out of the reach of the masses, compounded by the fact that Leica couldn\u2019t make enough of the BLK360 to satisfy the demand. Now Leica has a new product and its styling looks like it\u2019s from the future. Could this be the product that finally democratises point cloud capture?\nBLK2GO\nThe first thing that has to be said is that Leica must be employing an incredibly talented industrial design team. Both the BLK360 and the new BLK2GO are the most beautiful looking scanning devices that have ever been made. Even if you didn\u2019t know what the BLK2GO did, you\u2019d still want one and would want to hold it and show it to your friends like it was a Faberg\u00e9 egg.\nThe BLK360 is a static laser scanning device for rapid point cloud capture over short distances. In contrast, the new BLK2GO is highly portable and adapts SLAM (Simultaneous Localisation And Mapping) technology, which was originally developed for robots and autonomous vehicles.\nThere is a price for this liberating scanning technology over a standard tripodmounted laser scanner and that\u2019s accuracy. SLAM point clouds are typically accurate to around 20mm, which is still incredible when you consider a site that would take a day to capture through traditional methods, might be done ten times faster with a SLAM device. However, this limitation means there will be times when the BLK2GO isn\u2019t appropriate compared to traditional surveying.\nLeica\u2019s BLK2GO has been designed to be as simple to use as possible. Featuring one button operation, it wirelessly links with a Leica iPhone app, so the operator can see in real-time the data it gathers in both 2D and 3D.\nThe device features enough internal storage for 24 hours of scanning (compressed), 6 hours uncompressed and has an exchangeable battery which lasts for approximately 50 minutes. The range, however, is not huge \u2014 from 0.5m to 25m \u2014 but this is a SLAM device so your feet can do the work. It combines a 420,000 pts per second laser scanner with a 12-megapixel camera, together with an additional 3 camera panoramic vision system.\nLeica states that the BLK2GO can go down to 6-15mm accuracy and to achieve the highest accuracy, its case can be turned into a base for static scans. While the iPhone app is a useful tool for onsite feedback, the BLK version of Leica Cyclone is recommended for importation and scan clean up.\nEverything from the design of the device, to the ease of one button operation and the speed make the BLK2GO a highly desirable surveying and data capture tool. The nature of the scanned data makes SLAM scans look a bit different to traditional point clouds, with more visible banding in the variance of point cloud density, but this does not impact the quality of the data captured.\nSo what\u2019s the downside? Well, while this technology is something that everyone in the industry would absolutely love to have, it does cost around \u00a340k a pop. For a surveying firm, this is a no brainer \u2014 it has the potential to deliver a huge productivity benefit. However, as far as liberating and democratising point cloud capture within the industry, I am afraid this beautiful device is not going to be the one to make that happen.\nThe BLK2GO is very much aimed at the traditional surveying firms or, as Leica told us, Hollywood, as a number of films have used the device to capture sets and scenes between takes. It will certainly lead to quicker, less obtrusive, more frequent scans and might bring the cost of data capture services down.\nHxDR\nAt CES, Leica unveiled its first foray into the cloud, a browser-based service called HxDR which will be a platform for uploading, registering and accessing customer project point clouds or even city scale models for digital twins.\nWith the introduction of technologies such as the BLK2GO, the idea is that this data can be uploaded to the cloud from site for post processing and giving teams access to the captured data.\nAs it stands, the system is very much in development and will not compete with authoring tools like Cyclone, which would still be used to do a lot of the grunt work in filtering and processing the data for uploading.\nIn some ways, HxDR could be seen as the \u2018Unreal\u2019 or \u2018Unity\u2019 for point clouds \u2013 the ultimate aim being the automatic registration and production of an accurate, detailed and textured model with minimal user interaction, enabling rapid scan-to-model, especially from BLK products. This could be created from data derived from many scans and types of devices to create what Leica calls a \u2018supermesh\u2019. As an example of capability, Leica currently has the whole of Paris scanned, textured and in the system at high resolution.\nHxDR will be a subscription-based service, allocating a defined level of cloud disk space and a number of seats. Users can upload project work and use the service to share them. There will be many other possibilities as the platform and API develops. A company could use the technology to sell access to city models it has created, or there could be layers for AI post processing or the ultimate goal of having real Scan to BIM capabilities, which Leica is keenly looking at.\nAs part of the Hexagon group of companies, there are a number of initiatives to address Scan-to-BIM, including one based on AI that was announced between Leica and BricsCAD last October. When this eventually arrives, it will be a game changer.\nConclusion\nAs a company, Leica is certainly shaking off its image of perhaps being the least reactive of the scanning companies. It is trialling price points and delivering technologies and designs that are leading the industry, while aggressively looking at addressing rapid scan to textured mesh. BLK2GO and HxDR are two essential components in that plan but they also work happily in traditional desktop workflows. I\u2019m fascinated to see what the company does next.\nThe Leica BLK360 is still the closest scanner at what we would deem to be a point cloud liberation price point, and that product is so popular that Leica can\u2019t make enough of them to meet demand. Even the BLK360s that get refurbished all get snapped up.\nThis should be a sign to someone out there that there is a volume need for data capture. It might well be that the scanning revolution will really start from the consumer end of the spectrum, as our phones and tablet computers start to have LiDAR capability built in.\nWith every generation of phone, we know the quality and capabilities will evolve and rapid innovations in autonomous vehicles, robots and consumer AR are driving the development of low-cost LiDAR technology solutions.\nIt\u2019s all starting to look like it\u2019s within reach for all, but it might be trailblazed by games and consumer technology.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/dji-the-drone-ranger-scanning\/","title":"DJI: the drone ranger","date":1512000000000,"text":"In early November, Chinese drone developer DJI held its second annual US event, Airworks, in Denver, Colorado and outlined new commercial use-cases, as Martyn Day reports.\nIn today\u2019s society, the idea of drones gets a mixed response. People tend to think of military uses, sinister surveillance tactics, Amazon\u2019s work on using drones to make deliveries, or the potential risks that drones pose to commercial aircraft.\nThe reality is that drones are incredibly popular with consumers, but also have many possible industrial applications, most of which have yet to see mainstream adoption.\nChina-based DJI is the leader in a rapidly growing market for drones. According to Gartner, global drone unit sales grew an estimated 60 percent last year to 2.2 million, and revenues grew 36 percent to $4.5 billion. That market is projected to grow to almost 3 million units and more than $6 billion in revenue this year.\nDJI dominates the market. Its nearest competitors can each only muster single-digit market shares. The company has over 1,500 engineers working on research and development and seems to come out with a new drone about every six months, with each new introduction making gains in terms of numbers of sensors, speed, longevity and offering new sizes and capabilities. Now, DJI is focusing on expanding the commercial use of its drones within construction, infrastructure, agriculture, public safety and energy.\nIn recognition of the emerging role of drones in the AEC sector, CAD companies such as Autodesk and Bentley have accelerated their development of laser scanning and photogrammetry capabilities (Reality Capture in Autodesk parlance, ContextCapture in Bentley speak), enabling the capture of as-built structures, pre-existing terrain, building sites and infrastructure.\nThis has led to partnerships with drone firms, such as Skycatch, which offers drone surveys, model making and data analysis based on the DJI drone platform. As desktop applications become enabled to handle 3D data that has been captured, new opportunities are opening up.\nDrone use-cases take off\nThe race for new capabilities looks set to continue. With large firms now moving to deploy drones for inspection tasks, DJI has introduced a new cloud-based application called FlightHub to help keep track of pilots, tasks, drones and flight data across an enterprise, from anywhere on the planet.\nUsing a secure HTTPS link from the DJI Pilot application, data is sent to the cloud, where it can be remotely accessed. The cloud platform brings together flight logs, user information, real-time telemetry, an ability to see a \u2018live view\u2019 from drone cameras, together with team and hardware management tools. The subscription service comes in three flavours \u2014 Basic, Advanced and Enterprise \u2014 with prices starting from $99 per month.\nWhile DJI has many of its own engineers working on its drone technology, the company has also recently opened up its in-flight operating system with a software development kit [SDK]. This enables third-party, custom-built applications to be run in-flight that either access sensors or flight controls, support in-flight processing of data, or permit the inclusion of other hardware, such as LiDAR systems, thermal cameras, RFID scanners or additional GPUs for on-board processing.\nIt would seem that this has created a burgeoning ecosystem for the developer community to build commercial applications for mission-specific uses. For example, insurance firms are keen to use drones to quickly assess damage to areas with many buildings, speeding up damage assessment, when situations such as hurricanes, tornadoes and forest fires strike, as has been so common during 2017.\nUS network operator Verizon has been won over by drones \u2014 so much so that it has acquired several firms that help it manage and inspect its millions of antennas and poles. That has allowed Verizon to rid itself of 30,000 inspection trucks costing $50,000 each, replacing them with an army of drones priced at $1,000 each.\nFor DJI, SDK usage is exploding, with over 2.7 million activations in 2017, up from 1 million last year when the kit was first introduced. The latest version allows developers to tap into drones\u2019 visual recognition systems, helping developers code unique autonomous (in other words, self-flying) drone solutions.\nThe company\u2019s heavy-duty pro drones, the Wind 4 and Wind 8, can also be customised using the SDK for more complex tasks and unique sensor payloads. The Wind 4 can lift 13.5kg, for example, while the Wind 8 octocopter offers increased stability together with redundancy.\nAt DJI\u2019s Airworks conference, there was a lot of talk concerning big data and analysis. Collecting imagery is just the start of the process. While the computing power found on board the drone is focused on flying (making sure the drone doesn\u2019t hit anything and getting it back to where its mission started) additional processing can be added and accessed via the SDK. Alternatively, the data might be sent to the cloud and processed using Artificial Intelligence (AI) solutions such as IBM Watson, in order to identify all manner of situations \u2014 landslides, escaping gases, fires and so on. Data ecosystems are now being built around drone inspection.\nDJI also demonstrated an application it is developing for filmmakers to assist with pre-production and location capture. Called Project Vertex, users will be able to fly a drone over a site to capture an environment in photos that are then converted to a 3D model. Then, using the 3D model, it\u2019s possible to examine the location in detail in real time and plan camera paths, allowing for lenses with different focal points. When the director is happy with the flight path and view through a scene, this can be uploaded to a drone, which will meticulously follow the path. This could just as easily be used on construction sites and proposed mixed reality models for architecture.\nDevelopers get involved\nA number of interesting third-party developers exhibited at Airworks, including:\nPix4D offers mobile, desktop and cloud-based solutions that will capture, process, analyse and share models for surveying, construction, real estate and agriculture. The company has a new technology, Pix4Dbim, which delivers accurate photogrammetry from drone-captured imagery, delivering precise 2D orthomosaics and 3D mesh\/models using machine learning. Pix4D\u2019s system can identify buildings, trees, hard ground surfaces, rough ground and human-made objects and, at the the click of a button, automatically classify the contents of dense point clouds into these categories.\nPropeller Aero gave an interesting presentation on how it is helping landfill firms maximise the use of land with regular scanning, as well as assessing the height of landfill and calculating natural subsidence. The company offers construction firms the ability to generate topological maps, generate cut and fills and track project progress against design, measuring volumes, distances, grades and heights. Propeller Aero recently signed a deal with Trimble for global distribution of its cloud-based visualisation platform and is being integrated with Trimble\u2019s Connected Site solutions.\nDrone Deploy provides image processing, data storage, real-time sharable drone maps and 3D models through its enterprise platform. With a focus on workflows, the company provides accurate drone-based surveys, project monitoring and measurement. It also integrates point clouds with BIM workflows (it is an Autodesk Forge developer), enabling construction sites to be compared to BIM designs. The company has a free app for controlling automated flight paths, map generation and creation of 3D models, together with a portal for sharing data.\nPrecision Hawk has built a platform that supports both multi-rotor and fixed wing drones, together with a wide array of sensors (photo, video, 3 or 5 band, LiDAR, thermal and hyperspectral). The company enables the collection of data through autonomous drone flights, offers survey reviews in-field and rapid processing, modelling and reporting for additional analysis. Precision Hawk\u2019s drone safety platform provides real-time airspace and ground obstacle data for drone safety, to reduce risks to manned aircraft and airports. The company also provides clients with the services of experienced pilots and data wranglers and is the only firm authorised by the US Federal Aviation Administration (FAA) to fly beyond visual line of sight.\nLegal issues\nWith the explosion of consumer drones and with drone flights now outnumbering commercial flights two to one, there have been some negative stories of conflicts in airspace and misuse.\nThis was a running theme throughout the conference: presenters from the Menlo Park Fire Brigade, for example, reported that even they had trouble getting the FAA to let them use drones to track forest fires.\nThe industry is keen to train and certificate professional pilots, but current laws still mean that drones cannot typically fly beyond visual range and special permission is required to fly above crowds.\nThere was much talk about lobbying the US government and the FAA\u2019s work in testing drone impacts on cadavers, in order to estimate the levels of damage that a malfunctioning drone could cause. Other attendees spoke of implementing measures to liberate the market by introducing the same kind of professionalism that applies to more established forms of aviation.\nIn response to concerns around risk, DJI has introduced AeroScope, a ready-to-use system to enable authorities to identify, track and monitor airborne drones, especially near sensitive locations or in areas of safety concern, such as airports. The solution acts like an \u2018electronic license plate\u2019 for drones and helps to ensure they remain in safe and secure airspace, transmitting registration or serial numbers, as well as basic telemetry, including location, altitude, speed and direction. The company also is working on geo-locking technology, to prevent drones flying into commercial airspace or sensitive locations.\nIn the UK, the government has just announced proposals to make pilots of drones over 250g in weight take a safety awareness course and is looking to ban drones from flying over 400ft and near airports, with current limitations stating they must stay at least 40 metres from other people and 50 metres from buildings. This new legislation was based on 81 reported incidents during 2017, a figure that doubled from 2016.\nDrone futures\nWhile it\u2019s amazing to see what drones can do today, there was one presentation in particular that looked at what might be commercially available in the next few years. DJI has invested in AutoModality, a New York-based firm that is developing an autonomous bridge inspection drone, based on the DJI platform. The new system enables fully autonomous close-up infrastructure inspection and assists workers by accessing difficult and dangerous locations on a structure on their behalf, allowing them to remain safely on the ground.\nAutoModality adds additional sensors to the guards of the drone blades to help with assessing proximity to objects and utilises an attached LiDAR scanner, feeding an extra Nvidia GPU powered brain. The team built a mock-up of the interior of a bridge structure to test-fly the autonomous drone, occasionally pushing it with a stick to see if it would autocorrect. The drone flew down the narrow channel with just inches to spare.\nFor live trials, AutoModality first attempted inspection using manually flown drones but \u2018lost\u2019 two of these in the process. Large metal objects like bridges tend to warp the GPS signal or introduce blind spots, causing loss of control. With its new autonomous drone, the team achieved a full 70ft flight down a narrow channel on the bridge, avoiding collisions and returning with inspection footage.\nOne of the biggest limitations of drone technology remains battery life. By adding extra sensors and GPU processing, AutoModality shrunk the drone\u2019s flight time to just 15 minutes. Adding extra battery capacity for longer flight times, however, is perhaps one of the biggest issues for autonomous flight. Range is limited and inspection teams are forced to fly the drone in close proximity to its target. However, like autonomous cars, autonomous drones are clearly the direction in which the industry is heading.\nConclusion\nIts success in consumer drones has enabled DJI to invest in producing ever-smarter drones at remarkably low prices. But it\u2019s a double-edged sword, with drones unleashed in airspace with no real solutions to the challenges of monitoring or tracing owners. This has left institutions responsible for airspace management playing catch-up and is inhibiting professional drone usage.\nDJI is making efforts to develop controls and tools for the monitoring of drone flights, to lessen the risks they pose. With commercial drone usage predicted to grow dramatically, government agencies need to derive workable on-demand measures for professional missions.\nThe success of DJI\u2019s SDK, meanwhile, is bringing tailored applications to the construction and infrastructure market, from initial site capture prior to design, all the way to monitoring assets during their lifecycle.\nIn many of the talks given by developers at Airworks, big data processing was one of the most common topics, with some of this work being performed on-board, but the majority needing post-processing and analysis. With the many benefits of drones comes the problem of how to handle and use all the data they generate per flight.\nLooking ahead, with AI, machine learning and an increasing number of sensors incorporated into drones, the role of humans in piloting these machines is looking highly doubtful \u2013 just as we also see in driving. In other words, drones could be just another form of autonomous vehicle, operating above and alongside cars and trucks, with significant impact on our cities and their infrastructure.\nAccording to one speaker from Verizon, the company is contemplating a future where drones act as the cell masts of a city, intelligently steering themselves to where they are most needed in bandwidth terms, rather than being located in fixed positions. In this way, if a football game, for example, increased the density of cell demand in a certain district, extra drone-based antenna could be flown in, in order to meet temporary spikes in need. Over the coming years, many other organisations may allow drone technology to take them on similar flights of the imagination.\n\u25a0 dji.com\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/bentley-architectural-research-seminar\/","title":"Architectural Research","date":1188172800000,"text":"As part of its yearly BE event, Bentley holds a full day of seminars demonstrating how computer aided design and manufacturing technology is being used by the more adventurous practices and students. Martyn Day reports.\nI\u00ddve lost count as to how many of these events I have now attended but I have to say it\u00dds the highlight of the architectural calendar, as far as design technology presentations are concerned. While the projects on display will blow you away, it\u00dds not so much the end product that\u00dds the focus, but how the design was achieved. Refreshingly, while this is at a Bentley event, those taking the podium were not always Bentley customers.\nThe seminars have been the brainchild of Bentley\u00dds Director of Research, Dr. Robert Aish and they have provided a platform to demonstrate his Generative Component (GC) technology which has been in development at Bentley for a number of years. Some of the willing early \u00d9guinea pigs\u00dd also get to describe their experiences with the technology and these usually include industry luminaries from Foster and Partners, KPF, NBBJ, Arups and many others. Over the years the list of these interested parties has mushroomed, with this particular event now demonstrating the industry\u00dds enthusiasm for exploring new ways to use computers to assist in defining complex design.\n{mospagebreak}\nThe new ethos and demand from these practices is that CAD as a documentation tool is not good enough. And while the AEC market has been slow to adopt 3D, even these commonly available tools have been found to be lacking by the architectural early adopters. The main problem is that the current 3D tools are there to document the design in 3D and to ease the production of 2D drawings, not to enhance or optimise the model. If changes need to be made to the design, then it has to be, at best, partially rebuilt.\nTraditional 3D models do not include userfeedback, interactivity, automatic generation or variational design. Generative Components is a programmatic environment, based on top of MicroStation, which allows designers to build simple parametric skeletons, derived from standard MicroStation geometry, to produce designs and forms that can automatically generate or update, depending on the input to this underlying \u00d9frame\u00dd geometry. This isn\u00ddt the \u00d9walls, doors and windows\u00dd \u00d9intelligence\u00dd of products like Architectural Desktop, which is essentially about quickly generating 2D general assemblies from a 3D model. Generative Components works at a much lower level and isn\u00ddt bothered about the interaction of pre-defined recognisable building components. It simply concerns itself with geometry, complex relationships, control and applying complex, user-defined computations to a design.\nI have to admit it\u00dds not a technology for the faint hearted, as the learning curve requires some level of programming methodology together with an understanding of basic principles of geometry and a problem solving mind. It seems that most that have taken to using the tool are either young, ambitious and smart computer-literate architects or rocket scientists \u00b1 both of which seem in plentiful supply in leading London architectural practices.\nIf you look at the projects that have really dictated the current leading edge architectural vocabulary, from the likes of Foster, Liebskind, Gehry and Zaha Hadid, it\u00dds obvious to see that the ability to model complex surfaces and move this to digital fabrication are essential in their processes. This can only be achieved with increasing reliance on CAD systems and is a key reason why Generative Components, even while in extended Beta test, has already been used in the design of some very large projects. As I write this, Bentley has in fact, just announced that the first official version of Generative Components has shipped as part of its Select Subscription.\nSeminar program\nTo set your expectations, if you should venture to next year\u00dds Research Seminar, then plan for long day. The presentations kick off at 9am and should finish around 5.30, although this invariably runs over time as there\u00dds just so much fitted into the day. This year there were 13 presentations covering a number of projects, at various stages of completion. Here are some of my highlights.\nThe first presenter, Alastair Townsend from the intriguingly named Youmeheshe practice had the honour of going through how GC was used to design the first building to be built with the technology. The striking Cutty Sark Pavilion gets that \u00d9first-built\u00dd accolade, despite almost coming a cropper in the terrible fire that engulfed the historic tea-clipper in London a few months ago. The Pavilion is a stretched fabric design and miraculously escaped with a few minor burns which can be easily repaired. It was great to see how GC was used to optimise the building\u00dds form. We will be covering Youmeheshe and their design process in more depth in the next issue.\n{mospagebreak}\nHaving seen a GC-assisted building, the next speaker, Fabian Scheurer from \u00d9designtoproduction\u00dd Gmbh examined what I think is one of the more obvious missing links in this advanced computer geometry research, how do you fabricate your design? While there are many small rapid prototyping machines and CNC (Computer Numerically Controlled) cutters, the scale of architectural designs usually leaves current CAM strategies wanting. Architectural elements generated in the CAD system, need to be broken down into large numbers of smaller components so they can be manufactured. Scheuer gave a demonstration of how they go about automating and deriving these custom buildings systems for their CNC machines.\nWith two doses of real-world applications, Kaustuv DeBiswas a PhD student at MIT gave a demonstration on some fascinating and complex form finding work he had undertaken for dECOi\u00dds Bankside Paramorph project. DeBiswas had developed a technique to use GC to run an algorithm to find quadrilateral geometry that matched doubly curved surfaces.\nSOM is a practice that has been at the forefront of 3D. Its Freedom Tower is well publicised as being designed using Autodesk\u00dds Revit but it has also signed a massive deal for Gehry Technolgies\u00dd Digital Project, the Catia-based modelling tool. Neil Katz from the company gave a two-part presentation, inspired by the beauty that can be generated with mathematics, first by examining how GC could be used in deriving artistic tiling and surface parametric effects. The second part of his talk covered \u00d9morphology\u00dd and how surfaces can be generated to satisfy a variety of project requirements, including different materials and constraints.\nShane Burger of Grimshaws looked at the computer\/human interface, giving us an insight as to how these associative geometry models work in a team environment, given that it\u00dds not a natural fit. Grimshaws are in the process of developing a new design team methodology system that would allow models to be passed to a project team for manipulation, together with input from consultants and clients. The practice has already used some of this knowledge on the Museao del Acero in Monterrey, Mexico and on competition projects.\nNBBJ\u00dds very own Volker Mueller worked through various strategies researched for developing a complex fa?ade for a project in Kazakhstan, to aid constructability as well as economic and maintenance factors. The GC model allowed immediate feedback as to the non-planarity of surfaces.\nKPF\u00dds Lars Hesselgren didn\u00ddt give a presentation in the Architectural Research Seminar this year. I felt this was noteworthy as Lars is one of the GC pioneers and always manages to get on the presentation list! Instead two of his prot?g?es, Stylianos Ditsas and Mirco Becker stood in to give a commanding presentation on how they find form-oriented design an impediment to progress, preferring systematic approach to design derivation, being based on multiple performance constraints \u2013 radical thinking with beautiful consequences.\nThe design of the impressive Karachi Port Trust Tower was presented by Judit Kimpian of Aedas. GC was used to interrogate environmental, structural and architectural criteria and enabled the fine-tuning of this highly sculptured building Jeroen Coenders of Arup demonstrated the associative nature of structural systems built on GC. Jalal El-Ali of Buro Happold explored radical concept design using GC, with the model reacting to various environmental and structural inputs.\n{mospagebreak}\nFoster and Partner\u00dds Francis Aish, who is part of the Specialist Modelling Group, gave a presentation on design workflows which allow for the complexity of data that systems like GC will bring. If the geometry is linked to form finding and rationalising algorithms, together with performance simulation and analysis tools, then new user interfaces are required if all this information is to make sense and promote rapid evaluation for better decisions.\nFinally Axel Kilian, an MIT PhD and GC-scene regular gave a fascinating talk about is work on actuated structures \u00b1 structures that kinetically adapt after construction, using pneumatic actuators. GC was used to build possible configurations of the structure to analyse the complex control system.\nConclusion\nTo sum up this year\u00dds research seminar, there was something for everyone and on reflection, with the plethora of GC users and applications on show, the event was a subtle but very strong sales pitch for the benefits of GC technology. There were projects that were built, experts in going direct to manufacture, conceptual designs, experiences in optimisaton, first hand demonstrations on how GC eases complex geometry generation, fresh insights as to the impact on teamwork, applications in structural engineering, conceptual design and form finding. Put simply GC is allowing practices to experiment, interrogate and optimise designs that would not be feasible in any other out of the box\u00dd way and its application is extremely broad.\nwww.bentley.com\nwww.gcuser.com","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/project-one-blackfriars-road\/","title":"Project: One Blackfriars Road","date":1165449600000,"text":"Architect: Ian Simpson Architects\n(www.iansimpsonarchitects.com)\nSoftware: Bentley MicroStation V8; Engineering Configurations; Generative Components\n(www.bentley.com)\nOne Blackfriars Road is a proposed new landmark building for London. The scheme contains a 375 bedroom 6* hotel, 120 residential apartments and a publicly accessible sky deck. These elements are integrated into a slender, sculptural 65-storey tower and six storey podium building which frame a landscaped plaza.\nMicroStation was used for exploring the geometry of the form and producing the visualisations for client and public presentations.The shape of the tower is defined by a series of vertical curves that are assigned differing radii. At each level the floor is defined as the shape formed by connecting the resulting ellipses tangentially. This parametric definition of the form has meant that different solutions could be produced quickly and the resulting floor areas quantified. A mix of Generative Components and MicroStation VBA was used to generate the different versions of the tower based on the rules and compute the resulting floor areas. This allowed many more options to be explored then could otherwise have been attempted.\nWhen an option was chosen for detailed study, cladding and window panels were added to each floor plate. Again VBA macros were used to assist in this process. The resulting model was then used for visualisation studies, which were critical since this building will be a very prominent feature on the London skyline. Handling large volumes of complex geometry from early parametric studies through to a render ready model was central to this project. As a result Bentley\u00dds integrated software environment was a key enabling technology in this development process.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/keith-bentley-on-microstation-v8i\/","title":"Keith Bentley on MicroStation V8i","date":1229212800000,"text":"Bentley has just released the latest version of its flagship product, MicroStation, along with a comprehensive portfolio of applications for infrastructure. AEC Magazine interviewed Bentley\u00dds chief technology officer (CTO), Keith Bentley, on the advances made in this new version, which is called V8i.\nMartyn Day: The PR around MicroStation V8i claims it\u00dds the biggest release of MicroStation in the company\u00dds history. The same was said of V8 when it was launched. What have been the challenges?\nKeith Bentley: From this point on, I think each of our releases will be \u00d9the biggest release\u00dd for us as we grow and make more acquisitions. One thing about this release compared to, say, MicroStation V8 XM Edition, is that while there was a lot of big stuff in V8 XM, some of its main benefits were for our applications. V8i is the company\u00dds largest effort to date to get together the most programmers releasing the most code, affecting the majority of our products, being deployed by the largest number of our users.\nSo, it has been a concerted release with many more moving parts that all had to come together. I think our users will end up looking at V8i as a major breakpoint in the portfolio of Bentley products. In the V8i portfolio we have a lot of products that are used by a lot of people, and the products are interconnected. I don\u00ddt necessarily expect immediate adoption of V8i, because I don\u00ddt think overnight everyone is going to say, \u00d9I have to upgrade everything to V8i.\u00dd However, when an organisation does say, \u00d9Now is the time for us to move to V8i,\u00dd they won\u00ddt have anywhere near the issues they had in the past making a number of products work together. V8i is a big release for us. I\u00ddm sure in our press communications we say this about every release, but I really do look at this release as a big one!\nMD: With the release sounding like such an effort, customers may worry about the upheaval of upgrading.\nKB: As you can imagine, I\u00ddve been through a lot of releases and talked to many users about trying to encourage people to upgrade. As you know, in every software upgrade there comes some pain and some gain \u00b1 hopefully more of the latter. In V8i, I think the gain-vs-pain ratio is very high. For example, I think the pain point addressed by the ProjectWise \u00d9Delta File Transfer,\u00dd and the pain of slow file transfer that it solves, is bigger than the feature sets of a lot of previous generations. Companies can deploy V8i and literally the next day their projects are going to run much smoother. Moreover, the features and benefits in MicroStation V8i translate into profitability for our customers. We\u00ddre very excited about that.\nMD: The MicroStation release has been boiled down to five main features: intuitive design modelling, interactive dynamic views, intrinsic geo-coordination, integrated Print Organizer, and iterative Luxology rendering. What\u00dds new, what\u00dds updated, and what\u00dds been licensed.\nKB: I think each one has important benefits to offer our users and there\u00dds a combination of new, updated, and licensed. For example, we licensed the rendering engine from Luxology. The integration of that technology with MicroStation and with our vertical applications is going to change our market in much the same way that the digital camera changed the film market.\nTwenty years ago, there were professional photographers, who were very good at using a 35mm camera and at developing their own film. Photography was an art, and when you saw a really cool picture you were impressed.\nNowadays, people take a hundred pictures and if one of them turns out well they toss the other 99 and, in their minds, they are photographers. In terms of the way visualisation will be used in AEC projects, having something that\u00dds directly and easily accessible to all users \u00b1 and fast \u00b1 will fill an important need. One of the key features of Luxology is that it does a really good job amazingly fast. So, like with a digital camera, I think a lot more people will use it and become good at it.\nThe other items in the list of features are improvements or brand new content. Take, for example, dynamic views. Most people will compare dynamic views to Revit drawing extractions or synchronisation techniques. In differentiating between what we\u00ddve done with dynamic views and what Revit does, you need to understand that dynamic views use the same referencing capabilities that MicroStation has pioneered for a long time. The most important thing about that is the biggest downside with Revit: When you\u00ddre trying to keep your drawings in synch with your model, it all has to be stored in one place \u00b1 you\u00ddre always facing a limit of how big a project can be or how distributed a project can be. With MicroStation, you can subdivide the project whatever way you want and have as many files as you like. And with V8i, we\u00ddve taken the best of both worlds.\nIf you\u00ddre in a session of MicroStation and have, say, 30\u00b140 files that are all being referenced, and you need to edit some of them, in the past with MicroStation and AutoCAD there was one lock on one file. Now we can modify multiple files and have a transaction that spans multiple, independently lockable files. This is a big thing. You can have a large project spread across multiple offices and multiple computers and files.\nMD: Tell me a bit more about the new conceptual modelling tools \u00be what have you done there to expand MicroStation\u00dds capabilities?\nKB: We recognise that there are some incredible opportunities for using GenerativeComponents with other modelling tools: drawing modelling, component modelling, parametric modelling, even sketchy-type modelling tools. The cool thing is, we have made them all work together. In V8i, our objective was to make it so that in a distributed team, where people focus on aspects of the project using each of those techniques, when everyone comes together to collaborate, their work is all shared in DGN files.\nIn the past, we\u00ddve had people spending resources using a tool like SketchUp. SketchUp is a great tool, and it\u00dds been used by a number of our users. People say to us, \u00d9Can\u00ddt you make SketchUp work better with MicroStation?\u00dd and, yes, we do a pretty decent job of importing and exporting SketchUp files. But, we\u00ddre going to make those people much more productive by enabling them to use the powerful referencing, raster, point cloud, etc. tools in MicroStation. In other words, they can now use in their conceptual modelling, all the features that drive their decision to pick MicroStation for their engineering project.\nFrom an interface perspective, the goal was to make V8i easy to learn and use, for the people who say, \u00d9I don\u00ddt want to learn MicroStation,\u00dd or \u00d9I don\u00ddt want to install every application on my desktop; I just want to use SketchUp.\u00dd So we focussed on coming up with ways that you can use MicroStation and turn off all the features you don\u00ddt care about and just use it in a simplified way. Similarly, for users of GenerativeComponents, in MicroStation they are going to find themselves in that nice, easy environment, sketching in 3D mode, until they get to the point where they want to add more precision and use some feature modelling. Once they\u00ddve crossed into the solid modelling world, then they can use solid modelling techniques.\nWith V8i, with improved DGN referencing techniques, it will all seem pretty seamless. The two main scopes of our company \u00be comprehensive and integration \u00be are coming together in this world of design modelling.\n{mospagebreak}\nMD: Is V8i the first version based on a new foundation? Or are the foundations still being built?\nKB: No, V8i is the first version based on the XM foundation. A lot of our modelling tools, structural analysis tools, and building energy analysis tools have releases that are part of our V8i rollout. In fact, we now have the biggest array of solutions in the AEC industry \u00b1 literally. I think there\u00dds a lot more to the Bentley V8i release than Autodesk has in all of its AEC products put together.\nPlus, it all works together. In fact, we created something called the interoperability platform. In reality, that mostly happened in the XM release. So in XM, we started taking the parts of MicroStation, the parts of ProjectWise that are shared across lots of other applications, creating this programming tool kit that many programmers can use. But in XM, it really wasn\u00ddt deployed very well \u00b1 there wasn\u00ddt much that you could hold out as being of benefit for the user. But in V8i, a lot of our products incorporate the engineering interoperability platform.\nSo is V8i the first release on the new platform? V8i is actually the second release of the new platform. But when we got XM together, we didn\u00ddt have enough time to put any real meat behind all those new techniques and subsystems, beyond just getting it to work as it did before.\nThe platform was born in the generation that created XM, but most of our products didn\u00ddt roll out on it. The XM generation spanned multiple releases of MicroStation and our vertical products.\nMD: You called V8i the integration platform for your vertical products. Could you explain a bit more what this means?\nKB: Yes. I referred earlier to the interoperability platform that now exists in the V8i platform. One reason it exists is that when newly acquired companies would come to us and ask, \u00d9Now that we\u00ddre a part of Bentley, where do we go to download the integration tools that we use to do this or that?\u00dd we didn\u00ddt have a simple answer to their question.\nFrankly, every time we did an acquisition I was embarrassed by the involved explanation for how they could integrate with our other products. Everyone in a newly acquired company typically is very enthusiastic about being a good corporate citizen and just wants to know what they need to do to become one. Our answer before V8i was: \u00d9It ain\u00ddt that easy.\u00dd Now we have a solution.\nMD: Are you happy with Windows Vista as being a steady platform for CAD, or is it still an incredibly overweight, fat, system-hungry? What about 64-bit?\nKB: I think Vista has a bad reputation now, which frankly isn\u00ddt really deserved. I hate to be labeled as a Microsoft sycophant because there are a lot of things that I wish they\u00ddd do better. But, for MicroStation, Vista is the best choice.\nPerhaps the biggest problem we\u00ddve had is switching from OpenGL to Direct3D, which we didn\u00ddt necessarily have to do, but we did. Direct3D under XP can perform poorly when the user has two screens \u00be and a lot of MicroStation users have two screens. If you switch to Vista, the dual-screen support is definitely better. The 64-bit debate is two years out of date now. On a 64-bit operating system, MicroStation can take advantage of a lot more of the extra addressable memory. At this point, if you\u00ddre configuring a new computer, there aren\u00ddt really many reasons to install the 32-bit operating system. With the 64-bit version, you won\u00ddt run into as many problems with big models. Of course, with small models it doesn\u00ddt matter, but the distinction between small and big changes every year.\nMD: The DWG\/DGN announcement with Autodesk was probably the biggest announcement to hit the industry since CAD was invented. Obviously, it hasn\u00ddt made it into V8i because the announcement happened so late in the development cycle; but what timeframe are you going to be slipping that in?\nKB: The Open Design Alliance (ODA) toolkit that we use now does a pretty good job of emulating a lot of what\u00dds in AutoCAD\u00dds DWG format. Actually, the concept mismatches between MicroStation and DWG are now fairly few. Converting to Real DWG really isn\u00ddt as big a job to implement engineering-wise, but it certainly will require a lot of testing. We\u00ddve put a lot of work into making sure that when you\u00ddre editing a DWG file in MicroStation, you\u00ddre editing the MicroStation content and keeping all the DWG content updated concurrently.\nI would guess you\u00ddre going to see us implement Autodesk\u00dds Real DWG in a 2009 release of V8i.\nMD: Have you handed to Autodesk your MicroStation API in return?\nKB: Yes. They have our library, and we have theirs.\nMD: Do you have no fear about what they\u00ddre going to attempt do with the DGN access?\nKB: I think we recognise that they are going to be able to do as good a job with DGN as they want to. So file formats will not be either our or their differentiator. If anything, I think good DGN support in AutoCAD might be an advantage for us and our users. The MicroStation user will have the enviable luxury of saying, \u00d9Here\u00dds your DGN\u00dd when they work with other people who have AutoCAD.\nMD: So have you relinquished your membership in the ODA?\nKB: No, we are still members of the ODA. We still use their library; in fact, we\u00ddre still shipping it today. Yes, we\u00ddre going to replace the DWG library that we use in MicroStation, but we\u00ddre still a member of the ODA. We support them in their creation of the Open DGN libraries that they have developed from our published DGN specification documents.\nMD: What\u00dds the future of the ODA\u00dds Open DGN format now?\nKB: We answer the ODA\u00dds questions, but they don\u00ddt get any source code from us. We merely give them documentation about the format.\nMD: There\u00dds a lot of talk about cloud computing. Is there a space for CAD and Bentley in this cloud service based world?\nKB: I think there will be, specifically some of the analysis type applications that aren\u00ddt particularly interactive. I don\u00ddt necessarily think that most of what people think of as \u00d9CAD\u00dd applications will translate into cloud computing very well. But analysing data that you create, and even some data that you use in that model (e.g., environmental conditions), can be done via shared \u00d9cloud\u00dd resources. The theory of shared computing resources is old, but I think it\u00dds the future as well.\nMD: Bentley\u00dds Enterprise clientele and subscription models are very different from Autodesk\u00dds volume play. Moving forward, is this likely to fundamentally change?\nKB: Bentley\u00dds business model is an important factor in how we go about creating products and the way in which we target acquisitions. Our model is based on long-term relationships. Our subscriptions model is part of our DNA. We\u00ddre really motivated by things that marry us to users and make it a win-win.\nIf people using our products are successful, they are more likely to continue to subscribe to our programs so they can get more enabling technology through us. Our Enterprise License Subscription has been one of the most successful things we\u00ddve done in the last ten years.\nIf you want to predict the way in which Bentley will go, we\u00ddre focused on an ever-increasing scope of \u00d9things that people use when designing, building and using infrastructure\u00dd.\nWe\u00ddre not necessarily looking for license revenue; we\u00ddre looking for ways to partner with people and make them more successful.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-tom-greaves-dotproduct\/","title":"Video: NXT BLD London conference - Tom Greaves, DotProduct","date":1502150400000,"text":"Reality modelling with phones and tablets \u2013 NXT BLD London, June 2017\nUntil recently, we\u2019ve needed strong muscles, a fat wallet and patience to capture and consume the 3D reality of built environments, explains Tom. Laser scanning a construction site, for example, typically means a $50k+ investment and heavy equipment needs to be carted around the job site. In his presentation Tom shows how a new generation of mobile devices equipped with low-cost 3D sensors and computer vision software can deliver position tracking and 3D reconstruction in real time. And, best of all, they can be operated by anyone.\nView the other NXT BLD presentations\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Paul Nichols, Skanska\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Arthur Mamou-Mani, Mamou-Mani\nConstructing (and deconstructing) buildings with cable robots\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-hilmar-gunnarsson-johan-hanegraaf-arkio\/","title":"Video: NXT BLD 2019 \u2013 Hilmar Gunnarsson & Johan Hanegraaf, Arkio","date":1565049600000,"text":"Bringing architectural design into VR \u2013 NXT BLD London, June 2019\nFor decades architects have been using hand-drawn sketches, scale models and static 3D tools to create, study and communicate their designs. This process can be time consuming and early design decisions are often difficult to visualize, leading to costly misunderstandings between project stakeholders. Arkio is a collaborative VR\/AR design tool that makes this process more effective for everyone. With Arkio, designers can quickly prototype and validate their designs with project stakeholders, wherever they are in the world. All that\u2019s needed is a VR\/AR headset or just a regular tablet. Arkio enables designers to sketch out buildings and entire urban plans while experiencing and adjusting their designs at human scale. During our talk we will share some of the challenges and insights gained from developing Arkio from the ground up for architectural design in VR\/AR. We will furthermore give a live demonstration of how Arkio can be used to quickly sketch out various design options while working towards an overall program goal. We will show how easy it is for project stakeholders to participate, enabling everyone to be involved in finding the best possible design option in a more efficient way than possible before.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/16\/how-luminars-doomed-volvo-deal-helped-drag-the-company-into-bankruptcy\/","title":"How Luminar's doomed Volvo deal helped drag the company into bankruptcy | TechCrunch","date":1765843200000,"text":"In early 2023, Luminar was riding high. After going public during the pandemic and scoring a key deal with Volvo, the company had added Mercedes-Benz and Polestar as customers of its \u201clifesaving\u201d lidar sensors. Founder and CEO Austin Russell called it an \u201cinflection point,\u201d as Luminar prepped to have those sensors integrated into the first production vehicles.\nVolvo in particular was all in on the technology. The Swedish automaker, which spent decades building a brand around the idea of making the safest cars, was the first to jump at integrating the laser-based sensors in its vehicles. Volvo initially tapped Luminar to provide 39,500 lidar sensors over the life of a deal signed in 2020. In 2021, Volvo upped that to 673,000. And in 2022, Volvo upped it again, this time to 1.1 million sensors.\nThree years later, Luminar is now in bankruptcy. The company has already made a deal to sell off one subsidiary centered around semiconductors and is looking to sell its lidar business during the Chapter 11 process, which began on Monday.\nThe first batch of filings in the bankruptcy case shed new light on how Luminar\u2019s cornerstone deal with Volvo came apart \u2014 and how its undoing helped push the once-promising startup over the edge.\nBig promises, then big revisions\nLuminar made \u201csubstantial up-front investments in equipment, facilities, and workforce\u201d to meet the demand from Volvo back in 2022, according to a declaration written by Luminar\u2019s newly hired chief restructuring officer Robin Chiu. It built out a manufacturing facility in Monterrey, Mexico, and spent nearly $200 million to prepare to make its Iris lidar sensors for Volvo\u2019s EX90 SUV.\n\u201cVolvo was going to be a marquee customer, the stepping stone to introducing the company\u2019s Iris product to the broader automotive industry,\u201d one of Luminar\u2019s lawyers said during the first hearing in the bankruptcy case on Tuesday.\nBut, according to Chiu, problems were already brewing with Volvo. The automaker delayed the EX90 SUV because it needed to do more \u201csoftware testing and development,\u201d the automaker said in 2023. And in early 2024, Luminar says Volvo reduced its expected volume for Iris sensors by 75%.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nLuminar\u2019s other deals started to sour, too. Polestar (a subsidiary of Volvo) quietly gave up on integrating Luminar\u2019s lidar sensors \u201cbecause the vehicle\u2019s software ultimately could not use\u201d the features, according to Chiu. Mercedes-Benz terminated its agreement to buy Luminar\u2019s Iris sensors in November 2024 because the lidar-maker \u201cfailed to meet ambitious requirements,\u201d according to Chiu.\n(Mercedes-Benz struck up a new deal with Luminar in March 2025 for its next-generation Halo lidar, but Chiu wrote that Luminar has \u201cno go-forward projects\u201d with the German automaker at the time of bankruptcy.)\nThis left Luminar with Volvo as its lone flagship customer.\nThe company never diversified much beyond the automotive industry, shunning other applications like defense or robotics. In fact, Russell had founded Luminar in 2012 with the goal of taking lidar out of those sectors and into automotive to help accelerate the adoption of autonomous vehicles.\nIt wasn\u2019t until March of this year that Russell talked about expanding beyond automotive, as Luminar signed a deal with construction equipment company Caterpillar. Just two months later, Russell abruptly resigned following an ethics inquiry from Luminar\u2019s board of directors.\n\u201cMore bad news\u201d\nBy Chiu\u2019s account, Volvo kept promising that it would meet the lifetime order of 1.1 million units despite the reduced volume in 2024. So Luminar kept pressing forward under that assumption.\nBut signs of stress were showing. Luminar laid off 20% of its workforce in May 2024 and outsourced more of its lidar sensor manufacturing. It deepened those cuts and restructured some of its business in September 2024. Another round of layoffs came in May 2025 after Russell resigned.\nIn September, \u201cVolvo delivered more bad news,\u201d Chiu wrote. The automaker decided to offer lidar as an option on the EX90 going forward, instead of making it a standard feature as originally planned. Volvo also told Luminar that it was shelving lidar on future vehicles \u201cas a cost-cutting measure.\u201d\n\u201cThis change reduced Volvo\u2019s estimated lifetime volumes by approximately 90%,\u201d Chiu wrote.\nLuminar told Volvo on October 3 that it considered this a breach of the agreement the companies had first signed in 2020. On October 31, the dispute became public, as Luminar told shareholders in a regulatory filing that it was suspending sensor shipments to Volvo. The Swedish automaker sent Luminar a letter two weeks later, terminating the agreement.\nVolvo told TechCrunch in a statement Tuesday that it \u201cmade this decision to limit the company\u2019s supply chain risk exposure and it is a direct result of Luminar\u2019s failure to meet its contractual obligations to Volvo Cars.\u201d\n\u201cThe company\u2019s products can deliver a high level of safety and driver support, enabled by the cars\u2019 powerful core computing coupled with their advanced sensor set \u2014 with or without a lidar,\u201d a Volvo spokesperson said.\nLuminar, meanwhile, started selling lidar sensors meant for Volvo \u201cto adjacent markets in an effort to recover its sunk costs,\u201d according to Chiu\u2019s filing, but it was too little too late.\n\u201cAs its relationship with Volvo deteriorated, [Luminar] worked tirelessly to identify new customers, but was ultimately unable to enter into production with any new customers in a timely fashion,\u201d Chiu wrote. \u201cThe public Volvo dispute also resulted in a decline in sales due to broader market concerns over Luminar\u2019s financial future.\u201d\nNow the future of what\u2019s left of Luminar is in the hands of its creditors and the court. It\u2019s seeking the judge\u2019s approval to sell the semiconductor subsidiary to Quantum Computing, Inc. for $110 million, and hopes to court a number of bidders for the lidar business.\nLuminar has already had significant interest in the lidar business, according to the filing. In January, Chiu wrote, the company hired investment bank Jefferies to evaluate a sale after receiving an \u201cunsolicited acquisition proposal.\u201d Luminar received \u201cadditional unsolicited inbound expressions of interest to acquire the Company\u201d through the summer and fall \u2014 including one submitted by Russell through his new AI lab in October.\nAs TechCrunch reported Monday, Russell plans to keep bidding on Luminar\u2019s remains as the bankruptcy case moves forward. During Tuesday\u2019s hearing, a lawyer for Luminar said it is \u201cdeep into the sale process\u201d and \u201cin negotiations with\u201d several potential bidders.\nThis story has been updated with a statement from Volvo and information from Luminar\u2019s first bankruptcy hearing.","source":"techcrunch.com"}
{"url":"https:\/\/aecmag.com\/news\/tekla-structures-v14\/","title":"Tekla Structures v14","date":1219449600000,"text":"Tekla Structures is an end-to-end structural BIM solution providing a dedicated 3D modelling environment for conceptual design, engineering, round trip analysis, detailing and beyond. Greg Corke takes a closer look at the latest release, version 14.\nBIM (Building Information Modelling) is at a tipping point in the architectural sector with increased adoption of products like Revit, Bentley Architecture, and ArchiCAD. But while architects are only starting to realise the benefits of intelligent object-based design, their colleagues in the structural sector have been reaping the rewards for much longer.\nVirtually all steel fabricators rely on intelligent 3D models to produce accurate Bills of Materials (BOMs) and cutting lists \/ NC code, while consultant engineers exploit 3D to design and analyse increasingly complex structures. But getting the two disciplines to work together with the same 3D data is another matter entirely and one of the underlying ambitions for structural software developer, Tekla through its Tekla Structures product.\nStructural BIM, as Tekla defines it, covers the entire structural design process from conceptual design to detailing, fabrication and erection of all multi material elements. However, it\u00dds not just about the structural engineer passing on a model to the steel fabricator, but providing a single design environment in which concrete, steel and even timber can happily co-exist.\nTekla is a Finnish company by descent and its background lies in structural steel detailing through a product called XSteel, one of the first commercially successful 3D detailing applications on the market. In 2004 the company combined XSteel with its Xengineer product, a parametric 3D modelling package for engineers, and Tekla Structures was born.\nLike Revit Structure from Autodesk, Tekla Structures is a structural BIM application, which enables engineers to create intelligent physical and analytical structural models for design. However, it also offers a highly capable steel detailing element, combined with precast concrete detailing, reinforced concrete detailing, and into the realms of project management.\nTekla Structures is used in over 80 countries worldwide on a huge range of projects ranging from Wembley Stadium and the Beijing Olympic Stadium to New York\u00dds Freedom Tower and Leadenhall in the City of London. Its ability to model complex structures whilst keeping the file size small is a particular draw for the product. For example, the model for Wembley Stadium was only 70MB in size.\nThe system is modular and the number of options available is a little overwhelming, but this means users can tailor the application for many specific roles without shelling out for the entire software package if they are only interested in certain elements. There are seven configurations in total: Full Detailing, Steel Detailing, Precast Concrete Detailing, Reinforced Concrete Detailing, Standard Design (will be called Engineering in 14.1), Project Manager, and Viewer.\nDesign and modelling\n\u00d9Standard design\u00dd is the module of choice for consulting engineers and the starting point for any structural design project in Tekla Structures. It provides all the tools required to build up a structural BIM model and offers \u00d9round trip\u00dd analysis links to many of the leading structural analysis applications. Tekla Structures models can be built from scratch or based on referenced architectural or other 2D drawings or 3D models (DWG, DGN, IFC, etc). These 3D models can be made transparent to improve clarity, clip planes can also be used to expose elements through sections and clash detection carried out between models. As architectural models are continually changing there is a neat utility that enables users to compare two referenced 3D models and show only the changed objects.\nGrids are essential, regardless of whether referenced data is being used or not. These are even more important in Tekla Structures than in other structural applications as they not only provide reference points for structural components, but can be used to generate different named views for each of its viewports. For example, named views can be created for specific sections through a structure and it can also be used to define plan views at specific elevations or floor heights. This is likely to take a little getting used to but it\u00dds an extremely powerful capability.\nPretty much anything can be modelled in Tekla Structures: beams, columns, slabs, walls. These can be chosen from an extensive library of industry standard components and include standard UK & European sections, cold rolled, and precast concrete, but custom profiles and freeform components can also be modelled. This makes Tekla Structures particularly attractive for use on complex projects.\nIn a typical workflow components are laid down onto the grid, offset, or snapped to construction lines or reference elements. Users can work in 2D or 3D at the same time and even start in one view and finish in another \u00b1 it is that flexible.\nComponent properties, such as size, length, and material, can be defined prior to modelling or edited at a later date. Group edits are easy and users can choose which attributes to change. For example, it is possible to change the length of a group of beams without changing their individual shape or sizes. All the usual copy, move, and array commands are available to help users quickly build up the structure. There are very few limits.\nComponents do not have the same level of intelligence as they do in other BIM applications. For example, a beam does not automatically trim when it is attached to a column. However, there is a number of different ways to build in connectivity between components, so if a beam moves or changes shape the column adapts accordingly.\nWith more control over how components interact, Tekla Structures offers much greater modelling freedom. The downside is that users will have to invest more time to ensure that the geometry behaves how it should when edits are made.\n{mospagebreak}\nTekla Structures uses a central database to store all of its data. At the conceptual stages of design, this means several engineers can work in the same environment without having to break the model down into chunks. As data is saved a prompt appears to enter an optional revision comment explaining the changes. At the same time, users are notified of changes made by other engineers. Working in this way a full audit trail is made available for the project, but care still needs to be taken to co-ordinate work. Tekla Structures does not have a dedicated mechanism to enable engineers to communicate in real time. Instead, use of applications like Instant Messenger is common.\nSteel detailing is a major strength of the system and it includes hundreds of different types of 3D parametric connections. These can be generated automatically through customisable rules and are based on Green Book codes. They can be adapted by the user and parametric and non-parametric connections can also be created from scratch and saved in a custom library. A number of parametric custom components, such as Curtain Walling, Staircases, Trusses, and Precast Floors, are also included.\nAnalysis\nTekla\u00dds BIM ideology of sharing structural data throughout all stages of the design process is also applied to analysis. Round trip analysis is provided for a number of leading applications, including Robot, STAAD, S-Frame, and SAP2000. Engineers can export the BIM model from Tekla Structures, analyse the structure in a dedicated analysis application, tweak the design as necessary, then bring it back into Tekla Structures and accept or reject design changes (individually or globally). For analysis applications where direct links are not available (such as RAM) Tekla Structures also supports the CIS\/2 and IFC interchange standards. In this case design changes need to be manually updated in the Tekla Structures model.\nForces and load conditions can also be assigned inside Tekla Structures and taken into the analysis package. This streamlines workflow, particularly if there are multiple design iterations. However, many engineers still prefer to manage loading, and instead load cases inside an analysis application.\nStructural analyses requires a dedicated analytical model. Unlike Revit Structure, which automatically builds an analytical model in the background as the physical model is built, analytical models are created as and when required. The engineer can create multiple models for different types of analysis (steel frame, beam design, concrete slabs) and can define exactly which components are included in each. This means the best analysis application can be used for the job at hand.\nInside Tekla Structures Analytical models can be viewed alongside the physical model and the engineer is afforded much control over the positions of nodes (the top, bottom or neutral axis of a beam), the type of joint (fixed, pin) and the level of fixing. Additional nodes can be added if required and structural elements can also be combined as one affording much flexibility.\nDrawing and reports\nWith a central database containing all the structural information, drawings are produced as \u00d9snapshots\u00dd of the 3D model. If the model is amended, these changes are automatically highlighted in the drawing with a revision cloud. These remain until the drawing is re-issued.\nDetails and sections can be created from other views and if they are placed on another drawing sheet they are automatically referenced in the original drawing.\nWhen inside a drawing it is not possible to move beams, columns, etc so the physical model can\u00ddt be edited in this way. However, CAD tools can be used to add lines and text to drawings.\nDrawings can be based on templates with title blocks automatically populated with project information and anything in the model can be scheduled. Dimensioning is automated, as can be the creation of close-up views of details.\nThe engineer is afforded complete control over how each detail and component is displayed but if something has not been modelled to the required level of detail, drawings can also be augmented with \u00d9dumb\u00dd 2D details, though these are not linked to the project model.\nIn addition to drawings and schedules, Tekla Structures can generate a wide variety of reports in HTML, .XLS and .TXT formats, based on the model as a whole or for individual components. These can also be used to check the model for accuracy before it goes downstream. For example, an engineer could use a report to instantly see if columns are on a grid and then automatically zoom to the problem area.\nCNC code and steel management\nFor CNC (Computer Numeric Control) code generation, which is used to drive shop machinery tools to fabricate steelwork, the system supports DSTV standards. In addition, Tekla Structures supports FICEP\u00dds scribing technology, which scribes directly onto steel to indicate the required layout locations and aid in construction on site. Part marking, layout lines or welding symbols can all be added.\nTekla also works closely with Vela Systems to support its RFID (Radio Frequency IDentification) technology. This enables all components to be tracked and managed from shop to site. This information can be visualised in the model environment or over the web using the free web view.\nConcrete detailing\nWhile Tekla\u00dds strength historically lies in steel, its Structures product also includes 3D detailing tools for reinforced and pre-cast concrete. Whereas 3D steel detailing is commonplace, RC detailing has largely remained a 2D practice so doing everything in a 3D environment is an interesting proposition. As with steel in Tekla Structures, the modelling process is flexible and the detailer simply lays down bars with the system automatically recognising shape code and positioning according to pre-defined cover thickness. Attributes such as grade, size, and number of hooks, are all controlled via dialogues.\nThe system also features a number of standard objects and can automatically detail pads, strip footings and beams, among others. Detailing macros are provided for wall panels, windows and other details. A drawing wizard is included for producing reinforcement drawings, formwork drawings and rebar schedules, and NC code can also be generated for bars.\nConclusion\nTekla Structures is a vast product that covers the entire structural design process in a single environment. It enables a single BIM model to be used from the conceptual stages right through to fabrication and beyond and its ability to re-use data through the design, analysis and detailing phases and model in steel, concrete and timber is somewhat unique in the industry. While other structural BIM systems offer capable modelling and round trip analysis, they do not provide such powerful tools for the detailing phase, a capability on which Tekla Structures was founded.\nTekla Structures scores highly in structural design, but it loses out when it comes to sharing data with other AEC disciplines. While it supports the capable IFC standards, there is still no substitute for sharing data with architects and building services engineers in native formats, such as Revit and MicroStation, and this will become increasingly important in the coming years as BIM grows.\nIn terms of modelling the product is extremely powerful and users only need to look at the range of structures that have incorporated Tekla Structures to see how adept it is with dealing with complex forms. The downside of this modelling freedom is that structural elements are not automatically constrained, so a disciplined approach to modelling is essential to ensure connectivity is maintained. Without this users are missing out on the major benefits of structural BIM, such as adaptability, accurate drawings, BOMs and cutting lists.\nDelivering such a powerful system for structural design is a major achievement for Tekla. The software does an excellent job of bridging the gap between design and fabrication and through its single environment provides the perfect conduit for data to flow smoothly from engineer to detailer. Most importantly, though, it has the potential to simplify the management of an often fragmented process, leading to enhanced collaboration, better designs, reduced times, and of course lower costs.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-daniel-schmitter-mirrakoi-sa\/","title":"Video: NXT BLD 2018 London conference - Daniel Schmitter, Mirrakoi SA","date":1530230400000,"text":"Xirus: 3D CAD \u2013 From Biomedicine to AEC \u2013 NXT BLD London, June 2018\nDaniel Schmitter, CEO of Mirrakoi, gave a demonstration of his new Xirus surface modelling kernel which had origins in biomedical but is now finding an ideal home is the AEC industry, allowing the easy creation of complex organic shapes.\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/applanix-and-trimble-make-a-winning-combination\/","title":"Applanix and Trimble make a winning combination","date":1058832000000,"text":"Applanix, a provider of inertial systems technology (INS), recently coupled with Trimble\u2019s core GPS technology, to promise greater position accuracy in situations where trees, topography or tall buildings obstruct satellite transmission. Inertial sensors don\u2019t have dependencies like GPS, are self-contained and rely on gyros and accelerometers inside the system to providing positioning information. The combination of GPS with INS will improve the performance level for those customers engaged in surveying, construction, forestry and other key industries.\nApplanix shall complement Trimble\u2019s core positioning technologies, which include: lasers which help you determine an elevation of a plane or a point; GPS which gives you a 3D position and then we have optical total stations which are \u2013 when you are driving down the road and see a surveyor looking through an eye piece that\u2019s a total station; we have advanced technology called robotic total stations that allow the user to manipulate that total station without looking through the eyepiece. When one sees someone looking through the eyepiece there\u2019s always someone on the other end with a pole and prism. Robotic technologies enable remote control from the range pole, and it doesn\u2019t need someone looking through the eyepiece and the total station is controlled remotely from the pole.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/technology\/hp-omen-pro\/","title":"REVIEW: HP OMEN Pro (pre-production unit)","date":1431648000000,"text":"With a brand new mobile workstation, HP breaks the mould of its enterprise-focused ZBooks offering a uniquely styled machine that is big on performance.\nNot content with producing one ultra portable 15-inch mobile workstation, HP surprised everyone this April with the launch of a second. The fact that the HP OMEN Pro was not a member of HP\u2019s established ZBook family also got everyone talking.\nHP has spent years building up its Z Workstation brand, which spans both desktops and laptops, so to launch a CAD-certified mobile workstation under a different name at first seemed odd.\nWe were lucky enough to get our hands on a pre-production unit (the final shipping machine may be slightly different) and having seen the OMEN Pro in the flesh it all makes perfect sense now.\nLike the Dell Precision M3800 and MSI WS60 the HP OMEN Pro is based on a gaming laptop of the same name. As a result it was originally designed to tick two boxes. To be high performance (both CPU and GPU) and to be slim and styled for gamers.\nThis means it lacks many of the features that have become synonymous with ZBooks over the years: enterprise features, such as docking port and fingerprint reader, plus serviceability that has become the hallmark of all HP Z Workstations, both desktop and mobile. It is also less customisable, coming in a few base configurations.\nWith this in mind it would actually be a bit disingenuous for HP to call the OMEN Pro a ZBook.\nThe OMEN Pro also gives HP another string to its bow in this growing Ultra Portable market. The ZBook 15u prioritises battery life over performance with a Dual Core Ultra Low Voltage CPU and entry-level GPU, whereas the OMEN Pro is all about performance, with a Quad Core processor and higher spec GPU. In this respect it is very similar to the Dell Precision M3800.\nBoth Dell\u2019s and HP\u2019s machines feature Quad Core CPUs; the OMEN Pro having a slightly more powerful Intel Core i7-4870HQ (2.50GHz up to 3.70GHz) compared to the Precision M3800\u2019s Intel Core i7-4712HQ (2.30GHz up to 3.30GHz). Both have a Quadro K1100M (2GB) GPU and 16GB of RAM.\nIn terms of spec, storage is the big differentiator with the HP OMEN Pro having fast PCIe-based storage in the form of a 512GB HP Z Turbo Drive while the Dell has an mSATA SSD and, crucially, space for a 2.5-inch drive (SSD or HDD), which can be very useful if you want to store a lot of CAD data.\nWhile the HP Z Turbo excels in sequential read\/write tasks \u2014 i.e. copying large files or in workflows where exceedingly large files are the norm (e.g. video editing, point cloud processing and simulation) \u2014 mainstream CAD users probably will not notice much benefit. (See our PCIe vs SATA SSD article for more on this \u2014 tinyurl.com\/PCIe-SATA-SSD-AEC).\nBeyond the core specifications, similarities to the Precision M3800 end. While Dell\u2019s machine is a dead ringer for MacBook Pro, the HP OMEN Pro is like no other laptop we have seen before.\nIts striking angular chassis falls away at the front and sides with a sharply bevelled edge, giving the appearance of a much more slender machine. That is not to say it needs any visual trickery. The beautiful chassis, CNC milled from solid aluminium and anodised with a black textured finish, is only 15.5mm at the front and 19.9mm at the rear. It looks exceptionally smart but is prone to fingerprints.\nIts gaming pedigree becomes apparent as soon as you switch it on as you are greeted by glowing red lights. Colour emanates from everywhere \u2014 speakers, power button, keys and the exhaust grills at the rear of the machine.\nWhile gamers reportedly love this kind of thing, architects may not be so enamoured. The good news is, the lights are fully customisable and can be toned down to a calmer shade, including white, or switched off altogether. HP has said that lights will be switched off by default when the final machine ships this month.\nThe keyboard is excellent: firm, responsive and a pleasure to use. While there is no numeric keypad (none of the gaming-evolved mobile workstations have them) there are six programmable keys that sit to the left of the main keys.\nWhile these were designed with games in mind they could be put to good use as CAD shortcuts. Each key (P1 to P6) can be used on their own or in conjunction with four modifier keys (shift, FN, CTRL, ALT) giving up to 30 different combinations. Customisations can be saved to profiles.\nThe multi-touch touchpad is excellent. It feels very similar to the MacBook Pro\u2019s, but is presented in a much wider landscape format. The touch screen panel is also very useful for navigation, though not recommended for accurate CAD work. Of course, with Windows 7 64-bit pre-installed on the HP OMEN Pro as standard you will not get as much out of the touch screen as you would with Windows 8.1 Pro 64 (which is supported but cannot be pre-installed).\nThe panel itself is glossy so reflections and fingerprints show up more but the colours are extremely vivid and bright.\nAt 1,920 x 1,080 (HD) it may not match the pixel count of the 4K Precision M3800 but it is an impressive display nonetheless.\nBy having such a sharply bevelled chassis, which falls away at the front and sides, all of the I\/O ports are found at the rear. At first this looks like a neat solution for keeping unsightly cables out of the way, but it can be a real pain to plug monitors, mice and external storage. Rather than leaning slightly to the side to see exactly where the ports were, we found we had to turn the machine round a full 180 degrees.\nAt the rear you will find four USB 3.0 ports (all charging), mini DisplayPort and HDMi. With such a slimline chassis, there is no Ethernet port but a USB to Ethernet adapter comes in the box.\nPerformance is excellent. Having a full 0.2GHz over the M3800 (0.4GHz in Turbo) it has a clear lead in our multi-threaded, CPU intensive 3ds Max benchmark and is only a whisker behind the MSI WS60.\nWith the same Quadro K1100M GPU as the M3800 the performance increase is far less pronounced in our Creo and SolidWorks graphics tests.\nFans kicked in significantly under load, making a fair bit of noise. They also ran slowly in the background when idle but this was less noticeable than with the MSI WS60.\nWe actually experienced a few issues in our SolidWorks test with PhotoView 360 crashing regularly. It should be emphasised again that this was a pre-production unit and software issues like these are likely to be ironed out with the shipping machine and during CAD certification. Performance could also change slightly.\nOverall, the OMEN Pro is an excellent addition to HP\u2019s workstation portfolio \u2014 a beautifully styled, slimline laptop that delivers performance to rival most standard 15-inch mobile workstations.\nThere are some compromises, of course. Battery life is not great and some may find the storage too limiting and rear ports annoying but, apart from these minor gripes, the OMEN looks to be a very impressive machine for mobile CAD\/BIM and entry-level design visualisation.\nThis review is part of a group test of Ultra portable 15-inch mobile workstations.\nThose based on a business-class laptop chassis\nHP ZBook 15u\nLenovo ThinkPad W550s\nThose based on a 3D gaming laptop chassis\nMSI WS60 mobile workstation\nDell Precision M3800\nSpecifications\n\u00bb Intel Core i7-4870HQ (2.50GHz up to 3.70GHz) (4 cores, 8 threads)\n\u00bb 16GB (2 x 8GB) 1600MHz DDR3L SDRAM\n\u00bb Nvidia Quadro K1100M (2GB) + Intel Iris Pro Graphics 5200\n\u00bb Display: 15.6\u201d diagonal full HD WLED backlit touch screen display (1,920 x 1,080)\n\u00bb 512 Gb HP Z Turbo Drive (PCIe SSD)\n\u00bb Keyboard \/ Mouse: Full-size island-style backlit keyboard. Touchpad with multi-touch gesture support\n\u00bb Battery: 4-cell, 58 WHr 3.82Ah Lithium-ion polymer battery\n\u00bb Wireless: Intel Dual Band Wireless-AC 7260 802.11ac (2\u00d72) WiFi + Bluetooth 4.0 Combo\n\u00bb Ports: 4 x USB 3.0 USB to Ethernet Adapter (included)\n\u00bb Expansion \/ security: Fingerprint reader. Smart Card reader. Integrated 4-in-1 SD card reader\n\u00bb Display outputs: HDMI, Mini DisplayPort\n\u00bb Docking: No dedicated docking port\n\u00bb Size (W x D x H) 383 x 248 x 15.5 mm\n\u00bb Webcam: HP TrueVision Full HD WVA Webcam\n\u00bb Weight (machine + power adapter incl UK plug): 2.12kg + 0.63kg\n\u00bb Microsoft Windows 7 Professional 64-bit\n\u00bb 3-year Next Business Day Onsite\nCPU benchmarks (secs \u2013 smaller is better)\nCAM (Delcam PowerMill 2010) \u2013 i) 176 ii) 283 iii) 411\nCAE (SolidWorks 2010 Simulation) \u2013 N\/A\nRendering (3ds Max Design 2011) \u2013 273\nGraphics benchmarks (bigger is better)\nCAD (SolidWorks 2013 \u2013 SPECapc graphics composite) \u2013 4.26*\nCAD (PTC Creo 2.0 \u2013 SPECapc graphics test) \u2013 4.46*\nBattery test (PCMark 8) Compute intensive creative test (max & min brightness): 1h 50mins* 2h 07mins*\n(* Testing performed on pre-production unit so performance could change with final production unit)\nExternal Storage\nMainstream 15-inch mobile workstations can typically hold up to three internal drives but their slimline siblings are limited to one or two. This increases the importance of external storage for storing huge CAD datasets.\nTo read our review of some of the most popular storage devices, click here.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/cad-s-next-frontier\/","title":"CAD\u2019s next frontier","date":1473984000000,"text":"It\u2019s time for CAD tools to start sharing the burden of creativity, adding their own ideas to the ingenuity of human designers. Martyn Day reports on the latest developments in computational and generative design and examines how Autodesk is ramping up development\nProgramming and scripting in CAD tools is nothing new. In fact, it\u2019s a long-established principle, allowing users to create features absent from the as-sold toolset, from the small and simple to the hefty and complex.\nBack in the late 1980s and 1990s, any AutoCAD user worth their salt had a few AutoLISP skills under their belt and were happy to customise their drawing tools. These days, with Bentley Systems\u2019 GC, Rhino\u2019s Grasshopper and Autodesk\u2019s Maya, Max and Dynamo, users of most products benefit from rich suites of very capable tools.\nMost BIM applications, by contrast, are basically Lego sets: collections of predefined objects (walls, doors, windows and so on) that can be configured, edited and arranged into \u2018families\u2019 of made parts.\nFor some, that\u2019s been a problem, especially if they\u2019re looking to create non-linear buildings of interest, rather than rectilinear, functional \u2018blocks\u2019. Signature architectural practices, of the kinds led by Norman Foster, Frank Gehry or the late Zaha Hadid, have been less likely to use early BIM tools because of the constraints imposed by their geometry engines.\nMeanwhile, although early 3D modelling systems would allow the creation of interesting forms, these forms were static and needed to be remodelled to make changes, having little or no parametric capabilities.\nSignature architectural practices have been veritable hotbeds of scripting and computer-generated forms. They, along with a new generation of experimental architecture students have learnt scripting languages and played with algorithms to make geometry dance, relying on the computer to recalculate each and every change.\nThere are plenty of software options for exploring this, with GC, Grasshopper and Maya seemingly the most commonly used tools \u2013 and Grasshopper easily winning the popularity contest among students, thanks to its very visual and easy-to-use, diagrammatic style of programming.\nWhile Autodesk has had \u2018skin in the game\u2019 here with Maya, this software was still specialist and lacked BIM capability. So for some time, it was clear that AutoCAD, Revit and other Autodesk products might benefit from a generativestyle programming language.\nIt was a stroke of luck, then, that Dr Robert Aish, Bentley Systems\u2019 key developer of its GenerativeComponents parametric CAD software, was persuaded to jump ship to Autodesk in 2008. He went on to develop a new baseline parametric programming language called DesignScript.\nLater, Autodesk acquired Dynamo, created by Ian Keough, which brought a similar visual programming front end to DesignScript. Today, Dynamo is a fundamental capability spreading its wings from a standalone product to one that connects and drives Autodesk\u2019s core design tools.\nAt the same time, computational and generative design has expanded its scope from handling complexities of geometry and conceptual design, to the automation of common tasks and Autodesk\u2019s most advanced customers are starting to dabble with scripting their way to all sorts of benefits.\nDynamic Dynamo\nDynamo builds on the original development of DesignScript, which is a powerful, text-based parametric and associative language. Dynamo provides a visual layer, with visual nodes and connectors, which users use to define and build programmes that the computer can execute. In the background, these visual nodes, connectors and functions create the script. In short, it\u2019s programming without needing to know or understand the syntax.\nDynamo Studio (charged at a subscription rate of \u00a3280 per year) is a complete standalone environment that includes a modelling window and a powerful solids\/ surface geometry generation kernel.\nUsers can lay out grids, draw base geometry\/sketches and add points that form the basis of operations. These could be floor height, splay, pitch, strut width \u2013 any variable the user can imagine. They then apply constraints, formulas and transforms to create very complex solutions, just by defining basic rules and interactions.\nIntelligence can be added, so that if a span exceeds a certain length, for example, an additional column may be added automatically and then mapped to Revit.\nThis could be a complex roof truss structure or panellisation of a bespoke curtain wall system. It could be used to drive analysis of a design, to optimise fenestration areas or use feedback loops to ultimately derive the optimum solution.\nDynamo Studio also hooks into Autodesk cloud services for additional capabilities. There is a version of Dynamo for Revit, available free with Revit 2015, 2016 and 2017.\nAt the conceptual end of the spectrum, Autodesk has FormIt 360 Pro and Dynamo is making its presence felt here, too. FormIt runs on tablets in the form of native iOS and Android apps, inside modern web browsers and also as a desktop Windows application. It\u2019s fully compatible with Revit for a continuous flow of concept to detailed model and documentation, with access to cloud services for analysis, materials and more.\nWith the Pro version of FormIt, users can connect a Dynamo Studio script to the Windows app for FormIt. Dynamo content can be dragged and dropped into FormIt models to access the same Dynamo parameters. The Dynamo link and data is stored with FormIt for editing later.\nBeyond the toolmakers\nMost firms will only have a few employees skilled enough to use scripting tools \u2013 but Autodesk is looking to expand the use of Dynamo scripts beyond a highly trained elite. One function that it has demonstrated at a Revit Technology Conference (RTC) event is \u2018Playlists\u2019. These effectively remove the wires and boxes for the \u2018consumer\u2019 of a script and gives neater front ends for teams to call up tools created inhouse \u2013 perhaps to check a building for LEED glazing, for example, or for conformance to lighting standards.\nIn the background, Dynamo can be running through a complex script and making calls to other components, such as analysis tools and spreadsheets, but for the user, it\u2019s a much less \u2018messy\u2019 experience than loading and executing scripts. The Revit team is looking at how all this power can be more easily digested by users in the future. For now, Playlists are a demonstration-only product.\n\u201cThere\u2019s a lot of building knowledge lying around out there that\u2019s free, in academic papers, magazines but it\u2019s almost impossible to apply that to buildings as it\u2019s all \u2018siloed\u2019 or knowledge that\u2019s locked in design teams,\u201d says Anthony Hauck, director of product strategy at Autodesk. \u201cThat can now be captured in scripts and shared and developed further.\u201d\nIntroducing Project Fractal\nAutodesk has worked with Chicagobased architecture and design firm Perkins+Will on \u2018Project Fractal\u2019 (thebuildinglab. info). This involved Perkins+Will bringing together 20 employees from all over the US to consider the problem of computational space planning.\nAt the start of most projects, teams have an outline brief and need to tease out a number of possible options to best use that space. Using examples of past projects, the Project Fractal teams evaluated computational solutions to see how what they came up with compared with what was actually designed.\nOne example used was that of a hospital bed tower. A complex script was generated in Dynamo to explore the relationships and layout possibilities. The challenge was to see if the solutions that the Dynamo script suggested were similar to the ones that humans came up with \u2014 albeit at a much slower rate. This was all driven from an Excel spreadsheet: the spaces could be arranged with different weightings, which were taken into consideration throughout the computation cycles.\nBy working with real-world examples, the project showed that designers could let computers take the strain when it comes to suggesting optimum layouts, freeing up their own time to apply their knowledge and judge how designs met the client\u2019s brief and the building\u2019s constraints.\nIn future, designers will also be able to interact with the system during live calculation and add weight to different criteria, providing a true interactive design exploration experience. For now, Autodesk is experimenting with getting the system to compute maximum, minimum and average solutions.\nIn the case of a football stadium, the team used Dynamo to create optimum seating layouts, letting the system iterate the design as it met different weighted criteria. At the moment, the system shows all the results it gets \u2013 both wrong (in red) and right (in green) \u2013 but this could change, so that it displays only those solutions that meet the design criteria. At the top of the screen, the fit curve shows how strongly the derived design scores for each of the conditions.\nIntroducing Project Monolith\nStepping outside of traditional BIM, Project Monolith (monolith.zone) is a voxel-based engine that Autodesk is working on with the aim of solving f abrication problems in future, according to Panagiotis Michalatos, an architect, assistant professor at the Harvard Graduate School of Design and a principal research engineer at Autodesk.\nUp to now, he explains, most geometry engines were more concerned with the surface complexity of designs. If you are lucky, they may be a solid representation, but this bears no relevance to the material from which the solid is made. It\u2019s always assumed to be a consistent material.\nVoxel technology \u2018rasterises\u2019 solids by breaking them down into tiny 3D pixels. While they have been around as a concept for a long time, voxel modellers have been very CPU-hungry. Now that we have the power and are getting 3D printers that can make use of their ability to create solid out of multi-materials, this becomes an intriguing concept.\nThe latest 3D printer from HP, for example, is actually based on voxel technology: it breaks solids down into trillions of points, which it prints by inkjet, drop by tiny drop. The added benefit is that the machine can alter the material properties \u2013 colour, conductivity, elasticity, heat capacity, for example \u2013 at a micro level, making it possible to control the very make-up of manufactured materials.\nThis means that support structures or conductive wiring, for example, might be be built into materials. A traditional solid modelling CAD package would struggle to interpret and drive this capability for the next generation of digital fabrication machines \u2013 but these new developments open the door to creating some very interesting structures, probably best generated by something like Dynamo, but acting in Monolith.\nThe way forward\nAutodesk continues to surprise with the breadth of its developments. Dynamo may have started life as a visual scripting language, launched to compete with Grasshopper and the like, but as its reach expands to FormIt, Revit and other applications, it seems conceptual design and geometry is just one aspect of its application.\nWorking with customers to experiment with applying scripting to facilities design, room layouts and easier ways to interact with Dynamo-derived scripting will lead to new capabilities within design tools. For commonly experienced problems, meanwhile, there\u2019ll be communities where knowledge and experiences are shared that might have previously been in the hands only of the most highly skilled employees.\nMonolith is perhaps the \u2018odd product out\u2019 in this article \u2013 but instead of the programmability of design, we are in a new age where the physical properties of matter can be manipulated and programmed, not just at a large scale, but at a molecular level.\nTo get the benefit of these advances, our engineering analysis tools have to be liberated to solve real-world problems not only by surface, shape and reinforcing structures, but also by manipulating the way materials are formed in forthcoming 3D printing processes. This will have far reaching consequences on the way we make everything and anything.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/graphisoft-goes-multi-disciplinary\/","title":"Graphisoft goes multi-disciplinary","date":1595808000000,"text":"Version 24 of Graphisoft\u2019s flagship BIM modeller, ArchiCAD, includes the usual raft of features for customers, but this release also delivers four years of under-the hood development to revolutionise model collaboration, writes Martyn Day\nWhen analysing a yearly release of a BIM product, it seems natural to go with the biggest features which have been added to the software. With that in mind, ArchiCAD 24 is a bit of an unusual beast as it is probably the biggest release of ArchiCAD in living memory, in terms of development effort and scope. It\u2019s just that this new technology, previously called Project Everest, is so seamless that it\u2019s not particularly obvious, as it\u2019s a workflow enabling technology in both BIMcloud, the real-time collaboration platform, and ArchiCAD 24.\nWhen BIM first came out, it was sold on the concept of the Single Building Model (SBM): a single model structured database which contains all multi-disciplinary contributor\u2019s components, together with related building information. It was only when firms started to model in BIM that issues quickly arose, with a lack of collaboration, limits on model size, performance and interoperability, and the basic lack of decent standards.\nThe Single Building Model was a utopia and twenty odd years later the industry is still cutting up models and deploying workarounds and Common Data Environment (CDE) strategies to manage collaboration.\nFor the last four years, Nemetschek, Graphisoft\u2019s parent company, has been working on the problem of data portability within BIM ecosystems with a project called \u2018Everest\u2019. While a leading proponent of OpenBIM and Industry Foundation Classes (IFC), these open standards are not the same as working with the original data and require one way \u2018dumps\u2019 of data to be imported into other applications for processing.\nWith the cloud and modern computing, the whole concept of file exports, file imports and data surviving round-tripping is inelegant, time-consuming and could surely be improved?\nNemetschek set about developing a new foundational schema (structured data format), to establish seamless BIM data sharing for users with compatible Nemetschek applications, and hopefully those independent developers who want to plug in and participate through an open API. The technology is now delivered in Graphisoft\u2019s latest BIMcloud product and within the new ArchiCAD 24.\nBIMcloud revolution\nBIMcloud, Graphisoft\u2019s collaboration server, has seen a considerable evolution since its original launch in 2014. It securely connects teams that are working on common projects and limits data transfer by only sending the \u2018delta\u2019, the changes that each contributor makes, with additional secure local caches to reduce network delays. It offers controls and locks at the element level and now can act as the single source of BIM data for anyone, anywhere on the planet.\nThis release brings a new BIMcloud API and global availability for the SaaS variant. It works on a server or can be hosted in the cloud and enables permissions from the project to the element level, and only sends the data that has changed, not the whole model.\nThe new multi-disciplinary capabilities added to BIMcloud allow expanded integrated design opportunities, sharing one common model between teams \u2013 architectural, structural and MEP elements, together with the integrated analytical model. Also, Model Compare and Rollback are new features to quickly identify what has changed between models and revert to a previous state if necessary, together with a comments\/notes ability.\nThe new \u2018Everest\u2019 technology embedded within BIMcloud, is akin to having the ArchiCAD BIM schema extend to additional connected and compatible BIM tools, which may or may not be Nemetschek products.\nWith BIMcloud holding all the model data, changes flow between all connected applications to and from the BIMcloud. But here, BIMcloud is also smart, in that it will notify participants if their elements are altered or changed, so they need checking. And it is smart enough to only send the data that eco-system compliant applications, such as structural analysis tools, require.\nIn the case of a structural engineer\u2019s beam, it omits the fire protection coating or dry wallets which surround them when sent to the analysis tool. This bypasses unnecessary filtering, file exports, file transfers and delays. You get what you need, when you need it.\nWe talked with Viktor Varkonyi, Chief Division Officer at Nemetschek Group, about the Project Everest development, \u201cWe have a very strong vision for how the design profession will work in the future, because we strongly believe that the current roadmap means working in a silo. Everyone is managing their own data,\u201d he says. \u201cWe are taking a totally different approach and Project Everest is going to be a really disruptive approach.\n\u201cOur integrated approach means the architect can work on the architectural view, the structure modeller can work on the structural model, or the structural engineer can work on the analytical view, all in one model with protection for the different roles and responsibilities with a very robust management system.\u201d\nIt\u2019s important to point out that this isn\u2019t particularly intended to be a solution for integrating products such as Revit into ArchiCAD BIM workflows but it\u2019s not beyond the realms of the imagination, should someone want to do the work. This BIMcloud \/ ArchiCAD capability is very much targeted at data that is stored in the ArchiCAD schema and predominantly about building an ecosystem of apps which plug and play between teams and individuals. That said, at last year\u2019s Key Customer Conference, BIMcloud was demonstrated sharing geometry between ArchiCAD and Trimble Tekla, so an architect and structural engineer could \u2018model check\u2019 to compare each-other\u2019s geometry within their respective models.\nThe first instalment of this technology and benefit is for structural workflows. BIMcloud now enables Nemetschek\u2019s SCIA Engineer (multi-material structural analysis), RISA-3D (structural design) and Frillo (structural analysis and structural design) to be integrated into a seamless workflow, with data flowing forwards and backwards from each application.\nGraphisoft CEO, Huw Roberts explains, \u201cThis new technology is about connecting designers with structural, mechanical or electrical, with engineers who are carrying out analysis in perhaps over 300 different structural analysis tools \u2013 SCIA, RISA, STAAD, Midas etc. for thermal systems or connections.\n\u201cToday, every building project is shared by many people and every change could impact the column sizes or increase area and load, and the designers need to know the impact. The whole monolithic exchange of models that usually happens, contains way too much information and gives rise to long turnaround times between design, analysis and fix. This technology drastically reduces those iterative loops.\u201d\nMulti-disciplinary\nWhile the product is called ArchiCAD, as the years have gone on its scope and reach has broadened and its name has become a bit of a misnomer. The core strength is certainly architecture but ArchiCAD is an all-rounder for defining BIM projects and development of MEP and structural elements have been turbo-charged.\nStructural\nWhile you could model structural elements before 24, a lot of the development work and the first piece to get the benefit of the underlying \u2018Everest\u2019 changes is structural. ArchiCAD now can automatically generate a Structural Analysis Model (SAM) based on structural element definitions within the building model.\nThe software automatically checks that the structural \u2018stick model\u2019 elements are all connected and will update as the building design evolves. Here it\u2019s possible to see all loads, load-bearing capacities, connection points between load-bearing elements such as beams slabs and roofs etc. Graphisoft believes that if you see how this has been implemented and the benefits to collaborative workflows, you won\u2019t want to go back.\nUsing BIMcloud and the new seamless technology, model and structural data can be sent to engineers running Nemetschek\u2019s SCIA (structural analysis) and RISA (multi-material structural solver). The live SAM data can be used for all calcs and changes sent in either direction. This means no file exports. An architect can make model changes and, at the press of a button, a structural engineer can check changes, perform analysis and feedback any changes necessary to the architect, literally at the speed of electrons.\nThis could crush the iterative part of the design process from days and weeks to minutes and hours. The one stipulation is that your tools need to be able to connect to Graphisoft\u2019s BIMcloud. However, if not, they support the open source format called SAF (Structural Analysis Format) and IFC SAV.\nWhile the main example shown on the launch of ArchiCAD 24 was structural, the company will start rolling out other seamless integrations with all sorts of analysis tools. Another thing to note here is that Nemetschek realises that professions use other applications and that performing some function \u2018inside\u2019 a BIM authoring tool is probably not the best place for some of this to be occurring. There is an open API to plug into the \u2018Everest\u2019 schema and so other firms can also play in the seamless, data-roundtripping BIMcloud ecosystem.\nMechanical, Electrical & Plumbing\nMEP elements were available as an add-on to ArchiCAD, but now MEP is built into the core platform, so all users get the chance to create MEP components. There has been considerable development of the MEP capabilities and it looks to be fully featured and easy to use, placing elements creates holes in walls etc. If you are creating a model and all that is available is a basic MEP drawing, as opposed to an MEP model, this can be imported and used for reference, to quickly model up an MEP layout in 3D.\nBy including MEP in ArchiCAD, it\u2019s clear that Nemetschek is looking to address the dissenters who say that ArchiCAD is not an alternative to Revit as it\u2019s not multi-discipline. The combination of structural, MEP and architectural in its flagship product is more than a statement of intent.\nVisualisation\nBIMx, the visualisation and presentation tool, gets a major overhaul with the creation of a single BIMx app. BIMx is now \u2018freemium\u2019 in that it\u2019s free to create and share models, allowing the viewing, measuring and sharing of models, although you will need ArchiCAD to create them.\nBIMx is also now pretty much available everywhere \u2013 mobile, desktop or via browser. For paying users, BIMx can be enabled to stream huge models which don\u2019t fit into memory and can access API tools and save favourite views\/positions. BIMx models can be saved and accessed via BIMcloud or deposited on Dropbox or other hosted file stores.\nTwinmotion\nGraphisoft\u2019s partnership with Epic Games, the developer of Unreal Engine, continues to blossom with a deep integration for Twinmotion 2020, enabling amazing real-time visualisation and VR capabilities. The software is free to those on ArchiCAD plans. ArchiCAD 24 also has 500+ new surface materials along with a renewed residential library.\nAPI-tastic\nWhile ArchiCAD offers its own programming environment, GDL, and hooks into McNeel Rhino\/Grasshopper like no other BIM tool, Graphisoft is now adding support for Python API and the exciting looking PARAM-O for custom parametric content (currently only available in the Windows version). ArchiCAD is getting incredibly flexible in the options to customise and drive parametric model generation.\nThe PARAM-O plug-in, is worth mentioning in a bit more detail as it can create objects with nodes graphs to create GDL objects. This takes a few lessons from Dynamo and Grasshopper to liberate GDL, which is a rarely used, but powerful parametric language in ArchiCAD. Now powerful GDL components can be graphically scripted without any knowledge of the programming language just through wired up schematics. Graphisoft has turbocharged ArchiCAD to offer a fantastic array of expandability through today\u2019s most powerful parametric tools \u2013 Grasshopper, GDL\/PARAM-O and Python, driving BIM objects.\nConclusion\nArchiCAD 24 is a quiet revolution. At the launch, you would not hear the company talk about the underlying technology stacks or go into any great depth about how much work has been done in the last three\/four years. Indeed, all you would see and be shown is the new capabilities to make seamless Single Building Modelling possible.\nIn many respects, for regular readers of AEC Magazine, Graphisoft has pretty much delivered the type of functionality that Autodesk was talking about with Quantum in 2016, although Autodesk hasn\u2019t delivered anything yet. Graphisoft has managed to seamlessly integrate it into the current product line and has re-written part of ArchiCAD to utilise the new schema.\nWith this release, as a taster, we are seeing a big jump into integrating structural workflows with the architectural and MEP design software. While this benefits the Nemetschek ecosystem of products, the company is still supporting open standards and access to this new API. There are already a number of third-party developers who are looking to make use of BIMcloud and integrate their analysis tools into the solution. We will have to wait and see if other authoring tool developers are enticed into BIMcloud integration.\nI can imagine how Graphisoft may be concerned that people might think that it is not as committed to open formats as it historically has been with this \u2018Everest\u2019 powered ecosystem. However, this simply isn\u2019t the case, as the company realises that there will always be file-based workflows, for as long as there are a myriad of applications used in the design and construction industry. One only has to look at how the Nemetschek stable of products are individually branded and distinct technology groups are free to sell point tools which address all the types of mix and match workflows that are used in the industry.\nOne of ArchiCAD\u2019s biggest drawbacks, claimed by competitors, is that it is a product only for architects, as it\u2019s in the name. ArchiCAD 24 consolidates components and workflows that span all aspects of the BIM design process and is deeply integrated with advanced generative geometry tools like Grasshopper. It has clearly become a multidisciplinary application to rival products like Revit. But perhaps, as opposed to comparing authoring tools, it clearly offers something different and advanced in the cloud management and single model concept. Perhaps a fairer assessment of ArchiCAD would be to look at workflow and multi-user modelling. BIMcloud is what people should be looking at; it\u2019s a game changer.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/making-light-work-part-ii\/","title":"Making light work: part II","date":1170115200000,"text":"Darren Brooker introduces the second installment of his four-part comparison of the interior lighting methods available within 3ds Max, taking us through Global Illumination with mental ray.\nTo read the first half of this tutorial, click here\nIn the last issue of AEC Magazine we kicked off our look at the various interior lighting tools available within 3ds max by looking at radiosity, which is a Global Illumination (GI) algorithm. Of the other GI algorithms, photon mapping is the technique used by 3ds Max\u00dds alternative renderer, mental ray, which is where we\u00ddll pick up this month.\nMental ray is a production-proven renderer whose implementation within 3ds Max has in the past suffered from a lack of documentation and licensing restrictions. For these reasons, lots of Max users have given it a wide berth, but for complex lighting effects it\u00dds tremendously powerful. Not only can it do everything that\u00dds possible with the radiosity renderer, it can also go a stage further by addressing GI techniques such as caustics that the scanline cannot address without employing creative fakes. Furthermore, it is the only way within 3ds Max to generate ambient occlusion information.\n\u201d Mental ray is a production-proven renderer whose implementation within 3ds Max has in the past suffered from a lack of documentation and licensing restrictions, but for complex lighting effects it\u00dds tremendously powerful. \u201c\nIn the same way that we looked at the workflow for radiosity rendering last month, we\u00ddll first take a look at how GI calculations are processed by 3ds Max when using mental ray, which is a pretty straightforward process. The first thing that we need to do is to allocate mental ray as the renderer, which is done from the Render Scene dialog, accessed by hitting F10. Now, at the bottom of the Common tab is the Assign Renderer rollout, where you should choose mental ray as the Production renderer.\nOnce you\u00ddve done this, the tabs within this dialog will change to reflect the new renderer. Open the 02mentalRay.max file and go to the Indirect Illumination tab. You can see that there are only two rollouts within the tab, and apart from the sampling settings within the Renderer tab that can be thought of as the equivalent of the scanline\u00dds antialiasing settings, this is all you need to setup Global Illumination rendering with mental ray. First of all, enable GI in the Caustics and Global Illumination rollout. Leaving everything at its default setting, hit Render and you should be able to see from your results, that you have a GI rendering, albeit a very rough and ready one.\nLike the example in the last issue, you\u00ddll need to use the Exposure Control to compensate for the dynamic range of radiosity rendering. Hit the Setup button further down the same rollout and choose Logarithmic Exposure Control. If you render now, you\u00ddll see that the exposure looks correct, but that there are noise and sampling issues, as you can see in Figure 1. This is dealt with within the Final Gather rollout.\nClick the checkbox to enable Final Gather and hit Render again. This time you\u00ddll see that the render takes a lot longer, which is because the regather stage is a comparatively render-intensive affair. However, the Final Gather map can be stored, which saves on processing it each time a render is completed. To do this, check the Read\/Write File box within the Final Gather Map section of the Final Gather rollout. Once you are happy with your results, you then merely need to place a check in the Read Only box, and this Final Gather map will be used and not overwritten, which will speed your render up by an order of magnitude.\nOnce your rendering is complete, you should see something similar to Figure 2. You should also agree that it\u00dds pretty straightforward to get up and running with GI under mental ray. However, there are some subtle but important differences between this scene file and the one you used last month.\nYou\u00ddll see that this file is almost identical to the one we used last month to examine radiosity. There are a few subtle differences between these scenes; the first one is that the light in this scene is a Daylight system, which enables us to allocate a mental ray sun and skylight within one assembly. The light within last month\u00dds scene file was an IES Sun light, which works with the scanline, but needed to be changed to best work with mental ray.\nThe second difference lies within the scene\u00dds materials, which are all mental ray Architecture & Design shaders. This is noteworthy because there are some major differences in the way that raytraced reflections and refractions are calculated internally between the scanline and mental ray. Though mental ray works with the standard max raytrace map type, it actually uses it as a placeholder for its own techniques, so it\u00dds a better idea to work with these directly.\nYour rendering is of a decent quality, but is by no means a final quality solution, as we have used the default settings across the board so far and haven\u00ddt even touched the sampling settings. To help you understand how you should take these settings further you should open the 02mentalRayExample.max file, which is all set up to render at final render quality, just like our main image. Something you should take a close look at is the materials for this scene, which are all mental ray Arch & Design shaders, so differ from regular materials.\nThings to note are that, like last issue, the interior lighting uses photometric lights, which use a pre-defined photometric web which many lighting manufacturers provide for their products. Just like radiosity, mental ray works with physical lighting, so making sure your units are correctly defined is important as the lighting simulations obey its physical laws. You can imagine that a three-metre high space will look quite different to a three kilometre high space when lit by the same light fittings.\nMost of these tips were touched on last month when we covered radiosity, but they\u00ddre pretty fundamental when working with physical lighting so they\u00ddre worth reinforcing. Next month we\u00ddll continue with three-point lighting.\nTo read the first part of this tutorial, click here\nFor the third part of this tutorial, click here\nFor the forth part of this tutorial, click here\nDarren Brooker is a BAFTA award-winning lighting artist who has worked at many top UK studios. He works for Autodesk as a product specialist. His book, Essential CG Lighting Techniques with 3ds max, is published by Focal Press","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/pci-geomatics-signs-license-with-unb-for-new-advanced-pan-sharpening-algorithms\/","title":"PCI Geomatics signs license with UNB for New Advanced Pan-Sharpening Algorithms","date":1034726400000,"text":"October 11, 2002 \u2013 PCI Geomatics has announced the signing of their latest license agreement for the use of new and advanced Pan-Sharpening algorithms with the University of New Brunswick and Professor Yun Zhang. These algorithms will be built into a new add-on package for Geomatica, PCI Geomatics\u2019 single, tightly integrated software solution for remote sensing, photogrammetry, spatial analysis, and cartography. The new Geomatica add-on package will be available in time for the next version release, currently under development and scheduled for early 2003.\n\u201cWe are very impressed by the technology, which is clearly leading edge and very exciting.\u201d states David Stanley, Vice-President of Research and Development for PCI Geomatics. \u201cWe\u2019ll be providing a lot of new technology in our new release, and this is definitely one of the features that our customers are going to want to know more about, especially for clients using QuickBird, IKONOS, Landsat, and SPOT 5 data.\u201d\nMany new satellite sensors provide high-resolution panchromatic (black and white) imagery along with lower resolution multispectral (color) imagery. Pan-sharpening algorithms \u2018fuse\u2019 the high-resolution panchromatic and low-resolution multispectral imagery together to create a high-resolution color image. The high-resolution color image preserves the original color fidelity and allows for better visualization and interpretation. When coupled with PCI Geomatics high precision satellite modeling and new atmospheric correction capabilities, highly accurate orthocorrected pan-sharpened imagery can be produced from a wide variety of sensors.\nGeomatica is PCI Geomatics\u2019 powerful and complete geospatial information software solution, with strengthened raster and vector processing capabilities. A one-stop \u201cdo-it-all-right\u201d answer, Geomatica is built with the function and flexibility to meet all of tomorrow\u2019s demands for the Remote Sensing, Spatial Analysis, Cartographic, and Photogrammetry professional worlds.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-topcon-acquires-point-cloud-software-specialist-clearedge3d\/","title":"NEWS: Topcon acquires point cloud software specialist ClearEdge3D","date":1519084800000,"text":"Deal gives Topcon 3D modelling tool Edgewise and construction verification software Verity\nPositioning hardware specialist Topcon has acquired all the outstanding shares of ClearEdge3D, the developer of the AEC-focused point cloud software tools, EdgeWise and Verity. Terms for the deal were not announced.\nEdgeWise uses custom algorithms to automatically extract features from point clouds, such as beams, walls, windows, and pipes. The software comes in several discipline-specific versions, including a building variant that is fully integrated with Revit.\nVerity is a construction verification software that compares point cloud data from recently completed construction work against design or fabrication models, flagging out-of-tolerance or poorly installed elements.\nBoth products will retain their respective brands but will be bundled with a number of Topcon products.\nClearEdge3D was quick to allay fears that the acquisition would mean its software tools would be skewed towards Topcon scanners and software, stating on its website that it will continue to maintain a hardware agnostic\/vendor agnostic view of the market.\nRead AEC Magazine\u2019s in-depth review of Verity, where the construction verification software is tested out on a live construction project.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-london-announces-conference-program\/","title":"NXT BLD London announces conference program","date":1497830400000,"text":"Speakers from Gensler, Laing O\u2019Rourke Soluis, Mamou-Mani and Space Group head up AEC Magazine\u2019s exciting new conference at the British Museum on 28 June\nHaving scoured the architecture, engineering and construction community for those pushing the boundaries of AEC technology, NXT BLD is proud to announce 12 amazing speakers for the inaugural event on 28 June at the iconic British Museum.\nHighlights include:\nPhilippe Par\u00e9, Design Director, Principal, Gensler, will share research into VR and AR and how near instant physically-based rendering can be used to explore and validate early stage designs.\nRob Charlton, Chief Executive of Space Group, will look at how emerging Artificial Intelligence (AI) technologies will revolutionise project planning, management and delivery.\nDr Chris Millard, Technical Director, Manufacturing, will share Laing O\u2019Rourke\u2019s vision of the future for fabrication \u2013 robot research and modularisation.\nMartin McDonnell, Soluis, will present the Augmented Worker, a consortium of Autodesk, Microsoft, Laing O\u2019Rourke, AECOM & Doosan Babcock which is exploring the tools needed for effective use of AR on the construction site.\nArthur Mamou-Mani will present \u2018The DNA of Making\u2019, a future construction research project with Arup that uses new generation robots for construction \/ deconstruction.\nJohan Hanegraaf, ArchiSpace, will explain why the future of design will be immersive, presenting his vision for architectural modelling in VR and more.\nTom Greaves, dotproduct, will show the very latest development in reality modelling, capturing as-built conditions with handheld scanners.\nMike Leach, Lenovo, will explain how the latest development in workstation technology will enhance productivity in advanced AEC workflows.\nFaraz Ravi, Bentley Systems Fellow, will present the very latest developments in reality modelling and the opportunities in the advancing intersection of autonomous unmanned vehicles, sensing technology and algorithms.\nTim Guertjens, MX3D, will give his thoughts on the future of Digital Fabrication, including the use of robotics to print metal objects of theoretically unlimited size, focusing on an ambitious canal bridge project.\nDan Harper, Cityscape, will explore how Virtual Reality will change the value of the places we create forever and explain how VR and real time technologies can generate value within a design business.\nNXT BLD is a one day event, taking place in the exclusive BP Lecture Theatre at The British Museum. It is organised by AEC Magazine, in partnership with Lenovo.\nIn addition to the conference, there will be an exhibition with plenty of opportunities to get hands-on with the latest technologies.\nA limited number of tickets are available for \u00a325. Price includes admission, tea \/ coffee and food throughout the day, and drinks at an industry social mixer.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/the-emergence-of-bim-slcs\/","title":"The emergence of BIM SLCs","date":1586304000000,"text":"Abel Maciel and Leo Garbutt of the Construction Blockchain Consortium (CBC) explore the latest developments in Blockchain and its relevance to smart contracts for BIM and digital construction management\nAccording to Statista, by 2025, it is expected that the world will have 75 billion things exchang ing information in theever-growing Internet of Things (IoT). This data flow is becoming a detailed account of all and each one of us and how we interact with our environment. To prepare and respond to this new age of ubiquitous and embedded information, we have to carefully consider data ownership, data veracity and the application of fundamental technologies such as Blockchain.\nBlockchain, also known as the \u2018Trust Protocol\u2019, can be described as the amalgamation of a few technologies for secure decentralised data management and value transaction. The interest in Blockchain has been increasing since the idea of a decentralised cryptocurrency was established in 2008 and Bitcoin was launched anonymously in 2009.\nSince then, the potential of Blockchain has been a well-trodden topic. Put brie\ufb02y, the main arguments for Blockchain adoption are that its data architecture provides security, anonymity, provenance, immutability and purpose of data without any third-party organisation in control of data transactions. Now, the focus must turn to the specifc deliverables and reality of the technology. This article will explore its latest developments and relevance of smart contracts for BIM and digital construction management.\nThe post-BIM construction challenges As a highly project-based sector, the construction sector processes are characterised by tasks typically considered as non-repetitive activities where various professionals transact information to drive a series of events. This is a complex undertaking, with often sub-optimal coordination between the various stakeholders starting from its onset.\nConstruction is also a highly fragmented industry. This fragmentation can be described as informational and organisational, with the vast majority of businesses being comprised of dispersed Small-Medium Enterprises (SMEs). This is consistent with the European average of construction industry structures.\nEffective information coordination in construction projects is a persistent problem characterised by a disconnect between design and construction. This is where BIM plays a central role in improving the industry. However, the provenance of BIM information is still a problem. How can we guarantee we are using the right BIM objects and families? How do we know the contents of objects are accurate or have not been tampered with? Is it safe to give all federated models to the entire integrated team? The consequences are many, from statutory compliance and intellectual property protection to building assets\u2019 cybersecurity issues.\nFrom a wider perspective, circular economies in the built environment are becoming a priority for developers looking to curb the consumption of natural resources, prevent waste and increase effciency through the recycling and responsible sourcing of building resources. BIM platforms and protocols are the natural choice to accelerate changes in this area. BIM brings together project stakeholders under one collaborative platform to produce a federated digital design model and therefore a coherent dataset for the construction and management of a new asset. This has resulted in a fast-changing landscape for construction contracts.\nAbel Maciel is an architect and Senior Research Associate at UCL. He is the founding director of Design Computation Ltd, a specialist consultancy delivering BIM, Programmatic Design, and Smart Contracts. He is also a founding director of the Construction Blockchain Consortium (CBC).\nEnter Smart Legal Contracts (SLCs)\nIntelligent computational contracts appear as an ideal extension to BIM whereby the contractual performance itself becomes automated. Smart Contracts, as proposed today, are of short-term execution or of instantaneous effect. For example, buying a book online on websites like Amazon is automated from the moment the book is paid up to the delivery of the parcel at your doorstep. Design Computation envisions Smart Legal Contracts (SLCs) automating many parts of construction law processes and eventually replacing most or all conventional contracts currently being used in construction. SLCs are applications that run exactly as programmed without any possibility of downtime, censorship, fraud or third-party interference. They have the potential to become fully developed computational legal contracts and improve dramatically all aspects of construction project administration and payment systems in the sector. In fact, we have been prototyping and testing this technology in collaboration with major BIM authoring software developers. We believe SLCs can become the engine for smart infrastructure and the combined circular economy.\nThe use of collaborative BIM has created a necessity for the use of single project insurance. At the same time, collaborative procurement is a decisive enabler of digital transformation and is under intensive study. All the chronic issues identifed in the sector can bring cash \ufb02ow restrictions, such as late payments and misuse of retentions. To address these, enhanced project management and control, transparency and availability of accurate information in project governance are needed.\nSmart contracts operating with BIM processes offer a far more pro-active method for delivering projects. The interoperability of smart contracts and BIM can leverage effciencies in the allocation of accountability in seconds instead of days or weeks. Blockchain can address project complexity and in doing so reduce late payments, remediations and disputes that place companies under cash \ufb02ow risk. On the other hand, the intelligence leveraged from SLCs is desperately needed by contract administrators and law frms.\nA sector-generic and project-bespoke Blockchain-enabled solution can alleviate these repercussions and optimise cash \ufb02ow restrictions. Design Computation\u2019s current solution development has been addressing the growing diversity of Blockchains out there by multichain SLCs technology embedded in BIM authoring software. Our prototype can successfully exchange BIM data with smart contracts operating on 100s of cryptocurrencies and DID formats, therefore offering traceability of BIM objects in a truly interoperable and decentralised framework.\nLeo Garbutt is the Coordinator of the Construction Blockchain Consortium and editor of the CBC Whitepaper Series in association with the APPG Blockchain for the development of future policy and compliance.\nWhere next for BIM?\nAs BIM evolves and furthers the 3D design description model, adding extra dimensions of Time, CapEx, Carbon cost and lifecycle costing, and eventually include more complex dimensions of risk, fnancing and change control performance, asset management and project control performance, Blockchain seems to embody the data architecture necessary to enable the deployment of these new dimensions. This evolution leading to highly integrated work\ufb02ows and closer collaboration will demand for more and better professional transdisciplinary in order deal with future challenges and to future proof design and construction.\nBlockchain offers the means and opportunity to rethink fnancial, social and political relationships informing the built environment. It does so by providing digital assets with some of the properties and behaviours of physical objects. This represents one of the key technologies enabling the cyberphysical convergence in the Industry 4.0.\nThe rapid digital transformation of the construction sectors implies that lawyers struggle to keep up with the pace of innovation. DLT and Blockchain also provides a new foundation for machines and humans to interact and exchange information. As a consequence, we may see disruption in infrastructure management, energy and real estate to autonomous transport and water management.\nBlockchain can be used as an ID for assets for a circular economy, from design to delivery to operation and reuse. For its resilience, it is the favoured technology to connect people, assets and environments over long periods of time. It can create an \u2018automation of trust\u2019 with parties having certainty regarding identity, reputability and a price guarantee.\nConclusions\nIn order to accelerate the understanding and development of Blockchain technologies, more investment and collaboration is necessary. In the past three to four years, extensive industry analysis and education has taken place through major reports published by a number of companies such as PwC, Arup, Deloitte, Thomson Reuters, and the Joint Contracts Tribunal. Government research organisation Digital Catapult and institutions such as the Institute of Civil Engineers and the World Economic Forum have also contributed.\nAlongside continuing investment in piloting the technology, there has been a doubling of tech investment in construction in the past decade. There are a number of well-established open-source initiatives building frameworks such as Hyperledger, R3 and Ethereum. These efforts are increasingly underscoring Blockchain\u2019s potential for BIM and digital construction management.\nThe University College London based Construction Blockchain Consortium exists as a neutral platform to further these objectives through its series of white papers, providing knowledge transfer and assisting the development of use-cases. Furthermore, there are a number of ways to fnd out more about Blockchain and its application in the construction sector.\nThe CBC\u2019s website provides a number of resources, including links to our open source codebase and information about our seminars and annual specialist conference.\nThe CBC Conference 2020 will be held at the Bartlett School of Architecture in Autumn 2020. It was original scheduled for May but was postponed due to COVID-19.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/plant-extracts\/","title":"Plant extracts","date":1193011200000,"text":"This month Autodesk launched its first in-house developed P&ID (pipe and instrumentation diagram) application for AutoCAD. Martyn Day looks at the company\u00dds aspirations for the Plant market.\nNot that many years ago, Autodesk was a single product company. The company\u00dds market dominating 2D CAD tool, AutoCAD, was both a platform for application developers to build niche tools on top of, as well as being an end in itself, replacing the drafting board.\nWhile Autodesk concentrated on making the platform more powerful and feature-rich, the developers tailored AutoCAD to key vertical markets, adding functions that would be useful to the likes of mechanical engineers, electrical engineers, architects, civil engineers and plant designers to name but a few. Over the years this ecosystem grew and all profited, with some developers becoming major software developers in their own right, with their AutoCAD-based applications (DCA software, Eagle Point, Genius, EDA Autoplant, McNeel etc.)\nThen Autodesk decided to \u00d9verticalise\u00dd its development, which meant creating versions of AutoCAD to sell to specific engineering groups. New divisions were created in the company and Autodesk opted for Manufacturing, Building, Entertainment\/Visualisation and Geospatial. The teams could either develop their own applications, or purchase existing applications \u2013 it was obviously quicker to get to market if Autodesk purchased the market leading applications in its key areas. In Building, Autodesk bought DCA software in the States and created Architectural Desktop (ADT), the mechanical division purchased a number of applications, which became Mechanical Desktop (MDT). This was bad news for the specific third-party developers that were now in competition with Autodesk, their own platform supplier.\nSo why am I taking you on a trip down memory lane? Well the notable exception in the verticalisation strategy was Plant Design. Even though there were a number of application developers building on top of AutoCAD at the time, companies like EDA AutoPlant, Autodesk didn\u00ddt select to purchase any of them. At the time I do remember Autodesk helping its key two plant developers to agree terms to merge, providing some financial help. This may have been with a future purchase in mind but for some unknown reason, the successful new entity, called Rebis, ended up being acquired by one of Autodesk\u00dds main competitors, Bentley Systems in 2002. Now in 2007, Autodesk announces its ambitions to get into the Plant market, first with a P&ID application based on AutoCAD, which is now shipping and the company has a 3D system in development, both created from the ground up.\nWhile Autodesk getting into a new vertical market is news in itself, it\u00dds interesting that the company feels that even after opting not to acquire its leading developer, it still thinks it can make a impact after so many years. According to company spokespeople, there are still a lot of vanilla AutoCAD and LT seats in P&ID design. By developing an intelligent symbol library, with intelligent pipe routing and bills of materials, Autodesk hopes to entice enough vanilla customers to upgrade and get good old fashioned productivity benefits. However, that appears to be just the first step, with a non-AutoCAD-based 3D solution coming in the future.\nBeing a late entrant to the market, the existing main players don\u00ddt seem to be all that phased by Autodesk\u00dds latest move. Intergraph and especially Aviva seem to be co-operating with Autodesk. Aviva has even adopted AutoCAD P&ID as its base 2D package, to drive its mature 3D system. Bentley probably has the most to lose, as its AutoCAD-based Rebis installed-base is bound to come under some pressure. Autodesk\u00dds recent development of its own DGN translators, the core format of Bentley\u00dds MicroStation solution, are perhaps an indication of which vendor the company feels it is targeting.\nAutoCAD 2008 P&ID\nDeveloped in less than a year, Autodesk\u00dds P&ID flavour of AutoCAD is a feature-rich and intelligent first release. Part of this is due to extensive beta testing in the USA and the fact that Autodesk started with a clean sheet. Developed using the ARX application development layer in AutoCAD, the object\u00b1based system lent itself well to the key requirements of a library-based system, while providing the familiar interface of AutoCAD.\nThere are some enhancements to the standard interface of AutoCAD but nothing that would cause any problems \u2013 in fact they give the user great feedback as to the state of the design. Autodesk is keen to point out that AutoCAD P&ID doesn\u00ddt require a complex IT support department, which some Plant design systems do.\nAt the core of the P&ID system, lies a massive, intelligent Symbol Library. From the side menu, users simply drag and drop these symbols into the layout space. All symbols are intelligent and the Plant components automatically snap-to one another. The symbols conform to standards for PIP, ISO, and ISA. The AutoCAD P&ID components can be edited and moved using intuitive control grips. Components automatically align and snap into location when placed on process lines. When a process or signal line is moved or edited, components stick with the line, maintaining the right order, orientation, and relationship to the line. Users can customise and convert any group of geometric shapes or lines into distinct components or equipment to meet company standards. New symbols can be added to the project symbol library helping to ensure drawing consistency within the organisation. Substituting an existing symbol on a drawing with a new symbol of a similar type can be done with a single click.\nOf course, to connect your components you need piping lines and AutoCAD P&ID offers dynamic connections which the same intuitive grip editing and manipulation that the components have. It\u00dds easy to create, move, and snap lines into place. If you insert a component into an existing line, the line will automatically break and attach to components that are inserted on, or attached to, the line, as well as automatically mend when a component is removed. The lines intelligently re-route when connected equipment is moved, making complex layouts easy to arrange.\nAs you add objects to a drawing, AutoCAD P&ID maintains the uniqueness of the object across all drawings in the project. This is obviously pretty helpful and can be used prevent users from purchasing the same plant asset multiple times for the project by accident. All symbols and drawing elements have associated data properties which can be entered at any time. P&ID includes a useful Data Manager and Project Manager to set up projects, which may contain multiple drawings and provides the ability to track revisions and manage AutoCAD drawings. It\u00dds also useful to use these to assess the impact of changes and edits.\nWith Excel being used extensively in the Plant industry, AutoCAD P&ID can import and export to Microsoft Excel. It\u00dds possible to share drawing data with other teams by exporting the intelligent project data to Microsoft Excel, where it can be edited and then imported back in. The new updated information goes back the drawings. You can also electronically transmit P&ID drawing files containing embedded information without the need to query and filter data from a database. There\u00dds a powerful search and edit capability embedded into the spreadsheet interface. You can use the Data Manager to sort, filter, and find components in your P&ID drawings and quickly enter data properties specific to those objects. This is a great alternative way to browse project information and details. In this easy to use display, you can see Line numbers, component values, and other data. If you edit in the Data Manager, these are instantly updated in your P&ID drawings. There\u00dds also a cool \u00d9Zooming\u00dd feature within the Data Manager that instantly zooms your drawing window to the appropriate drawing object or record in the Data Manager.\n{mospagebreak}\nConclusion\nThe value proposition of AutoCAD P&ID is a very simple and compelling one. If you use vanilla AutoCAD for P&ID layout this tool will save a massive amount of time. In many ways it\u00dds similar to AutoCAD Electrical, where vanilla AutoCAD is used extensively and the automation provided by Autodesk\u00dds vertical extension delivers great advantages.\nIt will be interesting to see how aggressively Autodesk goes after the Rebis installed-base. Autdoesk representatives talk about disenchanted Bentley customers that want a solution from Autodesk, not Bentley, or customers that are worried Bentley will move their application to MicroStation only. I guess time will tell. Rebis\u00dd greatest asset is that it works both on top of MicroStation and AutoCAD for mixed environments.\nAutodesk is expanding into new areas. Plant, although mature, is a growth area, that now appeals to the new management team of Autodesk CEO, Carl Bass. Above all, P&ID proves that it\u00dds never too late for Autodesk to get into a market and the company\u00dds vast profits and share price means it can invest in product development and marketing. For now, Autodesk is happy to offer its own installed base a solid but basic P&ID solution but the company\u00dds aspiration is to go far beyond that over successive years.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-recap\/","title":"Autodesk ReCap","date":1401148800000,"text":"In 2012 Autodesk launched ReCap, a platform for the growing reality capture market. Since then, the company has expanded its capabilities, added a layer of cloud services and recruited third party developers.\nThe manual survey market was revolutionised by the widespread adoption of laser scanners, which can measure millions of 3D points in seconds, building dense point clouds of surfaces. But there remain a number of downsides to the technology.\nLaser scanners are typically hard to use, expensive and generate huge, unintelligent, process-intensive, files.\nAutodesk has recognised that the laser scanning software market was ripe for a wider audience, and that it could offer a competitive product and provide tools to enable the conversion of \u2018dumb\u2019 scan data to intelligent Building Information Modelling (BIM) models with its ReCap software.\nAt the same time, the laser scanning market has started to see some disruptive technologies, such as the Xbox Kinect, open up new possibilities to potentially lower price points for capturing real-world data in these point clouds. Typically, laser scanners can cost \u00a340K and upwards and the software to edit and manage the point clouds can cost over \u00a310K a seat.\nLaser scanner hardware provider Faro has been instrumental in introducing new, low-cost price points (to around \u00a320k) with its small professional scanners. This has increased adoption of laser scanning, and yet to date the software has remained an expensive but necessary add-on.\nNew low-cost scanners are typically hand held. They provide low resolution scans, and are therefore perhaps best used for interiors or individual objects.\n3D Systems Sense\u2019s 3D scanner costs just $399. It is aimed at non-professionals capturing small objects in 3D using the same technology as Microsoft\u2019s Kinect. 3D Systems software optimises its output for its 3D printers.\ndotproduct offers a sub-$5,000 hand held scanner, the DPI-7, which consists of a cradle \/ handle and an Android 7\u201d standard tablet. The Phi-3D software uses an indoor positioning system to build a 3D map from the scanner. Once enough points have been gathered to create a map, the screen turns green. Intel recently invested a large sum in the firm.\nZebedee\u2019s ZEB1 is an unusual looking handheld scanner with a spring mounted sensor. The scanner was developed by CSIRO scientist Dr Elliot Duff and his \u2018Zebedee Team\u2019, in honour of the children\u2019s TV programme, The Magic Roundabout. The device is able to survey enclosed spaces where GPS cannot reach, such as inside mines, factories & public buildings.\nAutodesk ReCap\nThere are two offerings in Autodesk\u2019s ReCap platform: ReCap Pro, a Windows-based point cloud registration and manipulation program and ReCap 360, an add-on service that uses capabilities in Autodesk\u2019s cloud to provide a portal for laser-scan or photo captured projects.\nReCap Pro is aimed at proving targetless registration of scans in the field and could run on a laptop or Windows Surface tablet, collating and importing multiple laser scans in real time.\nWhile ReCap includes some innovative targetless registration capabilities to assist in alignment of multi-scans, early adopters did wrestle with the accuracy of its results versus traditional target-based registration. In the latest version accuracy can be improved by adding survey points or known distances.\nEssentially, ReCap Pro enables the processing, cleaning, and visualisation of large data sets on the desktop from laser scans. It is hardware independent and supports the import of over 20 industry proprietary, text-based and open formats.\nThe software has a modern, easy to use interface and provides a suite of tools to clean up, edit and organise laser scans.\nThe data alignment between scans can be done by electing the same objects in the scene from different scans. The software uses powerful algorithms to provide registration between the scans.\nData points from surveys can also be incorporated into the model. These provide additional accuracy to ReCap Pro\u2019s registration capabilities.\nAs most scanners incorporate photo images with point cloud data, ReCap can be used to produce 3D interactive or single point panoramic visualisations.\nReCap 360\nWith firms doing more and more laser scanning and with point clouds generating huge files, local storage is not always an option. ReCap 360 provides additional storage space that can be expanded as needed.\nEdited point clouds can be viewed and shared via a web browser, where they can be annotated without destroying the original file.\nReCap 360 comes with 25GB of Autodesk 360 storage and 50 cloud credits for ReCap 360 projects, which can be used to cut processing time.\nFrom the cloud Autodesk manipulates the data remotely and streams the video to local browsers. This takes the processing weight off local machines, but is as interactive as if it were running locally, giving maximum image resolution even on huge multi-scans, which could be terabytes in size.\nReCap 360 offers additional tools such as bounding boxes for easier editing of big point clouds. Once cleaned up, high-density, mesh models can be created to provide a suite of mesh tools and options to create a meshed model that suits a target application or system. Again this all happens interactively through the browser in real-time.\nTo extend the ReCap platform, Autodesk is recruiting software and hardware firms to build add-ons or plug-ins to the ReCap platform. These include:\nCSA is perhaps better known in the process plant and nuclear markets and the Atlanta-based firm comprises PanoMap Laser Scanning Technology and Plant \/ CMS (Plant Configuration Management Systems.\nCSA plans to integrate ReCap with its PanoMap Server to transfer laser scan point cloud data from large-scale 3D laser scan databases to Autodesk products.\nZoller + Frhlich (Z+F) started producing laser measurement systems in 1994, with one of the first 3D optical laser systems for as-built conditions. It develops the Imager range of professional laser scanners and partners with a number of industry software companies. Z+F has joined Autodesk\u2019s Embedded ReCap OEM program for use in its LaserControl software.\nFaro is keen to capture market share with its low cost, small and lightweight Focus 3D scanners, which radically lower the price of entry. Despite having its own software solutions, Faro has a history of openly working with other software vendors, including Pointools and Autodesk. Faro has joined the Embedded ReCap OEM program for its Faro Scene software.\nKubit is an established Autodesk third party developer of surveying products for AutoCAD. ReCap Connect will enable kubit to integrate its PointSense (scanned data post-processor) and PhoToPlan (image rectification) products to access ReCap utilities directly from within the AutoCAD design environment.\ndot.product has already released a plugin for ReCap which allows ReCap to directly import the compressed, binary .dp files created with its DPI-7 hand held scanner, without having to go via bloated file formats such as PTX or PTS formats.\nAveva, a major plant, power and marine design software developer acquired 3D laser scanning software developer LFM in 2011. Aveva has signed up for the Embedded ReCap OEM program to include in its suite of LFM scan-data editing and collaboration products.\nUnder the banner of 123D Catch, Autodesk previously released a product and web service that could take multiple photographs or a Go Pro capture from a aerial drone of an object or a scene and turn these into a textured, meshed 3D model. A high-resolution version of this is built into ReCap 360, enabling photos to be uploaded and textured models produced at the press of a button.\nThere is a suite of tools for manually stitching photos together prior to the meshed model being generated. Survey data can be added to improve the accuracy of the final model.\nMemento\nAutodesk is currently running a Labs project called Project Memento, which is the company\u2019s innovative meshing technology.\nMemento is planned to be a all-in-one solution, taking in data from any reality input capture devices (photo, scans, structured and unstructured), to produce high quality mesh models which can be used for digital workflows.\nAutodesk has recently done work for the Smithsonian group of museums and research centres in the US, capturing artifacts, as well as for a Swedish museum using MRI scans of an ancient Egyptian mummy. The results of this can be delivered through the web or mobile, or possibly used in Autodesk applications like Maya, Mudbox or even 3D printed out.\nThird party\nOne of Autodesk\u2019s tried and tested routes to adoption is through third party developers who enhance the capabilities of Autodesk\u2019s platforms by adding specialised add-ons. With ReCap Autodesk is following that path and is actively recruiting to build on its platform with a ReCap Connect Partnership Program.\nFor now there is a small core and, as one would expect, those competitors with a lot of revenue to lose are not involved. But as ReCap usage grows the days of expensive registration software are numbered.\nThird parties can either write their own import plug-in to import data into ReCap using the Capture Codec Kit (CCK) that is available as part of the new ReCap desktop version, or join the Embedded ReCap OEM programme to export from point cloud processing software directly into Autodesk design products, or to ReCap.\nAs a third option, third parties can build their own application on top of the Autodesk photo-to-3D cloud service by using the ReCap Photo Web API. According to Autodesk, more than ten companies, serving markets ranging from medical and civil engineering, to video games and Unmanned Aerial Vehicles (UAVs), have started developing specific applications that make use of this capability, or have started integrating this capability into their existing applications\nConclusion\nReCap has a great interface and is one of the most innovative desktop \/ cloud service product that Autodesk offers.\nIn the 1980s AutoCAD beat competitors by offering 80% of CAD system functionality at 20% of the price. It is doing something similar with ReCap. Plus. there is also a healthy amount of disruptive new technology in the market to make users think differently about how they design and model.\nThe reality is that 3D modelling is time consuming and software providers need to find ways to limit that by modelling in context, or automating the creation of an as-built model. As a result, Autodesk sees point clouds being much more of a commonly used data format.\nHowever, the price of the scanning hardware still needs to come down considerably.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nAutodesk ReCap: alternatives\nCyclone & Cloudworx\nWith a considerable grip on the laser scanning market, Leica arguably has the most to lose from Autodesk\u2019s recent developments.\nCyclone is Leica\u2019s flagship point cloud registration and manipulation tool. Its Object database and client \/ server architecture is designed to handle very large sets of data and, as a mature product, offers a great depth of functionality.\nLeica also has the CloudWorx family of plug-ins which provide point cloud processing inside popular CAD systems including AutoCAD, 3ds Max, Revit, Microstation and Aveva\u2019s PDMS.\nLFM from Aveva\nTraditionally used in process plant and designed to deal with huge multi-scan point clouds, there are three components to LFM.\nLFM Modeller for creating as built models from laser scans, with a built-in pipes and structures standards library. LFM Server for hosting big databases with automatic registration of multi-scans and features such as clash detection.\nLFM Netview allows collaboration on private or public cloud hosted scans. e.\nFaro Scene\nDesigned for use with the Faro Focus3D scanner, Scene processes and manages scanned data by using automatic object recognition as well as scan registration and positioning together with automated target-less scan positioning.\nThe software provides editing, measuring, meshing, 3D visualisation and the ability to export to various point cloud and CAD formats.\nA \u2018webshare\u2019 publishing feature allows point clouds to be streamed to browsers.\nBentley Pointools\nAcquired by Bentley in 2011, UK-based Pointools was a leading, independent point cloud software developer. The company\u2019s point cloud pre-processing technology has since been incorporated inside MicroStation V8i, providing considerable capability to Bentley\u2019s core design tool and vertical industry products\nTrimble RealWorks\nThis is Trimble\u2019s point cloud editor for combining 3D scanned and position data to produce CAD models and video simulations.\nOriginally coming from the process plant world, the software includes \u2018automatic\u2019 modelling of pipes through a technology called Easypipe.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/new-utility-designed-to-automate-repetitive-task-in-revit\/","title":"New utility designed to automate repetitive tasks in Revit","date":1606435200000,"text":"Ideate Automation allowing Ramboll to perform previously fragmented automation tasks and increase its efficiency\nIdeate Automation for Revit is a new scripting tool for use with Ideate BIMLink designed to let \u2018repetitive, time-intensive, low-value BIM tasks\u2019 run silently in the background, freeing up designers and engineers so they can focus on higher-level work.\n\u201cWe were waiting with anticipation for the release of Ideate Automation and we have not been disappointed by its potential,\u201d said Michael Hartley, technical BIM Manager at Ramboll. \u201cWe have started implementing Ideate Automation and the tool is already influencing our workflows. Ideate Automation is allowing us to perform previously fragmented automation tasks and increase our efficiency whilst offering us new possibilities for data collection. This is one tool we have been missing and finally helps connect the dots!\u201d\nThe software is optimized to work with Ideate BIMLink, which enables Revit users to pull information from a file into Microsoft Excel for editing and then push volumes of precise, consequential BIM data back into the Revit model.\nIdeate Automation is available for $995 per company.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-agacad-introduces-concept-of-infinity-bim\/","title":"NEWS: AGACAD introduces concept of \u2018Infinity BIM\u2019","date":1489104000000,"text":"New approach to data management will help smooth path to buildings with artificial intelligence\nBIM software specialist AGACAD is working on a new approach to data management to help project participants get access to the information they need at just the right time throughout a building\u2019s lifecycle.\nThe Lituaninan company\u2019s \u201cInfinity BIM\u201d project centres on a new data-management platform that is designed to create and deliver data more efficiently, on a leaner real-need basis, just-in-time and in just the right amount for each participant.\nThe \u2018Infinity BIM\u2019 moniker came about as a reaction to the overuse of BIM \u2018dimensions\u2019, which will only expand more as the AEC industry moves towards artificially intelligent building processes and structures.\n\u201cThe \u2018big bang\u2019 for BIM was 3D design, a true watershed in AEC history,\u201d explains Donatas Aksomitas, AGACAD CEO. \u201cEmergence of the so-called fourth, fifth, sixth and seventh dimensions of BIM has been decidedly less revolutionary. They comprise, in roughly historical order, the addition to models of scheduling (4D), costing (5D), sustainability (6D) and facility management (7D) dimensions. We end up with a sort of periodic table of the elements, a list that grows as new BIM uses are popularised.\n\u201cEach new use brings new human and non-human actors and thus new needs for data exchange. Revit add-ons, clouds, project management software, ERP. CNC machines, panel manufacturers, 3D printers. Robots for layout and assembly. Virtual Reality systems in the office. Augmented Reality devices on-site. As-built BIM models linked to the Internet of Things: for building supervisors, security systems, maintenance crews, and tenants.\n\u201cIt\u2019s the onset of Industry 4.0 in AEC. And along this inevitable road to fully embracing AI, it no longer makes sense to add individual new BIM \u201cdimensions\u201d: 8D BIM, 9D BIM and so on. What\u2019s needed is \u201cinfinity BIM\u201d \u2013 technology able to effectively manage building information for any conceivable use.\u201d\nAksomitas recently launched a blog to examine the underlying methodological issues. AGACAD\u2019s hope is to have something to show on the platform side by the end of this year.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/geovue-announces-new-cfo\/","title":"geoVue announces new CFO","date":1185840000000,"text":"Woburn, USA, July 30, 2007: geoVue, a leading provider of dynamic location optimization software, today named Dan Kelly its CFO.\nKelly brings more than 20 years of financial management experience in software and services to geoVue. Previously, he was CFO of Ecora Software. Before that, he was CFO at Scribe Software. Prior to that, he was Corporate Controller at Clearway Technologies. Kelly also has Big Four experience.\n\u201cWe\u2019re seeing an extraordinary uptake in demand for our solutions because retailers, restaurants and other real-estate based consumer channels are coming to realize that we can provide them with a higher return on their capital investments,\u201d said Rudy Nadilo, CEO of geoVue. \u201cWe hired Dan, because it was important for us to have a seasoned CFO in place who could build and implement the required financial infrastructure to take geoVue to the next level.\ngeoVue goes beyond simple site selection. It optimizes locations. Using its innovative solutions and domain expertise, geoVue develops predictive store trade areas that allow clients to decide which markets to enter, expand or exit; how they should optimize store networks within each market; and how to localize marketing and merchandising.","source":"geospatialworld.net"}
{"url":"https:\/\/www.wired.com\/story\/trump-signs-executive-order-ai-state-laws\/","title":"Trump Signs Executive Order That Threatens to Punish States for Passing AI Laws","date":1765411200000,"text":"President Donald Trump signed a highly anticipated executive order on Thursday that sets in motion a plan to establish a national regulatory framework for artificial intelligence while undercutting states\u2019 abilities to enact their own rules.\nThe order, titled \u201cEnsuring a National Policy Framework for Artificial Intelligence,\u201d creates an AI litigation task force within the Justice Department to directly challenge state AI laws the administration finds to conflict with federal policy. It also directs the Department of Commerce to craft guidelines that could make states ineligible for future broadband funding if they pass \u201conerous\u201d AI laws.\nThe push for sweeping federal preemption of state AI laws has largely been fueled by AI investors, conservative policy shops, and tech industry trade groups. These groups have argued that a patchwork approach to AI regulation could stunt Silicon Valley\u2019s AI boom and reduce America\u2019s competitiveness on the global stage. White House AI and crypto adviser David Sacks has been one of the most vocal proponents of a light-touch approach to AI regulation.\n\u201cThe EO gives your administration tools to push back on the most onerous and excessive state regulations,\u201d Sacks told Trump during Thursday\u2019s signing ceremony. \u201cWe're not going to push back on all of them. For example, kids safety we're going to protect.\u201d\nThe order is similar in many respects to an earlier draft obtained by WIRED but with a few key differences. The executive order instructs Sacks and Michael Kratsios, the assistant to the president for science and technology, to prepare a legislative recommendation establishing a federal policy framework for AI. One of the new additions is a carve-out within this legislative recommendation asking Congress not to preempt state AI laws that aim to protect children, promote data center infrastructure, and encourage state governments to procure AI tools.\n\u201cWe want one central source of approval, and we have great Republican support. I think we probably have Democrat support too, because it\u2019s common sense,\u201d Trump said during Thursday\u2019s signing ceremony. \u201cEvery time you make a change, and it could be a very reasonable change, you still won\u2019t get it approved if you have to go to 50 states. This centralizes it.\u201d\nIn the absence of federal regulations, officials from states across the country have pushed through their own investigations and legislation to govern the use and development of AI. Trump\u2019s executive order specifically calls out certain state AI laws\u2014such as Colorado\u2019s SB24-205, which aims to limit \u201calgorithmic discrimination\u201d in AI models\u2014as an attempt to \u201cembed ideological bias.\u201d\nSeveral other state AI laws may also fall in the crosshairs of this executive order. California governor Gavin Newsom signed a law in September requiring large tech companies to publish safety frameworks around their AI models. In June, New York\u2019s legislature passed a bill that would empower the state\u2019s attorney general to bring civil penalties of up to $30 million against AI developers that fail to meet safety standards. That bill is currently sitting on New York governor Kathy Hochul\u2019s desk, awaiting her signature or veto\u2014though she\u2019s reportedly considering amendments that could weaken the bill significantly.\nShortly before the order was signed on Thursday, several state attorneys general were quick to criticize the Trump administration\u2019s attempt to curb state power.\n\u201cNow is not the time to let this new technology progress unchecked. AI development and deployment are happening quickly, and state attorneys general are the most agile regulators we have historically,\u201d New York attorney general Letitia James said in a briefing with reporters on Thursday. \u201cIt has always been collaboration, not conflict, between state legislatures and Congress that yielded some of the most critical federal legislation in our country\u2019s history.\u201d\nWhile the order may set a national tone for AI regulation, Trump does not have the authority to bar states from continuing to pass their own laws. Civil rights groups, including the American Civil Liberties Union, have called the order \u201cunconstitutional,\u201d and it\u2019s likely to be challenged in court.","source":"wired.com"}
{"url":"https:\/\/aecmag.com\/news\/news-vr-support-added-to-graphisoft-bimx\/","title":"NEWS: VR support added to Graphisoft BIMx","date":1480377600000,"text":"Users of presentation and communication app can now navigate buildings in VR using Google Cardboard\nGraphisoft\u2019s BIM presentation and communication app, BIMx (, now include Virtual Reality (VR) through support for Google Cardboard.\nUsers of BIMx with Google Cardboard viewer (version 2.0) for Android and iOS smart phones can navigate BIM models simply by turning their heads in the desired direction. To aid navigation, users can save views, stop walking, or take a step back.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nNEWS: Radeon ProRender for Cinema 4D + Blender\nNEWS: Visqueen BIM objects now available in NBS National BIM Library\nEnscape 3.1 gets AI upscaling to boost 3D performance\nHP bets big on AMD Ryzen AI Max PRO processor\nNEWS: Business Collaborator adds integrated BIM viewer\nISG transforms scan-to-BIM workflows with Pointfuse\nVideo: NXT BLD London conference \u2013 Martin McDonnel, Soluis \/ Sublime\nUnreal Engine to get reality capture boost\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autocad-2010-releases\/","title":"AutoCAD 2010 releases","date":1240704000000,"text":"It is that time of year again when Autodesk retires an old version of AutoCAD and introduces a new generation. Featuring 2D parametrics and conceptual 3D, AutoCAD 2010 represents a change of direction for the industry de-facto standard. Martyn Day and Paul Woddy give us their views of the product launches.\nThe first thing that is apparent about the launch of the new AutoCAD is just how many products Autodesk now develops and supports. Since Carl Bass took over stewardship of the company there has been an explosion of products, especially those that are 3D capable, together with analysis and visualisation. The days of Autodesk being a one horse town are well and truly over.\nThe 2010 product catalogue has swelled over recent years as Autodesk\u2019s AEC division has covered more industry areas and acquired more companies and technologies. Platform products include AutoCAD and LT for the base for AutoCAD Architecture (formerly Architectural Desktop), AutoCAD MEP (Mechanical Electrical and Plumbing), AutoCAD Civil and Civil 3D, AutoCAD Map and AutoCAD P&ID (for Process Plant). For project document distribution there is Buzzsaw, Design Review and NavisWorks. On the analysis side Autodesk now has Ecotect, Green Building Studio and Robot Structural Analysis. Design Visualisation is supported through 3ds Max Design and Maya, with Project Newport still in development. And these are just the products I can remember off the top of my head, without mentioning Revit Architecture, Revit Structure and Revit MEP. While we have not yet had the opportunity to review the products themselves, we can report back on the features identified at the launch events. However, Paul Woddy, our Revit Guru, has clocked up some time on a recent beta of Revit.\nAutoCAD 2010\nAutoCAD 2010 (Autodeskers call it \u2018twenty ten\u2019) has had 2,000 beta testers and some of the features were released to Autodesk Subscription customers last year. To go with all the powerful new features, unfortunately, 2010 introduces a new DWG file format but there is a built-in \u2018SaveAs\u2019 function that allows users to save drawings to and from AutoCAD releases using previous DWG formats. In addition, the SaveAs AutoCAD Release 12 DXF command supports releases prior to AutoCAD 98.\nThe first thing you will notice is a simplified and intuitive ribbon interface, which organises tools into tabs, which represent workflows such as creation, annotation and collaboration. Each tab contains a series of panels \u2013 a group of tools used to accomplish that part of the workflow. The ribbon is now consistent between most Autodesk programs allows users to move between programs more smoothly.\nLooking at 2D the most obvious enhancement is the introduction of parametrics. To the uninitiated, parametrics allow the user to apply persistent relationships between geometry, parallel lines remain parallel and concentric circles remain centred, all automatically. In AutoCAD 2010 these can be applied manually or automatically to any 2D geometry, with control over tolerances. As constraints are added icons appear next to the lines giving immediate feedback to the user. Constraints can even be added through dimensions, by editing the dimension, the drawing updates with the new length or angle. A management dialogue provides a comprehensive way to see all the constraints in a drawing where all the values can be seen and edited. This is brilliantly powerful stuff, with a very simple user interface and will be incredibly useful to many, many users.\nPDF has been updated. Users can now plot to PDF with much more control. AutoCAD 2010 supports higher resolutions, better font handling for true type fonts (making PDFs searchable). PDFs can now contain layer information and can be attached as an underlay. To export to DWF or PDF there is a combined dialogue for the selection of settings.\nThere are quite a few other improvements to the 2D feature set, like Hatch editing, so it would be worth checking the product data sheet or getting a demonstration from your local dealer. However, what is more impressive in this release is just how much AutoCAD has had its 3D capability beefed up, almost out of all recognition.\nAutoCAD 2010 now offers surface freeform modelling. The 3D workspace has been updated and users can start a session by selecting from a range of primitives. Simply push\/ pull faces, edges, and vertices to model complex shapes, add smooth surfaces, and much more. It is possible to interactively create any shape and it looks really fun and easy to use. The new modelling capabilities provide conceptual designers with something to stick their teeth into and attempts to blow away the modelling capabilities of products such as SketchUp.\nAutodesk has also enabled AutoCAD 2010 to connect to a remote 3D Print service, so prototypes can be delivered to your door, or if you are advanced enough to have your own desktop prototyping machine 3D print it there and then.\nAutoCAD LT 2010\nThe difference between LT and its big brother has been growing for a number of releases now. LT is most certainly a 2D workhorse. This time around, LT gets: the updated ribbon interface, the new DWG format with backwards capability, AutoCAD 2010\u2019s PDF improvements and underlay capability, enhanced External Reference commands for in place Reference editing and clipping, more Block Attribute commands and Align Objects.\nAutoCAD for Architecture\nFormerly Architectural Desktop, AutoCAD for Architecture has undergone a considerable realignment within the Autodesk AEC portfolio, specifically with regard to the growing Revit solutions and its exclusion from the world of Building Information Modelling (BIM). It has been noticeable that Revit has been getting many more of the new exciting 3D features, achieving greater product velocity than AutoCAD for Architecture. AutoCAD for Architecture gets the great conceptual benefits of the new modelling and surfacing tools in the underlying AutoCAD, as well as all the other UI and 2D goodies. These are certainly not inconsequential, but there are only a handful of enhancements flagged up for AutoCAD for Architecture.\nWalls have seen some neat updates for drafting productivity again this release. Trim and Extend have been joined by Fillet and Chamfer, providing more control in designing walls and end-caps as they will actually be constructed in the field. There is a new \u2019Space Separator\u2019 tool, allowing users to automatically divide spaces that are not bound by walls with plain AutoCAD linework. A new flip text position grip for AEC Dimensions has been added, giving users additional control over the placement of text. There is an improved stairs feature, allowing the inclusion of a distinct \u2018ramp\u2019 type, which comes complete with its own display and annotation tags (Percentage Tag, Degree Tag and Numeric Tag).\n{mospagebreak}\nAutoCAD Architecture software is interoperable with certain Autodesk products and third-party applications for specific needs, allowing for more effective collaboration with extended design teams. For instance, Part files and assemblies from Autodesk Inventor can be exported to AutoCAD Architecture software as MV Blocks, meaning that 2D and 3D views of the imported content is possible. Also when exporting designs via industry foundation class (IFC), relevant space boundary information is now attached for proper energy analysis for AutoCAD MEP software.\nAutoCAD MEP 2010\nTalking of AutoCAD MEP (Mechanical, Electrical, Plumbing), there has been a range of drafting and co-ordination improvements. Workspaces are better tailored for tasks such as HVAC and piping systems in ready-made formats, which are also highly customisable. Gravity based designs for Sanitary Drain, Waste, Vent and Storm\/Sewer drainage systems can now be drawn more easily with the new sloped piping functionality. Pipe routing preferences have been expanded to include parts with male and female connections.\nCatalogue support has been enhanced, supporting individual catalogues for different material types, allowing for easier part selection. There is also better migration of existing customised catalogues with tools to redefine the catalogue-based content in existing drawings, and upgrade and add new parameters automatically. It is also possible to import manufactured building components from Autodesk Inventor. As you would expect, new part content has been added and there have been enhancements to the Content builder, allowing users to create custom parametric parts with male, female or a combination of both connectors based on real-world piping requirements. Autodesk\u2019s Seek web service has also be integrated, enabling designers to search a vast library of 2D and 3D building products and publish customised designs to the service.\nAutoCAD Civil 2010\nOn top of the 2010 AutoCAD enhancements, Civil gets some attention to its surveying and Design components. A new \u2018Intersection Design\u2019 wizard helps take the complexity out of modelling typical intersections and facilitates the creation of dynamic 3D intersection models, helping to automate the creation of intelligent intersections that can be updated more easily when the design changes.\nSurvey data can be processed directly from the field without tedious translations or conversions and parcel generation offers a more streamlined workflow, based on frontage offset, minimum width, and minimum and maximum depths.\nTwo new design tools have been introduced: Alignment Offsets for synchronising offsets to the original alignment, and Alignment Masking for controlling alignment and label displays beyond the functionality of the alignment styles.\nConclusion\nAutoCAD 2010 is a major step for Autodesk. Internally it seems the product teams have decided that they can\u2019t suppress AutoCAD\u2019s functionality any longer and have added parametrics and relatively high-end modelling. This may impact Revit and Inventor migration, however the company has perhaps finally realised that many customers are happy with basic AutoCAD. This is especially pertinent given that the people behind TurboCAD, IMSI have launched a free-to-download AutoCAD clone called DoubleCAD which also features parametrics. With a tough economic climate AutoCAD seats and subscriptions will surely come under some pressure, it seems like the decision to beef up AutoCAD\u2019s functionality came at the right time.\nThere has also been a 5 percent price increase, across the board. It\u2019s best to contact your dealer to get local information on on this may affect you. Perhaps the worst impact will be on AutoCAD LT which already appears to be increasing in cost quicker than a bank bailout. With increased competition, an economic meltdown and a number of alternatives for 2D drafting it is perhaps not the best timing for this.\nThe DWG file format change will again cause disruption in the market and may well delay company roll-outs of the 2010 products. It\u2019s of note here that Autodesk didn\u2019t include the native MicroStation DGN file format technology that they cross-licensed from Bentley Systems last year, this will surely appear in the next release, if not streamed in to the new updates available to Subscription customers, delivered throughout the year.\nThe product Tsunami that Autodesk unleashes every year just gets bigger every time. With increased effort spent on addressing compatibility limitations though the product range, Autodesk is finally sorting out the muddle created by having so many products developed by different teams, on different code. 2010 marks the first concerted effort to enable models and drawings to be shared and swapped between the various AutoCAD flavours as well as the mechanical products like Inventor.\nAs to AutoCAD Architecture vs Revit, again, yet another release that makes it clear that Revit is the architectural product of choice with only a few minor 2D additions to the AutoCAD-based AEC flavour. Revit\u2019s functionality is expanding at an amazing rate and with a huge groundswell of architectural and structural firms piloting projects, it seems migrating to Revit is the only way to get a dynamically evolving AEC solution from Autodesk.\nWhile it may well take the rest of the year to work our way through reviewing these products, on paper at least, it seems as though Autodesk will deliver a good release of AutoCAD.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/digital-fabrication\/aectual-launches-service-for-custom-3d-printed-architectural-products\/","title":"New service for custom 3D-printed architectural products","date":1610582400000,"text":"Design-to-delivery platform enables rapid production of floors, panels, furniture and other architectural solutions\nAectual has announced the global beta launch of its \u2018design-to-delivery platform\u2019 that \u2018rapidly produces\u2019 bespoke 3D-printed XL architectural and interior products at industrial scale.\nThe Amsterdam-based company offers modern 3D-printed architecture and interior items, such as terrazzo artwork flooring, fixtures, wall panelling, columns, fa\u00e7ades, stairs, room dividers, planters and table screens \u2014 even entire buildings \u2014 that are all made from 100% circular, sustainable recycled and renewable materials.\nAccording to Aectual, its design-to-delivery process is said to reduce the cost of custom-made architectural products by 50%, is up to 10 times faster, eliminates waste, and reduces materials usages and CO2 emissions.\nThe Aectual platform utilises customisable engineered parametric products, an easy-to-use (customised) design dashboard and proprietary robotic XL 3D-print technology.\nAEC-industry professionals can upload and create their own product versions or simply go to the Aectual website, pick a terrazzo floor pattern, wall panel, room divider, or sun canopy, for example, and then customise it to their taste. Once ordered, Aectual 3D-prints and installs the product.\nPrices start at $24 per square foot (\u20ac200 euros per square metre). According to Aectual, items are typically more affordable than their conventional, custom-made counterparts.\nCommercial projects already deployed globally by Aectual include flooring in Amsterdam Schiphol International Airport, flooring in BMW World in Munich, printed display walls in Nike Town London, a tiny bauhaus (aka studio shed or she shed), flooring that uses recycled Budweiser bottles at Capital C offices in Amsterdam and the temporary EU building in Amsterdam.\nHedwig Heinsman, co-founder and Chief Commercial Officer, Aectual spoke at AEC Magazine\u2019s NXT BLD event in 2018.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-3d-laser-mapping-to-revolutionise-building-surveys\/","title":"NEWS: 3D Laser Mapping to \u201crevolutionise building surveys\u201d","date":1418342400000,"text":"New solution combines ZEB1 hand held laser mapping system with PointCab point cloud processing software\n3D Laser Mapping claims its new solution will revolutionise the production of highly accurate building floor plans. Combining the ZEB1 hand held laser mapping system with point cloud processing software, the company says the technology allows users to scan and produce detailed floor plans in a matter of minutes.\nThis announcement follows an agreement between UK based 3D Laser Mapping, worldwide distributors of the ZEB1 hand held mapper, and PointCab, a German software company specialising in the processing of laser scanned data.\n\u201cThe combination of ZEB1 and PointCab creates a truly powerful solution for building surveyors,\u201d commented Charlie Whyman, global sales and marketing manager, 3D Laser Mapping. \u201cBoth are easy to use with little or no training, both are fast and effective and both are well supported. Trials of the combined solution have shown that a multi room facility can be scanned, the point cloud data processed and a vector model produced, all in less than twenty minutes.\u201d\nFrank Torno, head of administration of PointCab added, \u201cThe ZEB1 is ideally suited for building surveys as it is truly portable and can be operated in confined spaces and difficult to access areas. The seamless workflow of raw data into PointCab combined with the unmatchable speed of processing make the ZEB1 PointCab partnership unbeatable for a range of applications including building surveys.\u201d\nDeveloped by CSIRO and commercialised by GeoSLAM, ZEB1 uses robotic technology called Simultaneous Localisation and Mapping (SLAM). The ZEB1 system includes a lightweight laser scanner mounted on a simple spring mechanism, which continuously scans as the operator walks through the environment. As the scanner loosely oscillates about the spring, it produces a rotation that converts 2D laser measurements into 3D fields of view. Its ability to self-localise is said to make ZEB1 ideally suited for use indoors, underground and in other covered environments where traditional solutions that utilise GPS don\u2019t function.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bentley-acquires-s-cube-futuretech-extends-concrete-design-capabilities\/","title":"NEWS: Bentley acquires S-Cube Futuretech for concrete design tools","date":1518134400000,"text":"Acquisition of Mumbai-based software developer will help build presence in India, South East Asia, and the Middle East\nBentley Systems has acquired Mumbai-based structural software developer S-Cube Futuretech. The acquisition will specifically help Bentley better support structural concrete building engineers and designers in India, South East Asia, and the Middle East.\nS-Cube Futuretech\u2019s applications, which include RCDC, RCDC FE, RCDC Plan, and Steel Autodrafter, cover structural concrete analysis, design and documentation. Automated concrete documentation is localised to regional requirements. Bentley says the tools are complementary to its current structural analysis, steel design, and BIM applications, STAAD, RAM, and AECOsim Building Designer.\n\u201cMore than two billion people will fill the world\u2019s cities over the next 30 years. This growth will continue to be concentrated in the developing markets of Asia, particularly India, China, and South East Asian countries. Efficient, automated, concrete design and detailing tools will be instrumental to every nation\u2019s ability to address this demand,\u201d said Raoul Karp, Bentley\u2019s vice president of design engineering analysis.\nSince 2014, Bentley Systems and S-Cube have collaborated to develop fully integrated concrete design and analysis applications.\n\u201cWe are excited to be joining with S-Cube\u2019s expert team whose proven technology provides interactive and automated concrete design and drawing capabilities to our STAAD, RAM, and AECOsim Building Designer users. S-Cube brings a wealth of experience in providing concrete design and drawing automation for the India, Middle East, and Southeast Asia markets,\u201d said Santanu Das, senior vice president of design engineering for Bentley.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/video-nxt-bld-2018-london-conference-bruce-bell-facit-homes\/","title":"Video: NXT BLD 2018 London conference - Bruce Bell, Facit Homes","date":1530144000000,"text":"Pre-fabrication has had its day \u2013 Digital Construction is the future \u2013 NXT BLD London, June 2018\nBruce Bell offered the most unique take on digital fabrication as his firm has been doing it since 2007. Client\u2019s bespoke houses are designed in Revit, from which G-code is derived through in-house secret sauce. On site, a shipping container, containing a CNC cutter, produces buildings in highly insulated, environmentally friendly wood box sections.\nBell took the concept of digital fabrication in factories to task. Given the number of houses the UK needs to build what kind of factories would we need to build? Looking at the Legal and General factory as a case in point, which aims to produce 3K prefabricated houses a year, to get that up to 50K a year it would take up to a third of all warehousing in existence in the UK. While part of the solution, it can\u2019t be THE solution.\nBell, then explained how Facit Homes is using digital manufacturing on a small scale to produce custom residential buildings, using digital tech to control every aspect of production to delivery.\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/the-rise-of-the-robots\/","title":"The rise of the robots","date":1449532800000,"text":"Martyn Day evaluates the latest projects and technologies involved in deploying robots and 3D printing in construction\nWhile the general media seems happy to stir up fears of mass-unemployment and social instability brought about by the infiltration of robotics into traditional middle-class jobs, the construction industry remains relatively untouched by automation, despite decades-old flagellation over market inefficiencies.\nBuilding Information Modelling (BIM) has been touted as a way to reclaim some of this inefficiency but, looked at a different way, it is also the entry point to robotic fabrication.\nIn the manufacturing world, the move to 3D modelling enabled Computer Numerical Controlled (CNC) machining and 3D printing, the core asset of which is the creation of a 3D model to drive the software, to drive the machines.\nAs a model-centric approach becomes more mainstream in AEC, it will inevitably drive the digital fabrication of components or complete buildings. However, today\u2019s attitude towards creating BIM models is more about documentation than driving fabrication and will need another step-change to enable model-driven fabrication.\nWhile much of this work resides in the research labs of universities, there are companies like Laings, which is actively seeking to deploy rapid fabrication technologies that hitherto have been the preserve of the automotive industry. Laings is doing so with its Design for Manufacture and Assembly (DfMA) approach to modular construction.\nA number of other highly-publicised projects are also seeking to demonstrate that robots and 3D printing can be utilised effectively on large-scale projects.\nModular and Prefab\nModular design and prefabrication has long been a popular subset of AEC. It has been proven to work for \u2018protocabins\u2019, McDonalds restaurants, emergency shelters and caravans, which need to be \u2018manufactured\u2019 rapidly and deployed in weeks.\nHowever, despite many attempts, prefabrication has generally failed to get much traction in construction until relatively recently. There are now a number of firms, such as China\u2019s Broad Sustainable Building (BSB) company, which are working out how prefabrication can have benefits without the old drawbacks.\nAutodesk VP strategic industry relations Phil Bernstein recently examined how technology changes are pushing the AEC industry towards embracing prefabrication.\nMr Bernstein envisions that buildings will be \u2018assembled\u2019 and then mass customised, enabling sophisticated design changes even though components are configured within a production line environment.\nMr Bernstein gave a number of examples that, he said, prove that it is possible to utilise digital fabrication and have a unique end result. For example, BSB built a 57-storey tower, with 800 apartments in 19 days (producing an amazing three storeys per day) by using prefabricated components.\nThe B2 Pacific Park building in Brooklyn\u2019s Navy yard had a very complex design that used prefabricated components to build 32-storeys, 363 apartments, and 930 modules.\nFacit Homes has developed a unique way to employ digital fabrication within its BIM process for domestic dwellings. Using Revit, customers work with the firm to design their individual home.\nThe Revit model is then used to generate GCODE to run a CNC milling machine, which is shipped to the construction site in a shipping container. The building is assembled from insulated wooden box sections, which are cut fresh each day wherever the building site is and controlled back at base in London.\nFacit Homes managing director Bruce Bell explained to AEC Magazine why the traditional view of prefab buildings from factories will not work in the residential sector. \u201cThere is a direct correlation between factory fabrication and repetition because you can\u2019t have factories sitting idle due to the overheads. So, as soon as you have a factory, you need turnover and in order to have turnover, you need standardisation and you end up producing the same thing over and over again.\n\u201cIf you build on-site, which the vast majority of buildings are, the constraints are completely different and fabricating on demand has benefits such as having no heating, storage costs etc, as running a factory would.\n\u201cThe economics (of prefab) just don\u2019t stand up. It leads to standardisation and people don\u2019t want the same, and every site has its own requirements. There is no one size fits all.\u201d\nThe robots are coming\nRobotics use in construction remains embryonic, with today\u2019s plinth-located manufacturing robots making Doctor Who\u2019s Darleks look positively advanced.\nHowever, there are a number of projects that aim to teach robots to weld and lay brick. To do this it is important to overcome platform immovability, limited arm reach, onsite spatial awareness and real time clash detection.\nSafety is also a major concern as these robots will be more than likely working alongside humans. (See box out on page 14 for some of the latest developments).\n3D printed Buildings\nGiven that 3D print technology has been around for such a long time, there remains considerable hype around its application. This is probably, in many ways, due to its fall in price to address the emerging \u2018maker\u2019 and consumer markets.\nNow that hoopla has died down, the reality is dawning on AEC professionals that those with engineering knowledge and CAD skills can manufacture with a growing range of desktop machines. While the print technology progresses slowly, there has been a great leap forward in materials that can be used.\nI have seen 3D printers that use chocolate, mud with seeds, plastic, cake mix, candy, ceramic, rubber, colours, UV curing liquid and various metals. It was only a matter of time before concrete and clay became available on the 3D print menu.\nMany 3D printed buildings currently come out of China, although many do not appear to live up to the 3D printed label. For example, Zhuoda Group claimed to have produced a 1,100 square metre \u2018neoclassical mansion\u2019 featuring multi-storey (five floors) and decoration in just 10 days. However, further investigation reveals that 3D print was used to generate components in a factory, which were delivered on site, not \u2018cast\u2019 in situ from a roving 3D print head.\nWinSun has developed its own system \u2014 a 3D printer array that stands 6.6 metres high, 10 metres wide and 40 metres long.\nThe \u2018print engine\u2019 sits in WinSun\u2019s factory and fabricates building parts in large pieces. These are shipped and assembled on-site.\nWinSun claims the process saves between 30%-60% of construction waste, can decrease production times by between 50% to 70%, and labour costs by between 50% and 80% percent.\nYingchuang New Materials claims to have \u2018printed\u2019 up to ten buildings in 24 hours. Each \u2018house\u2019 was made for less than \u00a33,000.\nSo far the Chinese company has spent 20 million yuan (\u00a32 million) and taken 12 years to develop its additive manufacturing device. The only sections not produced by the printer were the roofs.\nChinese companies are also keen to find new materials, such as using recycled concrete from unwanted buildings to produce new 3D printable concrete. However mixing concrete with fibreglass and different resins could lead to health issues should anyone actually live in these buildings. The materials science of 3D printed buildings is still some way off.\nAmsterdam\u2019s Dus Architects has been experimenting with plant-oil based materials to create a 3D printed house on its open source KamerMaker (room maker) 3D printer. Again, due to build size issues, 3D printing is used to create 2 x 2 x 3.5m high sections of the design, which are stacked up like Lego bricks to create a 3D printed equivalent of a Dutch gabled canal house. The project started in 2014 and is set to last three years.\n3D printing concerns\nThere are many challenges for 3D printing buildings. Physically there is a need to have a huge frame around which the 3D print head can move, otherwise it will remain as print sections and assemble. The materials need to be durable and fit for purpose and consistent \u2014 you do not want air bubbles or material weakness in supporting loads, for example.\nThere are also many problems with printing 3D buildings in concrete. The first problem is the model has to be constructed in a way to get the best fabrication success rate, which will certainly not be the same as producing a BIM model to produce drawings. In addition to the BIM model for architecture and a BIM model for construction, at the moment it would require a model for digital fabrication too.\nConcrete curing times have to be taken into consideration. The print head needs to travel as fast as possible and the material deposited needs to solidify and harden within minutes.\nSuddenly architects will find themselves being faced with questions that engineers and industrial product designers face every day when designing cars, planes and consumer products. When buildings are made of prefabricated components or 3D printed, they become more like machines, more an assembly that needs to be durable and repairable.\nAEC professionals need to consider how to build in structural elements, reinforcement and lighten non-supporting walls. Should the walls be fabricated in one long continual \u2018print\u2019 or be broken down? Should ducting or spaces for ducting be included in the 3D model and what would that mean to later refurbishment or alterations\/repairs?\nHow will the material consistency change over the time of the 3D print? Will the weather negatively impact cure times? How long is the material guaranteed for?\nWhat\u2019s the toxicity of the material? Can one material fulfill all criteria for each part of the design? What are the legal issues?\nThere are also fundamental problems with devising shapes for manufacture in today\u2019s AEC tools, which quite frankly were never designed with 3D print or direct manufacture in mind.\nThis area should improve over time as cement companies like LafargeHolcim experiment with extrudable and quick curing materials.\nComplex forms\nSignature Architects like Zaha Hadid and Foster + Partners find themselves drawn to the possibilities of the technology.\nWe are seeing an increasing use of non-standard fabrication materials and methodologies to achieve stunning forms that could previously never have been built for an acceptable budget. As many of these shapes are derived from generative and computational methods, connecting them to automated fabrication machines sounds like a good idea.\nMany of Frank Gehry\u2019s designs could not be built because the cost estimates from fabricators had huge \u2018risk\u2019 fees included as it was not totally clear from the 2D drawings how components could be manufactured. When Gehry\u2019s practice started using the CAD tool Catia to produce detailed 3D models his contractors and fabricators better understood the design and could reduced costs by using the model to cut the steel and aluminium. Gehry is still proud that he can have a sculpted wall for the same price as a straight one.\nFoster + Partners is part of a consortium set up by the European Space Agency to explore the possibilities of 3D printing lunar habitations. As it is prohibitively expensive to ship heavy materials to the moon, Foster + Partners is looking to process and print a lunar soil-based material into an inflated dome. Simulated lunar soil has been used to create a 1.5 ton mock-up using a D-Shape printer.\nSOM, together with the Oak Ridge National Laboratory have been working on research for a 3D printed structure made of C-sections called AMIE, which generates solar energy and has a symbiotic power sharing relationship with a 3D printed electric car.\nConclusion\nDigital fabrication is undoubtedly coming to the construction industry. With so many active research projects and investments being made in materials and robotic technology, some will eventually stick.\nHowever, it is going to take a while for the various dots to get connected, as changes are required to software, hardware, contracts and mindsets.\nThe idea of a machine or robots creating a building in a single 3D print still seems like science fiction still to me. At best, it may work in space, for quick military fortifications, or in emergency shelters or homes. But single continuous pour does not seem to make much sense. I also seriously doubt that 3D printing a flat wall is actually any faster or better than a traditional block wall, unless you have severe labour shortage. This is a misapplication of the technology.\nMuch of the 3D printing hype from China seems to really be a story about prefabrication and assembling it on site and most just look like concrete sheds.\nThere also needs to be considerable technological advances in all fields to make this work. 3D print industry guru Terry Wohlers said: \u201cWhen considering the time and cost of constructing an entire building, the skeletal walls are a small part of the project. You also need floors, ceilings, roofs, stairs, and kitchen and bathroom fixtures. Consequently, I cannot see how the use of 3D printing technology could save any time or money.\n\u201cWhen you factor in the added cost of a very large, expensive, and not very portable 3D printer, the cost of these walls are likely far more expensive and time-consuming than conventional walls. The use of 3D printing may be good for marketing and attention, but that\u2019s all.\u201d For now, prefabrication and assembly on-site can lead to incredible productivity benefits, if perhaps not to stunning architecture.\nRobots work best in environments that are controlled and predictable. They are therefore much more likely to be of use in a factory fabricating components.\nTo effectively employ these methods, designers will need to understand the limits of the actual construction, materials and fabrication technologies far better than they do today.\nEven in engineering, which is typically much more connected to fabrication, engineers still create designs that cannot easily be produced digitally or otherwise.\nThe rise of the robots \u2014 five technologies to keep your eye on\nHAL robotics\nOriginating from an academic Masters project development, HAL robotics offers a popular Rhino grasshopper plug-in to enable control of ABB, KUKA and Universal Robots machines. The software enables fabrication directly from a digital model, supporting hotwire cutting, milling and pick and place, without having to go through any additional programming steps.\nFounded by a multi-disciplinary team of architects and engineers HAL robotics is involved in over 200 research and commercial projects, which have both on and offsite fabrication, and may require collaborative robotic interaction or real time sensors.\nThe company is currently working on its next generation tool, which will be platform and format agnostic, so it could be pumped into Revit, ArchiCAD or a Raspberry Pi. As most of the major six-axis robot manufacturers run proprietary operating instructions, HAL is looking to produce a single tool that will be able to talk to them all. Called the grammar engine, it can translate into any required robot language for offline programming.\nHAL Robotics\u2019 VP of human machine interactions Sebastian Andraos explained, \u201cConstruction is the holy grail of robotics, it\u2019s very much in line with industry 4.0 \u2013 it requires autonomy of machines, awareness of their environment, collaboration between man and machine and error correcting in real time.\n\u201cAs things stand construction is an incredibly complex task, for the current generation of robots. For the time being it\u2019ll be complex, more expensive projects that will see the benefits in using digital fabrication. \u201cThere is huge inertia yet to be overcome for the adoption of digital fabrication.\nThere is a huge amount of education that is required before we start to see these kind of technologies used. If you use the example of BIM in the construction industry when considering inertia, ArchiCAD came out in 1982 as the first BIM tool. Here we are 35 years later and there are people that have still not got on board. And instead of digitally fabricating directly from the model we are generating 2D drawings to hand downstream where errors can be introduced.\u201d\n\u201cOne of the most interesting aspects about robots is their ability to change tools. So you could have one robot in a room, one minute it could be running cables through wall, then plastering, then painting.\n\u201cWe are already seeing many more experiments with 3D printing in construction. But nobody is really using it to its full extent, as most experiments are limited to extruding with the 3D printer, which is like using a nail gun to hang a picture.\n\u201cMeanwhile firms like Xtree are trying to experiment with forms and shapes. But I don\u2019t really see 3D printing as a way to create a whole building. On site construction will remain modular, with bricks, breeze blocks, and large panels with prefabrication of certain elements.\n\u201cThe real advantages will be from the logistics of timing the on site preparation with the production of prefabricated components. We could have half the building ready by the time we have planning permission and turn up on site.\u201d \u25a0 hal-robotics.com\nMX3D\nBased in the Netherlands, MX3D is a startup that has ambitions to build a steel 15-metre long bridge using multi-axis robots. The plan is to combine robots, printing in metal, and all the while doing it above water (something which most robots don\u2019t like too much).\nThe robot arms will be fitted with welding heads and fed with metal rods of material to produce layers of welds, one on top of the other, fabricating the computer-optimised structure of the bridge. The robots will either sit on the bridge as they weld, or on barges.\nThe project is estimated to take three months to complete. Autodesk, Heijman (a Dutch construction firm) and ABB, a Swiss industrial robot manufacturer) are all supporting the project. \u25a0 mx3d.com\nHadrian\nHadrian is an Australian brick and mortar laying robot, devised by Mark Pivac of Fastbrick Robotics, that can lay 1,000 bricks an hour at an accuracy of 0.5 mm by using site-wide laser scanning.\nHadrian seems to mainly comprise of a special attachment and feed system to a crane with a 28-metre boom, giving it incredible reach. It is estimated that Hadrian could produce a standard size house in two days. Although apparently it is not too hot at doing corners. \u25a0 fbr.com.au\nSAM (Semi Automated Mason)\nCreated by Construction Robotics of New York, SAM is a robot that works alongside a mason, taking the strain and enabling the laying of two to four times the amount of bricks that the mason could typically do on their own. Again, it does not like corners and excels at long straight runs.\nSAM sits in a self-contained metal box that has a small robot arm and brick\/ mortar feeder, which all together weighs 3,000 lbs, and runs on a propane generator. It moves along a scaffold wall and uses laser measurement so the system can dynamically apply mortar and place bricks. \u25a0 construction-robotics.com\nasmbld\nOne of the more bizarre projects is the Project Dom Indoors by New York-based construction robotics company asmbld. The concept is that rooms are broken down via a raised access floor into pixel like grids. A number of small robots move about within the raised floor to assemble structures that get lifted, raising the floor level to meet the design. This essentially means the room can be \u2018built\u2019 and sculpted on the fly like the terrain in minecraft. \u25a0 asmbld.com\nFabricating buildings with 3D printing\nXtree\nXtree is an impressive consultancy for advanced digital processes in architecture, which also offers fabrication services and technology development in the space. It has been involved in a number of European research projects for 3D printing complex walls, optimising lattice structures for 3D print. \u25a0 xtreee.eu\nD-shape\nUsing sand and a binding agent, the D-shape 3D printer creates layered \u2018sandstone\u2019 models, which the company claims is superior to Portland cement and does not require reinforcement. With a build volume of 2,500 metres squared, each sandstone layer is 5mm-10mm and can produce models from foundation level to the roof, including stairs, partition walls, concave and convex surfaces, bas-reliefs, columns, statues, wiring, cabling and piping cavities. \u25a0 d-shape.com\nWASP\nWorld Advanced Saving Project is an Italian project aimed at producing low-cost sustainable housing that does not just use concrete. The team has developed a number of products including PowerWASP \u2014 a 3D printer that mills wood and aluminium and can print ceramic mixtures. The six-metre tall Big Delta WASP printer can be assembled by three people in an hour and print modular reinforced concrete beams that eliminates the need for moulds and can print ceramic mixtures. The system consists of a large aluminium space frame and print head and the design will be able to use local materials, such as mud and clay and run off solar panels. The company has also really scaled things up with a 39ft-high, 20ft-wide WASP print structure, capable of producing large buildings. All the revenue from sales of the WASP machines is invested in to material research. \u25a0 wasproject.it\/w\/en\/wasp\nBranch Technology\nBranch Technology, a new start-up based in Chattanooga, Tennessee, has developed a process called C-Fab or Cellular Fabrication. The system uses a Kuka Robotics 12.5ft robotic arm attached to a 33ft rail. At the end of the robot arm there is a print head that uses a unique mixture of ABS plastic and carbon fibre, which produces 3D-printed matrix support structure, over which traditional materials can be layered. Branch Technology says its printer can cover an area of 25ft-wide by 58ft-wide. For now the company is focusing on interior spaces, art installations and exhibition structures but will eventually expand to produce load-bearing and exterior walls. \u25a0 branch.technology\nSkanska\nLoughborough University has signed an 18-month development programme with Skanska, Foster + Partners, Buchan Concrete, ABB and Lafarge Tarmac to develop a commercial concrete printing robot. Now in its second generation of development the system is fitted to a gantry and features a robotic arm and print head and can already print complex structural components, curved cladding panels and architectural features. Using this technology, Skanska hopes to reduce the time needed to create complex elements of buildings from weeks to hours and is seeking to develop a 3D printing supply chain.\nContour Crafting\nUniversity of Southern California\u2019s Dr Behrokh Khoshnevis has spent over 15 years developing what he calls the contour crafting system, which uses a fast drying concrete mix. The system has created walls up to 6ft high, with layers that are six inches high and four inches thick. It could be mounted onto a gantry frame and potentially do the plumbing, wiring and painting. \u25a0 contourcrafting.org\nApis Cor\nOne of the more intriguing 3D print technologies is the Apis Cor printer, a Russian design that consists of a single arm. Weighing in at 2.5 tons, and folding into a compact shape for transport, the printer can be assembled within half an hour.\nThe arm has a print zone of 192 metres square from a single point of print. The design features a rotating extraction head that allows for the creation of sloping walls both horizontally and vertically and uses as much power as five kettles. If there was a machine designed to convince me that you could actually print all the walls of the residential building in one go this would be it. \u25a0 apis-cor.com\nMinibuilders\nThe Institute of Advanced Architecture of Catalonia has been developing a community of small robots, which have their own tasks and collaborate to create big structures. The foundation robot is an extruder that creates he first 15 cm (20 layers) of the structure through 3D printing, using infra red follow lines on the floor. The second robot clamps onto the foundation layer and is called the Grip Robot. This 3D prints on the foundation and builds up the walls and can tilt to build walls that tilt in or out. The third robot uses suction cups to climb up faces and is called the Vacuum Robot. This robot can print on the surface of the 3D printed walls. \u25a0 robots.iaac.net\nMUPPette\nGensler\u2019s research project, the MUPPette (Mobile Unmanned Printing Platform) equips a drone with a 3D print head. The logic behind this is that drone-based printers can print as big as you want, in X, Y and Z axis. Started in 2014, the project consist of a hexocopter platform, a gimbal and a 3D printer attached to the gimbal. The gimbal is an essential element here to stabilise the print head from the copter base, which may be moving with the wind. The PLA plastic enabled print head can print in flight. A swarm of these could turn up and print a structure at a remote location. The reality of getting a precise build from this type of technology would seem next to impossible and the video of the Muppette in action suggests it is lucky to get more than a couple of blobs of plastic on an A4 sheet of paper. However I have to admire the ambitious concept.\nAADRl Swarm Printing\nArchitectural Association School of Architecture\u2019s Design Research Laboratory and the work of Robert Stuart-Smith from the Kokkugia experimental architecture research collaborative with students is certainly \u2018out there\u2019: Swarms of UAVs to thread fibres to create \u2018bridges\u2019 and spanned woven structures. \u25a0 kokkugia.com\nSwarmscrapers\nTalking of swarms, a research team at the California College of arts have been developing a series of autonomous robots that harvest on site materials to fabricate buildings. For now the machines just turn sawdust into termite-like structures using glue but this concept could be of use to NASA when requiring the fabrication shelters on moons and planets, where you do not have access to a handy builders\u2019 yard. \u25a0 instructables.com\nUC Berkeley\nUC Berkeley researchers, led by Associate Professor of Architecture Ronald Rael, who is world-renowned in this field, have devised a new 3D print process for buildings that lays down dry cement powder, an iron oxide-free Portland cement polymer, which is then sprayed with water to harden. The system does not need to extrude wet cement or clay-based materials. The advantage of the powder approach seems to be the quality of the finish, the precision of placement and the ability to make lightweight structures. \u25a0 iced.berkeley.edu\/resources\/digital-fabrication-lab\/\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/massmotion-flow\/","title":"MassMotion Flow","date":1426550400000,"text":"The design of mass transit and public building spaces presents architects with big challenges when dealing with high capacities and emergency egress. To help designers Oasys, has embodied years of simulation knowledge into its software\nBillions of people travel about their daily business, moving through train and metro stations, as well as airports, ports and bus terminals. Football venues like Wembley Arena are built to cater to crowds of 90,000 people on a weekly basis, while concert venues such as the O2 Arena have capacities in excess of 20,000. Globally, events like the Muslim Hajj attract two million pilgrims over five days to circle one shrine seven times. The challenge for designers to safely cater to the needs of these huge flows of people is too big to be tackled with guesswork or gut feel. This is where simulation can play a key role in the conceptual and detailed decision-making process.\nOasys is the software arm of multi-disciplinary giant Arup. Creators of popular products such as Mail Manager, the company offers very targeted solutions to the building market, including vibration and seismic analysis, 3D linear and non\u2013linear static and dynamic building analysis, concrete detailing, slope stability detailing as well as environmental and bridge design tools. As Arup develops bespoke solutions to solve its clients\u2019 problems, that knowledge gets distilled and turned into products by the Oasys group, providing industry tested solutions to the mass market.\nArup\u2019s work is well known in the world of mass transit, stadia design (Olympics) and its work on public spaces (Hajj). Oasys took some of the simulation software developed for the analysis of hundreds of thousands of moving \u2018avatars\u2019 in 3D spaces and built a product called MassMotion. Costing \u00a320,000 for a perpetual license and requiring a copy of the now defunct Autodesk Softimage, usage was once reserved for the lucky few. In its latest reworking of the product, MassMotion Flow, Oasys has rewritten the product, removed the need for Softimage as a geometry engine, added new functionality and priced a very capable version at a much lower price of \u00a35,000 for more widespread adoption.\nPlatform\nWe were very impressed with the previous version of MassMotion (tinyurl.com\/AEC-massmotion) although getting the geometry imported and prepared for analysis via Softimage was a bit contorted. With the writing on the wall for Softimage, as Autodesk owned all its competitors (3ds Max and Maya), it was pretty obvious that Autodesk was not putting development effort into Softimage. Oasys needed to remove MassMotion\u2019s reliance on the Autodesk geometry engine and with MassMotion Flow, the system features a brand new modelling space, which looks and feels very similar to the previous one.\nThe full version of MassMotion still exists and can be purchased, although, and by the time you read this, an updated version should be near shipping \u2014 obviously featuring all the graphics and new feature updates that the new MassMotion Flow has. As to the difference between MassMotion and MassMotion Flow, the full version allows for a greater capacity to include scheduled events and plug into databases to drive more complex simulations. Perhaps providing real world arrival and departure times of trains, or planes to drive even more realistic results.\nIn use\nMassMotion Flow can run on a reasonable Windows laptop or desktop but, as you would expect, the bigger the crowds involved in each scenario, the longer each analysis takes. A typical starting point is to import geometry from a CAD or BIM system such as AutoCAD, MicroStation, SketchUp, Rhino or Revit. The system supports 3DS, DXF, FBX and IFC, which is a good lightweight format with building metadata so the software can automatically recognise walls, slabs, stairs, etc.\nThis imported geometry is reference geometry for MassMotion Flow to automatically generate floors, stairs, ramps, escalators, etc. Work may be required, especially when modelling might not actually be as precise as expected or there are no appropriate IFC definitions for elements, such as baggage handling machines or security devices. MassMotion Flow can also create new geometry and be used to edit the models and filter out modelling errors and hide reference geometry. For heavy users of MassMotion Flow, or those that regularly send models for simulation, it is possible to create custom routines to assist in the import automation and include MassMotion Flow elements. The software comes with a library of intelligent geometry components, such as escalators and stairs, which can be used to swap out non-functioning elements or unrecognised components in imported models. It is also possible to create new intelligent elements. MassMotion elements, like escalators, can be easily edited to set direction or logic, such as timed open and close or repetitive tasks. For scheduled \u2018timetable\u2019 events driven by a spreadsheet, or complex process chains\/tokens, you will need the full version of MassMotion.\nPortals are placed to spawn \u2018agents\u2019 over time, which are highly configurable, as to the number of agents, their timings of generation and egress. Agents are spawned with individual objective destinations and each one is a mathematical point with a radius and can be displayed as animated \u2018walking\u2019 people, or moving pegs, should processor power be limited.\nOnce the model is complete and directions set on components like escalator travel directions, barriers, security, the simulation can be run. It is often a good idea to test the model first as if not set up correctly, agents may get trapped or bypass planned filters such as security checks, should an escalator direction allow a faster route to their destination.\nIn terms of processing speeds, it does depend on the complexity of a model but typically 15,000 agents can be calculated in real time \u2014 that is to say one minute of simulation will take one minute of compute time.\nThe more congested a design, the more computationally intensive the scenario; as agents will interact more with each other if confined in crowded spaces. The software is fully multithreaded so the more cores you have, the faster it will run. At the moment all processing is done locally on the computer but Oasys is evaluating the benefits of cloud-based options.\nOnce the simulation has been processed it is possible to watch the agents move about the space, navigate crowds and reach their destination. The real feedback comes with MassMotion Flow\u2019s wealth of report generation tools, agent density, level of service (how much space agents have around them), identifying points of maximum congestion, queuing level of service, traces of all agents pathways, vision maps to identify walls which are most viewed (for best advertising spaces) and many others. Reports can be graphs or agents can be coloured to indicate their level of service within the simulation. It is even possible to filter to just see one agent\u2019s experience throughout the simulation.\nGraphs and animations (at various quality) can be easily exported for including in reports or sharing with design teams over the web.\nTypically, Oasys recommends two days training to use MassMotion Flow, dealing with import of geometry and creation of simulations. Although, it has to be said that the knowledge required to properly understand all the analysed output is still viewed as being a specialised skill.\nRunning multiple scenarios in both best and worst cases can quickly identify problem areas within a design and help designers understand the capacity and comfort levels of people using the spaces. The software has many uses away from obvious train stations and airports, and has even been used to assist in the mapping out of school timetables to limit the congestion in corridors and canteens.\nConclusion\nMassMotion Flow is the next generation of pedestrian\/crowd simulation tools on the market and brings Oasys\u2019 previously high-end and relatively expensive solution to a much wider market. While still a specialist area and requiring some knowledge to drive and understand the results, it is now much more feasible for a practice that works on public spaces, schools or transport systems to get quantifiable feedback on the quality of design, starting at the concept stage and working through to retrofit and timetables.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/intelliwhere-launches-mobile-workforce-starter-kit-to-accelerate-data-automation-for-field-crews\/","title":"IntelliWhere launches Mobile Workforce starter kit to accelerate data automation for field crews","date":1034812800000,"text":"October 16, 2002 \u2013 IntelliWhere, a division of Intergraph Mapping and GIS Solutions, has launched the new Mobile Workforce Starter Kit, an out-of-the-box solution that enables organizations to automate the delivery and use of spatial and asset data to field crews using personal digital assistants (PDAs). Debuting at global exhibitions CTIA and InterGEO, the starter kit provides a discounted software bundle of industry-proven PDA mapping and GIS management tools \u2013 IntelliWhere OnDemand, IntelliWhere LocationServer, and Intergraph\u2019s GeoMedia software \u2013 for the rapid deployment of spatial intelligence for any organization with a mobile workforce. The limited-time-offer starter kit is specifically designed to provide an easy and efficient way for government, transportation, utilities, and military organizations to experience the technological and economic benefits of automating a mobile workforce. Empowering field personnel with PDA geospatial technology can improve an organization\u2019s responsiveness to customer demands, equip field crews to implement more intuitive solutions, and streamline data capture from the field \u2013 factors that can improve an organization\u2019s service while controlling cost.\nHome Innovations GIS IntelliWhere launches Mobile Workforce starter kit to accelerate data automation for field...","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/pushing-the-limits-in-stadium\/","title":"Pushing the limits in stadium","date":1214870400000,"text":"Now considered an urban essential, increasingly sophisticated sporting arenas benefit from Dassault Syst?mes\u00dd Catia design-to-build technology that generates 3D models for a better outcome.\nHundreds of new sports facilities are being built right now across the world. These will host a range of events from The Olympics Games and Premier League football to horse and motor racing, tennis, gymnastics and swimming, as well as multi-purpose venues that enhance national, local and community sporting activities.\nSome of the best of these facilities, including the 2008 Beijing Olympics Stadium, are developed using Catia advanced 3D model-based design-to-manufacture solutions from Dassault Syst?mes and with the Gehry Technologies\u00dd Digital Project software platform. In preparation for both Beijing 2008 and London\u00dds 2012 Olympics, Catia is used to ensure that complex structures for the Games and their many associated activities are delivered on time, on budget and to the right specifications.\nThe development of multi-purpose sports stadia represents one of the most difficult building problems, since they have to satisfy the needs of so many stakeholders. New bowl-shaped stadia housing up to 90,000 spectators have become symbols and icons of their locations that have changed from \u00d9bad neighbour\u00dd buildings, formerly sited out of town, disconnected from transport infrastructure and surrounded by huge car parks \u00be to city centre must haves.\nThe importance of new sports facilities in regenerating cities was stressed by the former Mayor of London Ken Livingstone, who recently said: \u00fdThe London Olympic Games in 2012 will be the greatest sporting spectacle the world has ever seen \u00be as well as transforming the Lower Lea Valley and creating a lasting legacy of cutting-edge sporting venues for the capital and the nation.\u00af\nOlympic glory\nThe foremost company working in this sector is Arup Sport, whose work includes the 2008 Beijing Olympics Stadium, The London Olympics Aquatic Complex, the stunning Valencia Stadium, and new world-class stadia for Ukraine and the Middle East.\nArup Sport Senior Structural Engineer, Kate McDougall, spoke of her work and the use of Catia 3D-based model technology. \u00fdThe stadia and sports facilities that Arup Sport is working on benefit from Dassault Syst?mes\u00dd 3D modelling because of the functionality and flexibility that this methodology provides. Our work covers architectural design services, structural, mechanical and fire engineering as well as many other specialist technical services. Using 3D Catia models, the information that we need is easy to access, update and integrate across the internal and external supply chains that we operate within.\u00af Kate added: \u00fdStadia are all unique and they always incorporate complex geometry.\nCo-ordinating their design, planning and construction involves making many changes and updates throughout the project lifecycle. This process is enhanced and facilitated through the use of 3D digital models. The software allows us to save costs by developing a route to manufacture early in the project and also by allowing us to make use of standard components to improve quality and make financial savings. The Catia 3D-based model methodology also makes checking efficient, easy and quick since complex geometry is modelled in three dimensions.\u00af\nThe spectacular Beijing Olympics Stadium is the most high-profile sports facility in the world and represents the current peak of innovative and complex stadium design. The stadium is built from more than 40,000 tonnes of steel enclosing 90,000 seats. The apparently random design is in fact very regular, but gives the appearance of a natural form hence its nickname \u00b1 the bird\u00dds nest. The geometry of the steelwork was defined using Catia to achieve the required absolute accuracy that both the design and its commissioners demanded. The structure has been engineered to accommodate wind and other loadings to which the building is subject.\nModel data was passed to subcontractors for component fabrication, assembly and erection with Catia data being used not only to produce the work but also to check build veracity. The building\u00dds exterior design looks much more complex than it actually is, and is constructed from flat plate steel formed into box sections (with increased thickness to cater for variable high stress areas of the structure). There is a very clear pattern to the construction, which is subtly hidden by a second fa?ade. Using Bill of Materials optimisation capability leads to more standard parts being developed and brought into the design of the impressive fa?ade. Due to its geometry, the building only has two-fold rotational symmetry \u00b1 there are only two of any joint type or element on the project.\nThe building had to be radically re-designed at a late stage to lower cost. This reduced the steel content by 20 percent and necessitated considerable alteration to the building in many areas. The use of a 3D central model made this work much quicker than it would have been by other means and allowed the building to be brought in on time and within the revised budget. Arup\u00dds Project Director Michael Kwok was recently quoted as saying that the Beijing National Stadium has introduced a new kind of structure to the world and that its successful completion showed the role engineers can play in defining what is possible in architecture.\nTeam effort\nSports facilities are now considered part of the urban fabric that has to be integrated into the existing and future city context. The use of Catia to help design sports facilities has the great advantage that, in using a core 3D model, the design can easily be changed to meet the often conflicting needs of those involved in planning, operating, financing, fabricating and construction as well as visiting, using and broadcasting from the facilities.\nHaving produced conceptual designs for a stadium using Catia 3D modelling software, and taken into account the different needs of all stakeholders, the model can be developed and made available to all concerned in a format that suits their needs and interactivity tools. Each participant can access the model drawing from it and the data that they need to conduct their work most efficiently. This can range from simple viewing with pan, zoom, rotate and mark-up facilities, to full interaction and, for example, direct NC machine programming to produce building components, fabrications and work instructions. The digital model has many uses as a central repository for ideas and the designs and methods that result from them.\n{mospagebreak}\nPlan to win\nPlanning is of very high value in the development, construction and lifecycle operation of sports facilities. Buildings must have flexible usage and, since their life may be more than 100 years, the needs of its users and other stakeholders will inevitably change. These changes can be best understood and addressed using a 3D digital model, which can be iterated easily to accommodate changes of use and function.\nSports stadia are increasingly being built as part of a broader visitor experience. A new football stadium in Porto, Portugal for example is planned to contain a shopping mall, a hotel, and a cinema complex along with deep underground car parks. It will integrate with an enhanced transport infrastructure planned for the city and offer sports fans unprecedented levels of luxury. By making this available, the stadium will attract visitors at times other than major events to provide a continuous revenue stream.\nThis sort of project was described by Andrew Watts MD of Newtecnic, a firm of architects specialising in very advanced building fa?ades \u00be important for producing strong visual impact along with structural functionalities. He said: \u00fdThe way modern buildings are designed has changed to accommodate ideas about how usage will alter over time. Consequently, spaces are much more flexible and non-specific. This means that the mechanical services of a building will change along with its usage. A 3D building model is the ideal repository for the design because it allows changes to be made over time, effectively extending the design period over the life of the building.\u00af\nHe continued: \u00fdThe process of making buildings is becoming more complex and the tools, such as Catia, are developing along with that process. The ability to change a building while retaining the integrity of the design vision allows us not only to design faster and more easily but to show interested parties how the design is developing in ways that they can understand and make best use of.\u00af\nRules of the game\nBecause Newtecnic\u00dds fa?ade systems have both structural and decorative functionality and Newtecnic sees designs through to construction, it has evolved a complete fa?ade manufacturing methodology. To handle this level of involvement, Newtecnic deploys Catia technology \u00be the same software that is used by major automotive and aerospace OEM companies to design and build the world\u00dds most advanced engineered products.\nAndrew Watts explained: \u00fdThe ability to turn conceptual designs into buildable structures while retaining the precision of the original creative intention requires the equally precise methodology that this software provides. This tool is liberating because it allows designers to iterate the design within a set of flexible governing rules. At Newtecnic, this methodology liberates us by allowing considerable creativity within the rigor of strict rule-based engineering discipline.\u00af\nGreen medal\nEnvironmental impact is a major theme of modern buildings, and their public nature means that large stadia and other sporting facilities must be exemplars of green credentials. Catia is being used in many buildings to design, refine and manufacture air circulation systems that reduce reliance on air-conditioning by using passive or mixed-mode ventilation with its greatly decreased energy requirements. Integration with Dassault Syst?mes\u00dd Simulia finite element and computational fluid dynamic analysis software allows designers to simulate airflows within structures improving comfort levels for sportspeople and audiences alike.\nCatia has been used to design many roof systems for sports stadia. These are becoming important showpieces in themselves with their dramatic operation and engineering ingenuity. The complexity of these structures that often include intricate but large-scale moving parts is ideal for development using Catia which, with its kinematic capability is able to show a simulation of the moving roof and indicate potential clashes or other problems. By deploying Catia in this situation it is possible to solve all potential problems through analysis and refine the design digitally before any physical manufacturing commences.\nKate McDougall, of Arup Sport, added: \u00fdRoofs have to clear certain envelopes and must also operate with maximum efficiency. Developing optimum geometry and sections is made easier with Catia because it enables the input of parameters that have an affect on the design, and allows automation of certain aspects in the design process.\u00af\nImportantly this methodology also enables parameters held on spreadsheets to drive designs in Catia. This often results in innovative and novel solutions that will comply with functional requirements \u00be leading to better and more advanced buildings.\nThe physical making of the building (once the design is finalised as a 3D model) can be carried out by subcontractors under the guidance and control of planners and project managers who use the central model as a data repository accessible by all parties collaborating to maximum mutual benefit.\n{mospagebreak}\nThe long run\nMultiple stakeholder interaction with 3D models was described by architects at Allies and Morrison. The firm was recently involved, with architects HOK, in The Emirates stadium and development for Arsenal FC. Despite being a complex multi-purpose facility including restaurants, bars, a future-proof flexible IT infrastructure in an iconic form, on a historic site that offers enhanced environmental benefit to its central London location, the stadium and associated development was finished ahead of time and below budget.\nAllies and Morrison remarked that the ability of Catia to integrate with climate and environmental data, allows their designers to model not just the structure of a building, but also the weather and light conditions to which it will be subjected. By taking into account this level of detail the eventual value of the building will be higher since the additional comfort that it proves will make it more desirable and popular.\nThe team at Allies and Morrison found that design intent is captured in the 3D model as a series of rules that the software maintains and applies throughout the design process. This means that conceptual frameworks stay intact throughout the many changes that the design is subjected to, and that all the while, development targets are retained and optimised.\nTaste of victory\nZaha Hadid Architects has been commissioned with delivering the spectacular swimming complex for the London Olympics. Partner, Patrik Schumacher, a champion and user of 3D modelling software recently said: \u00fdProductivity, creativity and elegance are available from Digital Project and the Dassault Syst?mes software.\u00af\nKen Livingstone recently commended the complex when he said: \u00fdZaha Hadid\u00dds exceptional winning design gives a taste of just what we can offer and makes London\u00dds bid to host the Games even more compelling.\u00af\nGeoff Haines, Managing Director of Desktop Engineering, the company that supplies and supports Digital Project and Dassault Syst?mes\u00dd 3D modelling software at many architects and engineering companies including Zaha Hadid, Arup, SOM and Allies and Morrison said: \u00fdDassault Systemes\u00dd Catia brings architects, sports facility developers and the AEC industry the proven benefits of large-scale design and manufacture software. The Catia-based Building Information Model (BIM) is a complete set of data that includes 3D design and manufacturing information, as well as associated rules, methods and knowledge that govern all aspects of a building or development.\u00af\nMind games\nLord Sebastian Coe, winner of the Olympic 1,500m gold medal in 1980 and Chairman of the London Organising Committee for the Olympic Games (the organisation in charge of overseeing the development of the Olympic Games), is someone who understands the significance of well-executed design in sports facilities. He recently spoke at the opening of a new training facility: \u00fdI know just how important that extra hundredth of a second can be and these facilities will enable athletes to develop and hone their technique allowing them to be at the top of their game.\u00af\nGreat athletes need great stadia. Sports Psychologist Bogdan Woolf of Institute Lacan claimed: \u00fdThe psychological effect of new stadia on the sports person is profound. For example, a recent study found that referees award 15 percent favouritism to home sides that have adapted their cheering to suit, and maximally exploit the acoustics of the setting. Using Catia, it is possible to incorporate these subtle but important influences in an aim to create the perfect environment for players and fans.\u00af\nPerfect pitch\nAnother example of advanced pre-build simulation is found in sunlight modelling that shows how the interior environment will react with the forces of nature. This is particularly important in order to maintenance both natural and artificial playing surfaces. In one celebrated example the constantly shaded grass-playing surface of a pitch in Japan is placed on a rolling platform and swapped with one that has the benefit of natural sunlight outside the stadium. This ingenious solution was developed using 3D modelling software in conjunction with sunlight calendar data to produce optimised playing surfaces.\nIt is well known that great facilities enhance sports to produce better outcomes. Across the world Catia users are developing and building ever-more advanced venues that help human beings excel at what they do best.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/mixed-reality-hololens-on-the-construction-site\/","title":"Mixed Reality: HoloLens on the construction site","date":1531353600000,"text":"Trimble is leading the pack in Mixed Reality for construction through industry partnerships and R&D projects to develop exciting new applications for the HoloLens. Greg Corke reports on what construction might look like in the future\nImagine stepping out on a construction site wearing a Mixed Reality (MR) headset and instantly being informed what\u2019s behind schedule, what\u2019s been installed in the wrong place or where there might be an issue or clash. This is automatic construction verification in real time using object recognition, machine learning and computer vision and could soon become a reality thanks to R&D work being undertaken by Trimble.\nWhen the HoloLens was first unveiled in 2015, Trimble was one of the first companies to get on board, working closely with Microsoft and collaborating with several leading AEC firms, including AECOM and Gensler, to explore real world applications of the technology.\nThe release of SketchUp Viewer for HoloLens in 2016 applied MR technology to design but the most exciting development \u2014 in our eyes at least \u2014 came in January this year with the launch of Trimble Connect for HoloLens, which finally brought MR to the construction site.\nThis is Mixed Reality in the true sense, where digital models are displayed in the context of the real world \u2013 so the wearer of the HoloLens can see where things will be constructed or, more importantly for identifying costly mistakes early on, where they should have been constructed.\nFrom design office to site\nTrimble Connect is a cloud-based construction management solution that acts as a collaboration hub for AEC projects. It can read in files from many different sources \u2013 Autodesk Revit, Tekla Structures, SketchUp and more \u2013 then be used to co-ordinate models and manage projects through change orders, to do lists and RFIs.\nThe workflow with the HoloLens currently starts in the office where a QA issue might be identified, and someone sent out on site to investigate. On site, that person puts on the HoloLens, sees a \u2018to do\u2019 list inside the MR environment and then inspects the issue.\nAt the moment, this is a very manual process. The worker first has to align the real and virtual worlds by telling the HoloLens that certain surfaces it has scanned in the real world are structural columns, staircases or elevator shafts in the digital model.\nOnce everything is aligned, on subsequent site visits the mapping sensors built in to the HoloLens do an excellent job of automatically recognising it is in the same location. However, as construction sites are continually changing with new elements added on a daily basis, alignment often needs to be done over again.\nTrimble is working on an R&D project to use machine learning to automate this process. The idea is the HoloLens is taught to \u2018recognise\u2019 objects in the real world \u2014 that a beam is a beam, a column a column, etc. \u2014 then automatically work out its location based on its knowledge of the design and align itself accordingly.\nAutomatic alignment would be an impressive leap forward, but the next step would be to use machine learning and AI to automatically identify issues in real time, a process that Trimble calls continuous alignment.\nRather than relying on human observation, Trimble is working on an R&D technology that would be able to tell the worker if something is being installed in the wrong spot, something wasn\u2019t installed and should have been, or if there\u2019s going to be a clash. This is automatic, real time construction verification, enabled through object recognition, where the technology not only knows that a column is a column, or a pipe is a pipe but the exact specification of that element. This will not only add a lot of value to construction, but also to asset and facility management.\nThe feedback loop\nAt the moment, Trimble Connect for HoloLens lacks a closed feedback loop for issue resolution, but Trimble is developing new functionality to address this.\nResponding to a work order on site, the worker will be able to review the issue in context, then give recommendations for resolution using voice commands to add audio notes, videos or still images to Trimble Connect for HoloLens.\nOnce this information is synced back to Trimble Connect, a change order can then be sent to the designer or engineer back in the office so he or she can make the necessary amendments to the 3D model.\nLocation, location, location\nThe HoloLens, in its current incarnation, has relatively limited storage and memory so it\u2019s not really practical to upload entire construction projects to the device. Trimble offers best practice advice to customers and it\u2019s more likely that only specific issues within a project are currently brought into the system.\nThis looks set to change soon. A Trimble spokesperson told AEC Magazine that in the last 18 months it\u2019s done so much R&D on things like streaming and dynamic Level of Detail (LOD) that in six months\u2019 time this will likely be a non-issue.\nAt the moment getting workers to the precise location of a potential issue still requires knowledge of the site. In the future, it\u2019s not beyond the realms of possibility that a worker could be guided to the next task by a virtual arrow on the surface of the ground. It could even plot the fastest route between issues.\nOf course, site safety is of paramount importance, so a beep from the HoloLens could be used to alert workers when, for example, they get close to the edge of a deck or an elevator shaft, as it knows at all times where it is on site. It could also identify potential dangers through object recognition.\nWhat you see is what you build\nApplying MR, machine learning and AI on an ever-changing site is a major challenge but Trimble is already making big leaps with its technology in more controlled environments to help workers perform repeatable tasks.\nIt is currently working with Consolis, one of the biggest pre-fab manufacturers in Europe, to help optimise the production process for its rebar cages with a view to replacing traditional, and potentially error prone, workflows with more productive and accurate solutions.\nOn the factory floor, instead of working off drawings, the workers use the HoloLens to see exactly where to place complex rebar within the cage. There\u2019s about 50 steps that need to be completed in sequence and, as the worker finishes each one, he or she can move to the next using a simple voice command. Everything that has been completed is greyed out until the whole cage is filled.\nA team of three led by Dr Marianna Kopsida, who has been heading up many of the Mixed Reality R&D efforts at Trimble, was able to complete the task without having any prior knowledge of building rebar.\nMR and machine learning could also bring QA to this process. Rather than having one team build the cage and someone else check it, the system could automatically perform quality control in real time, not only checking that the bar has been placed in the correct position but warning the worker if an incorrect bar is about to be used.\nConclusion\nTrimble looks to be making big strides forward in Mixed Reality for construction, with technology designed to help ensure design data is implemented correctly on site and deliver new levels of efficiency, accuracy and quality control.\nTrimble Connect for HoloLens is already being proven out on real life projects, but the most exciting developments look set to come in the future through the application of object recognition, machine learning and computer vision which will allow construction firms to use computers to track progress and identify issues on site, rather than relying solely on humans.\nThis isn\u2019t just about using technology to drive efficiency, shorten project schedules and bring down costs. With the global workforce becoming less skilled and the complexity of projects increasing, mixed reality could eventually transform construction itself in a \u2018what you see is what you build\u2019 future where construction workers are shown precisely where to place elements without having to refer to documentation. We are already seeing the first glimpses of this in Trimble\u2019s R&D collaboration with Consolis on its rebar cage project.\nWhile the HoloLens has been made \u2018construction ready\u2019 thanks to the Trimble Hard hat, one has to remember it\u2019s still very early days for the technology. In its current incarnation, the HoloLens is nearly three years old and has well documented limitations in terms of field of view and processing power, but HoloLens 2 is rumoured to be waiting in the wings to be unveiled later this year or next.\nAs we dream of a future with smaller, lighter and more powerful MR headsets, the important thing to remember is that Trimble, with its industry partnerships and R&D work, is laying a solid foundation for the future. 2019 looks set to be a big year for Mixed Reality as the industry moves out of the early adopter phase.\nSafety first \u2014 the HoloLens hard hat\nEarlier this year, Trimble, in collaboration with Microsoft, launched an official hard hat for HoloLens to ensure the Mixed Reality headset adheres to stringent on-site safety regulations.\nIt\u2019s a neat solution that distributes the weight of the HoloLens well and is a lot more intuitive to put on than the mixed reality headset on its own.\nFurthermore, as it looks and behaves just like an industry-standard hard hat, Trimble says it has been well received by construction workers.\nFor the time being, this type of device won\u2019t be standard issue and will most likely to be worn by those with specific roles such as BIM manager, VDC manager or site manger.\nThe retail price is $300.\nWhat is Mixed Reality?\nMixed Reality blends the real and virtual worlds in real-time. The Microsoft HoloLens leads the pack in terms of MR headsets and features semi-transparent glasses so workers can see 3D design or construction models aligned with real objects in the physical world. This is different to Virtual Reality (VR) where the wearer of a headset like the HTC Vive is completely cut off from the real world.\nWith HoloLens, users can simply look at objects to access data, use hand gestures or voice commands for control. The big advantage over a tablet, which can also be used for mixed or augmented reality applications, is that both hands are kept free which is useful when performing manual tasks.\nOne limitation of the current HoloLens is that it has a relatively limited field of view. It means the wearer of the device can only see holograms in their line of sight and not in their peripheral vision. This takes a bit of getting used to, particularly when you\u2019re used to a VR headset which is properly immersive.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/real-web-design\/","title":"Real web design","date":1243900800000,"text":"In the not too distant future, you may well end up launching your favourite design tool in a web browser, or never worrying about upgrading your workstation\u2019s RAM, graphics or hard disk. While investigating this story, Martyn Day has found out that the future is here.\nWe all know that \u2018everything is going online\u2019. From ordering groceries, buying entertainment, to planning holidays or even finding\/replacing a \u2018life-partner\u2019, it is now more possible than ever to never leave the house. We seem to have quickly adopted digital lifestyles and now take for granted the impact the Internet has had on our lives. However, I believe there is much more to come and a new technology announcement this month got me thinking about how the Internet could fundamentally change the way we use, access and pay for our design tools in the not too distant future.\nWhile browsing the BBC news site I came across a story about a new company called Onlive (www.onlive.com). Later this year, Onlive will offer an almost unbelievable online service to computer gaming fans. The company will allow customers to purchase a wide number of games, in a variety of formats, and will host these on its top-of-the-range servers and workstations. It will then blast the graphics of the game being played to a web browser running on any operating system. This means that you don\u2019t have to have a high-specification computer or a variety of consoles, at home but can still get the maximum graphical experience. There is also the benefit of never having to download the games, which these days come on DVDs and are huge. This got me thinking, if it can be done for a 60 frames per second, power-hungry game, it can surely be done for 2D and 3D CAD applications?\nAfter some further reading it seems that even the stock market took notice (for once) and industry analysts were claiming that if Onlive worked it could even be the death of such popular devices as the Sony PlayStation and Microsoft Xbox. With an Onlive account you would never need to have a specific games console, or ensure you have the latest graphics card, oodles of RAM or expensive processor. In fact you could probably even downgrade your current system. Onlive should give the same performance on an ATOM-based ultra portable, as it would on an Intel quadcore Xeon. Performance would only depend on the \u2018breadth\u2019 of your bandwidth.\nOnlive Technology\nFor those of you not into games, bear with me. In the last seven years, Onlive claims to have perfected an interactive video compression technique, with ultra low latency (lag) \u2014 it literally encodes live video streams into data in one millisecond. This algorithm has been hard coded into a custom processor that comes in a little black box you acquire as part of your Onlive subscription, together with a software plug-in for browsers. If you have a 1.5 Mb connection speed Onlive claims you will get real-time games at Wii resolution (852 x 480 pixels) but with a 4.5 Mb Internet speed, the company is promising to deliver 1280 x 720 at 60 frames per second. Onlive has already signed up the major software vendors: EA, Ubisoft, Eidos, Atari, Codemasters and THQ to name but a few, and is due to go live in September 2009 in the US.\nIf the technology works, this really could be the first industry changing implementation of so called \u2018Cloud computing\u2019. Cloud-based solutions are really in vogue at the moment, although not for delivery so much, more a discussion topic and as a future application development area. The basic principle offers dynamically scalable and virtualised (operating system independent) computing resources that are delivered over the web. There is no infrastructure required on the client side of the equation and everything is \u2018on-demand\u2019. The \u2018Cloud\u2019 vision SaaS (Software as a Service), IaaS (Infrastructure as a Service) and PaaS (Platform as a Service) are all seen as key future business models. Onlive delivers on all three.\nThere has been similar technology delivered on a local level by Hewlett Packard with its Blade workstations, which operate in a thin-client manner. The workstations can be kept at a remote location and accessed via a simple terminal over a TCP\/IP network. So, while the designers may be in India, the workstations they use and licensed CAD software could be, as an example, in Copenhagen. Onlive expands this concept to doing it over the public network (the Internet).\nFor a few years now, there have been conceptual ideas to try to utilise the Internet for running and hosting design applications but all have been limited by bandwidth and more fundamentally, the lack of native support for 3D in the standard browser applications. To get around this problem there have been attempts to develop and incorporate add-on 3D libraries for browsers but none have helped create a killer 3D application for the web. And should there be a 3D application running in a browser, it would still rely on the Internet bandwidth available, the graphics card in the machine and the grunt of the workstation\u2019s processor to run the application locally. The Onlive concept removes this barrier to the user by only streaming the video of the application.\nCAD online?\nThis Onlive technology surely must be interesting to CAD developers who typically have some experimental web-based solutions at various stages of development. Well, I am just back from Autodesk\u2019s offices in San Francisco where I raised Onlive as a talking point to the AutoCAD development team, to which I was told that Autodesk were one of the \u2018less visible\u2019 strategic investors in the company. Knowing the numerous web tools Autodesk has been experimenting with on its Autodesk labs website (labs.autodesk.com), Onlive seems to be a perfect fit and potentially a decent financial investment for the firm.\nAll web browsers are bad at 3D unless you add one of the numerous plug-ins. It is perhaps not surprising that our industry could not agree on a standard but there are five open-ish popular development tools:\n- VRML now reworked as a new standard, called X3D\n- 3DMLW \u2014 3D Markup Language for Web\n- COLLADA \u2014 COLLAborative Design Activity\n- Open GL ES 2 \u2014 a variant of Open GL\n- U3D \u2014 Universal 3D format, developed by Intel\nIf you look around you will find many small 3D applications that utilise one of these standards but really only to view and maybe rotate basic 3D models which have to be downloaded. The general consensus is that they lack the necessary oomph to form the basis of an in-browser CAD application. One CAD company CEO insider told me: \u201cMost of the 3D plug-ins are either junk or too specific. A ubiquitous one may come along but there is none now.\u201d\nHaving given short shrift to those emerging industry standards, I am going to add in some more jargon and mention RIA \u2014 Rich Internet Applications. This is an industry buzzword for providing enhanced development tools for browser or web-driven tools. Microsoft is pushing a .NET technology called Silverlight, while Adobe has AIR and Flash. These frameworks expand what is possible to be developed within browsers and provide web-tools with most of the benefits of desktop development tools, such as data persistence, access to locally stored data and integration capabilities.\nAll these standards and programming layers add hooks so the CAD developers can get their web-based applications to access the 3D graphics and other hardware in your machine. Web applications are moving beyond the browser and now appear to be hybrid applications. A little known fact is that this is being driven by modern phone operating systems, as developers need tools that will deliver applications for multiple target platforms, from low-power and thin-client to heavy workstation and servers.\nOn the actual delivery of a workable design application, everything at this stage seems quite experimental but eventually I am pretty sure the right combinations will create firm standards and open up rich web-based applications. That is, of course, unless cloud computing and video compression systems such as Onlive don\u2019t bypass these current software development limitations.\nOn that topic, Carl Bass, Autodesk\u2019s CEO told me: \u201cOnLive allows for all kinds of computer-intensive applications to run well on fully powered servers and low power clients and, when it comes to platforms, I think applications in the near future will run on a number of platforms and it is hard for me to guess in the longer run if one will emerge as dominant.\u201d\nWhile Onlive is not available yet (outside of beta), there are some impressive web tools that show what is capable with today\u2019s cutting edge add-on 3D web technology. Autodesk\u2019s Project Dragonfly (dragonfly.autodesk.com) appears to be one of the more advanced web-based Labs applications that I have seen so far, where users can design interior layouts and spaces using drag and drop and axonometric views. While its interiors focus is limiting, It is well worth trying out to see how fast it works as a browser-only based application on your system.\nThere are a number of other tools in development, including Project Showroom \u2014 a cloud-based rendering tool for visualising interior fittings and colours, and Project Freewheel for DWG viewing in a web browser. (More on these in the following two reviews).\nSolidWorks, the developers of the 3D product development system of the same name, also has a web-based DWG viewer, editor and creation tool called SolidWorks Blueprint (labs.solidworks.com). It is fun to try out and use as a DWG viewer but not particularly usable as an accurate drafting tool yet. SolidWorks offers a number web-based file distribution and collaboration technologies on its labs website.\nAt present there is not any serious commercial attempts at providing a 2D drafting tool, let alone a proper 3D solution in the Mechanical or AEC spaces. It would be interesting to see the capabilities of SolidWorks\/Autodesk Inventor\/PTC\u2019s Pro\/E or Dassault Systemes\u2019 Catia in a browser but that is probably still some years off. In fact, Dassault has told me that it is working on delivering Catia online in the future but estimated three to five years.\nConclusion\nWhile there may not be any serious commercial offerings in the market as yet, Onlive does offer the promise of fundamental changes in the coming years. We may not need to have workstations but share part of a \u2018super computer\u2019 that is situated somewhere in the middle of a desert. With a distributed model, the hassle of upgrading workstation components may simply disappear.\nNot to forget, of course, the cultural issues such a change would require. It is still an issue for companies to externally store their mission critical data, let alone stream it live via the web around the world, while relying on the security and stability of third parties.\nBefore I get too carried away, this concept is all based on one thing: the reliability of fast, cheap Internet bandwidth. Countries that do not have the technology infrastructure are going to suffer. Even if each session took 1.5 Mb of bandwidth, in a room of 20 engineers you would need at least a 30 Mb pipe and while our Internet providers promise us much, they rarely, if ever, deliver the bandwidth they advertise. For instance, I currently pay for 20 Mb but am lucky to get 7 Mb.\nAs with most technologies, the games industry will lead the charge and we will follow. The concept that everyone gets the hottest hardware running their games irrespective of what they have at home seems incredibly liberating.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-and-the-cloud\/","title":"Autodesk and the cloud","date":1531440000000,"text":"It\u2019s been nearly ten years since the cloud became the hot topic in the AEC sector. Martyn Day explores the current cloud developments at Autodesk and shares his thoughts on where things might be heading \u2013 for good and bad\nFor all the hype and hoopla, the impact of \u2018the Cloud\u2019 on AEC is still in its infancy. In the beginning there were worries that applications were going to be web-based only; the reality is, this isn\u2019t happening at the flick of a switch.\nFor all the architects, structural engineers, MEP engineers, civil engineers and construction workers, the world is still exceptionally file-based. Authoring tools sit on workstations and projects are all about managing files. It\u2019s only now, having seen the concentration of development on cloud applications by Autodesk and others, and some applications built on Autodesk\u2019s cloud-based Forge platform, that we are starting to piece together how our world is going to change.\nPhase one cloud\nPhase one of cloud benefits has been centralising data, repositories for project files, communication, viewing, sharing \u2014 the whole collaboration benefit. For firms that have had to invest in complex servers and data compressors, the near future indicates that, by and large, that can be outsourced to products like BIM 360 and the benefits of storing the data in Autodesk\u2019s cloud is that Autodesk will offer a suite of ever-expanding services which can work with your project data in the cloud. Assemble Systems, which Autodesk has just acquired, is a case in point.\nPhase two\ncloud Phase two is the Forge backbone, which is Autodesk\u2019s third-party development platform which resides in the cloud. Historically, developers would get access to the core AutoCAD engine through APIs. They would write some code to do something useful, customers would buy copies and it would be installed on each machine. Everything was local.\nFor CAD managers, every workstation on the network was a node that needed updating and maintaining \u2014 core applications plus the third-party add-ons. In a cloud-based world, with Forge, the third-party application is freed from the desktop and can reside in Autodesk\u2019s BIM 360 environment alongside your data. Up until this point if the application needed functionality that is within Revit, it still needed to reside or call on the Revit instance on the desktop.\nAutodesk is soon to add something called Revit.io to the Forge backbone, which allows developers to access Revit functionality within the cloud environment, which means that Revit data stored in BIM 360 can be accessed, interrogated, manipulated and, if necessary, edited in the cloud without the need of a desktop-based authoring application.\nTo be clear, Revit.io does not mean that users run Revit on the cloud through a web browser. It is, for now, something to enable deeper application development on Forge. Autodesk has been promoting a product called Fusion to manufacturing customers which runs locally, or on the cloud which was written from the ground up to be a cloud-based application. We don\u2019t see a Revit equivalent coming out anytime soon but perhaps Revit.io will be used to deliver some capability through the browser in the future.\nBIM 360, Forge and Revit.io point towards and represent a fundamental change in the relationship between project data and applications. Once models are in BIM 360, BIM managers will have a choice of applications they can turn off or on. The integration has already been done; it will just be a choice deciding the suite of Autodesk and Developer services that It\u2019s been nearly ten years since the cloud became the hot topic in the AEC sector. Martyn Day explores the current cloud developments at Autodesk and shares his thoughts on where things might be heading \u2013 for good and bad would benefit your workflow and switching them on. For BIM managers this will be a huge boon, as implementation of company- wide systems is a flick of the switch.\nOnce the project data is in the cloud and it can be sliced and diced, there will be Forge-based applications which will allow non-BIM trained participants to leverage the power of the models. Targeted applications for niche uses, such as fire prevention and evacuation are already in development from firms such as Xinaps, where Revit models are used and analysed on the cloud, with the results being served up and rendered in a highly visual way for non-CAD users interested in assessing a design\u2019s performance.\nAutodesk is also working on machine learning \/ AI solutions which will optimise building layouts and designs, presenting multiple \u2018solved\u2019 results, from which the best can be selected. What was available on the desktop with a downloaded specialised add-on, will be available in the cloud to everyone. Because of the cloud, the relationships between authoring tool, data, processing and access, are set to change dramatically over the next five years.\nThe gilded cage To come back down to earth, the cloud is also a gilded cage. For years we have been hampered by operating in data silos and the cloud will blow the walls away but with the interest in firms having Common Data Environments (CDE), hosting data authored in applications from multiple vendors is still a bit of a dream. By moving from your own hosted data management systems to a cloud-based service such as BIM360, you are letting Autodesk expand into controlling your entire process \u2013 not just authoring, team collaboration and project management but wherever that data touches or may be needed in the future.\nThe reliance on one firm, one subscription, removes a lot of the negotiation capabilities when it comes to renewal. Historically, Autodesk product has seen street competition in pricing. With subscription this has tended to go away. Should you be large enough to warrant a three yearly Enterprise Business Agreement (EBA), total reliance on one vendor does not give you much leverage. In many of our conversations with large firms, there is a fair bit of resentment at paying for products they don\u2019t need (subscription bundles, stuffed with products that don\u2019t fit into existing workflows) together with mandatory consultancy hours, which many do not want.\nThe cost of ownership is going up but Autodesk would argue that the benefits, the ROI and the value are increasing with the capabilities being delivered. However, the estimation of \u2018value\u2019 is really one that can only ever be truly assessed by customers. Autodesk needs to work harder to change this perception within its customers, as it moves from tool developer to service provider.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/tekla-structures-19\/","title":"Tekla Structures 19","date":1374019200000,"text":"The latest release of Tekla\u2019s structural BIM tool touches the entire building process from design to construction.\nTekla Structures started out as a steel fabrication tool, but has grown into an expansive structural Building Information Modelling (BIM) application. Engineers, fabricators and contractors use it to model, detail, fabricate and build \u2014 all the way from the air-conditioned design office to windswept site.\nTo support this broad spectrum of processes Tekla breaks the software down into different versions. There are specific configurations for steel detailing, precast concrete detailing, cast in place, engineering, construction management, drafter and others. Some configurations also feature different operational modes that are appropriate to the user\u2019s responsibility and task.\nOne reason for the different configurations is to ensure users are not paying for features they will never use. But it is also to streamline what can become a pretty expansive user interface. If you are focused on construction management, for example, you do not want to see the connection macros or detailing tools.\nWhile each configuration is focused on a specific role, the fact that everything is done in a single environment \u2014 in some cases a single model \u2014 offers big workflow benefits. An engineer models the structure then a steel detailer adds the connection details, drawings and schedules. The engineer is able to see all of the fabrication information \u2014 but is not able to edit or create it.\nModelling\nTekla has worked hard on the modelling and editing capabilities of the software in the last few releases and it is very easy to build up models around a grid, copying, pasting or arraying objects, linearly and radially.\nVersion 19 has new \u2018SketchUp\u2019-type features: users can grab handles, faces, and planes and pull them around, particularly useful in the formative stages of design.\nThe software can model pretty much anything: from beams, columns, slabs, and walls, chosen from an extensive library of UK and European standards, to parametric connections and components including curtain walls, staircases, trusses, and precast floors. A custom component editor lets users create their own parametric connections, details, and parts. Macros can also be recorded to automate common tasks.\nUsers can work in 2D or 3D at the same time, in multiple views, and even start in one view and finish in another. Specific views can be set up to help navigate around huge models, zooming in on a specific phase or storey.\nThe software is set up to support multiple users, working on a single model accessed directly from a server or via VPN. This makes it possible to work on different phases, or to support concurrent workflows, such as change orders, RFIs, modelling and drawing production. ID numbers of objects are tracked to avoid conflicts, but the process still needs to be managed and co-ordinated.\nThe software supports clash checking, which can handle any elements \u2014 steel, concrete, rebar and IFC data. While tolerance checks can be applied to connections to check that bolts can be placed, there are no generic proximity clash tools as of yet. However, this can be done in the free model-based collaboration tool, Tekla BIMsight.\nInteroperability\nOver the last few years Tekla has invested heavily in interoperability, working closely with Bentley, Autodesk, Graphisoft and others to improve links between Tekla Structures and third party software.\nIt can read in DGN, DWG, DXF and a host of other formats, including PDF, which has just been introduced in this latest release.\nThe big emphasis, however, is on Industry Foundation Classes (IFCs), because of its ability to hold object-based data and geometry. IFC files can be read in directly, but Tekla has also written plug-ins for Revit and ArchiCAD enriching the IFC file with a view to making the process more seamless. IFC export is also included.\nThe software includes some workflow management tools to help manage any changes that are made in the original BIM authoring tool. When re-importing IFC files it is possible to show what is new, old, unchanged, changed, and deleted so users can see very quickly what is different. It is also possible to track property changes, which is becoming just as important as geometry as the industry moves towards Level 2 BIM.\nTekla Structures is not just about sharing data with other BIM authoring tools. It also has strong bi-directional links with many of the leading structural analysis software applications, including Staad.Pro, SAP2000, Robot and Dlubal.\nThe model can be built in Tekla Structures, sent off to the analysis application with section sizes automatically adjusted in Tekla Structures based on the results. Various parts of the model can be routed to different apps, for example to handle steel and concrete.\nWhere direct links are not available Tekla Structures also supports CIS\/2 (CIMSteel Integration Standards) and SDNF (Steel Detailing Neutral Format).\nWith a background in fabrication, it is no surprise that Tekla Structures has strong links with CNC machinery via the DSTV format and precast machinery via Unitechnik, BVBS, HMS and SDI.\nConcrete\nTekla Structures might have started out life as a structural steel modelling and detailing tool, but can now handle most materials. When \u2018concrete\u2019 was added in the mid 2000s, the initial focus was on the pre-cast market, but this has now been extended to cast in situ structures.\nThere are a number of new tools in Tekla Structures v19 specifically designed for cast in situ concrete. A new \u2018auto union\u2019 feature automatically merges two concrete objects into one. So, if a pad and strip footing are placed next to each other or overlap they will automatically combine into a single volume of continuous concrete. Both objects retain their identity so if it is later decided that the foundations will be pre-cast this can be changed with a single click.\nFor cast in place work concrete can be split up into individual pours, which is a neat tool for planning the construction process downstream.\nTekla Structures has a range of tools to help contractors estimate the raw quantities used in concrete structures. Pour objects and formwork can be linked to time scheduling using the Task Manager, helping plan how concrete will be built on site and how much the construction process will cost.\nTaking control of data\nManagement of data has become a big focus for Tekla in the last few releases. Version 19 introduces the Object Browser, which is used for viewing or enquiring into the properties of selected objects.\nThe Object Browser can be used at any stage of the design and construction process: for checking object properties in the design phase or getting quantities out for estimating. Data is presented in a spreadsheet and can also be exported to Microsoft Excel for further processing.\nAnother tool, Model Organiser, is used to break down models into logical areas. Models can be classified by storey, object type, lots, sequencing and many more.\nModel Organiser is helpful for adhering to BIM protocols and delivering properly structured information for COBie data drops. It can also be tied into Tekla Structures\u2019 Task Manager to plan and manage project works.\nTask Manager is designed to help schedule projects and track progress, adding start dates, end dates, work duration, etc to the underlying model data.\nTo date, it has mostly been used by steel fabricators to help plan how to get different phases out, but is now being aimed more at contractors, particularly in relation to the new concrete pour functionality. Here it can be used to quickly price jobs, applying time and cost to tasks based on model data. Or to plan the erection process, including steel, concrete and temporary works.\nTask Manager includes bi-directional links to Primavera and Microsoft Project, which is useful if contractors are using a dedicated project management tool.\nMoving further downstream, Tekla Structures can also be used for setting out. The Layout Manager can prepare accurate model data, which is then exported to a field layout device. As Tekla is owned by Trimble, the obvious choice is Trimble Field Link running on a Trimble mobile device, which then communicates with a total station.\nWith Layout Manager it is also possible verify as-built conditions by feeding back information from site. If required, the model can be adjusted to suit site conditions. Everything is trackable and traceable.\nConclusion\nTekla Structures has come a long way since its humble beginnings as a steel fabrication tool. It focuses as much on design and engineering as it does on how steel and concrete will be constructed and managed on site. This has cumulated in this latest release with the introduction of pour management for cast in situ concrete.\nAs a result, it has a much further reach than Autodesk Revit Structure or Bentley AECOsim Building Designer, which are more focused on design and engineering.\nMoving forward, the biggest opportunities for Tekla Structures are undoubtedly at the tail end of the construction process. We would expect stronger links to be formed between the fabrication model and the reality of what is being built on site. The expertise that owner Trimble has in positioning technologies is sure to put Tekla in a strong position in this growing sector, perhaps with laser scanning and point cloud data playing a key role.\nPrice on application \/ tekla.com\nIf you enjoyed this article, check out our Tekla Structures 20 review","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/autodesk-world-press-day\/","title":"Autodesk World Press Day","date":1175558400000,"text":"In light of amazing growth and a change in CEO, Autodesk is seeking to reinvent itself. The company\u00dds recent press day in San Francisco offered a chance to hear the Autodesk 3.0 pitch. Martyn Day reports.\nMost engineers already have a view of Autodesk and its products, as AutoCAD is the de facto 2D CAD system and the majority of engineers were trained on, or at least used, AutoCAD at some point in their careers. The world\u00dds press, however, has had relatively little exposure to the design giant or its products and with a new CEO at the helm, the company has gone on a charm offensive to build on its consistent and dramatic growth over the last four years. Autodesk has outperformed any of its competitors and the world market for its products, growing in double digits, and appears to show little sign of abating. The company will have moved from a $1 billion company to nearly a $3 billion in under four years.\nAutodesk\u00dds World Press day was held in San Francisco and gave an opportunity for Autodesk to cover company strategy, market dynamics and introduce the new range of 2008 products. On the first day the company\u00dds VPs and top Executives got to give their overviews of the market, while the second day offered a chance to drill down into the different division\u00dds (Building, MCAD etc.) product mix and market strategy. There was a break-out area where hands-on product demos could be given and a series of one-on-one meeting arranged.\nThe net result of this appeared to be that the presentation sessions were all well attended but very few went on to seek a hands on demo. It seems that Autodesk is interesting to the wider press as a business success and industry force to be reckoned with, rather than the real interest in the products\u00dd capabilities or developments thereof. I guess some fairly specialised knowledge is required to do this and I should feel relieved there\u00dds little competition!\nIn general the strategy sessions were a repeat of last November\u00dds Autodesk University main stage presentations (which can be found here) but this time were given with added confidence \u2013 and I have to say that the VP team, which has had a little rearrangement this year, are shaping up to be a fairly formidable group. The new Marketing VP, Chris Bradshaw, moved from successfully launching the Civils products and was formerly with Bass while at Buzzsaw (an Autodesk spin-off, online hosting company that Autodesk bought back inside). Then there\u00dds the very bright Jeff Kowalski, who takes the long vacant role of Chief Technology Officer (CTO), who worked with Bass at Ithaca software prior to Autodesk buying them many moons ago. In the last few years of Carol Bartz tenure as CEO, Bass concentrated on restructuring and re-energising product development at Autodesk, leading to an appetite to produce new products and innovation. Now Bass has stepped up to the plate, it\u00dds obvious that \u00d9his kind of people\u00dd are taking more prominent roles within the company. I think in the past, Autodesk was more about spin, sales and branding. The new Autodesk is still inescapably very much about sales but there\u00dds now more substance backing this up, together with vision and an appetite to visit customers and see what problems need to be solved. There are still serious issues with the Autodesk customer experience that need to be addressed but it\u00dds hard for such a huge company to really get close to its customers and develop a sense of community. I can see that Autodesk is at least trying in these areas.\nJeff Kowalski, CTO\nAs I\u00ddve already really covered the keynote messages in previous articles, I thought it was worth relating the message of the new CTO, Jeff Kowalski. It was the first time I\u00ddd heard him present and he demonstrated some innovative technology.\nKowalski explained how technology is changing the way we design. He saw his job as trying to understand customers, their range of skills, culture and processes and try to augment their design capabilities through digital tools. Kowalski stated that many design tool developers ignore the environment in which design tasks are carried out, and success is when the technology is transparent.\nRarely is the first design the right design, requiring prototype and analysis. This iterative process could be seen as a \u00d9design conversation\u00dd, and for too long this has required expertise in the field to understand and communicate important design information. Kowalski sees it as Autodesk\u00dds task to break down this wall and come up with new ways to have this conversation, to allow accurate design information to be delivered in many different ways, from industry experts to home owners.\nThe other key focus will be to drive intention into the design. So rather than just documenting creation then providing performance analysis, the CAD system will have some prior knowledge of the performance requirements of a design and will actively assist in meeting those requirements. This has been termed Functional Design by Autodesk and is one of the driving principles behind the company\u00dds MCAD products.\nDuring his presentation, Kowalski demonstrated two interesting technologies that the company had in development \u2013 neither of which the company had any plans to commercialise but were fascinating as they were hardware-biased. One was a very large LCD screen that could be touch activated and manipulated by fingertip, improving man\/machine interaction and the other was a multi-axis arm which had a screen attached to it, and as the display moved, the image moved around a virtual, rendered model accordingly. The last time Autodesk demonstrated any hardware to me it had developed was well over ten years ago and was a Virtual Reality system that occupied a whole room.\nIn a conversation after the event, Kowalski told me that his job is to unify the interface experiences of the many, many products that Autodesk has, as well as to further develop these multi-representational formats to liberate the data which is stuck inside its products.\n{mospagebreak}\nAutoCAD 2008\nAnd so to products. It\u00dds that time of year again when Autodesk stops supporting one version of AutoCAD and releases another. AutoCAD 2008 on paper actually looks quite dull. In the past when Autodesk has announced a release based around the AutoCAD User Group wish list, it has usually signified that the company has run out of ideas. The reality of this release is that there\u00dds actually a lot of useful stuff in it.\nThe key features of 2008 include: annotation improvements, layer properties per viewport, enhanced tables, enhanced MText, and multiple leaders. None of these sound too fantastic, like last year\u00dds massive boot to 3D capabilities, but what\u00dds been added has been done very well and we will review the changes in the next issue.\nWhat has since occurred though is a broad sweep of price increases, for which you had better check with your dealer. It seems that Autodesk Subscription pricing has gone up about 30%, which is deeply shocking. Since its launch in the UK, I calculate that Subscription is up around 60%. With dealers now telling me that subscribers are regularly questioning the value they have received from subs. This is perhaps a price hike too far. We will report on this next month.\nBuilding Products\nAutodesk\u00dds Building Division has re-organised and renamed a number of products, as well as getting a few more added to its roster. It all makes good sense.\nAutoCAD Architecture 2008 was formerly known as Autodesk Architectural Desktop; AutoCAD MEP was formerly known as Autodesk Building Systems; and AutoCAD Civil 3D has been added to the Building division. On the Revit side of things to align all the products Autodesk has modified the names of several Revit-based applications. Revit Architecture 2008 used to be Revit Building, then there\u00dds Revit Structure 2008 and Revit MEP 2008 (which was formerly Revit Systems).\nThe move confirms that AutoCAD Architecture (ADT) is 2D and Revit is 3D. AutoCAD Architecture features: new annotation scaling, auto space generation, drawing compare, photometric lighting, Sun\/Sky systems and procedural rendering. For Revit colour fills have been greatly improved, Revit Groups for repeating units or rooms of similar size has been introduced, and all instances can be edited at once. Dependent Views allows projects to be segmented across multiple sheet views, and Max and Viz can now import Revit 3D DWGs, maintaining materials and camera positions while moving back and fore. There are graphic overrides for object representations and new drag and drop file linking tools and clearer views of nested links. The \u00d9Google Earth\u00dd plug-in has also been included in the shipping version.\nIn the past Revit development has been driven by flagship projects like the Freedom Tower and functionality has been added to help skyscrapers grow \u00d9vertically\u00dd. For this release Autodesk has actually looked at the design of buildings over a larger area, such as schools and hospitals, where you have a series of large interconnected buildings. The 2008 Revit release features far better management for these types of projects, regardless of whether you\u00ddre working on one large file or a number of separate files for individual buildings.\nThe amount of development work that the Revit suite has had compared to AutoCAD Architecture appears significantly out of balance. In short, if you are thinking of moving to 3D go the Revit route. If you predominantly are 2D then stick with an AutoCAD environment. This isn\u00ddt good news for people that invested in ADT and hoped to adopt a model-based process based on it.\nStructural and MEP\nAt Autodesk, structural and building services engineering have historically been the poor cousins of architecture, but things are changing. For the 2008 releases there\u00dds been a lot of work done in those areas, some might say to the detriment of applications such as AutoCAD Architecture.\nIn line with Revit Architecture 2008, the ability to model non-standard components tops the list of enhancements. A powerful parametric structural truss generator \u2013 which was originally available in Revit Structure 4 as an example of what you can do with Revit\u00dds API \u2013 has now been incorporated into the core product. Curved beams (and openings in beams) are also new and warped structural slabs are now possible. In the past slabs had to be of a constant thickness; now you can easily incorporate multiple slopes for draining.\nIn terms of interoperability, Autodesk has been working with third party structural analysis developers to build stronger bi-directional links with Revit Structure, which are still fairly basic in some applications. In relation to this, with Autodesk\u00dds proposed acquisition of Robobat falling through late last year many had expected development work between the two companies to slow down. However, we\u00ddve learned that the integration between Robot and Revit Structure is more advanced than ever. Finally on the structural side of things, there are still no plans for an \u00d9AutoCAD Structural\u00dd \u2013 the future is certainly with Revit.\nOn the MEP (Mechanical Electrical Plumbing) front, AutoCAD MEP 2008 (formerly Autodesk Building Systems) is for documentation, while Revit MEP 2008 (formerly Revit Systems) is for systems information modelling \u2013 so says Autodesk marketing, at least. As Revit MEP is a much younger product, there\u00dds still a gap between the two, with AutoCAD MEP a more rounded product, though this is largely down to AutoCAD MEP\u00dds mature parts library. The majority of UK content in Revit MEP is metricised imperial libraries, though this will change in future releases and discussions are already underway with several manufacturers.\nHowever, both products form part of the AutoCAD Revit MEP Suite, so anything you can\u00ddt yet do in Revit MEP, you can do in AutoCAD MEP, though this will be at the expense of optimum interoperability with other Revit products.\nFinally, in addition to standard analysis functionality, such as pipe sizing and duct sizing, Revit MEP now includes heating and cooling calculations, which is the result of a collaboration with Glasgow-based IES (Integrated Environmental Solutions). To extend this further into the realms of lighting and solar calculations, Revit MEP 2008 also links out to IES\u00dd\nIn the May\/June edition of AEC Magazine we will be taking a closer look at the new Autodesk 2008 products and their functionality.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/iot-impact\/","title":"IoT impact","date":1474243200000,"text":"After a slow lift-off, the Internet of Things is making its presence felt in almost every aspect of the AECO industry and these technologies have already started to deliver, writes Peter Marchese, senior technical evangelist at Microdesk\nThe term \u2018Internet of Things\u2019 (IoT) has been around for years, but like many promised technology disruptions, it\u2019s taken some time for the concept to find its legs and live up to the promises made for it.\nIn 2016, however, the IoT is finally starting to be implemented in many different industries \u2013 not least the architecture, engineering, construction and owner- operated (AECO) industry, which is incorporating IoT ideas and technologies into how buildings are designed, constructed and managed.\nTake, for example, the field of architecture: here, we see buildings that use IoT principles to become more \u2018transformative\u2019, adapting to users\u2019 needs based on changing internal and external environmental conditions.\nThe RMIT School of Architecture and Design in Melbourne, Australia, for example, is covered in thousands of small, circular glass plates that rotate to improve interior conditions, providing increased shade and ventilation. This is an innovative application of IoT ideas that not only helps designers create more efficient buildings, but also helps owners save money through energy efficiency.\nAnother innovative example might be the adaptive skins on the Al Bahr Towers in Abu Dhabi, United Arab Emirates. These are reactive to the sun and dynamically adjust throughout the day to manage sunlight and heat gain, again saving money through energy efficiency. And at the same time, from a designer\u2019s perspective, the exterior skins also give the building a unique aesthetic.\nIoT for engineers\nIn civil and structural engineering, different uses are being found for IoT concepts and technologies. They might be used, for example, to improve people\u2019s safety, particularly when it comes to building structures and bridges based in seismic zones.\nHere, maintenance can be an issue. In the US state of California alone, there are over 24,000 bridges that need to be inspected. But inspectors face significant constraints \u2014 in many locales, they work in small teams of two, for example, and only spend two days a week in the field.\nWith intelligent sensors and devices, it might be possible to collect and analyse data relating to the condition of these structures through an automated, regular process. By identifying any issues, engineers could prioritise maintenance tasks better and prevent future accidents.\nIn mechanical, electrical, and plumbing (MEP), meanwhile, IoT technologies are being integrated into building environmental systems. That gives MEP engineers the ability to track the status of the wide variety of systems that one might find in a medium to large-sized construction project, building up a more holistic view of multiple disparate systems and driving more intelligent processes for owners, relating to the management and maintenance of their properties.\nFor example, a building might have sensors installed that take temperature and pressure readings from mechanical equipment. If those readings start to climb, they may reach a point where an inspection is required. With IoT technologies, on-site maintenance staff can be alerted to the situation. They may even be guided to the correct location via a 3D map or model. If parts need replacement, a service order might be automatically generated. The whole process is more efficient, saving time and money.\nThe potential here is huge: the IoT will help to create better built environments, using data that gives us insight and automation that speeds up processes and makes them more accurate. It can simplify workflows and provide new solutions to hard-totackle problems. We are still in the beginning stages of seeing how IoT will impact AECO projects and the industry itself, but it\u2019s already started to deliver on its promises.\nMicrodesk helps AECO firms implement and utilise design and construction technology. The consultancy has 12 offices worldwide, including a recently opened office in London, UK.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/the-light-fantastic\/","title":"The light fantastic","date":1188000000000,"text":"The way we see our world is thanks to the way light is absorbed and\/or reflected by the objects around us. In Revit and 3ds Max this can be re-created by raytracing and radiosity rendering.\nWhen recreating photo-realistic images from virtual, modelled environments the concept of applying a texture or material to an object in order to represent the finish is generally understood, but the properties of absorption and reflection also need to be considered, and two pieces of jargon often banded around are Raytracing and Radiosity.\nRaytracing is a common rendering algorithm used by software packages including Accurender, a cut down version of which powers the rendering of images from within Revit. Ray Tracing is a global illumination based rendering method; it traces rays of light projected from the eye through the image plane into the model scene. The rays are tested against all the objects in the scene to determine if they intersect any other objects. If the ray does intersect with an object, the pixel is then set to the colour values returned by the ray. If the ray misses all objects, then that pixel is shaded the background colour. This technique handles shadows, multiple specula reflections, texture mapping, reflective surfaces, and transparent or opaque materials. However there are limitations to this technique when it is used in isolation.\n{mospagebreak}\nRadiosity is a method of illumination which takes into account distribution of energy via reflection and absorption of light. The first pass will identify the sources of light within a scene. After the first pass each surface is treated as a light source whose intensity is defined by the energy distributed from the first pass sources, minus the effect of the surface finish. The process is repeated until the residual energy falls below a certain tolerance and the lower this tolerance is set, the higher the level of realism achieved. High-end visualisation packages such as Viz and 3D Max are able to \u00d9bounce\u00dd light until the eye cannot distinguish the result from a photograph.\nIn rendering packages such as that within Revit, raytracing may be performed on its own but will be significantly enhanced by the use of radiosity, with a couple of provisos:\n- Radiosity is only used for internal scenes, this is because light needs surfaces to bounce from; using an external scene will cause the light to bounce around infinity, causing unrealistic illumination and also very high white spots in places.\n- Radiosity is not very good when used in animation as it requires exposure controls which can cause flickering in a movie. Accurender appears to suffer quite badly from this phenomenon, whereas Max and Viz have got some exposures that are more suited to animation.\nIf the situation suits the use of radiosity then this should be carried out first and the resulting light model is used to influence the results of the raytracing and provide more realistic shadows.\nFor example\u00cd\nTo further understand the process, consider the simple example of a shiny red ball sitting on a white surface. Light striking the ball casts a shadow which is obvious enough, but a certain quantity of light energy is reflected by the ball which then acts as a secondary emitter, sending an amount of red tinted light to the surrounding surface. The result is to give the otherwise white surface a reddish hue within the immediate vicinity of the ball. This effect is often referred to as \u00d9colour bleeding\u00dd and subtle as it is, we are accustomed to the result in the real world; hence radiosity helps to create the illusion of realism within computer renderings.\nThe radiosity process produces a 3D light model of the scene with basic colour and absorption\/reflection properties assigned to geometry. The light model is then used to enhance the image produced during the raytrace procedure.\nThe next two images show how radiosity calculations applied in conjunction with raytracing produce images with more subtle and accurate lighting than those rendered with raytracing alone.\nIn Figure 1, an image produced using raytracing alone, light is emitted and intersects with the objects in the scene lighting them and casting shadows beyond, but due to the way that the light rays are tested and returned, the scene appears unnaturally dark as objects are only lit by rays coming directly from the primary light source. Similarly shadows are only created where rays do not hit an objects surface.\nIn Figure 2 when radiosity is used in conjunction with raytracing to calculate the scene lighting the results are much more realistic. Light is reflected back from the surfaces and bounced \u2013 or to put it in more technical terms, re-gathered \u2013 around the scene, whilst \u00d9bleeding\u00dd colour from the objects as it goes. Shadows also appear more subdued as light is reflected from surfaces behind objects where before, no direct light fell. In Figure 3 you can see the effect of using Radiosity on its own without raytracing.\nRevit uses a highly simplified version of the Accurender rendering engine and the interface has been designed with architects in mind, so unlike specialist rendering products on the market, users need not be visualisation gurus with pony-tails; it is easy to use and still produces good results, albeit not in the same league as more advanced applications. With almost no user intervention, the engine calculates indirect light, hard \/ soft shadows, colour bleeding, blurry \/ sharp reflections, translucency, transparency, depth of field, attenuation and reflections.\n{mospagebreak}\nThe down side\nUsing Radiosity does require additional processing time for still images, although for animation, the same light model applies for the entire scene so a new Radiosity calculation is not required for each frame. Also, as it creates a static, 3D light model, any alteration or movement of objects within the scene make the light model obsolete and the Radiosity solution must be re-calculated.\nThere are some other issues for consideration associated with using a radiosity solution besides the render time; anomalies within an image are often referred to as artefacts.\nOne of the most common types of artefact is Light Leakage which is due to light seeping between surfaces. Imagine a fish tank whose glass sheets butt up to one another without any sealant; water would seep between the joints.\nAdvanced options\nThe results of the radiosity calculation can also be used to analyse the designed lighting of your building.\nRevit allows for accurate lighting data from the manufacturers to be attributed to a Revit light fitting to replicate the illumination provided by certain designs. For example, the Erco web site allows you to download IES data files for every light fitting in their catalogue. The IES file contains all the photometric data for a particular light fitting such as the hotspot angle, falloff, etc. and when a lighting fixture family is directed towards the file it applies this information to the light model.\nConclusion\nSo as well as being one of \u2013 if not the \u2013 most powerful design package for the building design industry, Revit allows the user to generate photo-realistic images of the concept with accurate lighting. The most important factor in this however, is that the rendered images produced are a by-product of having developed your design and hence produced your plans, elevations and schedules etc. Many Architects that adopt Revit are so used to having to outsource the production of presentation graphics that they overlook a very accessible and useful aspect of Revit, assuming it to be overly complicated. What this article attempts is to explain some of the concepts of rendering, but also to encourage users to have-a-go. A full-colour well-lit scene can speak volumes to a client.\nThat said of course Revit-produced images will not compete with the quality achievable in advanced visualisation packages such as 3ds Max, and for the really important images, it is still relevant to incorporate such requirements in your plans, but at least the Revit model can be imported into Max without having to re-model. The Radiosity calculation from Revit can also be saved as an external file, and then re-used within Max to save re-calculating the scene (although, if any materials are edited, or objects moved within Max, then the scene should be re-processed to reflect the changes made).\nRevit really is a one-stop design tool for architects and designers, offering the ability to not only design buildings quickly and accurately, but also to show the concept off to its best advantage with still images, walkthroughs and panoramic vista with little additional effort or skill required!","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/puretech-systems-selected-to-protect-southwestern-u-s-water-utilities\/","title":"PureTech systems selected to protect Southwestern U.S. water utilities","date":1122249600000,"text":"U.S. based PureTech Systems Inc. has been selected to provide its PureActiv surveillance solution at a Southwestern U.S. water treatment plant as part of the site\u2019s total security upgrade. PureTech has also been selected to supply and install a PureActiv system as a pilot project for another major city in California. PureTech is the developer of a wide-area surveillance system known as PureActiv, designed to safeguard the assets and employees of firms with expansive outdoor or geographically distributed operations. PureTech incorporates its technologies which include GIS mapping, advanced image scene analysis, intrusion sensor integrations, patent-pending multi-camera steering algorithms, and policy-based alarm notification and response engine, into its PureActiv solution.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/surroundbim-for-small-projects\/","title":"BIM for small projects","date":1489536000000,"text":"What works for building the Crossrail project may not work for the development of an artist\u2019s studio in Cornwall, but small and medium-sized projects can still benefit from BIM, writes architect Jacob Down\nFor architectural practices that work on projects of a predominantly small-scale nature, it\u2019s easy to see why the adoption of BIM Level 2 may not be at the top of the agenda.\nTake, for example, just a few of the key documents involved. There\u2019s BS 1192:2007+A2:2015, PAS 1192-2:2013, PAS 1192-3:2014, BS 1192-4:2014, PAS 1192-5:2015. The list goes on, and you need to implement these alongside the Uniclass 2015 classification, the NBS BIM Tool Kit and the CIC BIM Protocol.\nDespite all the industry hype surrounding BIM\u2019s improved productivity, coordination and efficiencies, the sudden adoption of a myriad of standards, specifications and protocols could potentially be paralysing to any project without sufficient prior expertise, experience and knowledge.\nThat said, there are still many benefits to be exploited through the utilisation of BIM technologies on small projects without the need to delve deeply into the realms and complexities of BIM Level 2.\nThrough the simple adoption of a BIM software package, in lieu of conventional 2D drafting and 3D modelling software, architects and designers can quickly tap into some of its inherent advantages and efficiencies.\nReality-captured site geometry\nOver the last 18 months, there has been a fundamental shift in the tools and techniques used by surveyors to capture the spatial volumes and site features of landforms and buildings.\nWith the rise of reality-capture technologies like 3D lidar scanning and aerial drone photogrammetry, now common across the construction industry, aligning one\u2019s design tools to enable full utilisation of this site-specific data makes logical sense.\nAccess to such a wealth of accurate site geometry and high-definition visual content is unprecedented and utilisation of a point cloud-compatible BIM software package enables this data to be referenced and interrogated directly within the design\/model space.\nFor the small-scale architectural project, this opens up huge design opportunities through the potential creation of highly bespoke spaces that begin to truly respond to site-specific conditions. It\u2019s a powerful resource on projects involving alterations to existing and listed buildings, sites of a highly sensitive nature or contentious sites where party walls or rights to light maybe involved.\nThe ability to visualise and reference the reality-captured context within accuracies of +\/\u2013 1mm precision will provide a new level of design confidence that has yet to be fully explored within the industry.\nOver the past twelve months, Poynton Bradbury Wynter Cole Architects has utilised the benefits of reality-captured site geometry on a number projects, enabling proposals to be digitally interrogated and visually represented from any viewpoint.\nThis not only includes external viewpoints, like a typical approach or street perspective, but also the illustration of future views through the windows of the proposal itself or existing views from neighbouring properties.\nThis has provided clients, planners and members of the public with a much better understanding of the proposal in question. The practice\u2019s ability to confidently generate such information on demand to millimeter precision, meanwhile, has aided these projects to successful planning outcomes.\nDesign coordination\nPutting aside the data rich content of the BIM model and focusing on its 3D geometry as the primary means of coordination is another way BIM technology can be utilised.\nThe 3D model space provides a digital environment not only to construct architectural elements but also to coordinate and interrogate the design proposals of different disciplines.\nOn projects of a small-scale nature, the biggest barrier seems to be the ability for the design team consultants to possess or acquire the relevant expertise and skillsets to contribute to the production of 3D information in a compatible format, with many still opting for 2D drawing outputs.\nThus, in order to successfully coordinate this information, key structural and M&E elements can be modelled into the 3D model space purely as a validation and coordination exercise.\nFrom the architect or lead designer\u2019s perspective, this not only facilitates the detection of any potentially clashes, but also helps inform and control aesthetic design decisions through the visual interrogation of the digital geometry and proposed materials.\nLuckily, on projects of a small scale nature like the ones being discussed in this text, this should be a fairly simple undertaking \u2013 one in which the benefits of having a coordinated digital 3D model far outweigh the labour involved.\nI commonly undertake that kind of exercise for the structural elements of projects, where the appointed structural engineer is not yet operating in a BIM environment. As BIM becomes more widespread among smaller-scale practices and design team consultancies, I anticipate that such exercises should in turn become obsolete.\nAutomation and human error\nWithin BIM software packages lie a whole series of automated functions that are geared towards the efficient production and coordination of construction information.\nMundane tasks such as the scheduling of doors, windows, sheets and room areas are automated. Drawing revisions are tracked and recorded within the project file, and drawings are extracted directly from the 3D model geometry. As long your 3D geometry is correct, you can be confident that the 2D geometry in the resultant plans, sections and elevations are an accurate representation of that 3D geometry.\nThese automated functions omit tasks that might have historically been done by human input and, in doing so, greatly reduce the margin for human error, resulting in the efficient production of higher quality and more accurate information.\nAutomation capacity and data management can be extended furthermore through the use of third-party plug-ins and open source visual scripting tools; dissolving the barriers of the pre-packaged BIM software and facilitating the creation of unique solutions that may become the workflows of the future.\nAn example is my own recent discovery of XRev UpRev, a third-party plug-in for Revit 2017. This enables the user to apply a revision to multiple sheets in a single action, something not possible natively within the software. As trivial as this sounds, it is a godsend when large lists of sheets are involved, especially with deadlines looming.\nAdding value through 3D visualisation\nInstrumental to BIM software is the production of a 3D model from which design information is coordinated and project data is extracted. The inherent existence of this 3D geometry facilitates the production of multiple 3D representations and visualisations \u2013 from simple perspective lines drawings, exploded axonometrics and rendered imagery, to 3D animations, stereoscopic panoramas, real-time VR and mobile BIM viewing. Such outputs are either facilitated through the software itself, or are very attainable through additional software and applications.\nThese more immersive forms of architectural representation have been embraced in recent years in playful, creative ways by Poynton Bradbury Wynter Cole Architects.\nAs far back as two Christmases ago, we used augmented reality (AR) to impose the digital BIM models of recently completed projects onto the top surface of wrapped Christmas presents. These presents were given to clients and guests, enabling them to freely rotate and move in physical space to explore scaled-down digital models of proposals, via an iPhone.\nMore recently, non-locomotive stereoscopic virtual reality (VR) has been implemented through coupling of iPhones and Google Cardboard-like headsets, to present proposed interiors not only for client review but also for end-user consultation and educational purposes.\nThe existence of this 3D geometry is likely to lead to a reduction in the outsourcing to third-party rendering service providers, as architects and designers will now have the tools in-house to create their own glossy productions and VR experiences, directly from their BIM geometry and material data.\nThese modes of architectural representation and communication can be highly seductive and add significant value to any project, regardless of its size. In conclusion, BIM Level 2 is a result of over two decades of government reporting, with the intention of improving cost, value and carbon performance through the use of sharable asset information on large-scale government projects such as Heathrow Terminal 5.\nBIM Level 2 provides the industry with a suite of specifications, standards and protocols to best achieve this.\nIt\u2019s certainly true that what works for the \u00a314.8bn Crossrail project across central London is not necessarily going to work for a \u00a350k artist\u2019s studio in St. Ives, Cornwall.\nThat said, many benefits of BIM are scalable \u2013 downwards as well as upwards. It is through their adoption on small to medium-sized projects that the potential exists for a more organic evolution towards BIM Level 2. And that evolution should be driven from the bottom up, rather than imposed from the top down.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-university-2010\/","title":"Autodesk University 2010","date":1297987200000,"text":"The end of November saw the annual migration of CAD fans to Las Vegas for Autodesk\u2019s huge University event. As usual, AEC magazine was there in force to see the latest design technology, writes Martyn Day.\nThere are very few cities even in America with the necessary infrastructure to host Autodesk University (AU). In the past attendance has been as high as 12,000 but with the recession and the availability of many of the main stage presentations now online, attendance has dropped to around 7,000. That is still a lot of people to fit in a room. I am told that Autodesk University had over 40,000 people sign up and it is available all year for those that are interested (au.autodesk.com)\nThe show is a week-long mix of general presentations, training classes, industry sessions and meetings, not to mention the lunches, cocktail evenings and parties. It is almost an Olympic endurance sport.\nAutodesk has settled into yearly releases of its products, usually around March, with mid-year (October) additional functionality for customers on subscription. AU falls at the end of the year and so while some new technology is on view, future technology is often disguised in the presentations, obliquely referenced or not mentioned at all. It is always a good game to spot never-been-seen products.\nOf course this is also limited, as Autodesk Labs online is mushrooming with technology demonstrations of future products. This year was no different and Autodesk Labs had some new technology previews to share with interested customers, but more on those later.\nThe cloud\nThere has been a lot of talk about cloud computing and its future impact on how we will access design software in the future. At the moment, software is bought and installed on a local PC. In the future software will be hosted on powerful servers and will be accessed over the Internet remotely.\nAs the software will be hosted, there will be no need for customer-installed updates, or a powerful local computer. Software and processing power becomes a service \u2013 this is termed SaaS (Software as a Service).\nNow, there are many questions that this architecture poses. What happens if my Internet connection is slow or fails? Where is my data stored? What happens if I am on the move? How much will this cost? The truth is nobody really knows yet as it is still in development.\nAutodesk has been pretty up front about its development in this area, with remotely hosted sessions of AutoCAD and Revit through its Project Twitch technology. There was Project Dragonfly, which became Autodesk Homestyler \u2014 the first major cloud-based application from Autodesk for interior design (sold only in America at the moment), as well as: Project BlueStreak (online collaboration) and Project Photofly (photographs to 3D models). It is clear that there is a lot of research into cloud-based applications at Autodesk.\nAutodesk University 2010 was the first time that Autodesk brought this message to the main keynote sessions. Using the term \u2018Infinite Computing\u2019, company CEO Carl Bass and CTO Jeff Kowalski explained how this phase change in computing would benefit its customers.\nBy moving applications to the cloud, software would run on the most fast, fully equipped servers, running 128GB of RAM with banks of GPU graphics processors providing the ultimate in performance. When running computer-intensive tasks such as rendering, analysis or simulation, solutions would be delivered to the desktop in seconds as opposed to minutes or hours.\nThe technology even allows for multiple results to be delivered at the same time, perhaps giving a range of options to aid decision-making. The computer moves from being a documentation tool to truly optimising the design by providing more options that meet the stated requirements. There is a big future in this technology and every major CAD developer is trialling systems.\nThe interesting question in all this, is what happens to desktop machines and operating systems? If all you need is a computer that can run a browser to access all your software then what will future devices look like?\nFrom what I can see, the Apple iPad and Google Chrome machines indicate that we will be operating on low-power devices that have very simple operating systems. The upshot of cloud computing could well be the death of Microsoft Windows as we know it.\nAutoCAD WS\nThe second major cloud application that was released last year by Autodesk was AutoCAD WS, which runs on Apple iPads, iPhones and web browsers. Customers upload and share DWG files through the cloud, where AutoCAD WS can load them. This means you can have access to your drawings wherever you are in the world, and it is free.\nThe initial release of AutoCAD WS required that there was a constant connection to the Internet, which was less than ideal. In time for AU an update, 1.1 came out which cached files locally on the machine. This means you can take an iPad loaded with drawing and go onsite or to clients and interact with the drawings, which works really well.\nAutoCAD WS is also a proto-CAD system, in that it can edit and create some basic AutoCAD entities. At the moment this capability is embryonic but Autodesk is quite keen to see how far it can push this technology and ideally would like to have something like AutoCAD running on mobile devices such as an Apple iPad.\nThere are many other tablet PCs coming out this year and I understand that the way AutoCAD WS has been written means that it can be ported to any platform. I am assuming that is Google Android and Windows Mobile. So, soon there will be many platforms from which AutoCAD WS could be run. This is actually really good technology and should be evaluated. AEC will have a full review of AutoCAD WS in its March\/April edition.\nRevit\nHaving only just delivered the Subscription Advantage packs, many of the product demonstrations were concerned with the additional functionality that had been added.\nThe AEC keynote concerned itself with mainly demonstrating how Autodesk\u2019s products now share information seamlessly: using multiple AEC products to get a project done. The one little sneak I did notice was the version of Revit being used supported point-clouds natively \u2014 something which the current version cannot do. I suspect the next release will have this capability.\nPoint-cloud support was added to AutoCAD last year; it makes sense for Revit and will enable \u2018scan to BIM\u2019, which will ideally create a building model from laser scanned data.\nLabs explosion\nWith the decision a few years back to be more open about new technology, Autodesk has now made a wide range of tools available for download for architects and engineers to try out and give feedback on:\nProject Galileo\nlabs.autodesk.com\/utilities\/galileo\nBased on the next generation of Autodesk Land Xplorer, Project Galileo is a planning tool for creating 3D city models. The database can collate and overlay buildings, civil and geo-models, enabling users to sketch conceptual infrastructure ideas within a multi-kilometre model space. The technology provides conceptual visual renders of huge models and can be used to better understand the impact of major infrastructure projects.\nProject Neon\nlabs.autodesk.com\/technologies\/neon\nProject Neon is a rendering service that provides a lightning fast turn-around on delivering photorealistic images. While rendering is available on the desktop, Neon is powered by state-of-the-art 4-core servers. In the future, instead of waiting for a local render to complete, near instant results with higher quality and resolution will be available.\nProject Vasari\nlabs.autodesk.com\/utilities\/vasari\nNamed after the famous 16th Century architect and painter, Vasari is a stand-alone tool for creating building concepts with integration into analysis tools for energy and carbon footprint. The design can be easily imported into Revit to flesh out the details. The geometry modelling tools are parametric and are about performance-based design. The interface is fantastic and lets an architect or planner get valuable estimates on the suitability of a design concept.\nProject Photofly\nlabs.autodesk.com\/utilities\/photo_scene_editor\nPhotofly seemed like mad science and it still does.\nIt allows users to take pictures of items destined for\na 3D model with a basic camera. Upload the images to the cloud and in seconds you get a fully textured 3D DWG back.\nThe initial version was impressive, as it had to do a huge amount of mathematical equations. Autodesk has significantly improved the technology with new algorithms. The end results are a lot more predictable and this eventually could rival expensive laser-scan surveys for getting geometry into AutoCAD and Revit sessions. I highly recommend a play.\nConclusion\nAs usual, Autodesk University is one hell of a ride, with new products, new ideas and many new people with which to mingle. It is a great opportunity to meet the people who develop the tools you use and if you are very nice, you may get a heads up of what is in store.\nThe future is all about the cloud and Infinite Computing. How we access our tools will be only a small part of this. It is more about how much more \u2018grunt\u2019 we will have at our fingertips and how clever developers will be at utilising this in their products.\nIf our design tools are constantly running analysis on what we create, suggesting improvements or alternatives, at no loss of speed then what we produce will be \u2018greener\u2019 and have better quality that anything we could create with our currently processes.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/smartgeometry-conference\/","title":"SmartGeometry Conference","date":1207612800000,"text":"If you are into advanced form design in architecture, then you know what Generative Components is all about. Smart Geometry is the industry\u00dds leading event for advanced applications of computer technology. Martyn Day reports.\nOne of the Architectural highlights of the year is without doubt SmartGeometry. It\u00dds the one event where you can see totally cutting edge technologies applied to Architecture and structural design. Although these are, in the main, research projects, a handful of innovative thought leaders also present how they are using new methodologies to create some of the most radical buildings of our generation.\nThe founding fathers of the Smart Geometry organisation are Lars Hesselgren (KPF), Hugh Whitehead (Foster and Partners), J Parrish (Arup Sport) and Dr.Robert Aish, formerly of Bentley but now, somewhat shockingly, at Autodesk. Each event is a chance for the best students and most talented architects to come together for tuition on Bentley\u00dds parametric Generative Components technology, which was developed by Dr. Robert Aish and is based on MicroStation. Although this sounds Bentley-centric (and I guess it is), the conference element is highly non-vendor centric, and you\u00ddll see presentations on projects using technologies ranging from McNeel\u00dds Rhino to 5-axis Robots used to lay bricks!\nThe Autodesk shift\nThis was the first GC event to run since the \u00d9father\u00dd of GC, Dr.Robert Aish, jumped sides and moved to Autodesk at the start of the year. The move shocked Bentley and many wondered what the future of GC would be. Would it continue to be developed? What would Aish do at Autodesk? It appears Bentley took this event to make a very bold statement \u2013 that GC was alive and kicking and continuing to grow in usage, despite only leaving a prolonged beta state last year. If anything, Aish\u00dds departure has forced Bentley to change down a gear and ramp up the marketing and awareness of GC in the industry, and the company looks to be more committed.\nThere appear to be no hard feelings between Dr.Aish and Bentley, as Aish was warmly welcomed by the Smart Geometry directors, as well as Bentley\u00dds Huw Roberts, Global Market Director for building. Aish stayed for the duration, listening intently to all the presentations but could not be drawn into a discussion as to what he would develop at his new masters, Autodesk. It would be foolish to not introduce some conjecture on this particular point, as Autodesk hired him on the basis of what they saw with Generative Components. Parametric form generation is absolutely catching on at leading practices and Autodesk realises that it doesn\u00ddt have anything that can be sold as a solution. Products like Inventor, Maya and 3D Studio Max have been used with lots of hand-penned scripts but there\u00dds little for mere mortals to get stuck into. It\u00dds a fair assumption that Autodesk wants and needs some Form generation capability, this may be based on Maya or Max, as I have reports of \u00d9Aish sightings\u00dd at Autodesk\u00dds Toronto offices, which is where Alias used to reside. It appears that Aish is getting the scope of what technologies he has at his disposal for product development. For now, Bentley has to get as far ahead as it can with GC, before Autodesk shoves its millions of dollars into the market.\nSo, enough competitive talk, what was the event like?\nThe 2008 conference was held in Munich, at the incredible BMW Welt (BMW World) building. Itself an outstanding curvaceous form, the building is a BMW museum, a display area and a community space. Innovation continued into the theatre area where a rising back wall (entrance and exit) came out of the floor \u00b1 all very cool and a parametric environment in its own right! The sub title of the event was \u00d9Architecture in a Parametric Age\u00dd. And before I relay a few of the presentations, I have to say it was far and away the best Smart Geometry conference program I have seen and really depicted the realities of parametric modelling by the leading design firms, many on live or built projects. There was also a healthy dose of theory with the practical, as well as many research presentations leaving you thinking what on earth will happen next in this exciting area. Also of note was the pre-eminence of rapid prototyping models that were on display and the clear focus on post model fabrication, which indicated the traction that was now being forged between modelling and reality.\nThere were far too many presentations to go through in detail, so I will concentrate on ones that made the biggest impressions. There were presentations from the likes of: MIT, Zaha Hadid, KPF, Gimshaw, Ecotect and Foster and Partners, together with students from the previously held workshops.\n{mospagebreak}\nThe event kicked off with the SmartGeometry directors answering as a panel, some audience generated questions. It was clear that everybody has different ideas about how this technology will be adopted. To the question \u00d9will all architects become script writers,\u00dd Hesselgren (KPF) answered \u00d9almost certainly!\u00dd. Parrish (Arup Sport) felt that artistry would always take over from being a geek and in the future systems would require, hopefully, less scripting. Whitehead felt that the technology has a long way to go before it matures as a platform and that it will attract people from different design disciplines and while the current concentration is on building performance and sustainability, that\u00dds a natural progression for the technology\u00dds capabilities. Whitehead added that despite thoughts that parametric design in architecture would decrease team size and increase output, it\u00dds actually increasing the size of design teams due to its complexity and requirements for diverse skills in projects these days.\nTo the question on best practice of deployment of Generative Components, the answers from all the directors seemed to indicate that specialist modelling teams in architect\u00dds practices now appear to be the most common way to push this kind of technology into a practice. These teams produce their own tools to capture knowledge. But here there is a divide \u00b1 the knowledge resides in the architects that are 40 years old plus, where the technology skills are in the kids in (or leaving) universities today.\nAsked what still inspires them, Hugh Whitehead answered \u00d9Poetry in pragmatism\u00dd. Hesselgren liked to be around people who like what they do, while J. Parrish still got a buzz out of designing a stadium in a computer and having someone build the 1:1 model which he could go and sit in. The team wrapped up with a humorous reflective quote \u00fdThe older we get, the better we were\u00af.\nPatrik Schumacher, Zaha Hadid\nWhat can be said about the amazing designs of Zaha Hadid? Probably most would not be possible if it were not for the use of cutting edge technology. The practice is a heavy user of Rhino and a multitude of CAD applications, including Catia. Patrick Schumacher gave a visionary talk on Parametricism as an architectural style\u00dd. This takes the concept of using parametric form design from the production of a one off building and applying it to a cityscape. Should the practice be so lucky and influential, which it is, huge swathes of land could be dedicated to an architect\u00dds brief, allowing computational generative forms to be applied to standard geometry or skeletal systems to produce a cityscape of individual and unique buildings in response to applying formulas derived from any number of sources. Taking a lead from topology of the curvature of a nearby river could be used and applied to an array of buildings, which would morph and form in the computer to automatically respond to the operands that the architect chooses. This is amazing to watch and really takes parametrics to another dimension, although I have major reservations about what it would be like to live in such a generated building. It would certainly be a unique building and would fit in context by formula but I did find myself wondering \u00fdwhy??\u00af.\nHugh Whitehead, Foster and Partners\nWhitehead has the tool maker role at Fosters but this year went one step ahead and looked at the \u00d9Architect as the lawmaker\u00dd. Using generative tools, together with digital topology, Whitehead was looking at the concept of being able to \u00d9drop\u00dd a generative model of a building into a simulated environment, with the correct latitude and longitude, and having the building create the best form for the orientation. And all by taking into consideration the lighting, natural air conditioning, prevailing winds, envelope allowance, and louvres etc. Here the architect would have created the original form and its response to certain conditions, the environment, being the defining factor. It\u00dds a fantastic thought and we are probably not that far away from this. The architect would then be the aesthetic judge on the variations that the computations generated.\nGeorge Jeronimidis from Reading University\nIn an absolutely spellbinding presentation, Jeronimidis took us through the world of fibres as a building material. The spider is a great builder, working in 3D it\u00dds an architect and fabricator all in one. In today\u00dds buildings, joints are always a weak point and give rise to torsion issues and sometimes failures. So what can be done with fibres instead of point, lines, surfaces and solids? When you are fabricating with fibres the material is the structure. Is there anything wrong with knitting the Eiffel Tower? One only has to look at nature to see what can be done with fibres and nature always develops effective solutions. Using nature as an example Jeronimidis thinks that the future will see increased growth of fibrous composite designs that can be designed to intrinsically handle loads better, as the structure can be mapped to stress paths and carried deeper within the materials.\nWhile I am rapidly running out of space to relay much more of the presentations, Bentley has made the event an online event, visit www.smartgeometry2008.com, where streaming sessions will become available.\nConclusion\nMy final thoughts on the event are that it was truly inspirational and is certainly not to be missed. The one presentation that has been making me think was from a University presentation that was using a 5-axis robot to lay bricks. In itself not exactly a stunning use of a Robot but the team then started playing with the bricklaying, applying transforms to brick walls producing truly complex designs with different sized bricks, generating images, odd alignments and letting the robot just get on with it. The final designs were stunning. The only thing missing was mortar, which was kind of essential and every brick had to be placed by a human hand to the area where the robot could grab it. All these could be easily sorted I am sure with more funding. I think with this event it\u00dds clear that technology in architecture is dragging the industry from dumb 2D to something that we only expected in science fiction and it\u00dds almost so close you can touch it.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-zwcad-releases-parametric-design-tool-for-architects\/","title":"NEWS: ZWCAD releases parametric design tool for architects","date":1418860800000,"text":"DWG compatible software features associative plans and 3D models and parametric walls, doors and windows\nZWCAD Design, a Chinese developer of DWG-compatible CAD tools, has released an architecture-specific version of its ZWCAD+ software called ZWCAD Architecture 2015.\nFeatures include associative plans and 3D models, parametric building components including walls, doors and windows, roofs, room areas, and organized schedules. Users can also visualise designs within Google Earth.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nNEWS: VR aids on-site health and safety training\nNewforma launches Info Track\nHP Anyware comes out of beta\nLooq AI links to Trimble Business Center\nNavLive adds Control Points to handheld scanner\nOpenSpace to bring \u201cStreet View\u201d style documentation to construction\nBond Bryan Digital renames as Createmaster Information Management\nAI extracts knowledge from past projects\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-builds-more-automation-and-visibility-into-plangrid\/","title":"Autodesk builds more automation and visibility into PlanGrid","date":1562112000000,"text":"Advanced RFIs and Project Hub designed to boost project visibility and extend workflows to site\nAutodesk has added two new features to its PlanGrid platform with a view to giving construction teams greater visibility into a project\u2019s progress and unforeseen challenges.\nAdvanced RFIs is designed to automate the Request for Information (RFI) workflow to speed up the process.\nProject Hub provides an actionable picture of key project activity in one central location within PlanGrid\u2019s software.\n\u201cOn a typical project, we\u2019re managing vast quantities of information, and we need a way to make this information manageable and easily accessible,\u201d said Rob Winklepleck, General Manager, West Brothers Construction. \u201cIf my team doesn\u2019t have the insights we need, work stalls out and it can cause a domino effect on interrelated resources across the project.\n\u201cThe new features in PlanGrid not only help speed up the building process but also show me exactly what needs to happen to move the project forward. My team is able to get a quick response to pressing issues, and I spend less time calling or walking back and forth from the trailer to the field to verify work.\u201d\nPlanGrid\u2019s Advanced RFIs automate the RFI process by giving construction teams a visual and structured workflow to manage and distribute questions and answers efficiently and intuitively. Any project member can quickly draft a question from a log or sheet and attach photos, and then track RFI progress all from within PlanGrid. Reviewers are notified and can respond by email, and answers are immediately distributed to all critical team members. Responses are also automatically added to the RFI history within PlanGrid\u2019s platform to decrease miscommunication.\nProject Hub is a centralised place within PlanGrid\u2019s platform where project engineers and managers can get a snapshot of their project in real-time and take action on the most critical site operations. With an uncluttered interface it is designed to deliver a \u2018holistic picture\u2019 of all project activity, with a view to providing instant answers to questions such as, \u201cWhat has changed on the project? Does everyone have the most up-to-date information? What tasks need to be assigned or are still incomplete?\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-autodesk-invests-in-ai-tech\/","title":"News: Autodesk invests in AI tech","date":1502236800000,"text":"Smartvid.io technology can be trained to recognise AEC objects in video and stills\nBoston-based Artificial Intelligence (AI) developer Smartvid.io has closed its Series A funding round for $7 million, with money coming from Autodesk, Borealis Ventures, Castor Ventures, in addition to its original funders.\nAutodesk sees this as a strategic investment in machine learning within the AEC space and Smartvid.io is now integrating applications based on its \u2018VINNIE\u2019 engine into Autodesk BIM 360. Borealis Ventures also has a proven track record in AEC investment having been early investors in SketchUp, Revit and Flux.io.\nSmartvid.io was AEC Magazine\u2019s booth buddy at last year\u2019s Autodesk University and founder Josh Kanner gave us a very impressive demonstration of Smartvid.io\u2019s capabilities. Using speech and image recognition, the technology makes photos and videos searchable for AEC equipment, material and material defects \u2013 just about anything it\u2019s taught to identify. This can be applied to images, drone footage or recorded video.\nThe company\u2019s core focus is safety and its \u2018VINNIE\u2019 engine can be used to automatically tag people, hardhats, ladders and other things from uploaded videos and photos. Once these items have been processed with tags, photos and videos can be searched for items and people.\nIn another example, video was shot going through a concrete lined tunnel. The AI identified the cracks and, after being trained by experts over a number of iterations to grade the cracks on a scale of severity, Smartvid.io was able to automate the process of finding, analysing and identifying concrete cracks which needed urgent remedial work. There was no laser scanning; it was all identified from a Go Pro video.\nOn the investment by Autodesk, Kanner commented, \u201cThe best thing about Autodesk is how in tune they are with our mission of integrated data. Our partnership with them in no way limits \u2013 and in fact expands \u2013 our capacity to build integrated solutions with all our partners, both new and existing. Autodesk fully understands how important \u201cdata neutrality\u201d is, not just to our products\u2019 success, but as a business imperative to the ever-increasingly interconnected industrial world of tomorrow.\u201d\nThe team behind Smartvid.io were the original creators of Vela systems, which was acquired by Autodesk to provide the core technologies which emerged as BIM 360 Field.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/imagina-2010-preview\/","title":"Imagina 2010 Preview","date":1264377600000,"text":"Fancy a gamble on the design visualisation industry? Discover what could be a sure bet at this year\u2019s Imagina in Monte Carlo, writes Stephen Holmes.\nImagina 2010, being held at the Grimaldi Forum, Monaco from 3\u20135 February, promises to be one of the largest European visualisation and simulation conferences of the year.\nWith the use of 3D technology in architecture, visualisation and landscaping as the backbone of the occasion, Imagina is shaping to be a key opportunity to assess how the industry is moving forward through the application of new techniques and technology.\nThis year\u2019s keynote speech is to be given by leading French architect and urban designer Jean Nouvel, who was awarded the prestigious Pritzker Prize \u2014 regarded by many as architecture\u2019s highest honour \u2014 in 2008. Having worked on more than 200 projects around the world, he is best known for the Arab World Institute, Paris; the bullet-shaped Torre Agbar in Barcelona and the Guthrie Theatre in Minneapolis.\nFor the second year running, Imagina 2010 will play host to a specialised Digital City and Territory Village for local authorities, towns, local governments and regions.\n\u201cWith the use of 3D technology in architecture, visualisation and landscaping as the backbone of the occasion, Imagina is shaping to be a key opportunity to assess how the industry is moving forward through the application of new techniques and technology.\u201d\nEach is invited to an allocated stand to present their territory management initiatives using a 3D mock-up, explaining its applications and demonstrating how computer-generated imagery made with geo-referenced 3D data can be used in communications and public consultation.\nThe Imagina exhibition floor will be bigger than ever. There is a strong French contingent among the exhibitors, with the likes of Dassault Systemes and The French National Geographic Institute (IGN) entertaining visitors. Meanwhile, international giants such as Nvidia and Wacom will be cast alongside interesting smaller companies. Below are six exhibitors that are worth a visit:\n3DTV solutions\n3DTV solutions creates, develops and commercialises software and hardware systems dedicated to glasses-free 3D visualisation. 3DTV Solutions has designed a complete range of products for audiovisual, multimedia and advertising professionals: A camera and editing software and a plug-in for 3D software, such as Autodesk 3ds Max.\nBionatics\nBionatics is a French software publisher specialising in real time 3D modelling and visualisation tools for large territories and their evolution over time. The company presents at Imagina version two of its flagship software LandSim3D for sustainable city development and countryside conservation.\nEnodo\nPreviously known as IMAGTP, Enodo adapt hi-end video-game technology (Cryengine) to industrial needs. Its has expertise in virtual reality, providing cutting-edge solutions for real time and interactive manipulation of industrial data. Whether for industrial simulation, urban planning, transportation or serious games, the French company\u2019s team stretch 3D graphics and add artificial intelligence to individual projects.\nNOOMEO\nAn innovative company that develops a new generation of ultra-portable 3D scanners dedicated to industrial applications. The solution, OptiNum, is based on a photographic technology that allows total autonomy of movement.\nSpaceyes\nSpaceyes provide a third-party real time fly-through and model making tool, compatible with most GIS, CAD and 3D software on the market.\nVertice\nVertice will be demonstrating its Nova product, a standalone real time rendering solution, which also works directly with 3ds Max and Revit. Also on show will be Nova Server, a cloud based solution which offers real time navigation of 3D scenes in a web-browser without the need of a plug in.\nAfter the exhibition closes guests are treated to a shimmer of Monte Carlo glamour at the annual Imagina Awards, recognising and rewarding the very best use of 3D modelling, simulation and visualisation applied to a particular sector. This year\u2019s categories include Architecture and Urbanism, featuring works such as the new MediaCity UK by Andrew Li Creative; and the Landscape and Territory category, with entrants including Glasgow City Council for their urban model of the city.\nShould any of this year\u2019s entrants leave empty handed, they can always try their luck later on in the city\u2019s famous casino.\nLaunch of the ethical 3D design charter\nUsed in territory management, 3D data allows realistic digital mock-ups to be created. Communication, public consultation and projects are indispensable to governmental bodies. As such a shared \u2018operating manual\u2019 defining the context, aims and criteria for the use and representation of 3D data was felt to be important resulting in a project for an ethical charter.\nA definitive document will be finalised this autumn and various parties involved in the project have been asked to sign the charter at the Imagina 2010 event in Monaco.\nThe charter is to be used as a universal reference document that establishes recognised and shared principles, giving three-dimensional representations the benefit of a long-term guarantee of quality and legitimacy.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/transformation-engineering\/","title":"Transformation engineering","date":1145577600000,"text":"Chuck Hoberman is an unusual mix of inventor, artist and mechanical engineer. He has become internationally renowned for his innovative folding structures, art installations, medical tools and toys.\nEarlier this year, I attended the SmartGeometry Forum in London. This group of predominantly architectural designers and computer technologists have formed a research group that explores new ways to design and interact with CAD models based on parametric design principles. The technology being explored here is applicable to all form-based design and one speaker in particular had a very interesting multi-disciplinary approach to introducing mechanisms to structural design.\nChuck Hoberman is a New York-based inventor, artist and mechanical engineer. He designs products, toys, art installations and structures that all offer, what he terms \u2018transformability\u2019. Hoberman\u2019s designs change shape, size and form, such as a geodesic sphere structure that folds into a 16 inch space, or something the size of a hatbox that deploys as a 5ft tall, two person tent. It\u2019s not just about space saving design, it\u2019s the ability for a design to move and change states, perhaps adapting to climactic conditions or morph through a number of positions as an art installation. The images here give a taste for the kinds of things Hoberman creates but as they all move in some way, a visit to the company website and a look at some of the videos is well worth the time (www.hoberman.com).\nOn examination, the designs all use an exquisite understanding of geometry, together with standard and unique mechanical components and manufacturing techniques. Hoberman holds both Art and Science degrees and even worked at a Robotics company prior to setting up his design studio to apply his fascination with moving mechanisms within many fields. Hoberman Associates has worked on projects covering military applications, home and office products to stadium and museum installations \u2013 in fact wherever Hoberman sees his theories and many patents on transformable designs can be applied. Talking at the SmartGeometry forum, it\u2019s obvious that he sees many applications of his principles to modern architectural design, providing retractable coverings and adaptable fa?ades \u2013 perhaps building on the concept of the building as a machine for living in, adapting and changing to the environment or usage. It\u2019s also typical of today\u2019s \u2018signature\u2019 architects, like Sir Norman Foster and Lord Richard Rogers, pushing structures of steel and glass to their limits, only made viable from using tools such as Catia to drive the manufacturing process and make technically complex, custom buildings financially possible.\nI caught up with Chuck Hoberman on the phone in his New York workshop to find out more about he designs and manufactures his transformable structures.\nYou have qualifications spanning art and science, together with applied experience of both. What interests you about design?\nChuck Hoberman: The starting point of my whole career was looking at this transformability from the most general perspective. What I was interested in from the onset was working at a systems level, that is to say, working with the fundamental geometry and really trying to elucidate mechanical and structural principles, where all these areas come together. It\u2019s really an outcome of my education which is in the arts and sciences. It\u2019s an inherently multi-disciplinary role.\nI didn\u2019t start out as, let\u2019s say, an expert in tent design, looking to improve the design of tents. It was more from a general design perspective and I\u2019ve been going from general to specific application through the course of my career. I think one would come up with a different solution to the problem if I were to approach the design from a specialist point of view.\nI guess you have come up with a range of ways to transform structures through the years. Is this knowledge always the starting point for your designs?\nCH: It\u2019s a combination of these principles that form part of the 18 or so patents pending that I have. It\u2019s a library of geometric techniques. Certainly when a client comes to us for a design, I dig into those techniques. On the other hand it\u2019s also about a highly rationalised, engineering-based approach but at the same time open to new creative directions and inspiration. In many cases while my team and I are always drawing on the core principles that we share, often when a client comes to us with a problem, we only have part of the solution at hand, so we have to develop something new. It\u2019s a combination of the two.\nTypically, what kind of customer comes to you for a \u2018Transformable\u2019 solution?\nCH: The critical factor is that there is a need and enthusiasm and a requirement that our technology can address. In the case of the rapidly deployable military tent, the requirements were very stringent but we felt from the onset of that project we could meet those requirements and the result is now a product on the market has raised the bar.\nYour solutions are unique. How do you work with clients and manufacturers?\nCH: It\u2019s a process that parallels other types of design. We set up a team with a client, a small group to really understand the aspects of the brief in-depth. We then go back and work with my team, develop two or three preliminary approaches to the problem and then the challenge for us is to find the intersection between the brief, the programmatic requirements, the pools of solutions and technology that we already have to bring. We will usually develop lots of tools to convey our ideas, which could be some rapid prototypes, CAD designs\/models, renderings, or flythroughs.\nWe are small company and we are very hands on. Our approach is very grounded; we get a lot of mileage out of standard software, and standard techniques. Once we have what all sides consider is a suitable direction then we will always play a significant role in subsequent design and engineering of the product. Primarily we will handle the engineering from a design and mechanical standpoint and from a structural view we will work with one of the many structural engineering firms out there, like Buro Happold, Ove Arup or local firms in and around New York.\nWe have a number of fabrication houses that we work with, some are large, some are small. Very often they are firms working for the Aerospace industry or maybe theatrical, working on theme parks.\nYou mention working with Aerospace manufacturers, is that due to tolerancing of your designs?\nCH: From a construction point we are working with very close tolerances. In the manufacturing world, many fabricators are used to working to the tolerances we need. There are process issues and there is an equilibrium to be found with the tolerance and scale of the project. Very often if you are building large structures you have to be aware of the economic costs of over-tolerancing a design.\nWe mainly get our projects machined, with some casting. Our toys are injection moulded, of course. Sometimes we can use surprising low-tech solutions for fabrication. For the piece we did for the Utah Winter Olympics, we needed a big arch to support our mechanical curtain and that was a series of large welded trapezoidal frames that bolted together. These had slides on them, which had to be oriented carefully to keep the mechanism aligned. The actual layout was done on the factory floor, they laid out a grid, 4ft by 8ft, which is the size of sheet of plywood. They took these sheets of plywood onto the CNC machine and etched the outlines of these trapezoidal shapes and laid them out across the grid, so there was a big one to one image of this arch on the floor (around 80ft long). They used this as their template and it gave perfectly adequate tolerancing for what we were looking for and the cost was minimal. Our fabricator came up with that, it was a smart practical solution.\nMany of your designs use hinges and joints, are these bespoke or standard parts?\nCH: We really don\u2019t need to develop components; most have been pre-manufactured for us. It\u2019s all out there \u2013 they aren\u2019t all that \u2018standard\u2019 but they are out there. I am happy that we don\u2019t need to innovate at the detail level! We are about the global integration of all these components, at a systems level, to get something that performs.\nYou use Autodesk Inventor as your modelling tool. How do you use 3D CAD?\nCH: I\u2019m not an advanced CAD user but I have been using CAD for 20 years and when I started designing my structures I had no access to any 3D software at all, so I just wrote code to create the geometric representation, as well as render the output. This was on a Macintosh. I had to do the transforms to project the geometry onto a picture plane and come up with my own shading and hidden line algorithms. It was ridiculous, but it was out of necessity. If I wanted to show someone a picture, I\u2019d print this stuff out on a dot matrix printed and tile the plots out! But it was a good experience as I got to know what all these tools are about.\nThe use of Inventor is a legacy as we grew up with AutoCAD \u2013 it\u2019s a natural follow on. We take advantage of the parametric capabilities at a component level, which means we can adapt the design easily and most of our structures use families of parts, or parts that have similar features but are geometrically different, so that\u2019s useful.\nWe build assemblies with mechanical constraints so we can simulate the kinematics, the mechanical action of our structures. However, we don\u2019t include the forces or do force analysis in Inventor. Instead we do a fair amount of paper, pencil and spreadsheet calculations!\nAs the project is being finalised, that\u2019s when we go outside to a structural house and generally speaking, even then, they are not doing a full-motion dynamic calculation, they will do a static analysis. If, for instance, we have a cable driven structure, they will pin the structure into one position, by \u2018fixing\u2019 the cables, then run the analysis, get the forces and repeat that in a series of positions. It\u2019s actually going into a fairly standard analysis package from their point of view.\nInternally, we have a prototyping shop where we will use laser cutting, STL, machine parts, any technique that\u2019s useful for a given project. To be honest on the software side of what we do, we tend to stay with fairly standard packages.\nDo you send 3D CAD data to the fabricators or 2D drawings?\nCH: It still depends on the fabricator but things have certainly improved over the last five or six years. We can hand them the 3D file and most have the software to generate all the tool paths. I am of the mind that if we understand the manufacturing technique we can have a meaningful dialogue with manufacture and work out how to do it economically.\nIt\u2019s impressive to see that there is a strong intent to unify Autodesk\u2019s wide variety of solutions and have all these huge development teams work within a well defined standard framework.\nWe don\u2019t just do the mechanisms, we also produce the electrical wiring controls, and the flow chart for integrated programming for lighting and motion. I am a mechanical engineer, I worked for six years in industrial robotics. There is so much of what we do that is similar to that industry, we are working with the same firms that do motion controls systems, robotics systems but putting it into a different context.\nWhere do you see your Transformable designs being used in an Architectural context?\nCH: There\u2019s a spectrum of architectural possibilities; a quick rapid set-up and modular approach, a fixed installation that offers variability and movement of the building over time, or something structural, like the roof may open and close. The process that these ideas share is the fundamental way of looking at changeability and movement. It\u2019s a very complex equation with design, functional, and economic issues to be assessed. The role that I am trying to play is really to seed the discussion offering new practical, technical solutions and also just trying to spark people to think about design in a broader way.\nSmartGeometry appears to be about complex form and shape development. What is it that is applicable to transformability.\nCH: The reason we are interested in SmartGeometry is to push parametric design to a much earlier part of the design process. So, instead of using our specialised techniques for form finding and creating geometry, we can automate that process and hopefully get to design solutions a lot quicker and in an easier way.\nI\u2019ve been aware of smart geometry for about a year, from some of the engineering firms here in New York. There could be enormous value for us, as we work with highly defined rule-base systems for developing designs. Since we know our rules really well, they may not be that simple and have many steps but it could be possible to develop algorithms for these. If what you can do is find a free form for a building shape by easily adjusting that form, and have the computer take care of the geometry and connections, then our approach is a poster child for that. We have been doing that in a more low-tech way because that\u2019s where the current technology is at.\nSmartGeometry is a different mindset for design. If you are starting with an algorithmic approach to design, where the first thing you set up are the rules, then you are in a different place than a paper and pencil sketch. I think it\u2019s a matter of time, until people become accustomed to the techniques. The other area where this whole highly-parametric approach interfaces with the transformability of designs, is the time component. When we design a structure, it has an infinite number of states, which in a sense in just one more parameter, but you can bring that transformable shape parameter into direct relations with any other design parameter that you may be looking at. It\u2019s a powerful idea for us. For instance, if you see people working in these parametric programs, they generate a field of variable design possibilities. Within each of these possibilities there\u2019s this time component, where any one of those designs can be set into motion, going through all its changes in configuration. What this does, is takes what is inherently complex and breaks it into a very rich experiential design process giving many different possibilities and automatically optimising for all the different constraints in the project. Everyone here in the office is intellectually engaged with the concept.\nSo do you think this highly-parametric and algorithmic approach will impact your designs?\nCH: When I present to designers, they say \u2018Your stuff looks pretty symmetrical. What can you do that\u2019s asymmetric, that has more complexity or more interesting forms?\u201d I think certainly, there would be a benefit from deploying these kinds of extremely parametric systems. Our transformable design techniques are adaptable to asymmetric systems but the time to design is higher and there\u2019s a cost penalty for going in that direction. However as designers our job is to respond to the vision of the architect. Transformable design is not about a particular look or feel or design, it\u2019s a flexible design approach that will fit with many types of designs. If the architect is looking for blobby, we can do blobby, if the architect is looking for something curved, our techniques are adaptable and that\u2019s what interests me.\nSo what do you hope for transformable design in the future?\nCH: With regard to Hoberman as an entity, we are technologists and technology holders. The future is to move it more towards a design tool, approach and philosophy that is accessible for advanced designers and forward looking designers. I\u2019d like it to have a much broader base.","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/origin-gis-wins-award-at-2002-esri-business-partner-conference\/","title":"Origin GIS Wins Award at 2002 ESRI Business Partner Conference","date":1027123200000,"text":"ESRI, a provider of GIS software, and Power Delivery Associate has announced that Origin GIS was awarded the distinction of first place in the category of Best Productivity Enhancement Solution in the ESRI ArcGIS Challenge Contest.\nThe ArcGIS Challenge Contest was designed to highlight ESRI business partners who have developed solution products based on ArcGIS, ESRI\u2019s flagship GIS software. The contest was judged at the 2002 ESRI International Business Partner Conference held in Palm Springs, California, February 24-26, 2002.\nOrigin GIS is a MultiSpeak Compliant enterprise solution for electric utilities built on ESRI\u2019s ArcGIS 8.1 technology. Used by electric utilities throughout the country, Origin GIS provides a user-friendly interface within ArcGIS for the creation and maintenance of electrical facilities data. Origin GIS enables utilities to quickly and accurately create a detailed electrical connectivity model, which permits tracing within the ESRI environment and seamless data exchange with engineering analysis, outage management system, customer information system, and work order automation software systems.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-bimstore-launches-augmented-reality-app\/","title":"NEWS: Bimstore launches augmented reality (AR) application","date":1422489600000,"text":"bimstore Eye pitched as a fun, new way for specifiers to visualise construction products\nbimstore Eye is a new augmented reality app from BIM content provider BIMstore. The iPhone and iPad app is designed to help specifiers explore products for use in BIM models \u2013 simply point the camera at a manufacturer\u2019s brochure image or catalogue where the BIMstore Eye logo is displayed and a 3D representation appears on screen. The model can then be viewed in various orientations and sizes, including to the actual product scale.\nNot all bimstore products currently work with the app but BIMstore says that more will be added every day.\n\u201cbimstore Eye is a fun, new way for specifiers to visualise construction products,\u201d states bimstore founder Adam Ward. \u201cBecause bimstore Eye is part of our growing ecosystem, we are hoping it will help manufacturers and their sales teams show off their BIM content, whilst specifiers can get a first look at their downloads.\u201d\nbimstore has provided a number of example brochures for users to test out. They can be downloaded here.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nNew service for custom 3D-printed architectural products\nTurboCAD 14\nSpaces V2 launches with Pro subscription offering\nNEWS: LFM adds intelligence to laser scans and improves access via the cloud\nRhino 8 boosts architectural modelling\nNasuni helps smooth transition to remote working during lockdown\nNavVis IndoorViewer integrates with Construction Cloud\nWorkstations and the future of AI in AEC\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/anchorage-reducing-wildfire-risk-with-remote-sensing\/","title":"Anchorage reducing wildfire risk with Remote Sensing","date":1058227200000,"text":"Alaska, the 49th state to join the Union, is known for its picturesque landscapes, pristine bodies of water and abundant wildlife. What isn\u2019t widely known is that over the last few years, insects less than one-fourth inch aggressively were wiping out spruce trees in Anchorage, Alaska, USA. The spruce bark beetle infestation on Anchorage\u2019s wildlands left over 85,000 acres of dry, rotted, forestland in its wake, creating a major wildfire threat. This threat resulted in the Municipality of Anchorage (MOA) being named a community at risk by the USDA Forest Service in 2001; and prompted the Municipality to take action to reduce its wildfire risk using remote sensing technology. The Wildfire Mitigation Project became a collaborative effort between the MOA\u2019s Data\/GIS Resources Division, Anchorage Fire Department, Alaska Department of Natural Resources (DNR) Division of Forestry, Bureau of Land Management (BLM), Anchorage Soil and Water Conservation District (ASWCD) and the University of Alaska Anchorage. The Municipality contracted IGIS Technologies (IGIST), Inc. (San Diego, Calif., USA), a GPS, GIS and remote sensing solutions company, to achieve the following objectives: assess the forest fuel structure in Anchorage\u2019s wildland\/urban interface areas; develop predictive fire behavior models; and assess wildfire risks.\nIGIST created a landcover map and fire hazard map of the wildland\/urban interface around Anchorage. Using IMAGINE Professional V8.5, IGIST ran an unsupervised classification on a Landsat TM image of Anchorage taken in 2000. Forestry inventory labels were determined using data from multiple resolutions, including: one-meter aerial photos, 4-meter IKONOS, 50-meter USGS landcover, and GPS transects adjacent to the classified imagery. The classified imagery was used as the central component in creating a spatial model to determine fire hazard. In conjunction with the MOA, Department of Natural Resources (DNR), and the Anchorage Fire Department, IGIST created a spatial model that consisted of seven key inputs:\nThese inputs were combined within the IMAGINE Spatial Modeler. The final output ranked each class with numerical weights that related to fire risk level.\nFor example, if an infested area is close to a water source, it was given a low fire risk weight, and if the area is far away from water, it was given a larger weight, meaning that area has a moderate-to-high fire risk.\nThe DNR Division of Forestry combined land cover layers with FARSITE, a GIS-based program that simulates growth and fire behavior of fires as they spread through different terrain under changing weather conditions.\nVegetation layers were incorporated into FARSITE and the results were converted into fuel models. (Fuels include grass, shrub or timber that escalate the threat of wildfire.) Analysts are manipulating and analyzing the classification process to help develop custom fire behavior models that best represent forestland conditions in Anchorage. This will be an ongoing process as better imagery and data is obtained and forestland conditions change. The MOA will continue to obtain better images and data over the years, as forestland conditions will continue changing over time. The steps of this project will be repeated to ensure the Municipality has the most up-to-date information needed to minimize wildfire threat to human life, property and natural resources. This project demonstrates how Anchorage is successfully combining GIS software and procedures with effective firefighting policies. The MOA now has the information and guidelines needed to improve forest health management, pinpoint wildfire risk hazards and keep the public informed.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/the-revolution-is-here\/","title":"The revolution is here","date":1563840000000,"text":"Our 3rd annual exploration of the future of design and construction technology took place last month in London, examining a broad array of disruptive ideas and technologies that are set to have a huge impact on our industry\nOver 17 years ago, AEC Magazine was our response to the potential of Building Information Modelling (BIM) to move the industry forward to 3D and a model-centric approach to defining buildings.\nAEC Magazine\u2019s sister publication, DEVELOP3D covers the manufacturing and product design sector, one that moved from 2D drawings to 3D models and digital fabrication twenty years ahead of the AEC industry.\nWith BIM-capable tools commonly owned by AEC firms (although not necessarily deployed in an exemplary BIM process) AEC Magazine is now examining the next set of technologies and processes which will disrupt our industry. The key topic areas include robotics, digital fabrication, offsite production, modular approaches to design, robots, artificial intelligence (AI), augmented \/ virtual reality (AR\/VR), real-time photorealism and computational design.\nOn the evening before NXT BLD 2019, I sat down to write my introduction to the conference. I was going to open by making some predictions as to what will happen in the future of the AEC industry, then realised that the speakers that would come directly after me were already delivering those things \u2013 robots, automated production of buildings, etc. In Nostradamus comparisons, being able to see five minutes into the future didn\u2019t seem like much of a vision to boast about!\nThe robots are here\nOur keynote talk came from Boston Dynamics, who had flown over especially for the event, bringing two four-legged Spot Mini robots with them.\nBoston Dynamics sees its roving robot as a sentinel for on-site construction and is about to launch its first commercial product into the construction space. The robot can work autonomously or be operated by remote control. Hopefully you\u2019ve all seen videos online of Spot Mini walking around construction sites and even dancing to uptown funk!\nNothing really prepares you for the first time you meet Spot. It truly is an incredible piece of hardware. Michael Perry from Boston Dynamics was only 10 minutes into his presentation when Spot walked into the auditorium, up the stairs and on to stage. At that point the entire audience had their phones out to video this incredible work of engineering art.\nThe articulated arm on top of the robot, features a grabber and a camera, and the body behind has plenty of space to mount sensor devices. In fact, while at the show, Faro and Topcon took full advantage of access to Spot Mini to do some live tests in the exhibition area, where we had the first laser scans executed on specially designed 3D printed mounts. We\u2019re convinced that Spot Mini will be a big hit in the world of reality capture, enabling the automated collection of scanned data for surveying and 4D analysis.\nMelike Alt\u0131n\u0131s\u0131k is a fantastic architect who creates beautiful flowing designs and runs her own practice, MAA, in Istanbul. Formerly at Zaha Hadid Architects, you can really see the influence in the curved and innovative forms which is typical of the work which MAA produces. A keen user of advanced technology, Alt\u0131n\u0131\u00d0\u0131k has been innovating in her approach to construction and assembly.\nHer work came to our attention when she won the Seoul Robot Science Museum competition and has designed a building for robots, to be assembled by robots. Another of MAA\u2019s iconic projects will be visible from everywhere on Istanbul\u2019s skyline, the futuristic 369m metre-tall telecommunication tower, which utilised revolutionary construction methodology.\nVisual edge\nJust prior to NXT BLD, Epic Games announced that it had acquired TwinMotion (which is powered by Unreal Engine) and that it would be giving the architect friendly real-time viz tool away for free until November (ArchiCAD customers get even a better deal).\nEpic Games\u2019 Ken Pimentel talked on this and how the development work in the popular video game Fortnite is driving the development of Unreal, for it to handle city-scale models and collaborative technologies. Unreal does offer one of the most realistic shaders out there and we are edging ever closer to their promise of real-time photorealism which will totally change the architectural visualisation market. Ken\u2019s presentation gave delegates a great taste of what to expect in our dedicated viz and VR stream which ran in parallel to the main stage.\nAdvanced fabrication\nJust weeks prior to the event I came across a fascinating article on a designer who had come up with a way of folding metal for construction by using origami techniques. After a brief conversation, Tal Friedman was initially just going to give a short 10 minute demo of his technology but we were fortunate to be able to give him a full 20 minute slot where he looked at generative building optimisation, complex fa\u00e7ade design and these incredible self-standing and optimised origami metal forms.\nExtra-terrestrial architects\nXavier De Kestelier of Hassell made the case for the practice of architectural design in the design of a future Mars-based habitat. Looking at utilising technologies such as robots and 3D printing, the design team came up with a whole process of a flock of robots constructing a 3D printed dome, from Martian soil. The idea is that the dome will shield an inflatable habitat from cosmic radiation, which gets through the atmosphere due to the weak magnetic field of Mars. This presentation really was a tour de force in architectural design story telling.\nThe new Bauhaus\nETH Zurich is the Bauhaus of the 21st Century. The University is incredibly well funded and has a laser focus on experimenting with digital fabrication and innovative ways to make buildings.\nLast year we had Stefana Parascho and Andrei Jipa talking robots and concrete. This year Mariana Popescu of the Block Research Group talked about her revolutionary approach to using knitted forms to create in-place formwork for concrete. This new process, called Knitcrete, combines digital fabrication, computation and structural design with one of mankind\u2019s oldest fabric creation processes. Popescu gave deep insight into the CNC knitting technique, its capabilities and a project completed with Zaha Hadid Architects at Museo Universitario Arte Contempor\u00e1neo (MUAC) in Mexico City.\nDesign inside VR\nAt our first ever NXT BLD, we had a young architect from Holland called Johan Hanegraaf from Mecanoo Architects, who presented the work he\u2019d done in the evenings programming a Unity-based VR environment. This was for architectural modelling inside VR, as opposed to using VR as a just a viewing tool.\nNow two years later he has teamed up with Icelandic developer and entrepreneur Hilmar Gunnarsson and at NXT BLD we got an exclusive viewing of Arkio, a new VR tool for architects specifically designed to support collaborative modelling. It runs on a desktop or a tablet and you just need one of the new low-cost headsets to get going in VR. The demo was exhilarating and allowed two designers to simultaneously work on a single model, generating geometry at a building or city scale.\nDigital fabrication\nFor the afternoon, we were graced with Richard Harpham from Katerra, a US firm widely tipped to become the \u2018Tesla\u2019 of building companies. Starting at zero, within four years the company is now worth many billions, employs thousands and has an order book like no other house builder.\nKaterra is driven to use digital fabrication methodology to produce buildings as components, shipped to site and is leading the charge in CLT (Cross Laminated Timber) buildings in countries that are adopting this material. Katerra has a range of configurable buildings, from apartments to eight storey office blocks which can be ordered online and delivered. London South Bank University is host to DARLAB, an advanced architectural fabrication research institute, headed up by Federico Rossi, that explores both subtractive and additive processes, combined with large-scale industrial robots. The project work of students and academics was shown at its full force through Rossi\u2019s varied presentation. The algorithmic statue work for the World Wide Expo which went on for three months non-stop was incredible.\nFlorian Frank works at international architectural practice Herzog & De Meuron in the Digital Technology (DT) department, which develops and investigates new creative software tools and processes for CNC, visualisation, VR, BIM, Rhino\/Revit\/CAD and coding, providing solutions for the architectural groups.\nFrank gave us practical examples as to how the DT group supports design 3D model development and we saw how physical models still play an important role within all the design teams. As he is a natural experimenter, he developed a 5-axis parametric environment in Grasshopper to generate milling strategies to bypass the old unintuitive CNC software to make fabrication less stressful.\nOur final speaker was a special guest who literally flew in from New York for the day. Marc Fornes and his practice \u2018THEVERYMANY\u2019 have been using computational design and digital fabrication to make extremely thin, complex, self-supporting forms. The design team appears to be constantly experimenting with form generation, pushing the limits of the current technology while creating their own vocabulary and design aesthetic. A must see for all geometry explorers!\nConclusion\nFor those that missed the event, I think you get the gist that there was an amazing array of inspirational presentations. For an industry that is much maligned for being technologically backward, or lacking investment in technology, NXT BLD proves that this is no longer the case. In many respects, the level of investment and experimentation has become infectious, with many firms now evaluating digital fabrication and off site workflows. While generating complex forms and using the new methods of production may seem highly risky, the use of computational methods helps minimise that risk and even improve the quality. In many ways, while SketchUp and BIM adjusted the course from 2D to 3D modelling, the key deliverable is still a set of 2D drawings. The next step for the industry will be to embrace digital fabrication as the core output. You can rely on NXT BLD 2020 (London, 9 June 2020) to bring you the best of these early adopters and the latest in research.\nReal-time ray tracing comes of age on the design viz and VR stage\nIn less than a year Nvidia RTX has gone from a demonstration technology to one that is being used to great benefit on real-life projects. NXT BLD\u2019s dedicated design viz and VR conference stream provided the perfect platform for digital artists and architects to share their experiences of real-time ray tracing and for the technology developers to give a sneak preview of where things are headed in the future.\n<\nCarlos Cristerna of US marketing agency Neoscape dived straight in with a live demonstration of real-time ray tracing in Unreal Engine. Cristerna showed how the technology was currently being pushed to the limits on a live New York skyscraper project that had originally been configured for offline rendering, creating stills and animations with 3ds Max and V-Ray.\nDoing a live demo comes with its risks but Cristerna perfectly illustrated just how fast an RTX-enabled viz workflow can be, toggling on reflections, refractions and global illumination on the fly with near instant results. Equally impressive was the fact that he was doing everything on a mobile workstation. The real time ray tracing demos that we have seen in the past have been multi GPU on powerful desktop workstations. Cristerna was using a 15-inch mobile workstation, Lenovo\u2019s ThinkPad P53 with Quadro RTX 5000 graphics, which we review here.\n\u201cIn a matter of minutes, you can do as many renderings as you want. If you have a presentation for a very important client tomorrow, you can put together a pretty decent presentation,\u201d he said.\nCobus Bothma, applied research director at Kohn Pedersen Fox (KPF), gave an architect\u2019s view of rapid visualisation and how it is being used to accelerate design decisions. \u201cWe don\u2019t have a specialised visualisation team inside our office. There\u2019s no one to go to, to say \u2018can you render this stuff for us,\u201d he said. \u201cWe train our designers to render, to compute, to use BIM etc\u2026 they all get involved in communicating their design and visualisation.\u201d\nBothma\u2019s presentation gave a fascinating insight into KPF\u2019s visual design process using a broad set of tools including V-Ray (Project Lavina), Enscape, Unreal Engine and Nvidia Holodeck. This wasn\u2019t about generating glossy marketing images; it was about using visualisation for communication and to influence the design process.\nBothma explained that it\u2019s the workflow that leads up to render process that is most important; the rendering takes care of itself through hardware and software. For the biggest scenes, he explained, these can be pushed down to clusters or bigger scale GPU machines, so precious time doesn\u2019t have to be spent optimising models.\nBothma made the very important point that in design, real-time is relative. \u201cWe don\u2019t need to run 11ms a frame, we can actually wait a little bit for it. If we wait half a second, it\u2019s OK. If we run ten frames a second, it\u2019s also OK. It\u2019s about communicating exactly what you need to communicate at the right time.\u201d\nNvidia launched RTX last year but it\u2019s taken a while for the technology to appear in commercial applications. Much of the focus has been on Epic Games and Unreal Engine but Enscape and Chaos Group, which have a longer history in the AEC sector, are set to bring their \u2018real-time\u2019 ray tracing, RTX-enabled applications to market soon. Presentations from both firms gave NXT BLD delegates a chance to see where things are heading.\nMoritz Luck, co-founder of Enscape showed how RTX technology is currently being implemented into its push button viz tool. Architects love Enscape because it is so easy to use. In a live demo, Luck showed how this ethos will continue with the RTX-enabled version of Enscape, by enhancing the visual quality of a complex scene at the push of a button with near instant ray tracing.\nSimeon Balabanov from Chaos Group showed the latest developments in V-Ray for Unreal \u2013 specifically, how users of the popular photorealistic arch viz renderer can harness their V-Ray workflow for the real time engine, or simply get high-quality ray traced imagery out of Unreal. Balabanov also gave a glimpse of Project Lavina, its new ray traced rendering engine for real time previews that takes advantage of Nvidia RTX GPUs and their ray tracing cores.\nIf delegates were left wondering what the magic was behind these advanced viz workflows, Nvidia\u2019s Sandeep Gupte had the answers. As part of a broad presentation on the expanding role of GPUs in the AEC sector, Gupte explained how RTX works and the technology behind its new Quadro RTX GPUs. He also gave a glimpse into the future of collaborative VR through the impressive Nvidia Holodeck.\nLenovo\u2019s Mike Leach also showed how RTX technology has now made its way into mobile workstations as well as giving an excellent overview of Virtual and Augmented Reality and the many options available to AEC firms. The Oculus Rift and HTC Vive might get most attention but there are several other powerful options for architectural VR.\nHerzog De Meuron\u2019s VR lead Mikolaj Bazaczek continued the VR discussion with a fascinating insight into the Swiss architecture firm\u2019s VR journey and workflow, w hile Nassim Saoud of Trimble Consulting turned the discussion to Mixed Reality and its growing influence in the AEC sector.\nFinally, Alexander Le Bell, CEO of Finnish firm Tridify closed proceedings by introducing a new technology designed to make BIM model data instantly available to anyone, everywhere in VR or on mobile phones.\nGreg Corke\nWatch NXT BLD 2019 on demand\nMissed a talk or would like to see a favourite again? The good news is you can now watch all of the presentations from NXT BLD 2019 (and our 2017 and 2018 events) on AEC Magazine\u2019s YouTube channel.\nSave the date NXT BLD 2020\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre in London on 9 June 2020. More info at nxtbld.com over the coming months\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/bridging-the-gap-engineering-construction\/","title":"Bridging the gap: engineering & construction","date":1512000000000,"text":"Greg Corke reports on the maturing partnership between Bentley and Topcon that aims to streamline engineering and construction through a connected data environment in the cloud\nThis time last year, Bentley and Topcon introduced the idea of \u2018constructioneering\u2019. While the phrase might not be everyone\u2019s cup of tea, no one would contest the objective \u2013 to optimise the fragmented workflow between engineering design and construction. As it stands, data in digital engineering models is often lost, inefficiently recreated, or does not flow at all.\nSpeaking at Bentley\u2019s Year in Infrastructure event in Singapore last month, Topcon CEO Ray O\u2019Connor shared some shocking figures that highlight the construction industry\u2019s inefficiency. Since 1945, he said, productivity increases in agriculture and manufacturing have been 1,500% and 760%, respectively. Construction, on the other hand, has only managed a miserly 6%. It\u2019s obvious that something needs to change.\nConstruction is one of the largest manufacturing industries in the world, and the least automated, notes Murray Lodge, senior vice president and general manager of Topcon\u2019s construction business unit.\nO\u2019Connor admits that when he first joined the industry, he could not understand why construction was not automated like factory operations. After all, it is just like CNC machining in a machine shop, he said, except that construction machines the face of the earth.\nEmbracing automation\nThings are starting to change. Machine control automation is currently experiencing exponential growth. Five years ago, none of the major equipment manufacturers offered machine control for guiding construction machinery. Now, they all do. Price points have also dropped. However, O\u2019Connor acknowledges that islands of automation like these are not going to address the broader efficiency challenges facing the industry.\n\u201cIf we don\u2019t get the engineering side and the construction side calibrated together, we won\u2019t solve the entire workflow problem,\u201d he said. \u201cThis is not about solving just one area of the process. This is about solving the entire process \u2013 and that\u2019s why Bentley and Topcon have partnered in order to achieve that.\u201d\n\u201cConstructioneering is when engineering and surveying and construction are continuous processes that are not handed off from one to the other,\u201d added Bentley Systems CEO Greg Bentley.\nMerging clouds\nThe foundation to the Bentley\/Topcon partnership is the fluid exchange of information between design office and site, and vice versa.\nTo facilitate this, the companies have connected their cloud platforms \u2013 Bentley ProjectWise for engineering project delivery and Topcon Magnet Enterprise for construction contractors.\nOnce data is shared seamlessly in the cloud, information can then get fed down to the appropriate desktop or mobile application, whether that\u2019s in the design office or on site.\nBetween them, the two companies have most bases covered for technology used on a typical construction project. Topcon focuses on surveying, on-site execution and the capture of as-built assets, while Bentley concentrates on engineering design, project information management and asset maintenance. This makes for a compelling story, outlined below in one possible workflow.\n1) Capture the digital content of a construction site with a Topcon UAV (drone).\n2) Use the Topcon Magnet Enterprise cloud service to bring that context, processed by Bentley\u2019s ContextCapture software, into an engineering-ready reality mesh.\n3) Use the mesh as the basis for the engineering design in Bentley OpenRoads Designer or OpenRail Designer.\n4) Send the design model back through ProjectWise and Magnet Enterprise and turn it into a construction model.\n5) Push the construction model out to a machine control system using Topcon Positioning devices for automatic grading.\n6) Use the positioning device\u2019s record of what was actually built to feed back into the Bentley ProjectWise or AssetWise engineering and asset management environment.\nHistorically, most of these workflows have not been automated, others have been dumbed down to go via paper drawings and some did not really exist at all. Even contractors that have invested in a machine control system may not initially have access to a 3D model, notes Lodge, which is needed for a machine to know where it is on site, and where the blade needs to move up and down to get to finished grade. Lodge describes a familiar construction industry workflow, where a contractor takes 2D engineering plans and then creates a 3D model. But this, he says, is not only slow, but introduces potential for errors, one of many that can happen in a fragmented workflow.\nOne area that is often overlooked is the accurate capture of as-built data. This could include documentation of everything on site from the exact location of a pipe, including how far below the surface it is, to the position of valves. \u201cWe could give that information to the owner because, at the end, if you know where everything is exactly, not where the plans say it is, but actually really what you\u2019ve put into the ground, you can have so much information to help you lower the cost of maintaining the project over its lifecycle,\u201d says Lodge.\nAs-built surveys have traditionally been costly and capturing this data with drones can dramatically increase efficiency. Bentley is exploring new ways to bring down costs further. Earlier this year, it carried out an R&D project where it attached some GoPro video cameras to an excavator to dynamically capture work carried out in a trench.\nThe resulting videos were fed into Bentley ContextCapture, which produced an accurate reality mesh showing the size and position of pipes, along with the surrounding manholes, drains and houses.\nLooking to the future, the next obvious step is to apply Artificial Intelligence (machine learning) to automatically classify objects within the mesh (see ContextCapture article here).\nOf course, reality capture can also be used throughout the project. Drones can be flown daily or weekly to collect data to monitor construction progress, then the resulting mesh analysed and compared against design data to check it is within the correct tolerance.\nReal-time change\nIt\u2019s a familiar scenario: an issue found on site halts work until a design change can be made. Having a fully digital, optimised workflow can help minimise this downtime as data can filter through to site very quickly.\nFor example, changes made to the design model in Bentley OpenRoads could be sent directly into the cabin of the grader operator, via Topcon\u2019s Magnet Field and Sitelink 3D.\nThe driver will be notified that there is an update to the design, which can then be downloaded so that work can begin immediately. The blade on the excavator will simply adjust, according to the new grade.\nEarthwork planning\nBentley and Topcon are looking to optimise a number of different workflows between design and construction, another being the scheduling and planning of earthworks. Model information created in Bentley ConceptStation, for example, can be fed through to Topcon DynaRoad.\nDynaRoad can consume that data, then do some mass haul calculations associated with the project. It can help define the number and type of trucks needed, and optimise the movement of the material in relation to the design that has been passed on.\nAccording to Bentley Systems\u2019 Dave Body, this process can help reduce earthwork movement costs by anywhere from 7% to 10%. And when you are talking about a large greenfield project, where the earthwork costs alone can be in the region of $200 million to $300 million, the savings can be huge.\nEducation\nDeveloping optimised workflows to help surveying, engineering and construction work together more seamlessly is one thing, but the effort is ultimately fruitless if the industry does not embrace them. The problem now is not the technology, admits Greg Bentley, it\u2019s convincing a very conservative group of owners and engineers of the advantages.\n\u201cLow margins on projects mean it\u2019s often difficult for companies to take the risk in investing in new technology,\u201d adds John Foster, European business development manager for BIM at Topcon Europe Positioning. \u201cEarly adopters are helping to highlight the clear cost and time-saving benefits, which means more widespread adoption is now creeping into the industry.\n\u201cIt\u2019s a mindset change. All project stakeholders need to share in the risk and rewards of investing in new technologies and need to demonstrate and share the ROI stories within the industry.\u201d\nTo help drive this much-needed change, Bentley and Topcon recently announced the Constructioneering Academy to provide opportunities for construction professionals to learn best practices in integrated engineering and construction.\nBoth companies have been proving out a number of constructioneering workflows on real construction projects over the last year. This has helped shape the \u2018curriculum\u2019, which will be implemented through existing learning centres located in Livermore, California (Topcon), Houston, Texas, and London, UK (Bentley). More academy locations will be opened in the coming months, including Pune, India; Dubai, UAE; and Beijing, China. Constructioneering Academy offerings will also be available online through universities with leading construction degree programs.\nThe Constructioneering Academy is not just about educating Topcon and Bentley customers. \u201c[It] will not only teach people how to get the most out of both of our products, but will teach us about where the gaps are in order for us to solve the problem going forward and really create automation across every segment of the workflow for construction and design,\u201d concludes O\u2019Connor.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-arthur-mamou-mani-mamou-mani\/","title":"Video: NXT BLD London conference - Arthur Mamou-Mani, Mamou-Mani","date":1502150400000,"text":"Constructing (and deconstructing) buildings with cable robots \u2013 NXT BLD London, June 2017\nThe future of construction will be in robotic technologies, explains architect and digital fabrication specialist Arthur Mamou-Mani, as he introduces a project he\u2019s carrying out with Arup to develop a robot capable of constructing and deconstructing buildings. Arthur captivates the audience as he demonstrates the cyclical nature of the building process and how demolition can be anticipated in the spirit of our new circular economy.\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Paul Nichols, Skanska\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/render-legion-releases-corona-renderer-for-cinema-4d\/","title":"Render Legion releases Corona Renderer for Cinema 4D","date":1549324800000,"text":"New visualisation tool said to bring artist-friendly controls and affordable photorealism to C4D community\nRender Legion has launched a version of its Corona Renderer for Cinema 4D. The company says the software simplifies visualisation by reducing the rendering process to a few clicks.\nCorona Renderer for Cinema 4D builds on the strengths of the 3ds Max version, aligning product standards with tools specifically designed for Cinema 4D. Through a tight integration, Render Legion says it has tied Corona Renderer deep into Cinema 4D, making it an easy addition to any architecture or design-based workflow.\nThe software offers interactive rendering, where artists get \u2018immediate feedback\u2019 as they work. Whether modifying geometry, lighting, camera location or materials, each change appears in photorealistic quality within the Corona VFB (or Cinema 4D Picture Viewer).\nOther features include LightMix, where users can interactively adjust the colour and intensity of scene lights before, during or after rendering, allowing for subtle or extreme changes with just one render; and AI Denoising, where users can choose between High Quality CPU or interactive GPU-based AI denoising to speed up renders.\nFinally, the Corona Node Material Editor is said to offer a simpler way for artists to create, edit and reuse shaders between channels or materials. The editor works with all Corona and many Cinema 4D material sets and introduces the ability to spread changes across multiple materials at once.\nCorona Renderer for Cinema 4D is available now for macOS and Windows, and compatible with Cinema 4D R14 to R20. Prices start at $28.50\/month, with yearly rates available as well. To celebrate the launch, yearly licenses will be 25 percent until February 7.\nA free 45-day commercial trial is also available at: https:\/\/corona-renderer.com\/download.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-eleni-papadonikolaki-ucl-bartlett-school-construction-blockchain-consortium\/","title":"Video: NXT BLD 2018 London conference - Eleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium","date":1530144000000,"text":"Beyond crypto: Digital transformation in construction through blockchain technologies \u2013 NXT BLD London, June 2018\nBlockchain is not just about crypto currency. It could also have a huge impact in construction. Dr Eleni Papadonikolaki of UCL \/ Bartlett is also a board member at the Construction Blockchain Consortium. With a deep background in the construction supply chain, Dr Papadonikolaki highlighted key issues, with adversarial, non-collaborative, transactional and governed by mistrust. Blockchain is one way to circumvent a lot of those issues by providing certainty, security and building trust, but there are many ways to implement an encrypted ledger system.\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/grabert-targets-bim-documentation\/","title":"Gr\u00e4bert targets BIM documentation","date":1612137600000,"text":"In the BIM world we hardly ever talk about the evolution of 2D in pursuit of the ultimate model definition. While industry leader Autodesk stagnates the development of AutoCAD, competitors such as Gr\u00e4bert are sensing an opportunity to improve the way documents are produced from BIM models, writes Martyn Day\nThe CAD industry is entering unknown waters, in terms of technology changes. The cloud, for many software firms, is seen as the ultimate destination for everything: applications, data, communication, processing, AI \u2014 you name it. However, the design software world is still predominantly about applications which run on local hardware.\nSure, the cloud has been used to aid collaboration, share data, manage licences and as a project repository, but we seem a long way off the all-singing cloudutopia which software firms envisage.\nCompanies like Autodesk, which have exceptionally successful, but mature desktop applications, have the strongest visions for how the cloud will be all things to all designers.\nAutodesk has spent billions on developing its Construction Cloud offering, which is growing every year in scale and capability. The issue is, what does the roadmap of getting everything in the cloud look like and how soon?\nRegular readers of AEC Magazine will know of the consternation felt within the Revit community on its lack of development. This actually could be expanded to users of other important Autodesk desktop applications, such as Navisworks, as well as Autodesk\u2019s flagship software application, AutoCAD.\nThe reality is that these are mature products. The incentive to completely rewrite a desktop software application has vanished; the next generation will be written in the cloud. This period of stagnation is a danger, even for a company as mighty as Autodesk.\nWhile there are very few alternatives for Revit at the moment, AutoCAD users are more fortunate. Both BricsCAD and Gr\u00e4bert have mature DWG applications that are not only substantially cheaper but are also developing at greater velocity, providing more capabilities.\nIn recent years, these firms have identified that deeper integration with BIM, to provide greater productivity savings in automatic document automation, would be their focus. While the cloud is seen as a useful infrastructure, it\u2019s not necessarily the only destination for all software.\nGr\u00e4bert is based in Berlin, Germany and has a suite of DWG drawing tools which come under the Ares brand. The desktop application \u2014 Ares Commander \u2014 is available on Windows, MacOS and Linux. Gr\u00e4bert has also enabled access to, and editing of, DWGs on desktop, tablet \/ mobile (Ares Touch) or through a web browser (Ares Kudo). It\u2019s possible to access your designs or create new ones wherever you find yourself, on whatever device is close at hand. This is unique in the industry.\nBefore looking at what is new in the latest releases, it\u2019s worth noting that Gr\u00e4bert has identified that BIM users need better drawing production tools than are available today.\nThe whole promise of automated 2D output from 3D models was never really delivered, and plans, sections and elevations are regularly edited in products like AutoCAD and LT, breaking the automatic updates should the model change.\nFor the last two years Gr\u00e4bert has been expanding Ares\u2019 capability to readin and work with BIM models. As this work goes on, new DWG workflows are emerging, aiming to take a lot of the drudgery out of creating detailed drawing sets and healing the disconnect between iterative BIM design changes and any ongoing 2D editing.\nThe ultimate goal being associativity between model and DWG throughout the design and documentation processes, even if they are from two different applications, from different software houses. It seems like a big issue to chew on but at least recognising the frustration that users have with BIM documentation aligns Gr\u00e4bert\u2019s intentions with the industry feedback we have been getting for the past few years.\nBIM features\nIn last year\u2019s release, Ares had significant BIM capabilities added. It could read in RVT and IFCs, read BIM properties, extract BIM data (into tables or CSV files for quantity take-offs and cost estimation), filter BIM entities and navigate through models.\nThe new \u20182022\u2019 version, coming out in March, supports the creation of floorplans, elevations and sections, layouts with multiple views, BIM labels and dimensions, and materials. Most importantly, it has the ability to refresh the drawing when the BIM model is updated. Gr\u00e4bert states that its ultimate goal is to increase the automation of drawing production from manual to semi-automatic by a factor of 10x.\nSome of the automation capabilities can be seen in the way Ares automates tedious tasks, including the insertion of symbolic graphics such as doors, stairs or swing symbols.\nCallout generation is also automated, referencing the other BIM drawings. When the spatial volumes represented by two or more drawings overlap, callout symbols \u201ccall\u201d other drawings and indicate their position, drawing number, and sheet ID. This is done according to the logic of overlapping drawings.\nThe BIM drawings in ARES Commander retain all the BIM information and keep associativity with the BIM model. Consequently, automated dimensioning is also available. This is especially powerful when there have been changes to the original BIM model, as Ares will automatically update any dimensions within the drawing.\nTo find out more about Gr\u00e4bert\u2019s views on BIM and how it intends to improve BIM workflows, AEC Magazine caught up with Robert Gr\u00e4bert (CTO). \u201cWith BIM, you\u2019re going do drawings multiple times. Designers need support here, because they are going to make five sets of drawings, as every month the model will keep changing. And here, you\u2019re not ever 100% sure it\u2019s going to be the final set of drawings. Reworking persists over multiple iterations and that is the problem we want to solve,\u201d he says.\n\u201cIf you refresh the model, after making changes, say the metadata on a door, or the geometry, all those will update in Ares, as will the dimension chains appropriately. The next step we are actively working on is to deliver this across the design workflow.\n\u201cAn architect may think spending a week creating a set of boring drawing plans is not a great use of time. But they accept it. The real frustraton is doing it for the second, third and fourth times! That\u2019s the problem we want to tackle first. We\u2019ve been working with architects and having them report the time they spend producing drawings from BIM models. We\u2019re just looking at those and seeing which ones of those we can tackle.\n\u201cWe have so much data from the original BIM model, if we really use that to our advantage, we understand what these things are, because all the properties are in there, I think we can do a lot here.\u201d\nMoving on to the topic of Apple Macs, it\u2019s also worth noting that Gr\u00e4bert is ahead of the game when looking at the new Apple silicon machines. The company is already building and testing an ARM64 compatible beta of Ares 2022.\nConclusion\nIn the past it\u2019s been easy to look at what are essentially AutoCAD clones and not expect to see much in the way of innovation. The primary reason for buying a clone has been to switch to a drawing tool with a lower cost of ownership.\nThis is no longer the only reason. Autodesk\u2019s competition is now actively developing new capabilities beyond the functionality of AutoCAD, targeting inefficiencies in current BIM workflows, integrating more tightly with models and workflows and using innovative approaches to deliver on the automation of drawing output.\nIf the cost of ownership hasn\u2019t been a compelling enough reason to switch DWG tools, the increase in automated documentation productivity in successive releases of Ares should concern Autodesk, especially as cost of ownership increases and AutoCAD development velocity falters.\nFor now, you may not be aware, but there is a CAD arms race happening to better automate, if not fully automate the 2D part of the documentation process. Customers want it and from the efforts so far from Gr\u00e4bert and Bricsys, the developer of BricsCAD, it\u2019s coming sooner rather than later.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-melike-alt-n-s-k-melike-alt-n-s-k-architects\/","title":"Video: NXT BLD 2019 \u2013 Melike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects","date":1565049600000,"text":"Dialogue between architecture and robotic construction \u2013 NXT BLD London, June 2019\nTechnological developments of the new construction methodologies play an important role for the innovative architectural design solutions to generate new type of structures and spaces. Through the recent works of her practice MAA, including Seoul Robot Science Museum and Istanbul\u2019s futuristic 369m metre-tall new telecommunication tower, Melike discusswa how to design the architectural formations which combine elegance and beauty with math and geometry through advanced engineering techniques for robotic construction.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/the-road-ahead\/","title":"The road ahead","date":1311465600000,"text":"In North America, InRoads and Geopak remain the software of choice for 90 percent of US Departments of Transportation, a majority of the Canadian Ministries of Transportation, and many municipal governments. Internationally, countless transportation agencies and engineering consultants tend to prefer MX or InRoads, now offered in a localised format and re-branded as Power Civil for Country, with \u201ccountry\u201d indicating the region for which it is localised.\nBentley acquired InRoads in 2000, Geopak in 2001, and MX in 2003. Since then Bentley has continued the development of these three premier civil engineering design product lines, each with 20-plus years of 3D modelling credibility on some of the world\u2019s most intricate transportation infrastructure projects.\nWhile we generally don\u2019t differentiate among these product lines, we do recognise that each offers unique and valuable techniques, processes and workflows specific to user needs and disciplines. The products share common functionality; however, their distinctive characteristics are integral to what drives our users\u2019 businesses and maintains the project productivity that engineers and designers demand and have come to expect from Bentley.\nAEC: Maintaining three code streams must be a big challenge. How much cross fertilisation of technology goes on between the three tools and are there any future plans to consolidate the technology?\nSC: Maintaining three code streams is a significant challenge. Bentley could have taken the simpler route, one chosen by some of our competitors in fact; however, our goal is to develop products that enable users to meet their business objectives. Starting from a blank piece of paper and defining a \u201cnew product\u201d would have risked the many years that Bentley, and our users, have invested in developing refined, tested, and proven processes.\nBentley\u2019s goal is to help lower users\u2019 exposure to risk; save time, effort and money; and increase the value of project deliverables. We want to help users increase the quality of the infrastructure assets they are designing, constructing, and maintaining. Safeguarding users\u2019 capital and manpower investments across all Bentley civil product lines is a non-negotiable requirement of our product evolution.\nBentley stays attuned to users\u2019 concerns to identify the need for new capabilities. We develop that capability once and apply it to all three products, as evidenced by our new tools for data acquisition, geometry (as in roundabouts), and reporting enhancements. Superb technology \u2013 such as Quantity Manager, Roadway Designer, and our overlay and rehabilitation tools \u2013 that starts out in an individual product is cross-pollinated to the other products. Likewise, new technologies, like scalable digital terrain models (DTMs), are developed to serve civil as well as other verticals.\nAEC: Both MXRoad and InRoads have been in use in some form or another since the 1970s \/ 1980s. How much have the products changed in this time and what have been the most recent innovations?\nSC: MXRoad and InRoads have changed dramatically over the last 25-plus years, and specific enhancements made are far too numerous to mention for one product let alone three. However, significant milestones include the move to Windows, running inside MicroStation and AutoCAD, interoperability between what were competing applications prior to their respective acquisitions by Bentley, and integration with Adobe Acrobat and Google Earth.\nBoth MXRoad and InRoads have become more dynamic, interactive, and intuitive. Processing is faster than ever, even though data file sizes have grown with the increased accessibility to data sources \u2013 LiDAR for example. Built-in design checks and warnings help users identify potential problems at the earliest possible stage. Visualisation is no longer considered \u201cnice to have\u201d, but rather is often a prerequisite to winning detailed design work. Additionally, product integration with engineering data management and collaboration systems like ProjectWise becomes tighter with each release.\nInRoads and MXRoad, as well as Geopak, leverage the combined experience and innovation of the now-combined development team. Bentley continues to enhance and support each product line, as demonstrated by the V8i (Select Series 1) and (Select Series 2) releases of the products, which include a range of tools and functionality.\nData acquisition tools, for example, enable users to import, reduce, map and manipulate data from multiple sources, including raw survey information, ASCII data, and LiDAR XYZ and LAS data. Users can work with large data sets without \u201ctiling\u201d surface data or taxing performance. They can drag and drop data into a \u201cdata tree\u201d hierarchy to view any given surface or DTM and understand which data sources were used to build the model. In addition, with a simple right-mouse click, users can display and edit items and reveal context sensitive operations before saving data for immediate use in Bentley Civil products, or third-party applications where they are part of our users\u2019 workflows.\nFor drafting efficiency, Civil AccuDraw, a civil- specific version of MicroStation AccuDraw, provides a precision drafting tool that anticipates user intent.\nIt streamlines drafting processes by supporting civil- specific conventions like stations and offsets, bearings and distances, azimuths, and more. As workflows can vary widely depending on the project, Civil AccuDraw offers users the flexibility to work in a way that best suits their needs.\nHelping put designs in context, all Bentley Map functionality is included in all Bentley Civil design products, including Bentley Rail Track, our application for 3D design of rail infrastructure. Bentley Map functionality addresses the challenging needs of organisations that map, plan, design, build, and operate transportation infrastructure. It empowers users to add engineering precision to their GIS data or easily use existing GIS data in the engineering decision process.\nContinuing the mapping theme, in the V8i release, Bentley provided the first single-point solution to publish geospatially located 2D\/3D models directly to the Google Earth environment in MicroStation.\nWith Google Earth integration, the entire project team can gain a new perspective, via the ability to publish projects to the most relevant environment for review by a wider audience. At the touch of a button, users can publish DGN\/DWG models from their desktop to Google Earth and zoom to any place on the planet.\nFinally, in addition to the traditional tools within each of the Bentley Civil engineering design products, users can access the same in-context, smart design of 3D road corridors using Roadway Designer. This tool streamlines the complex development of every aspect of the roadway, in a single, parametric presentation.\nUsers can move along a corridor, interactively edit superelevation, compute end conditions, and calculate volumes to balance cut and fill. Benefiting from the immediate visual feedback, and accessing state-of- the-art design automation to promote conformance to standards, users dynamically design all roadway components in concert.\nComponents can include kerb and gutter sections, footways, asphalt and aggregate layers, slopes, and ditches. Users can easily modify and create these intelligent components \u2013 without programming \u2013 applying design constraints as part of a sleek 3D modelling process. Roadway Designer automatically creates surfaces for use when creating cross sections, performing volume calculations, or merging components to create a single design surface or model of an entire corridor to aid visualisation and rendering.\nAEC: With the UK government cutting councils\u2019 road budgets by \u00a3165m, how can Bentley software help those working within these tougher financial constraints?\nSC: With the UK construction industry in a state of stress due to reduced budgets and workloads, margins are being squeezed as firms fight for the projects that continue. Companies need to become more efficient and competitive if they are to prosper in the current economic climate. At Bentley, with a viewpoint shared by the UK government, we believe a significant way to address these challenges is to work in a more coordinated and collaborative manner.\nCollaborative work is a foundation base that can be built upon by additional organisational initiatives, such as Building Information Modelling (BIM) and Integrated Project Delivery (IPD). Numerous technical and professional challenges require consideration when applying BIM in the civil world, but the principles for sharing information and common modelling are equally applicable to building and civil projects.\nBS 1192:2007, the British Standard that establishes a methodology for managing the production, distribution and quality of construction information, including CAD deliverables, uses a disciplined process for collaboration and a specified naming policy. Bentley, with ProjectWise Essentials for BS1192, is the only company with a data management solution that supports the standard. ProjectWise Essentials for BS1192 bundles ProjectWise with custom software add-ons, out-of-the-box ProjectWise configuration, installation and training to quickly get users up and running using ProjectWise and supporting BS1192 methodology.\nUsing processes and techniques that cut misunderstanding and duplication of work, and facilitate the early involvement of operators and constructors can lead to cost savings in the order of 15 percent on any size project. This figure was backed up in a recent HM Treasury report that pointed to savings of \u00a3600m being made on the London Olympic Project, and around \u00a3600m of anticipated savings on the initial cost estimate for High Speed 2 (HS2).\nAEC: Where does Bentley see growth potential for transportation software, not only in the UK, but globally?\nSC: In February 2010, Bentley announced the acquisition of Enterprise Informatics Inc., with its\neB Insight software, and Exor Corporation, with its information modelling software. These strategic acquisitions signalled the launch of our AssetWise platform and a new focus on value creation for owner- operators.\neB Insight ensures that asset information is governed, secure, controlled, and can be trusted \u2013 delivering relevant information in context to users and turning information into an organisational asset. Exor information modelling software provides for the management and operations of linear networks, including roads, railways, and water networks, as well as all components connected to them.\nBentley\u2019s objective through AssetWise is to empower owner-operators to leverage the value of information throughout operations and maintenance, to take full advantage of information modelling and realise the true potential of \u201cintelligent\u201d infrastructure assets.\nBentley sees a tremendous opportunity \u2013 driven by advances in computing and information modelling \u2013 to substantially increase owner-operator return on investment through AssetWise. We are confident demand will grow with users continuing to gain new value as data reuse will always drive return on investment.\nAEC: What challenges does the industry have in terms of interoperability? What role does LandXML play in this and how mature is this schema?\nSC: LandXML has provided a significant and more generic means of data archival and exchange. However, like all preprocessor formats and processes, its limitations are driven by a basic principle that all data must be \u201cdumbed down\u201d to the lowest common denominator.\nDesign teams are often forced to execute projects in an IT environment that includes a host of software applications from several vendors, each with differing file formats. When using LandXML, as with any conversion process, there is a potential loss of efficiency and accuracy. Users of LandXML data may even be forced to accept compromised data as not every application understands the complex geometry encountered in high speed railway alignments. For Bentley or any vendor to provide anything near to complete interoperability, proprietary information has to be stored under custom definitions in the file, adding to the bloated inefficiency of this already heavy file format.\nBentley\u2019s commitment to interoperability, however, was reflected in its early and continued support for multiple file formats like LandXML, DXF and DWG.\nWe believe with the introduction of the i-model (a container for open infrastructure information exchange) in 2010, and subsequent launch of our iWare Apps site in March of this year, that we have created the foundation for a new level of interoperability.\nBentley\u2019s i-model is a digital container that enables users to combine disparate data, providing walk-through controls and automated clash detection information from the original design data. To facilitate interoperability, our new iWare Apps site (www.bentley.com\/iware) provides fast and easy access to apps developed by Bentley, and other organisations, free of charge. Each is Bentley\u2019s contribution to infrastructure professionals, allowing them to wield the interoperability benefits they deserve.\nAEC: How has highway engineering benefitted from laser scanning and the ready availability of off the shelf digital terrain data?\nSC: Laser scanning is not a new concept; however, technology advancements in scanners, digital imaging and computing mean accuracy is increasing as costs come down. This places large volumes of relatively accurate data at most engineers\u2019 fingertips, and is bringing this technology to age for civil engineering.\nAirborne scanning from fixed-wing and rotary aircraft make the acquisition of large-scale data, combined with digital imagery, common place. Recent advancements in mobile scanning mean we are on the cusp of seeing laser scanning for resurfacing and rehabilitation projects at the required accuracy. And, terrestrial scanning of bridges and other infrastructure assets, combined with digital imagery, brings structural modelling from as-built data to the screens of every designer.\nIt\u2019s easy to see that laser scanning can greatly enhance the quality and speed of engineering, but another huge asset is increased safety. With laser scanning, we reduce the necessity of placing people in harm\u2019s way to collect that last segment of data to complete the design. As we watch laser scanning and digital imagery reshape our industry, this may well be one of the most exciting times to be a civil engineer.\nAEC: What is the future of road design software?\nSC: Recent business trends reflect an increase in design- build and design-build-maintain projects; subcontracting by transportation departments and authorities; and the use of automated machine guidance requiring a complete 3D model. Ageing infrastructure means many projects involve the resurfacing, reconstruction or rehabilitation of existing highways. And the assessment of carbon footprints throughout design, construction and operation of infrastructure projects is growing.\nAt the same time, technology outpaces the industry\u2019s ability to consume it. Traditional design- bid-build and linear- and paper-based workflows are quickly being superseded by dynamic, real-time, digitally-based best practices. Users are becoming more geographically distributed, while each has the possibility to deploy software specific to his or her discipline or function, and applications from a variety of software providers may be deployed across a project.\nFor the evolution of our civil products, Bentley evaluates each industry trend, challenge and opportunity. We see a growing demand for transportation software based on technology proven to work on projects of all sizes that supports established engineering practices and theory \u2013 software that leverages state-of-the-art technology to work in 3D and is dynamic, constraint-driven, intelligent, and intuitive.\nThese capabilities and more are available today and found in all Bentley civil engineering applications via Roadway Designer\u2019s smart, in-context 3D design tools, offering the state-of-the-art roadway design automation. Roadway Designer is the only \u201cbattle-tested\u201d technology geared to keep engineers in full control, meet their requirements, and accommodate the way they work. Developing this technology is only part of Bentley\u2019s plan for the future; we see road design software going a step further and anticipating the engineers\u2019 design intent.\nBy \u201canticipating design intent\u201d Bentley does not mean that the software simply updates the road, railway or bridge design in the background without the interaction or knowledge of the user. For example, if the centre line of a road or railway is moved by 20, 10 or even 5 metres, and the software is allowed to update everything in the background, as engineers we must ask if it is safe to assume the engineering intent will be the same. There will be exceptions, but the answer will almost certainly be no. Instead, when software is designed with the engineers\u2019 learned or acquired knowledge in mind, it can truly begin to anticipate their intent, reduce repetitive tasks and checking cycles, and\/or warn if the \u201cintent\u201d is superseded or violated.\nBentley\u2019s objectives will continue to save our users\u2019 time, effort and money, improve the quality of the end-project deliverables, and protect their investments in software and services. Bentley is not new to this industry. We have the experience to know what really matters, and I firmly believe that as a company we will continue to lead where others follow.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/civil-engineering\/autodesk-acquires-water-modelling-firm-innovyze-for-1-billion\/","title":"Autodesk acquires water modelling firm, Innovyze for $1B","date":1614124800000,"text":"Autodesk buys Portland-based Innovyze for $1 Billion, to enter smart water infrastructure modelling market\nIn its first acquisition of the year, Autodesk has spent $1 billion cash on Portland OR based developer Innovyze. The 35 year-old software company is a specialist in smart water infrastructure modelling, simulation, and predictive analyses technologies.\nAccording to Autodesk this acquisition helps it \u201cto provide end-to-end solutions for design, construction, and now, operations of water infrastructure, enabling them to accelerate sustainable outcomes and help communities build resilience for the future.\u201d\nInnovyze\u2019s modelling, simulation, and predictive analyses solutions enable cost-effective and sustainable water distribution networks, water collection systems, water and wastewater treatment plants, and flood protection systems. Further, Innovyze\u2019s solutions centralise infrastructure asset visibility to optimise capital and operational expenses.\nAutodesk stated that \u201ccombining Innovyze\u2019s portfolio with its own design and analysis solutions, including Autodesk Civil 3D, Autodesk InfraWorks, and the Autodesk Construction Cloud, offers civil engineers, water utility companies and water experts the ability to better respond to issues and to improve planning.\u201d\nExecutive comments\n\u201cWe can achieve a more sustainable planet, but we can\u2019t do it without responsibly managing our use of water and securing its future,\u201d said Andrew Anagnost, Autodesk president and CEO. \u201cAn estimated $1.9 trillion is required to address global water infrastructure needs by 2030, and by fundamentally changing the way systems are designed, constructed, and operated, we are best positioned to overcome this challenge and realize the better world we\u2019ve imagined.\u201d\n\u201cNearly nine trillion gallons of water are lost each year worldwide due to prolonged leaks and pipe breaks, but we cannot manage or fix what we cannot see,\u201d said Amy Bunszel, executive vice president, AEC Design Solutions at Autodesk. \u201cInnovyze\u2019s portfolio of operational analytics, distribution modelling, and asset management solutions provides the insight needed to identify this and other potential problems before they become a crisis.\u201d\n\u201cFor thirty-five years Innovyze has been a hidden part of the daily lives of millions of people around the world, helping to deliver fresh, clean water, managing sewage and flooding in our communities, and turning wastewater into safe water,\u201d said Colby Manwaring, CEO of Innovyze.\n\u201cSimilarly, if you look at the built world around us, Autodesk\u2019s design DNA is found in just about every structure you see above ground and below, so it makes strategic sense to bring together our complementary organizations critical to much of the world\u2019s population. We look forward to completing the acquisition and getting to work, together.\u201d\nAEC Magazine thoughts on Innovyze acquisition\nBy any measurement, this is the biggest deal so far under CEO Andrew Anagnost. Since 2017 Autodesk had already spent $2.4 billion acquiring firms and technologies predominantly for its construction cloud play. Innovyze appears to be a much more strategic purchase, with regards to entering deeper into the Civils space, targeting specifically water.\nIn Autodesk\u2019s justification slide deck, it states that Innovyze\u2019s lets it better address $1.7B total addressable market. Mainly due to the fact that Innovyze is the largest pure-play global water infrastructure software provider with regulatory stamp of approval. The company has 240 employees and is conveniently located in Portland Oregon, which is also the home of Autodesk\u2019s mechanical CAD division.\nTo date, Autodesk\u2019s presence in infrastructure and plant, has tended to less than market dominating. Autodesk feels it has the building space, but has tough competition in transport roads, plant, rail and water, against players such as Hexagon, Trimble and Bentley Systems.\nAutodesk\u2019s intentions now go beyond competing in the design offering space, as it looks to build a compelling, cloud-based, digital twin business. This acquisition of Innovyze takes the companies relevance from design and construction all the way through to operation and decommission. While Autodesk has the potential to democratise digital twins in the building space, it\u2019s lack of dominance in the infrastructure market is an issue.\nAutodesk\u2019s digital twin technologies are also still embryonic, with Autodesk Tandem, now in public beta, and yet to be released. Clearly Autodesk is not going to stop at building digital twins of above-ground installations, but wants to also document what goes on under our feet. However, to do this it has a fight on its hands with the incumbent competition to win market share.\nWhile Autodesk has now spent $3.4 billion acquiring firms in the last three years, we are not sure how this news will be received by Revit customers who have been complaining of lack of development and increased cost of ownership. While Autodesk has promised to re-double its efforts on delivering a broader three-year roadmap for Revit, it seems to prefer to spend much more money on acquiring firms for its construction cloud and now digital twin\/ civil engineering play.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-ideate-introduces-view-management-tool-for-revit\/","title":"NEWS: Ideate introduces view management tool for Revit","date":1519084800000,"text":"ViewCreator helps users batch-create multiple Revit views and comply with company BIM standards\nIdeate ViewCreator is a new application designed to improve control, quality, and the efficiency of Revit model View creation. The software allows users to create multiple Revit views at the same time based on smart rules and create copies of plans from one level to many levels. According to the developers, it also helps ensure view properties, templates, and naming standards are used.\n\u201cViewCreator solves a problem experienced by both large and small companies that use Revit software,\u201d said Ideate software director of software development, Glynnis Patterson. \u201cUntil now, they haven\u2019t been able to batch-create multiple Revit views while complying with company-established BIM standards. ViewCreator solves this problem and perfectly complements our other Sheet and View tools: Sheet Manager, Clone, and Align. When used together, these tools support a high degree of automation and consistency for Revit sheet and view management.\u201d\nIdeate ViewCreator and the other Sheet and View tools are included in IdeateApps, a collection of applications designed to simplify the daily Revit activities.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/graphisoft-archicad-15-p2\/","title":"Graphisoft ArchiCAD 15 p2","date":1317427200000,"text":"Last issue AEC reviewed the new renovation tools in the latest release of ArchiCAD from Graphisoft. This month we conclude the review, highlighting the other functional improvements to the popular architectural BIM modelling tool, writes Martyn Day.\nAs AEC Magazine has reported previously, the interest in Building Information Modelling (BIM) has recently been accelerated thanks to the Government\u2019s construction advisor, Paul Morrell announcing that BIM will be mandated on all Government projects after 2016 with a gentle phasing in over the next five years. This decision has probably brought fear to the firms that specialise in government run education and healthcare but with BIM adoption being fairly low, the majority of the industry will be learning on the job.\nWhile Autodesk has done a good job in marketing Revit as being BIM, the reality is that there are a number of applications that offer similar BIM deliverables, Bentley Architecture and Graphisoft ArchiCAD being two popular choices in the UK.\nFor those not familiar with ArchiCAD, it is a Hungarian developed architectural 2D and 3D modelling application, which is the most mature BIM solution on the market, having been launched in 1987. I specify architectural because unlike the competing solutions, ArchiCAD focuses its main feature set on the single discipline (although there is an add-on for MEP). Another unique feature is that ArchiCAD runs on both Apple Mac and Windows PCs which, with the rise in popularity of Apple computers, is a clear advantage.\nBefore diving into the Release 15 features, it is worth mentioning that Graphisoft also recently launched an online design community and mobile communication tool called BIMx for iOS. Using this application it is possible to send models to people using iPads or iPhones who can then explore and remotely walkthrough the 3D designs without having an ArchiCAD license.\nThe software offers four different rendering engines, gravity, fly mode, individual building element information, distance measuring and pre-saved walkthroughs. BIMx is included as part of the ArchiCAD 15 trial download.\nGraphisoft themes its yearly updates to ArchiCAD like no other vendor. The major two issues addressed in this release were the renovation market and expanding ArchiCAD\u2019s ability to handle complex geometry. In the July\/August issue we looked at the effective workflow that had been implemented covering the creation and deletion of elements, clearly showing what was to be demolished and what the new design would be.\nWith tough market conditions out there for architects, it seems most vendors are adding some capability in this area, although like its collaboration server technology, Graphisoft has excelled at reducing the complexities of use.\nArchitects continue to look to innovate and create designs using new freeform geometric shapes and structures. Release 15 expands the geometric creative freedom of ArchiCAD and has addressed longstanding issues of user interface when comprehending 3D space and applying workplanes.\nComplex geometry\nArchiCAD 15\u2019s key geometry enhancement, the new shell tool, is all about providing design freedom. The shell capability supports three methods: extrude, revolve or ruled. All work in a simple or detailed way allowing quick creation of basic forms or generation from complex profiles.\nThe software automatically creates a \u2018membrane\u2019 for this structure based on its defining components (core driving geometry). The thickness of the shell can be defined and composite structure can be applied and it will be displayed. These composite definitions can be applied to all shell structures in whichever method was used to create them.\nThere are a range of other capabilities with shells; such as the ability to alter the base height, apply distortion, flip, grow, rotate, mirror, segment, drag, and morph.\nIt is possible to define a single contour polygon for a shell in 2D section or elevation that can be used to shape basic geometry into more complex shapes, such as roof elements. This can also be done in 3D, using other elements in a model. It is also possible to create any number of holes within a shell, which can also be easily manipulated. The edges of shells (even holes) can be assigned different materials.\nIn combination these can be used to create really complex, detailed geometric forms, with intelligent structure information that can be applied to the Information generated for the model.\nMany times in BIM tools, complex geometry, like roofs have been defined in products such as McNeel and Associated Rhino and simply imported as dumb geometry, which break the whole concept of BIM.\nArchiCAD 15\u2019s shells also play happily with standard ArchiCAD elements, such as skylights. They can be placed in 2D or in 3D and when they are placed they automatically oriented themselves with the shell and can be moved within the shell body.\nThe gravity feature of ArchiCAD can also be applied to shells. This means components that are drawn over them will automatically sense the height of the component underneath. A shelled component can be selected as the external or internal surface, enabling the \u2018embedding\u2019 of elements, like walls into the shelled structure.\nWith the introduction of complex shells, ArchiCAD has new intelligent connections, which consolidates previous connections for items such as curtain walls. There is some new naming here too, Trim means a connection that creates association between components (all will update if edited), while cropping will cut a component to another element but without making a direct association (components are independent of editing). There is a new trim elements to roof and\/or shell command, allowing for fast modelling with walls and columns.\nThis release brings substantial new 3D capabilities for ArchiCAD and, as usual, have been implemented in a very easy to use manner. Creating complex geometry still requires some thought, although here it is mainly considering which of the many 3D construction methods best fits the shapes required. I understand that the foundation modelling technology for all this is brand new and will possibly enable development of ArchiCAD towards the generative design capability, which is the forte of Bentley\u2019s Generative Components (GC) and Rhino Grasshopper.\nEnhancements\nArchiCAD 15 has new blue, auto-orienting Editing Planes, which make element selection and modification much easier. They can be applied to selected components or moved around the 3D space to start the creation of new geometry and support independent grid and snap points.\nUsing a new dimming effect it is possible to get a better sense of depth in 3D element creation and modification. The creation and placement of reference guidelines have been significantly enhanced aiding both 2D and 3D creation, providing visual feedback.\nThere are also new GDL parametric libraries of 3D components with some lovely boats, solar cell and heating, wind turbines, shelving units, kitchen appliances and all the other typical household and business items.\nEcoDesigner\nWith the growing need for sustainable architecture, Graphisoft offers EcoDesigner for ArchiCAD, which has been updated for release 15. EcoDesigner, can quickly and easily analyse designs for energy efficiency. Providing invaluable feedback on the building\u2019s performance means the architect can make better decisions on how to conform to regulations and satisfy the interests of the client and the operator of the building.\nIn release 15, EcoDesigner gets an improved Building Energy Evaluation Report PDF, adding 40% more information versus the previous version. The software also outputs to Excel for a detailed breakdown can now be exported to the Passivhaus energy calculator. Weather data has been improved to include ASHRAE-compliant city data.\nUsers can now override U\/R values to experiment with different material properties, which will be useful in the early design phase.\nOpen BIM\nGraphisoft is probably the biggest supporter of the Industry Foundation Classes (IFC) standard for BIM and has been since its onset. Interoperability is potentially the biggest problem that the construction industry will face as competing BIM systems really lack a credible open standard. IFC is all the industry has and it is still playing catch-up.\nArchiCAD 15 has enhanced IFC output; MEP elements can be saved as complex yet lightweight BREPs (boundary representation) geometry, renovation status saved to IFC, ArchiCAD and IFC models can be merged. IFC properties are now native elements in the ArchiCAD database and settings can be edited in the element settings dialogue.\nThe final point to mention is that this version is native 64-bit on Windows and Mac OS, IFC and BIM server. This means much more memory can be addressed, which means larger models and faster performance.\nConclusion\nThis is a big and beefy update to ArchiCAD. For existing users who create mainly rectilinear buildings there are speed and feasible benefits to many of the new features despite the concentration on freeform geometry.\nMoving to 64-bit to have larger models and address lots of memory is worth it on its own.\nThe real benefit will be to those designers who have struggled to design shapes that have proved difficult in previous versions, or have had to create dumb geometry to define the surface. With the increasing use of computerised manufacture the cost of producing these free forms is drastically reducing and become more popular.\nWith ArchiCAD 15, it now has the 3D power to allow much greater freedom in geometric exploration.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-marc-petit-unreal-enterprise\/","title":"Video: NXT BLD 2018 London conference - Marc Petit, Unreal Enterprise","date":1530144000000,"text":"The journey to real time. \u2013 NXT BLD London, June 2018\nReal time viz is on the rise. One in five arch viz firms now use Unreal Engine, explained Marc Petit, general manager of Epic Games Enterprise division. He also examined next gen tools that will enable real time rendering without sacrificing visual fidelity, including ray tracing at 24 frames per second.\nPetit also explored the benefits of using Unreal as a conduit for Revit BIM model data, architecture, structures and MEP, 3D and 4D, for interactive design sessions with import through the Datasmith API. VR is not just for clients and marketing but has a role to play in the design process.\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/revit-structure\/","title":"Revit Structure","date":1124841600000,"text":"The BIM concept will always remain a pipe dream, so long as the tools are unable to handle the thousands of components that make up today\u2019s buildings. So far the focus has been on architectural elements. Autodesk has just released a product for Structures.\nProduct: Revit Structure\nSupplier: Autodesk\nPrice: \u00faPOA\nI feel I should start off this article by first apologising that the product I\u2019m about to write about won\u2019t be available in Europe til next year. So, I will be teasing the structural engineers amongst us as yet again, Autodesk is developing products primarily within the USA, first for the home market and then adapting to the local conditions found in Europe and Asia\/Pacific. I guess this make business sense, as the US is a very large market and while the EU may be as big, or larger, it is not homogenous. Still, I do have concerns that while the products aren\u2019t probably designed to be US centric from the outset, the fact that the first two or three revisions are fuelled by US-customer feedback can\u2019t help but lessen the flexibility when it\u2019s time for release in the EU.\nSo, enough grumbling. Revit is Autodesk\u2019s \u2018next generation\u2019 AEC design tool. Bought for the princely sum of $133 million, in 2002, the product has found it tough going against the core 2D products, on which the AEC community doggedly hold on to. The concept of Building Information Modelling (BIM), where a single 3D model provides all the 2D plans, elevations and sections, has still to catch on. There have been successes in small architectural practices and with SOM on the Freedom Tower in New York, but Revit\u2019s architectural sales means it is going to take a long time to payback its purchase fee.\nInstead of drawing out everything with lines, circles and arcs, Revit uses a component-based design system, where the designer places walls, doors and windows. Previous revisions of the software have seen the complexity of components on offer mushroom, greatly enhancing the variety of architectural designs Revit can be used for. However the greater goal for BIM is to have a single model which contains components of all the building assets, structural and building services. To date Revit has only offered some dumb structural components for visual placement, while ADT has already seen a building services (HVAC) module be developed. The recent launch of Revit Structure now means that ADT can do architectural and services, while Revit can model architectural details and structural elements. This decision means neither product quite yet completes on the BIM concept of total building definition. However, Autodesk is developing a Structural package for ADT, as well as a building services package for Revit. At some point Autodesk will have two complete modelling packages, although the AutoCAD-based ADT is still more aimed at documentation-based design, than a modelling tool. Revit\u2019s parametric engine also ensures it\u2019s of greater use, earlier within the design stage, making short work of project-wide edits and updates.\nWithin that the design process, structural analysis and the changes that the results can have on the overall project outcome, make Revit a great place to deploy this type of technology. Basically, Revit Structure gives structural engineers a single environment in which all the structural members can be created in 3D and held together with the analytical model. Autodesk has developed an API (Application Programming Interface) which allows third party developers to take the Revit Structural model and analyse the data. As things stand, structural engineers may use a multitude of analysis packages, perhaps having to model the steelwork or nodal model many times. With this solution, Revit acts as the interface for multiple analysis packages, saving time and simplifying the data task. The results of the analysis can be fed back into Revit Structure and used to improve the structure, which may impact the architect\u2019s original design \u2013 and as this is all in Revit these changes will automatically feed back to the production drawings. Obviously the desire here is that the design was created in Revit Building, however Revit Structure has been designed to work just as well as a stand-alone structural interface.\nCurrently only available in North America, Revit Structure is linked via an API to several US-based industry applications for building analysis: RISA-3D from RISA Technologies (www.risatech.com), ETABS from CSI (www.csiberkeley.com) and ROBOT Millennium from RoboBAT (www.robot-structures.com). Autodesk is currently speaking with equivalent EU-based developers for the products release here next year, however Revit Structure is already in use and proving valuable on real projects.\nADT and Revit\nAs I pointed out ADT and Revit seem to be getting different bits of the BIM jigsaw at different times. However there is an increasing degree of compatibility between the products (but perhaps not as much as some would like). It\u2019s possible for a structural firm to import an ADT model directly into Revit Structure, using the architect\u2019s model for design reference. Although I understand that many Structural engineers in the UK tend to remodel architect\u2019s drawings to iron out any errors.\nStructural components in Revit Structure that are not supported in Architectural Desktop are exported to ADT as mass elements. So a complete structural model from Revit Structure can be shared directly with an architect using Architectural Desktop \u2013 enabling the architect to review the structural design inside the architectural model and check it for interferences with architectural elements, directly inside Architectural Desktop.\nSimilarly, users of Autodesk Building Systems (based on ADT), can import structural members as intelligent building objects. When exported, these can be used directly in Autodesk Building Systems, enabling HVAC engineers to layout their pipe\/duct systems within the context of the structural model. Revit Structure can also import \u2018dumb\u2019 ACIS geometry and objects generated by Autodesk Building Systems, allowing structural engineers to view the geometry of the building environmental systems within their structural model. That said the opportunity for \u2018true\u2019 collaboration will be between an architect using Revit Building and a Structural Engineer using Revit Structure, enabling the parties to work on a combined model, or allowing powerful cross-linking between the parametric Revit models.\nPerhaps, unusually for Autodesk, Revit Structure is more useful as a design tool, proofing the concept, as opposed to a pure documentation tool. There are more competent and complete steel detailing packages already out there. Revit Structure is looking to play a key role in structural design optimisation, while improving the level of co-ordination between architect and structural engineer.\nThe underlying differences between ADT and Revit, means that true model integration between the two competing systems, including geometry and intelligence, is really an unachievable goal. Autodesk has come up with a workable compromise, especially given the typical workflows, where each discipline edits its own data but the other disciplines merely need to be able to \u2018see\u2019 their collegues\u2019 geometry. In time each application will get it\u2019s own building systems and structural add-ons. At that point, the BIM concept becomes more than just an interesting technology, enabling projects and design\/build firms to standardise on a single 3D platform and interface.\nConclusion\nI think Revit Structure is the right product at the right time in the structural market. The modelling power of Revit, combined with the parametric editing capabilities make it a great front-end for an iterative design process such as structural analysis. While in the past Autodesk has avoided developing an API for Revit, Revit Structure has necessitated this. Autodesk has settled on offering Revit as a front-end for the structural engineer, while the hard maths is done by the add-on applications, which the company is busy developing links with.\nUnlike the Architectural community the payback of a Revit-based workflow is more immediately beneficial; reducing the need for multiple models and representations, offering a single easy to use environment and a tool which is capable of updating the model after analysis and quickly sharing this information back to other project members. The key benefit here for Autodesk is that the structural market is already on-board with the 3D message but have not been particularly well-served with easy to use products, particularly in the US. I think the UK has two or three key developers that have managed to cover the market and it will be Autodesk\u2019s task to bring them on board over the next year.\nWhether Revit Structure will really bring on the adoption of BIM across the AEC landscape, I am really not sure. It will be good news for those few companies that are using Revit already. The nature of the fractured building process means that disciplines use their own tools for their own purposes. There\u2019s little trust in the quality of data that they are initially provided and the common denominator is 2D DWG files.\nObviously, the Revit Structure does not do everything required, it\u2019s an early release, but it will improve over the coming year before it\u2019s unleashed in Europe. Its release will be synchronised with releases of Revit Building, so the same data types will always be available in each. I am not sure how this will work with ADT\u2019s timings but Autodesk appears to have most product ready around March of each year. We will keep you informed of any news on the further development of this product.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-university-2011\/","title":"Autodesk University 2011","date":1327881600000,"text":"Late 2011, Las Vegas; time for the annual migration of Autodesk fans to attend the CAD industry\u2019s biggest conference. AEC Magazine looks at how the company\u2019s technology has raised the stakes.\nThroughout the week-long CAD love-fest that is Autodesk University (AU), the one word not mentioned was AutoCAD. For an IT company that was seen as the 2D specialist, this was probably the first Autodesk event that demonstrated how far its product suite has diversified.\nUnder the leadership of Carol Bartz, Autodesk went from being a teenager to a corporate giant with one product and an attitude problem. With technologist CEO Carl Bass and team it has more customers using new products on mobile devices than it does using traditional CAD. Autodesk is now a company of experimentation, rapid development and broad ambitions.\nThis year\u2019s AU concentrated on how innovation, technology trends and an uncertain global economy are impacting lives. There was also a gaggle of inspirational thinkers devising different approaches to design, engineering, education and sustainability.\nThe benefits of cloud computing was a big theme, which took the shape of calming fears over software vanishing from CAD workstations. The expanded processing and collaboration benefits of using cloud tools will mean not only near instant results for building or structural analysis and data, but will also unburden workstations for design work.\nAutodesk envisages a cloud system with smart super computer systems in the background that are constantly processing designs, providing analysis and photorealistic renderings on demand. Autodesk is already developing analysis tools that will offer multiple solutions instead of a single solitary result.\nDocuments in the cloud\nIn 2007 Carl Bass went on record denouncing the manufacturing \/ engineering product lifecycle management (PLM) industry as \u201ca solution in search of an industry\u201d. At AU 2011, Autodesk joined in that search, with Mr Bass backtracking and saying that he did not want Autodesk in PLM until it could be done right. A fair point given that the company has gone through a torrid time trying and failing to develop document management systems.\nWhile PLM is seen as an engineering process the same underlying technology is going to be rolled out for Architecture, Engineering and Construction customers.\nThe forthcoming cloud product, Autodesk 360 Nexus, expands Autodesk\u2019s portfolio of document management solutions, Autodesk Vault and Buzzsaw. While Buzzsaw simply shares and distributes drawings, Nexus is a service that offers easy configuration of powerful workflow automation and hooks into document management systems running on the inside of customer\u2019s firewalls.\nAt the show, Autodesk Manufacturing took Nexus central stage but the AEC team talked about its future use as part of its slightly confusing \u2018BIM 360\u2019 suite of products.\nJim Lynch, vice-president, architecture, engineering and construction solutions, explained. \u201cToday\u2019s AEC projects are often hindered by widely dispersed teams relying on inefficient communications and collaboration services. Autodesk 360 for BIM can have project teams up and running with collaboration and data management in a matter of days. It represents a significant step toward enabling a BIM workflow from design through to construction.\u201d\nAutodesk\u2019s 360 document management suite includes the following products: Autodesk Vault Collaboration, an on-premise data management solution that lets project teams manage and track digital models without leaving their BIM design tools, including Revit Architecture, MEP & Structure, AutoCAD Civil 3D and Navisworks. Documents and content are centrally managed and integrated with enterprise systems like Outlook and SharePoint. Integration with Autodesk\u2019s Buzzsaw cloud service provides more external collaboration and mobile access.\nBuzzsaw is cloud-based project collaboration for exchanging designs and documents with distributed partners and teams. Through Buzzsaw Mobile on an iPad or Android tablet, users can access the most up to date information while working remotely.\nAutodesk 360 Nexus is planned to support configurable AEC workflows and business process management. No release date as yet.\nAEC content\nAEC-related presentations were thin on the ground. The biggest news release was that the former head of the division, Jay Bhatt, had left the company at what seemed short notice. In the summer the marketing director had also left and not been replaced. These moves would have had considerable impact on the potential for the production of content at AU this year.\nThere were three strong product take-aways, the first being that Project Spark \u2014 a potential Revit LT \u2014 is doing well on Autodesk Labs and Autodesk is experimenting to find the right level of Revit sub-functionality to attract architects to get into BIM.\nSecond, the excellent Project Storm had just been launched and allows structural analysis of members using a tablet device (available through iTunes).\nThird, Navisworks will get a port to tablets. Autodesk products and data are going to be available everywhere it seems.\nDesign computation symposium\nAutodesk director of software development Robert Aish holds a regular AU day for experimental architectural forums and designs. Computational design states that the computer does not simply record designs. Using scripting and applying parametrics and associations, it assists in generating the complex forms or conditions that are required. Edits do not involve redrawing but are computed and regenerated by the program.\nDr Aish demonstrated the power of his new programming language for AutoCAD, DesignScript, which could rival Bentley\u2019s Generative Components and Rhino Grasshopper when launched. AEC will have an in-depth look at this in the next edition.\nThe rest of the presentations mainly followed an ecological theme covering experimental green strategies covering: adaptive design, feedback systems, bio-mimicry, designs made from recycled materials, research, sustainable living, simulation, faade cooling and performance-based consumption.\nThe final session concluded with a presentation of a project with NASA to trial 3D printing machines in space. Autodesk tested various 3D printing machines on the so-called \u2018vomit comet\u2019, a plane which achieves zero-G by flying parabolas. Positive results mean that 3D printers are on thier way to the space station in the not too distant future.\nOther forward-looking discussions included the splicing of genes to create \u201cuseful living things\u201d. One suggestion was to design a tree that would grow into the shape of a house.\nAutodesk \u2018innovation forums\u2019 were new this year. There was a short presentation from Sir Ken Robinson explaining how the education system kills creativity. Saul Griffith of Otherlab shared some of the designs he has created (including \u2018soft robots\u2019, electric vehicles, a fast dinghy, bicycles).\nConclusion\nGone are the days when Autodesk was a one-product company, supporting one operating system and one platform. Its tools are now liberated and run on many operating systems and hardware platforms.\nWhile Autodesk\u2019s AEC team kept a low profile this year, Autodesk is clearly developing a strategy where there will be more services included in its subscription portfolio. Where once subscription was literally paying ahead for the next release, it will be a key to new productivity enhancing capabilities.\nAutodesk University is now less about product and more about ideas. In that respect, AU felt more like TED than it did an Autodesk product platform. For content it was one of the best conferences of 2011 and demonstrated that computers and engineering software combined with humans can do so much more than draw.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/hexagon-acquires-melown-technologies\/","title":"Hexagon acquires Melown Technologies","date":1562025600000,"text":"Acquisition intended to strengthen the 3D visualisation capabilities of the company\u2019s Geosystems division\nHexagon\u2019s (Leica) Geosystems division has acquired Prague-based Melown Technologies to strengthen its 3D visualisation capabilities.\nThe company specialises in mass-scale computer vision-based modelling of urban and natural landscapes from aerial imagery. As a major part of this process, it has developed an advanced 3D visualisation technology which allows interactive web-based rendering of these landscapes, from street-level detail to planetary scale.\nMelown Technologies also offers Vadstena, a fully automatic 3D natural and urban landscape reconstruction software system. From a single set of aerial images, it can produce a geometric 3D model, orthoimagery, DEM and land cover in a single unified, automatic process.\n\u201cAt Hexagon, we make the world machine readable and subject to the power of analytics to ultimately boost efficiency, productivity and quality for our customers,\u201d says Juergen Dold, President of Hexagon\u2019s Geosystems division. \u201cWe are very excited that Melown Technologies is joining our organisation as it strengthens our competence and technology portfolio to accelerate the accessibility of 3D digital realities that are captured with our broad portfolio of reality capture solutions.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/q-a-with-richard-brotherton-of-allplan\/","title":"Q&A with Richard Brotherton, MD of Allplan","date":1517788800000,"text":"One year on from his appointment as managing director at Allplan, Richard Brotherton speaks to AEC Magazine on how AEC technology is evolving and the company\u2019s plans for the future\nQ: Historically, Allplan has been strong in Germany, with its BIM products for architecture, engineering and facilities management, but less well-known in the UK. You have. How do your customers break down by sector?\nA: Allplan benefits from offering one platform for architecture, structural and civil engineering and, while different users will naturally work with different aspects of that platform, it provides a single environment for all users. In particular, we believe we offer a unique workflow that combines 2D and 3D processes without compromising the quality of drawings. This, in addition to our comprehensive solution for rebar detailing, makes Allplan a compelling solution in the engineering market, with over 250,000 users across the disciplines. Our products are being used on everything from the world\u2019s longest three-tower, cable-stayed bridge, the Queensferry Crossing in Scotland, to the world\u2019s longest tunnel, the Gothard Base Tunnel in Switzerland, as well as many landmark buildings such as the Sky Tower in Germany.\nQ: In the early days of BIM, Allplan stood out for its focus on cost management. How has this influenced product development over the years?\nA: Cost management has always been central to Allplan\u2019s offering in countries like Germany, but in reality, costing processes are very region, which makes this workflow difficult to bring to other countries. However, it\u2019s still very much at the heart of our thinking and we fully support initiatives to integrate costing solutions with Allplan. As an example, we work very closely with our sister company Nevaris [also part of Nemetschek], and with our cloud CDE [common data environment] solution Allplan Bimplus. This way, we\u2019re able to link to specific solutions our clients require.\nQ: In 2017, Allplan opened a new UK office \u2013 why the UK and why now?\nA: The UK has been at the forefront of BIM processes and digital construction for many years and is supported through the UK government\u2019s BIM initiatives. As a result, we see the UK market as a key player on the international BIM stage. I believe BIM is on the verge of transforming the industry and, with Allplan\u2019s software solutions, we believe we can help the industry achieve its goal of Level 3 BIM.\nQ: In the UK, we have some very specific industry initiatives to make BIM a country-wide standard. What developments and capabilities has Allplan included in the software to assist customers to meet standards such as the UK version of COBie, for example, and achieve interoperability? And how does this compare with what you see globally?\nA: The goal of delivering Level 3 BIM is to provide complete traceability of every object, in order to support the entire lifecycle of the project. This is something Allplan has been able to do for many years. The properties of individual drawing elements and objects can be tracked through the entire design and delivery process. In addition, Allplan is intrinsically linked to our CDE and collaboration tool Allplan Bimplus, an open platform for managing models and all associated project data with the entire project team. We also support export of BIM data via IFC4 and the open BIM approach for maximum interoperability.\nQ: Many new members of the Allplan team, yourself included, have a background in structural engineering and fabrication (at Tekla, CSC and AceCad). Should we read anything into this, in terms of the direction in which you intend to take Allplan?\nA: No \u2013 this domain knowledge just helps to ensure we continue to provide the right solutions for our clients. We, of course, have sister companies such as SDS\/2 for steel fabrication and PRECAST for concrete fabrication that work together with Allplan to ensure a comprehensive solution for structural engineering. Again, these are managed seamlessly through our cloud-based CDE platform Allplan Bimplus to ensure users can use best-of-breed tools without compromise, enabling an effective workflow.\nQ: Software developers have been very keen to jump on the cloud bandwagon and, at AEC Magazine, we can see how this helps collaboration, but we also see hurdles that must still be overcome. How does Allplan handle group working and distributed team collaboration \u2013 and how else do you use the cloud?\nA: We\u2019ve developed Allplan Share, a cloud-based collaboration tool that supports multi-user working, even when users are in different locations. Based on Allplan Bimplus, it opens up flexibility in your project teams and removes the obstacles associated with working virtually in the same location.\nQ: Coming from the Nemetschek stable of building design products, which also includes well-known brands such as Graphisoft ArchiCAD, Vectorworks and MAXON Cinema4D, how does Allplan differentiate itself from its stablemates \u2013 particularly those that offer BIM tools such as Vectorworks and ArchiCAD? And how is the group integrating and cross-pollinating technologies from its different brands?\nA: As part of Nemetschek, we are in a unique situation, as some of our offerings might be seen to compete with those of our sister companies. However, we see this competition as healthy and something that drives our technology forward. It also provides customers with choice, enabling them to choose the right tool for the right job. More often than not, we actually complement our sister companies and our combined technologies provide customers with enhanced workflows. An example of this is the work we\u2019ve done on data exchange with SCIA Engineer, whereby a direct connection simplifies interactions between modellers and engineers considerably. This is recognised in the group as a whole, and we have a number of cross-brand initiatives underway to bring enhanced workflow to our clients on platforms such as Allplan Bimplus.\nQ: Architects, in particular, seem to be broadening their tool palettes by mixing numerous products together in the realisation of their designs. What kind of capabilities does Allplan offer for collaborative open BIM working and how do you see this changing in the future?\nA: Allplan Bimplus supports a CDE across both open standard formats, such as IFC and so on, and closed BIM methodologies \u2013 for example, different API and SDKs. This means that numerous applications can be utilised via this cloud technology across many stakeholders, disciplines and applications, be it for architecture, engineering, costing tools or newer technologies such as HoloLens.\nQ: Many users would agree that, when creating BIM models, automated 2D output can leave a lot to be desired. That can lead to a lot of reworking to get 2D output into the state in which customers would want to see it. Some users break up the process and take their drawings into AutoCAD, which means they lose the benefits of quick changes driving new drawings. Allplan has extensive documentation editing tools; do customers accept the default option, or is document-editing of sections and elevations still a significant part of the process?\nA: I believe Allplan is in a unique position: its extensive 2D drawing functionality has been in the market for many years and combined with the requirements of 3D modelling. Therefore, we don\u2019t have to compromise on drawing quality or the 3D modelling as a number of our competitors do. We can actually work predominantly in 2D and still derive the 3D BIM model. Some applications are mainly layout tools and not created to generate deliverables for detailed reinforcement, hence the user is required to manually create 2D deliverables or use alternative systems, thus breaking BIM workflows and introducing potential errors. With Allplan, the solution is not just about creating the layout but also using familiar working methods in the 2D\/3D environment to detail reinforcement, thus delivering full manufacturing data and quality production information. Even better, with Allplan, users can detail almost any shape imaginable using a wide range of bar bending codes, including BS8666.\nQ: We\u2019re seeing a current trend in which users are using CDEs to \u2018data wrangle\u2019 proprietary data into a single, synchronised environment. What are your feelings on this?\nA:Engineers and architects need flexibility; they also need to deliver constructible data, which often will involve numerous software solutions to ensure that no compromise is made on the quality of the project data. Also, it is simply more efficient to use the correct tool! We therefore believe that we need to be flexible and be able to communicate with many native and open file sources. It\u2019s the philosophy behind Allplan Bimplus to be completely open to other solutions that our clients need.\nQ: Model sizes are a common problem in BIM projects, with models growing as more detail is added. What\u2019s a typical file size for a customer project for Allplan? What kind of RAM would a typical user require and does Allplan have methodologies for breaking up models into smaller work packages?\nA: Our recommended RAM requirement for a standard PC is at least 16GB. Graphic cards are also important if you wish to take advantage of 4K resolution, which is new for Allplan 2018. An overall model can be broken down into a series of structured sub models, thus enabling users to only work on the sub models they need to, even in a multi-user environment. This makes the workflow quicker, more efficient and more manageable and we believe keeps file sizes comparatively low. In reality, therefore file size does not tend to be an issue.\nQ: ArchiCAD has a live connection to Grasshopper. Does Allplan have any similar capabilities\/workflows for computational geometry?\nA: Allplan supports computational design with the import of the resultant model from the Grasshopper script.\nQ: Could you give us some details on how Allplan supports parametrics?\nA: Allplan is a full parametric solid modelling solution. We also have enhanced parametric intelligent objects controlled by Python called Pythonparts. An intelligent Pythonparts editor interface enables users to create their very own parametric objects and combine basic elements from pre-defined building objects \u2013 including columns, beams, foundations and stairs, for example \u2013 to create individual assemblies. In this way, recurring tasks are completed in far less time.\nQ: It\u2019s no secret that learning a BIM process can be a massive undertaking and thus a barrier to adoption for some potential users. What strategies does Allplan use in order to speed up training and keep the product from becoming excessively complicated?\nA: Training is a vital element when implementing a solution such as Allplan. To maximise return on investment, training is always advised. We have a dedicated training centre in all our offices, in addition to telephone\/email support from technical services and online self-help forums. All these resources help clients get up to speed as quickly as possible. We also have dedicated BIM consultants who offer both BIM consultancy courses and certification as well as BIM templates to companies and public-sector bodies. We publish an annual BIM Compendium [www.tinyurl.com\/BIMComp] and invest significantly in BIM-related research at universities including the Technical University of Munich (TUM), the Leonhard Obermeyer Centre (LOC). Finally, we are of course the founders of OPEN BIM.\nQ: With an increasing focus on digital fabrication in AEC, what\u2019s Allplan\u2019s vision in this area?\nA: This is an exciting area as we see more and more technology enter the market from robots, 3D printing, even electronic bricklayers! It\u2019s our vision to interface with such solutions, so that the BIM data created by an engineer is fully utilised downstream. We have invested in numerous projects around Machine Learning and Artificial Intelligence, which we hope to be bringing to market in the coming years.\nQ: Finally, looking to the future, what technologies and processes do you think will drive adoption and improve the benefits already delivered by BIM?\nA: First, we need to listen to our customers\u2019 requirements and help them make the transition to BIM as easily as possible. This means ensuring we help them digitise core deliverables such as drawings, while introducing new workflows to support BIM adoption. And there is no doubt that cloud-based technology has its place and will continue to drive digital processes, with open collaborative platforms such as Allplan Bimplus.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-simply-rhino-gears-up-for-rhino-uk-user-meeting\/","title":"NEWS: Simply Rhino gears up for Rhino UK User meeting","date":1495584000000,"text":"London event focuses on architecture, parametric and generative design, visualisation and more\nSimply Rhino has announced details of this year\u2019s Rhino UK User Group Meeting. The one day conference will take place on Wednesday June 14, 2017 at The Crypt On The Green in Clerkenwell, London.\nSpeakers highlights include:\nArthur Mamou-Mani, Mamou-Mani and Anthony Buckley-Thorp, Flux will present \u2018The DNA of Making\u2019, an exciting new project taking place at Arup\u2019s headquarters that features a giant cable robot that assembles and disassembles structures. Mamou-Mani is collaborating with Flux on an online dashboard for the project, created from Grasshopper3D.\nYavor Stoicov, a 3D Artist from Chaos Group, will present \u2018Parametric, Meet Realism: Real-time rendering for parametric design with V-Ray 3 for Rhino\u2019.\nBedir Bekar & Ian Shepherd from Elliott Wood, a design led civil and structural engineers, will present \u2018Making the Battersea Pump House Pavilion\u2019. Bekar and Shepherd specialize in the design and modelling of traditional and complex geometry building projects.\nDaniel Piker will give the low down on the forthcoming v2 release of Kangaroo, a Live Physics engine for interactive simulation, form-finding, optimisation and constraint solving.\nOther speakers include Oliver Salway, Softroom; Emma-Kate Matthews, EKM Works; Moritz Waldemeyer, Studio Moritz Waldemeyer; Carlos Perez, McNeel; Carsten Astheimer, Astheimer and Bart Radecki, Digits2Widgets.\nTickets are available from the Rhino Web store.\nStandard tickets cost \u00a340 + VAT\nStudent tickets cost \u00a320 + VAT\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-graitec-delivers-new-tools-for-bim-and-steel-fabrication\/","title":"NEWS: Graitec delivers new tools for BIM and steel fabrication","date":1434499200000,"text":"Software includes new productivity add-on for Autodesk Revit plus new Steel Fabrication Management Information System (MIS)\nGraitec has launched four new and enhanced products focused on BIM and fabrication: two productivity extensions for Autodesk Revit 2016 and Autodesk Advance Steel 2016, a new release of the structural analysis solution, Advance Design and a new Steel Fabrication Management Information System (MIS) called Advance Workshop.\nThe Advance PowerPack 2016 for Autodesk Revit 2016 toolset is designed to deliver increased functionality, better control and more automation for Revit users across all industries \u2013 structural, architecture and MEP.\nIt is said to include practical, everyday utilities focused on speeding up modelling, simplifying family management and predefining annotations, dimension and drawing views to better automate processes and documentation.\nThe Advance PowerPack 2016 for Autodesk Advance Steel 2016 introduces what Graitec describes as a wealth of localised manufacturer profiles and more predefined default settings. New tools for creating splines of complex helical stair stringers are said to expand the toolset for high-end stair manufacturers, while tools for automatic creation of floor coverings, based on standard grating sizes, are more focused on Graitec\u2019s plant users.\nThe 2016 version also introduces cloud connectivity and integrates bi-directional links with Advance Workshop, Graitec\u2019s newly launched Steel Fabrication MIS solution, enabling Advance Steel users to retrieve and graphically display colour-coded production statuses in their models.\nAdvance Workshop 2016 is described as a versatile, well-rounded fabrication management solution (MIS), perfect for optimising, controlling and managing every aspect of the process with high levels of accuracy and quality.\nIt is designed to give steel fabricators complete control over every stage of production including inventory management, traceability for quality control, human resources and CNC processes to increase efficiency and reduce costs.\nAdvance Workshop manages digital manufacturing and production data, derived directly from the BIM model and uses this to simulate a \u201cvirtual workshop\u201d helping ensure all aspects of the production process are optimised.\nThis software has the ability for full integration with high-end ERP systems and is said to support all major 3D steel BIM software including Autodesk Advance Steel, with built-in drivers for major equipment manufacturers to support a BIM to fabrication workflow.\nMeanwhile, the new 2016 release of Graitec\u2019s structural analysis solution, Advance Design comes with a new \u2018ergonomic\u2019 ribbon-based user interface designed to make it easier to navigate and access tools more efficiently.This version also introduces US, Canadian and Polish design codes together with a EC8 German appendix expanding the reach of Advance Design to additional global markets.\nNew design tools have been added for rigid diaphragms for slabs and reinforced concrete ribs design. The bi-direction link between Advance Design and Autodesk Revit has also been updated to enable the sharing of BIM models containing building performance data, including support for transferring the actual FEM results to the Revit environment.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/maporama-announces-maporama-partner-gateway-2003\/","title":"Maporama announces Maporama Partner Gateway 2003","date":1059782400000,"text":"Maporama, launches Maporama Partner Gateway 2003, an product designed to interface, in a totally seamless, transparent, and non-intrusive fashion, internal datasources of a company with its location-centric applications to provide end-users and\/or employees with ever up-to-date pertinent information. Fully integrated with all Maporama\u2019s products, in particular with Maporama Partner Hub 2003, the real-time and non-intrusive bridge laid by Maporama Partner Gateway 2003 provides companies with an efficient means to update automatically their address databases used for their location-centric applications with fast-changing data stored in their corporate databases, Enterprise Resource Planning (ERP), Customer Relationship Management (CRM), GIS and\/or Supply Chain Management (SCM) systems. All information updated via Maporama Partner Gateway 2003 is then directly and automatically accessible from their Maporama-powered location-centric application. \u201cMaporama Partner Gateway 2003 was specifically designed for companies needing to share fast-changing location-centric data coupled with powerful location-centric applications in order to reduce their mobile personnel-related costs, manage efficiently their vehicle fleets and\/or turn the visitors of their websites into customers by guiding them straight to the closest outlet having a specific product in stock.\nAn Internet user willing to reserve a hotel room from a hotel chain website has to enter the travel date and the destination address. Maporama provides, in a single click, full coordinates and detailed situation maps for all nearby hotels available for the specified date. Timesavings through powerful data treatment automation Maporama Partner Gateway fully automates data update process, freeing the time spent on manual data updates for other more productive tasks. Total control of the operations Maporama Partner Gateway operates as a totally non-intrusive tool: Maporama Partner Gateway does not connect to Maporama\u2019s customers\u2019 IS \u2013 they use this exclusive product to export only the data they decide to, and at a frequency they define. This way, they preserve full control of their data and the actions carried out on their IS and those effectuated on their Maporama address database. The customer also has the possibility to define the employees to grant access to Maporama Partner Gateway 2003.SecurityMaporama Partner Gateway 2003 is totally secured by a login and password for each user. Compatibility with market standards Maporama Partner Gateway is perfectly compatible with all database systems, including Oracle, Informix, DBR and SQL Server. Maporama Partner Gateway is ideal complementary module for ERP systems such as SAP, JD Edwards, Peoplesoft, Siebel and Oracle Applications.Multi-source data feed functionality. Maporama\u2019s customers can automatically update their address databases simultaneously from multiple information sources. Customers using previous versions of Maporama Partner Gateway benefit immediately and automatically from Maporama Partner Gateway 2003.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/skanska-to-develop-first-commercial-3d-concrete-printer\/","title":"NEWS: Skanska to develop first commercial 3D concrete printer","date":1416960000000,"text":"Skanska is leading an 18-month programme to develop the world\u2019s first commercial concrete printing robot.\nThe company will use 3D concrete printing technology developed through research at Loughborough University, and apply it to real applications with a view to \u2018revolutionising the design and construction process. The 3D printer is fitted to a gantry and a robotic arm and deposits a high\u2013performance concrete precisely under computer control. It works by laying down successive layers of concrete until the entire object is created. The printer is said to be able to make things which cannot be manufactured by conventional processes, such as complex structural components, curved cladding panels and architectural features.\nA team in the School of Civil and Building Engineering at Loughborough, led by Dr Richard Buswell and Professor Simon Austin, have worked on the development of 3D printing technology for the construction industry since 2007.\nRob Francis, Skanska\u2019s Director of Innovation and Business Improvement said: \u201c3D concrete printing, when combined with a type of mobile prefabrication centre, has the potential to reduce the time needed to create complex elements of buildings from weeks to hours. We expect to achieve a level of quality and efficiency which has never been seen before in construction.\u201d\nDr Richard Buswell from the Building Energy Research Group at Loughborough University comments: \u201cThe modern construction industry is becoming more and more demanding in terms of design and construction. We have reached a point where new developments in construction manufacturing are required to meet the new challenges and our research has sought to respond to that challenge.\n\u201cWe are pleased and excited by the opportunity to develop the world\u2019s first commercial 3D concrete printing robot with Skanska and their consortium. We have been convinced of its viability in the lab, but it now needs the industry to adapt the technology to service real applications in construction and architecture.\u201d\nFoster + Partners, Buchan Concrete, ABB and Lafarge Tarmac are also working with Skanska on this project\nAEC first reported on the project in 2010.\nMeanwhile, read our in-depth article on Italian engineer, Enrico Dini, who has developed a 3D printing technology that prints buildings out of sand.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/geoeye-announces-strategic-partnership-with-spadac\/","title":"GeoEye announces strategic partnership with SPADAC","date":1182729600000,"text":"Virginia, USA, 13 June 2007: GeoEye announced a strategic partnership and investment with SPADAC, a provider of geointelligence and predictive analysis solutions for government and private industry. Under the agreement both the companies can use each others tools in research and development efforts.\nGeoEye\u2019s partnership is aimed at providing its customers with predictive analysis and geointelligence solutions of SPADAC in combination with M.J. Harden\u2019s aerial imagery capabilities after the launch of their GeoEye 1 satellite which will provide images up to a spatial resolution of 0.45m. The combination of high resolution image by GeoEye 1, tools provided by SPADAC and M J Hardene is expected to form a suite for value-added offerings to commercial and government sectors.\nAbout SPADAC:\nSPADAC is a provider of geointelligence and predictive analysis solutions, including applied research and development, to customers primarily in defense, intelligence and homeland security agencies. SPADAC\u2019s predictive and geointelligence analysis products and services has enabled its customers to make better decisions faster, leveraging patented technologies, visualization services and domain expertise.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/workstations\/video-nxt-bld-2018-london-conference\/","title":"Video: NXT BLD 2018 London conference - Mike Leach, Lenovo UK","date":1530144000000,"text":"Enhancing performance \u2013 NXT BLD London, June 2018\nMike Leach addressed the burning issue of how to get the right specification workstation for the right job. Not all software is created equal and some crave CPU, others eat RAM and some demand GPU. It\u2019s totally down to workflows and software product mix. Leach defined handy recipe cards for four types of workflow \u2013 3D design\/ BIM, BIM & Design Review, Viz and VR, AI & Deep Learning. Each specifying processor type (speed and number of cores), RAM, graphics and SSD.\nView the other NXT BLD 2018 presentations\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/www.wired.com\/story\/nvidia-becomes-major-model-maker-nemotron-3\/","title":"Nvidia Becomes a Major Model Maker With Nemotron 3","date":1765756800000,"text":"Nvidia has made a fortune supplying chips to companies working on artificial intelligence, but today the chipmaker took a step toward becoming a more serious model maker itself by releasing a series of cutting-edge open models, along with data and tools to help engineers use them.\nThe move, which comes at a moment when AI companies like OpenAI, Google, and Anthropic are developing increasingly capable chips of their own, could be a hedge against these firms veering away from Nvidia\u2019s technology over time.\nOpen models are already a crucial part of the AI ecosystem with many researchers and startups using them to experiment, prototype, and build. While OpenAI and Google offer small open models, they do not update them as frequently as their rivals in China. For this reason and others, open models from Chinese companies are currently much more popular, according to data from Hugging Face, a hosting platform for open source projects.\nNvidia\u2019s new Nemotron 3 models are among the best that can be downloaded, modified, and run on one\u2019s own hardware, according to benchmark scores shared by the company ahead of release.\n\u201cOpen innovation is the foundation of AI progress,\u201d CEO Jensen Huang said in a statement ahead of the news. \u201cWith Nemotron, we\u2019re transforming advanced AI into an open platform that gives developers the transparency and efficiency they need to build agentic systems at scale.\u201d\nNvidia is taking a more fully transparent approach than many of its US rivals by releasing the data used to train Nemotron\u2014a fact that should help engineers modify the models more easily. The company is also releasing tools to help with customization and fine-tuning. This includes a new hybrid latent mixture-of-experts model architecture, which Nvidia says is especially good for building AI agents that can take actions on computers or the web. The company is also launching libraries that allow users to train agents to do things using reinforcement learning, which involves giving models simulated rewards and punishments.\nNemotron 3 models come in three sizes: Nano, which has 30 billion parameters; Super, which has 100 billion; and Ultra, which has 500 billion. A model\u2019s parameters loosely correspond to how capable it is as well as how unwieldy it is to run. The largest models are so cumbersome that they need to run on racks of expensive hardware.\nModel Foundations\nKari Ann Briski, vice president of generative AI software for enterprise at Nvidia, said open models are important to AI builders for three reasons: Builders increasingly need to customize models for particular tasks; it often helps to hand queries off to different models; and it is easier to squeeze more intelligent responses from these models after training by having them perform a kind of simulated reasoning. \u201cWe believe open source is the foundation for AI innovation, continuing to accelerate the global economy,\u201d Briski said.\nThe social media giant Meta released the first advanced open models under the name Llama in February 2023. As competition has intensified, however, Meta has signaled that its future releases might not be open source.\nThe move is part of a larger trend in the AI industry. Over the past year, US firms have moved away from openness, becoming more secretive about their research and more reluctant to tip off their rivals about their latest engineering tricks.\nA recent report from OpenRouter, a company that gives people access to different models through a single user interface, shows that open models accounted for around a third of all tokens\u2014units of text and other data\u2014sent through its systems in 2025. Chinese firms including DeepSeek, Alibaba, Moonshot AI, Z.ai, and MiniMax regularly release powerful open models and publish details about their research advancements that make their offerings more appealing for engineers to experiment with.\nThis could prove troublesome for Nvidia. The company\u2019s hardware has become so important in the world of AI that its silicon has become a bargaining chip in Trump\u2019s trade dealings in China. The US government recently said it would allow Nvidia to export H200 chips\u2014the best of its previous generation\u2014to China, but the Chinese government is keen to achieve greater technological independence and has taken steps to push Chinese companies to use home-grown chips. This could mean Chinese AI models become more closely aligned with Chinese silicon, which could undermine Nvidia\u2019s position.","source":"wired.com"}
{"url":"https:\/\/aecmag.com\/news\/openspace-to-bring-street-view-style-documentation-to-construction-sites\/","title":"OpenSpace to bring \u201cStreet View\u201d style documentation to construction","date":1565740800000,"text":"Images from 360\u00b0 cameras attached to construction worker\u2019s hard hats are stitched together by AI\nUS startup OpenSpace is using patent-pending, artificial intelligence to automatically create navigable, 360\u00b0 photo representations of a building site.\nConstruction workers attach a small off-the-shelf 360\u00b0 camera to their hardhats and walk the site as normal, with OpenSpace passively capturing imagery in the background. Imagery data is then uploaded to the cloud, where OpenSpace\u2019s algorithms map the photos to project plans and stitch them together, creating a visual representation of the site similar in style to Google Street View. This data then accumulates over time, providing builders with a \u201ctime machine\u201d that allows them to review site conditions as they were a day ago, a week ago, or years ago.\nWith OpenSpace, project stakeholders can conduct virtual site tours, communicate easily about change orders, and review the photo record to see changes over time.\n\u201cTishman Speyer has piloted OpenSpace at MIRA in San Francisco and now at our ongoing construction of the Spiral, a three million square foot development in New York City, and we see value in scaling this program more broadly,\u201d said Jenny Wong, Managing Director at Tishman Speyer. \u201cAccess to accurate, immediate data is essential in the development and building process, and we are excited to be partnering with OpenSpace.\u201d\n\u201cAt OpenSpace, we\u2019re using AI to augment workers\u2019 capabilities by making documentation fast, easy and complete,\u201d said Jeevan Kalanithi, co-founder and CEO of OpenSpace. \u201cWe started this company by collaborating with builders, on their job sites. We knew that any documentation solution needed to be totally passive in order to fit into the extremely busy day-to-day of a construction site, and this is what inspired us to create OpenSpace\u2019s unique tap-and-go approach.\u201d\nThe company has just received $14m in Series A funding, which it plans to use to continue to scale up its operations, including sales and marketing expansion, as well as for development of new computer vision powered products.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/3d-printing-in-construction\/","title":"3D printing in construction","date":1580947200000,"text":"Construction practice is changing. Old workflows, methodologies and practices are being challenged, as offsite, modular and design for manufacture concepts are under development. Could 3D printing technology be part of that change? Martyn Day investigates\nAs a species, we certainly have our flaws, but we are eminently resourceful at making things. We are natural engineers. When it comes to providing shelters, we have always used the resources around us to construct structures; wood, soil, clay, stone, straw. Our tools evolved, we made axes and chisels from metals, and we further refined and processed our raw building material, carved joints, planed surfaces, and baked bricks. We experimented with geometry, built towering pyramids with 2.5 tonne blocks, invented plumbing, heating, glass, built Great Walls visible from space. Humankind, \u2018masters of construction\u2019 since 9,000 BC. Not bad for a bunch of jumped up monkeys. We are always innovating.\nIn the 21st Century, it looks like we might well move \u2018beyond the brick\u2019. Concrete actually dates back to around 700BC when lime mortar was first used in Jordan and Syria. The Nabataeans even had concrete floors and waterproof concrete cisterns, which still survive. Obviously, the Romans turbo-charged the whole concrete thing and made domes, arches, aqueducts, amphitheatres etc. but they never figured out reinforcement.\nWe had to make it through the middle ages and into the Industrial Revolution for Portland cement. An unhappy French gardener, Joseph Monier, miffed at his easy to break concrete pots, embedded an iron mesh in his next generation flowerpots. He went on to patent the process and made, reinforced tanks, beams and bridges. Now we produce over 10 billion tonnes of concrete a year for the building industry, and the steel rebar market is worth $121.1 billion alone. Concrete is now the most commonly used man-made material on Earth.\nAs always, while our materials have evolved, so have our tools. Look out spades and shovels, the robotic revolution is upon us. And with precision motors, computer modelling \/ analysis and automation driving the next industrial revolution, construction is also at the forefront.\nIn manufacturing, 3D printers have survived the hype curve and are now moving from prototyping plastic parts to making real plastic and carbon within the traditional mix of processes. However, despite becoming faster, more reliable and production- capable, 3D print is a long way from replacing traditional methods of mass production such as plastic injection moulding. It\u2019s still carving its own niche.\nWith 3D modelling now standard within construction, in the late 1990s there were a number of research projects examining how 3D printers could be used to speed up the process of fabricating buildings and components, both onsite and offsite. Behrohk Khoshnevis, director of the Center for Rapid Automated Fabrication Technologies (CRAFT) at the University of Southern California (USC) looked at the then formative 3D print process and realised that while manufacturing parts in 3D layers was a novel approach, making buildings in layers was actually a continuation of the traditional construction process. He went on to develop a layered, 3D print fabrication technology he named Contour Crafting\nToday there are over 20 firms selling 3D print manufacturing services, cement 3D printers or 3D printed buildings, from fixed gantry-based systems to portable robot arms. That said, it\u2019s still very early experimental days, with the most common use of 3D printing being the production of wax moulds for items like panels, an example being Laing O\u2019Rourke\u2019s FreeFab wax 3D print methodology which was used recently in some stations for London\u2019s Crossrail.\n3D printed buildings\nThe \u2018dream\u2019 solution is that one day buildings would be printed in concrete, onsite, with robots and gantry systems laying down fast drying cement, working 24\/7 to deliver at great speed, affordable structures for housing, offices, barracks, hotels and more. The reality of the limitations of the technology has meant there is more development in the deployment of 3D printing offsite, in a controlled environment, where the temperature and moisture levels can be maintained. This comes with build-size limitations for 3D printed components, as they have to fit on a trailer to be shipped to site.\nThe real hype for 3D printed buildings took off when China\u2019s WinSun demonstrated in 2013 that it could make 10 single storey buildings in 24 hours, using prefabricated 3D printed components. This indeed was an impressive feat, however the design component of the \u2018architecture\u2019 (four walls with a roof) left a lot to be desired.\nWinSun carried on its rapid development and created a 3D concrete printer which measured 20 feet tall, 33 feet wide, and 132 feet long and had created its own special liquid concrete from recycled building material, which was given the name CMS (Crazy Magic Stone). WinSun won over the Dubai royalty with the idea of having the first 3D printed office and so WinSun teamed up with Gensler to design a 250m2 concrete C-shaped cassette structure, with metal reinforcement between the layers, costing $120,000. This was eventually shipped and assembled in Dubai.\nTo hear a first-hand testimony on the project, Jorge Barrero (formerly at Gensler, now at HKS) talked at DEVELOP3D Live in Boston and I highly recommend 30 mins of your time, it\u2019s both informative and hilarious. WinSun has gone on to build massive buildings which have drastically improved on its initial forays and continues to champion 3D printed buildings.\nAI Space Factory is one of those Teslakind- of-firms. Set up in 2017 with the main purpose of competing in an \u2018X Prize like\u2019 NASA competition to design 3D printed habitats for Mars, which the company won in 2019 with a very interesting conical design, material and print process. It is now moving on to design habitats for Earth.\nThe firm is eschewing traditional forms and has devised TERA, TERAmini, ASTRA and EVO habitats, ranging between 500 and 1,000 sq ft, ranging from 1.5 \u2013 3 floors and up to 24 ft high. The company is claiming green credentials using reclaimed and local materials (basalt and biopolymers) and looking towards a carbon negative model.\nIn November 2019, the firm started to print a TERA habitat in-situ at Garrison, New York, on the Hudson River. The birch plywood interior is being made in a factory and will be inserted when the outer shell is complete. It will operate as a short-term rental as they test the insulation materials and longevity. I think these designs are gorgeous, but have no clue as to how liveable they would be.\nUS-based 3D print specialist Apis Cor is one of the leading firms in delivering on the 3D print utopia. i.e. actually printing all walls and floors onsite. It has completed a number of buildings which go way beyond many of the 50m2 demonstration projects you will find on the web. It has developed its own print technology, software and gypsum-based material. It recently completed a twostorey, 640m2 curve-laden administrative building for the Dubai Municipality. The 3D printing arm was moved around the site by crane to build the concrete walls. The company has even worked out a process as to how to print floors and roofs. i.e. horizontal printing over voids. The company estimates a 250 sq ft space takes eight hours to print and would cost $5,100 in print material.\nIn all the case studies I have mentioned, you will have hopefully picked up that if it wasn\u2019t for super-rich Middle- East royalty or mind-blowing space competitions, 3D printed buildings, while capturing the imagination, have yet to gain any real industry acceptance. And there is rarely a repeat order. Someone could easily argue that 3D printing in construction is a solution looking for a problem. But many people thought that about 3D printing in manufacturing and after the hype curve, it is now starting to make the impossible, possible.\nRoyal BAM\nOutside of the University research labs, 3D printing in construction has seen development and limited use in big firms such as Skanska and Laing O\u2019Rourke. But a lot of the development is coming out of the Netherlands.\nTU Delft University, Eindhoven University of Technology, DUS Architects, MX3D, Houben \/ Van Mierlo Architects and Aectual have all been experimenting and building at 1:1 scale, refining their processes. The Netherlands is amongst the global leaders in 3D print research and certainly a leader in the commercialisation of the process.\nRoyal BAM has been working with TU Eindhoven and Weber Beamix since 2015 to develop an onsite and offsite 3D print capability, which uses ABB robot arms. In 2017 the company printed the world\u2019s first fully structurally pre-stressed concrete cycle bridge (8 x 3.5 metres) in six sections, in two days. By 2019, BAM had opened a dedicated 3D print facility in The Netherlands, which can build elements up to 3.5m in height, but theoretically could go up to 25m. However, the limitation is the weight and size to fit on a lorry.\nThe key point with BAM is that while most others see the prize as building cheap houses, the company is primarily focussed on solving the hard problems, like performance, weight and reinforcement because it wants to apply the technique to civils projects which operate under higher loads. Figuring out the strength issue will open up all the existing areas where concrete is used. BAM\u2019s 3D print technology can simultaneously lay metal wire within each layer of a 3D print. Primarily this is to stop cracking, but it does act as a kind of reinforcement as it is added layer upon layer.\nBAM and TU\/e are currently printing and assembling a \u2018freeform\u2019 29 metre pedestrian and cyclist bridge by Michiel van der Kley, for the city of Nijmegen. A Rhino and Grasshopper model had to be created, to allow analysis and optimisation of the expressive form. It required a lot of experimentation and structural analysis to develop the best componentisation and sectioning for assembly and transport.\nFor now, printing offsite, in a factory, is the preferred method, as the environment is completely controlled, but BAM foresees a time when printing onsite will also be common practice.\nAt a recent demonstration, it was pretty clear that BAM believes this technology is now ready for commercial use. It\u2019s currently in talks with Heathrow to use it on the next phase of expansion and London Underground and Thames Tideway have visited to see the \u2018portable\u2019 version in action. Over time, BAM expects the process to become more economical than the traditional concrete methods, perhaps as low as 50% in a few years\u2019 time. And as usage grows, BAM has said it would open factories in the UK and Ireland.\nPrint pros and cons\n3D printed structures offer a range of potential benefits. The primary ones are cost and speed. Should this be making components offsite or printing onsite, there is no doubt that a whole lot of time could be saved in replacing brick and block work.\nIt\u2019s pretty basic, but as there is no need for formwork or shuttering, a whole load of time, effort and on-site wastage can be avoided. According to TU Eindhoven, this could lead to a financial saving of 35% to 60% compared to traditional methods.\nWith the construction industry facing labour and skills shortages, a technology that reduces head count and delivers more with consistent quality, with less, deserves further investigation.\nFor eons we have been slaves to the right angle and those that have gone curved have paid an additional price, 3D printed forms can bring in interesting geometry at no extra cost and produce unique facades.\nWith the greenhouse gas issue, traditional concrete is not our friend. New green materials are in the pipeline which will make 3D printing a better alternative to using today\u2019s cement. Using algorithms, it will also be possible to \u2018lightweight\u2019 structures and optimise material usage.\nIn the negative camp, there are some key problems which need a solution: reinforcement and tensile loads have limited the number of floors 3D printed buildings typically can have, as well as the loads structural elements can bear. There have been some advances in doing this, but performance is still a critical concern.\nRelated to performance, certification and testing of concrete is baked into the way we build. It is challenging to monitor the material to check for stability and strength, even in the manufacturing world, where printing in metal has become an affordable reality. With 3D printed metal voids can form within the unfused material, altering the strength characteristics and potentially resulting in a future stress failure in service.\nIn the half-a-job-bob category: 3D printed buildings can\u2019t really finish a complete building \u2013 at best you get a shell. Work still needs to be done for structural placement, rebar placement, electrics, plumbing etc. A 3D printed house in France took just 54 hours to print \u2013 but it took four months for contractors to add windows, doors and the roof. One only has to look at offsite construction methodologies from firms like Katerra, where wood walls arrive on-site and everything is inside ready to go.\nThere are limitations to what geometry can be printed. In the manufacturing space, 3D prints regularly have to be printed with support structures to hold a 3D print together and are removed afterwards. This is not so easy with cement. Most print jobs will need a strategy of which way round it should be and how to optimise the print to get the required shape.\nWhen printing, you have about ten minutes to put down the next layer before the underlying layer cures to the point that the two won\u2019t properly fuse. I\u2019m not sure what happens if there is a power cut or a nozzle jam and your print is compromised halfway through.\nFinally, the finish. To me, it looks like dry elephant skin. The waffled edges of layers are incredibly rough. While all might be homogeneous within the printed concrete, the outside really needs extra work to smooth out the finish. Even if you didn\u2019t like the 1950s grey Brutalist buildings, the surfaces were at least clean and had an aesthetic.\nConclusion\nAs the construction industry modernises, it is fantastic that major firms are literally rethinking every step of the process to improve delivery time, quality and price.\nWithout moving to 3D, whether that was SketchUp, Rhino or Revit, I don\u2019t think the industry would be considering connecting its design systems directly to fabrication machines. 3D printing is perhaps the most esoteric of these, as it\u2019s been happening in the more traditional steel and wood components for a decade or more.\nIt has to be said that it took humans a while to realise that moving nine metre-tall, 25 tonne granite rocks, 25 miles to the Stonehenge construction site was probably not the best way of delivering material to site. Bricks and blocks have been our \u2018go-to\u2019 solution for over 2,000 years and, guess what? They are optimised for the size of our hands! It\u2019s only right that we should keep looking for alternatives and despite the current drawbacks I am convinced that 3D printing has a role in the new mix of technologies that will be adopted, especially offsite.\nWith major construction firms such as BAM working with Weber Saint Gobain, as well as other building innovators in construction like Skanska and Laing O\u2019Rourke, the technology is set to go mainstream as these firms add 3D printing to their list of available processes. There will be projects where it fits well and others where it doesn\u2019t. As architects continue to push the complexity of form with generative architecture, construction firms are going to be looking at all options to bring them to life.\n3D printing in manufacturing\nManufacturing is decades ahead of construction in its use of 3D and factory-based assembly. It still has a long way to go, but 3D printing has evolved steadily over the last decade from an over-hyped novelty making plastic parts into a more commonplace manufacturing technology, printing in metal.\nLow-volume, high-value parts have risen in popularity among automotive and aerospace manufacturers, with material properties, finishes, testing and quality control hurdles gradually being overcome.\nThe 3D printing hardware itself is developing faster, now that may of the leading names in the technology, such as HP and GE, are also their own biggest customers.\nHP 3D prints parts for its own 3D printers, because the volume\/cost ratio makes sense. GE, meanwhile, has placed almost 300 3D printed parts into each of its new GE9x plane engines, helping push the entire industry forward in accreditations for aerospace.\nTrials by companies like Volkswagen, which enlisted HP and GKN Additive, are acting as a learning curve for mass manufacture \u2013 churning out over 10,000 metal AM model cars for an exercise in part management and quality control.\nFootwear designers are also using production grade 3D printing materials for manufacturing sneakers. Carbon has been working with Adidas to develop an elastomeric polyurethane material used for midsoles and New Balance has partnered with Formlabs to develop \u2018Rebound Resin\u2019 from springy lattices. But mass production is still far away.\nWith 3D printing comes the ability to \u2018print in\u2019 performance, should that be light-weighting or strength. Design tools for this topology optimisation and generative design are now incorporated into the major CAD software for product design as standard.\nThe next big step is happening now, with new software utilising AI to add speed, automation and intelligence to the actual print process \u2013 promising to make even existing machinery work faster with more efficiency.\nStephen Holmes\nThe 3D printed metal car\nHackrod is a California-based company on a mission to 3D print in metal a car chassis. The firm has built a massive 3D print bed, with a combined metal deposition print head and CNC milling arm. As the metal is deposited it is simultaneously machined to produce a refined finish.\nUsing computer simulation, derived from forces when driving a real race car, the chassis geometry was actually \u2018grown\u2019 by generative design. As the aim was always to 3D print the chassis, the design was unencumbered by the limitations of traditional manufacturing processes, which typically comprises metal tubing.\nThe company\u2019s technology is still in development and a presentation on the vision can be seen in this DEVELOP3D Live video by Hackrod founder Mouse McCoy.\nConstruction 3D printing firms\nXtreeE \u25a0 xtreee.eu\nApis Cor \u25a0 apis-cor.com\nCybe Construction \u25a0 cybe.eu\nWASP Crane \u25a0 3dwasp.com\nContour Crafting \u25a0 contourcrafting.com\nMX3D \u25a0 mx3d.com\nVertico \/ University of Ghent \u25a0 vertico.xyz\nAECtual \u25a0 aectual.com\nAI Space Factory \u25a0 aispacefactory.com\nCOBOD \u25a0 cobod.com\nBetaBram \u25a0 betabram.com\nSPECAVIA \u25a0 specavia.pro\nD-Shape \u25a0 d-shape.com\nWinSun \u25a0 winsun3d.com\nBranch Technologies \u25a0 branch.technology\nAi Build \u25a0 ai-build.com\nIcon Build \u25a0 iconbuild.com\/technology\nMudbots \u25a0 mudbots.com\nTotalkustom \u25a0 totalkustom.com\nSQ4D \u25a0 sq4d.com\nTwente Additive Manufacturing \u25a0 twente-am.com\nConstructions-3D \u25a0 constructions-3d.com\n3D printing timeline\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/progress-energy-equip-its-workforce-with-mobile-mapping-field-automation\/","title":"Progress Energy equip its workforce with mobile mapping & field automation","date":1058572800000,"text":"Progress Energy has selected MapFrame\u2019s FieldSmart\u00ef\u00bf\u00bd technology to equip its workforce with GIS mapping data and automate redlining and routing processes. MapFrame will implement a set of four tools at Progress Energy \u2013 FieldSmart View, FieldSmart Sketch, FieldSmart Route and the server solution, Field Flow Manager \u2013 to create productivity gains and manage dataflow between office systems and mobile users.\n\u201cProgress Energy provides electricity to 2.8 million customers across three states. With FieldSmart, the teams will soon have GIS maps and facility data covering this entire territory contained within a single program,\u201d said Jean Jerger, Progress Energy\u2019s Manager Technology Systems Support. \u201cThis will improve response times \u2013 particularly during outages or storm restoration \u2013 and complement our efforts to consistently deliver quality customer service.\u201dAll FieldSmart applications are based on the core map viewing capabilities of FieldSmart View. View maintains a neutral interface to display data from multiple systems (including Progress Energy\u2019s Intergraph GIS, Andersen CUSTOMER\/1 CIS and MDSI Advantex mobile dispatch) in a single, spatially based viewing package. Field Flow Manager operates as the server component, handling data compression and synchronization between end users and Progress Energy\u2019s back-office systems. Additional functionality will be layered over FieldSmart View through Sketch and Route. FieldSmart Sketch is a mark-up system for adding text notes, symbols or annotations to a map. FieldSmart Route creates point-to-point routing by highlighting the optimal path and turn-by-turn directions on screen. Field service workers will transmit redline files and receive map updates across a combination of wired and wireless LAN connections. The FieldSmart software also includes a standard interface to Progress Energy\u2019s mobile dispatch system, MDSI Advantex r7. A FieldSmart mapping link geocodes street addresses from Advantex work requests to display the service location, or route from one work order to another. Users can also add notes through FieldSmart Sketch and send this information with a completed work order.\u201cProgress Energy is a pioneer in field automation, far ahead of the curve in terms of mobile deployment,\u201d explained Paul Wilson, MapFrame President. FieldSmart will quickly help Progress Energy derive greater value from earlier technology investments, such as GIS, work management and mobile dispatch, providing a significant operational advantage.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/reality-capture-modelling\/vercator-slams-the-cloud\/","title":"Vercator SLAMs the cloud: point cloud processing","date":1612137600000,"text":"UK start-up Correvate offers cloud-based registration for laser-scanned point clouds. With a growing trend to mix and match scanning technologies, the service recently added support for SLAM, terrestrial LiDAR and UAV captured data\nLaser scanning is rapidly becom ing a commonly used technology within AEC. While it has traditionally been the realm of surveyors, smaller and lower cost devices have started to drive wider adoption.\nInnovations such as SLAM technology (Simultaneous Location and Mapping) liberate scanning devices from tripods, to enable real-time data capture on the move. This rapidly speeds up capture, lowering the cost, but sacrifices some degree of accuracy.\nWith portable scanning becoming a common practice, users are starting to mix-and-match scanning technology on the same projects. SLAM is also set to benefit from the huge amounts spent by automotive firms, developing automated driving systems and in aerospace for unmanned UAVs, where it provides mission critical \u2018vision\u2019.\nLast year London-based Correvate launched its Vercator service, a cloud platform that automatically registers static-captured, scanned data from a series of overlapping scans.\nThe registration engine, which uses technology developed by University College London\u2019s Department of Electronic and Electrical Engineering, finds multiple features within each scan and auto-aligns the huge data assets.\nCorrevate estimates its cloud service is 60-80% faster than manual alignment and has a simple token-based system for processing data. You can read more about it in this AEC article from April 2020.\nSLAM dunk\nCorrevate has now incorporated SLAM into its service, so AEC Magazine caught up with Correvate\u2019s Charlie Cropp, a laser scan industry veteran, to find out more. \u201cThe first thing to say is that we\u2019re not solving the SLAM algorithm. We\u2019re leaving that to the likes of GeoSLAM, Paracosm and NavVis,\u201d explains Cropp. \u201cWe\u2019re just taking their processed point cloud, once it\u2019s been captured and solved, and uploading the point cloud into our system to then align with other scanned data.\n\u201cWe\u2019ve seen a couple of big shift changes. The first is that people are deploying different arsenal. Firms who\u2019ve traditionally always had a Leica or a Faro static scanner, are starting to run with GeoSLAM or Paracosm scanners and doing data capture to capture different levels of detail, different accuracy, tolerances, to match different project requirements.\n\u201cThere is a realisation that they don\u2019t need to be sending \u00a350\/60\/70k pieces of kit to site when they could do so with a \u00a315,000 to \u00a320,000 solution. So that\u2019s been a shift change in how the industry is working with SLAM data.\n\u201cSecondly, we are also seeing people actually use these devices in the same way they would a traditional scanner. So instead of having long, large linear scans that potentially have drift, they are truncating their capture and keeping that data section quite small.\n\u201cThe challenge with doing that is they end up with lots of small sections \u2014 chunks of scan data which their hardware provider\u2019s software can\u2019t actually manage. They then have to look for an alternative software programme for processing, such as CloudCompare, which is less than ideal.\n\u201cWe\u2019re seeing firms producing \u2018hybrid\u2019 datasets; capturing core areas with a higher accuracy scanners and filling in other areas with a GeoSLAM (or equivalent SLAM) scanner.\n\u201cWe get a lot of people coming to us with mobile data or the SLAM data captured from drones, handheld scanners, and they\u2019re wanting to align them all together. Our core algorithm initially came out of the static scan world where the centre point, the zero point, was in the centre of the scan. With SLAM that centre point can be outside of the scan data and we would typically see that data fail. We\u2019ve been able to enhance our algorithm and go from a 0% success rate to 100% success rate, enabling automated registration for SLAM data as well as static data.\u201d\nSLAM \u2013 Data trade-offs\nSLAM has always been seen as \u2018dirty\u2019 or \u2018noisy\u2019 data as it\u2019s far from clean and usually not colourised. Cropp shared her thoughts, \u201cLaser scanning has always been a compromise. There\u2019s always been trade-offs. Do you want data quality? Do you want speed of capture? How much data do you want? How much can you work with? And those three key elements are still relevant today.\n\u201cLooking at a GeoSLAM scanner, what actually are you trying to do with it? I get asked quite a lot \u2018which scanner should I be buying?\u2019 And I always say \u2018ignore the hardware for the moment, start with the workflow\u2019. Start by thinking, \u2018what am I trying to do? What\u2019s the purpose of scanning? What\u2019s going to be 90% of what I do and capture, and why and what\u2019s the deliverable I need?\u2019 If you need to deliver to Autodesk Recap, work back from the deliverable and that will then dictate your hardware.\u201d\nIt\u2019s clear Correvate is seeing a change of attitude in scanning. While SLAM might not be as accurate as it would like, firms are weighing that up. Do they need to be as accurate as a Leica or as clean as a Z+F scanner and spend the \u00a370\/80k on each scanner? Or can they actually increase productivity using SLAM to do more work, while still achieving what they need from a deliverable perspective. If you can be four times more efficient or productive because you are able to capture or generate four times the number of projects, many firms are easily making that call.\n5G potential\nWhile scan registration in the cloud means firms get the benefit of large processing power, the other advantage would be having real-time registration fed live from the survey site.\nWhen we first met Correvate, 5G had yet to be rolled out and it was somewhat of an unknown. Now that 5G is starting to become available and phones are capable of submitting large amounts of data quickly, the reality of feeding Vercator from site is a possibility.\nAEC Magazine asked Cropp if there had been any uptake of 5G. Cropp explained, \u201cNot as yet. I think we will start to see a difference when there is actually a hardware shift. With something like the Z+F scanner, they have a tablet which can synchronise to the scanner. It essentially pushes out the scans complete from the hardware to the tablet.\n\u201cWhat we anticipate, and what we\u2019d like to see a bit more adoption from the hardware vendors, is actually making that push out from the scanner to the tablet, and from the tablet to the cloud. If that starts to become commonplace, that will drive 5G use.\n\u201cThere\u2019s two things here. The first is getting data into the cloud in the first place, and the second is connecting the cloud to your downstream production.\nIf we can get data into the cloud more readily, and here there will be the naysayers saying \u2018laser scans in the clouds, they\u2019re too big\u2019 but actually if we can send the data up in truncated chunks, almost without their knowledge, the data is in the cloud ready to work with. \u201cIf we can then connect that data to our algorithm, which actually knows the order of the scans, from the timestamps, you can plot that route and resolve the registration.\n\u201cThe other thing that will be key for us will be the downstream connectors. If we can securely connect to Autodesk\u2019s or Bentley\u2019s cloud via APIs, we can start processing point cloud data that\u2019s held there. That will also be another huge turning point in the market.\u201d\nScan-to-BIM\nAt the moment Scan-to-BIM, where point clouds are automatically turned into building information models, is still a pipe dream. However, there are a number of companies working on trying to bring this to reality.\nCorrevate has declared that it intends to compete in that market. This is data alchemy, turning the dumbest data possible \u2014 3D points \u2014 into intelligent walls, doors and windows.\nCropp told AEC Magazine that the company is aiming for its first beta in March, as in-house development resources scale up. To improve the quality of scan-to-BIM, it\u2019s important to get as much data out of the scanner as possible. This requires access to APIs, colourised registrations etc. and some scanner companies are reluctant to give potential competitors deep access.\nCorrevate is vendor neutral but will work best with firms who are open with their developer communities. Some scanning firms are very defensive of their high-cost point cloud software. Registration is the start point, but the company intends to offer additional segmented offerings from deep analysis of identified components within a scan.\nConclusion\nFor a long time, we have been waiting for better price competition in the laser scan market. Faro was the first to bring out a highly portable scanner for around \u00a320,000, then Leica brought out the \u00a315,000 BLK360, but we have not seen a rush to commoditise professional point cloud capture.\nWhile we are now starting to see LiDAR on phones and tablets, it\u2019s a long way from the accuracy and density required for professional applications. For now, the market is adopting a hybrid approach and using the best technologies depending on need.\nThe SLAM market appears to be winning the productivity stakes and is carving its own niche. With Correvate\u2019s Vercator cloud based services now being able to handle SLAM scans surely this will only accelerate that trend? AEC Magazine looks forward to seeing Correvate\u2019s Scan-to-BIM technology later this year.","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/sirfstariii-powers-garmin\/","title":"SiRFstarIII powers Garmin","date":1125532800000,"text":"SiRF Technology Holdings, a provider of GPS-enabled location technology, has recently announced its flagship SiRFstarIII architecture is being used by Garmin International Inc., a unit of Garmin Ltd. to provide GPS positioning capabilities for the Edge, a new line of integrated personal training systems designed for recreational and advanced cyclists. The Edge is the first Garmin device to incorporate SiRF\u2019s flagship SiRFstarIII architecture. With its 200,000 correlators combined with sophisticated navigation algorithms, the SiRFstarIII architecture sets a new benchmark for fast and deep GPS signal search and track capabilities, making it possible to achieve very fast and robust positioning even through dense foliage and urban canyons.\nEmploying a completely new design specifically for cyclists, the Edge 205, Edge 305HR and Edge 305CAD are compact, lightweight, waterproof units that monitor pedaling cadence, heart rate, vertical profiles, climb and descent, altitude, speed, distance and time. SiRF\u2019s SiRFstarIII is designed to provide fast, accurate location data for all of the new Edge products. In addition, the Edge 205 relies on the SiRFstarIII for accurate altitude information. Because the Edge is GPS based, the information may be overlaid onto a cyclist\u2019s course map, including hills and curves, making it easy for cyclists to analyze every aspect of their workout. SiRF Technology, Inc. is a wholly owned subsidiary of SiRF Technology Holdings, Inc. and is a supplier of GPS enabled location technology for high-volume mobile consumer devices and commercial applications. Founded in 1995, SiRF is headquartered in San Jose, California, U.S.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/visualisation\/designviz-central-bank-of-iraq-in-baghdad\/","title":"Designviz: Central Bank of Iraq in Baghdad","date":1605571200000,"text":"In the case of the new 172m high Central Bank of Iraq, the futuristic setting devised by creative studio Arqui9 only increases the drama of its curving form\nThe brief for this visualisation came as part of a series of studies to showcase the engineering side of the buildings featured; emphasising the client Newtecnic\u2019s ingenuity for constructing the future.\nIn addition to the typical focus on the structure, renders feature futuristic construction systems, robotic materials delivery and holographic blueprints \u2014 highlighting potential upgrades that the building could embrace in the future thanks to Newtecnic\u2019s engineering.\n\u201cEach portrait represents a particular phase of construction in the retrofit process,\u201d says Arqui9 creative director Pedro Fernandes.\n\u201cAlong with Newtecnic, we thought long and hard on how we could integrate various types of technological advances into each image, making sure we kept with the integrity and language of the project.\u201d\nIn the visualisation, elements like the hybrid solar voltaic facade panels are enhanced more than they would be in other renders, as well as revealing the constructive systems below.\nFernandes says that it was important for Arqui9 to consider that buildings \u2014 such as the CBI, which has a delivery date of 2021 \u2014 have a huge lifespan and will need to adapt to the environment and times throughout in order to remain relevant and sustainable.\n\u201cWe imagined a world where most of the works would be automated and symbiotic, where engineering takes a step forward to aid humanity and its inherent need for advancement.\n\u201cFocusing on the importance of good engineering for a sustainable future of the landmark building.\u201d\nFollowing initial storyboarding of ideas and scene concepts, the model data was initially provided to Arqui9 as a Rhino model from Newtecnic, which was then exported into FBX format and into Autodesk 3ds Max.\nThe studio, based in London, then began to add 3D elements, compose the scene and add the humans and objects that inhabit the render.\n\u201cExcluding the building we had no context to go off and had to reinvent the future of the place and the surrounding buildings and context machinery, which also proved to be quite tricky at times,\u201d recalls Fernandes.\nAided by the realtime rendering in Corona Renderer, Fernandes\u2019 team was able to develop and iterate several quick mood concept designs.\n\u201cWe explore the scene and find a composition that fits best with our idea,\u201d he says. \u201cMany times, formulating a quick sketch on these in order to quickly develop and compose a few key shots.\u201d\nFrom these, a pair of images might move through to the next stage, with client approval, to the adding of textures and other refinements of the 3D model.\nThe next stage takes place in Adobe Photoshop, in order to add additional atmosphere and mood. \u201cThis is where we like to say \u2018the magic happens\u2019!\u201d laughs Fernandes.\n\u201cWe commence with a raw render straight out of 3ds Max, compositing various render passes, including reflections passes, ambient occlusion passes and Z-depth pass.\n\u201cWe complement these with a lot of digital painting, of clouds, haze and atmosphere as well as using additional images from our photo reservoir, like sun and sky, as well as overlays.\u201d\nThe goal of focusing on futuristic technical elements created some challenges for the team \u2014 namely imagining how the world would look and how construction work would be carried out in the near future.\nArqui9\u2019s creative director concludes that from the very beginning they knew they were working with future thought provocateurs in Newtecnic.\n\u201c[They] wanted to push and test the boundaries of what engineering and architecture represented for humanity and its potential forthcoming technological advances.\u201d\nExpertly, Arqui9 matched this with images that encapsulate a gritty and atmospheric future that is perfectly believable.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/structural-engineering\/data-driven-design-for-structural-engineers\/","title":"Data-driven design for structural engineers","date":1591660800000,"text":"Parametric design can offer many benefits to structural engineers, from the automation of repetitive tasks and easier handling of last-minute design changes to the opportunities it provides for the creation of the geometrically complex, explains Trimble\u2019s Stuart Campbell.\nParametric design, or data-driven design, is perhaps more com monly associated with the architectural discipline. \u2013 However, while architects have been open to and familiar with parametric design for many years, this is not to say that engineers are completely new to the idea. Whereas architects have made use of parametric modelling to achieve geometric complexity in their designs, an engineer can perhaps achieve far wider and greater benefits with this parametric or data driven approach \u2013 benefitting not only their business and its processes but also their service to clients.\nRather than architects being the dominant leader in parametric design and modelling, therefore, it would be perhaps more accurate to say that, while both disciplines are familiar with the approach, it is engineers who are increasingly starting to explore and further develop the value that such a data-driven and parametric way of working can deliver. Nor is parametric design only reserved for the major, signature construction projects; instead, it is becoming widely accessible across the engineering sector, used on the everyday developments as well as the extraordinary.\nAutomation\nWhile every construction project is different, and no two buildings are the same, the processes involved, particularly from a structural engineering perspective, are often identical; therefore, lending themselves well to automated processes.\nTo demonstrate, any building, regardless of its function, will often be repetitive in its design, featuring any great number of identical steel components, such as beams, columns or plates.\nFrom a modelling perspective, each of these identical components would have to be added into the model separately, which is not only hugely repetitive, but would also take a considerable length of time. However, practically speaking, if manual and repetitive tasks and calculations could be automated, the amount of time and resources able to be invested more efficiently elsewhere on a project would be significant.\nWhile existing CAD tools, such as Tekla Structures, may already have some element of parametric and automated capabilities, parametric design can take this functionality to the next level.\nConsider a series of steel columns, each identical in form and required to be a set distance apart from each other. With a parametric design tool, such as Grasshopper, you would simply be able to input the required parameters \u2013 including the x-axis value \u2013 and multiple, identical columns would be automatically generated as an output in the corresponding model, spaced out in accordance with the written script. However, the value of automation is perhaps most keenly felt during the structural analysis. As engineers, a large majority of time will be spent reviewing analysis data, whether to verify that the building meets a dictated set of structural requirements or to determine the set of variables that provides the optimum structural design. Indeed, with clients becoming more demanding by desiring the most efficient building design in terms of mass material cost and carbon footprint, engineers are under increasing pressures to satisfy these requirements. With a manual workflow, determining the most efficient design is ordinarily an incredibly time-consuming process, involving the engineering team having to manually change each variable, run an analysis, note the results and then repeat an indefinite amount of times. Whereas, by incorporating parametric design into your workflow, this process is transformed.\nFor example, on a recent project, the engineering team used the Grasshopper and Rhino live-link with Tekla Structural Designer to analyse deflection in a proposed truss system. Through the Grasshopper script, the team were able to run various design iterations at speed. The software then listed the deflection results for each and automatically highlighted the set of variables that resulted in the most efficient design \u2013 in this case, the optimum balance between minimum steel weight and minimum deflection.\nA significant time-saver, parametric and computational design signals a shift from manual and repetitive work to more creative engineering. Even using a single automated mathematical function in the place of a repetitive task would have a notable result, enabling you to spend more time on providing the client with added value services on the project.\nGeometrically complex designs\nThis idea of creative engineering is another exciting benefit of parametric design. Just as the time and resources saved, as a result of automation, can be used on improving a building\u2019s cost or carbon efficiency, the same time and resources can also be invested in creating a more geometrically complex and aesthetic form from the start. Used correctly, parametric modelling can be a means of allowing computer software to drive geometry creation for structurally complex steelframed buildings, ones that would have been incredibly challenging \u2013 if not impossible \u2013 to model and analyse manually.\nAs well as allowing engineers more time to be creative and push the boundaries of structural design, parametric design also enables otherwise challenging structures, such as complex building facades, to be modelled at a far faster rate.\nSantiago International Airport in Chile is but one example. Manually designing the soffits and new, curvilinear roofing system for the first terminal building, took the engineering team six months to complete in Tekla Structures. For the second terminal building, the Grasshopper live link was used in conjunction with Rhino and Tekla Structures and the soffits and roofing system were modelled in just one week \u2013 a huge time saving\nCivil engineering, such as the construction of road and rail bridges and tunnels, is another area where parametric design is particularly beneficial, often being challenging and structurally complex in nature.\nWhen designing the Paris Metro line 11 extension, due to the track variability and the tight tolerances involved, it was clear that a manual insertion of all elements would be a time-consuming process. As such, a bespoke Grasshopper file was developed, by using the live-link between Tekla Structures and Grasshopper, in order to automatically place the elements, so that they were correctly aligned with respect to the rail track. The Grasshopper file included a coordinate transformation, which allowed the structural elements in Tekla to be close to the origin of the model, as well as give the georeferenced coordinate of each element modelled. By doing this, it enabled close cooperation between the BIM modeller and the surveyor, who was responsible for tracking the installation points of the elements.\nLast-minute design changes\nA common challenge on any construction project, last-minute design changes can be incredibly time-consuming for structural engineers to resolve; as well as involving a high degree of repetition, particularly on the more complex BIM models. As the very name implies, such requests will commonly occur towards the end of the design stage, where the model, drawings and schedules are almost complete and contain a high amount of information-rich data. As such, while the change may initially be to just one structural component, the likelihood is that there will be a significant knock-on effect on all of the connected components, making resolving the change and ensuring that the updated model is still constructible both a long and intricate process. With parametric design, the building or structure has already been modelled in a connected way. All parameters, inputs and outputs, are interconnected and the data relationships are maintained, meaning that if you have to change a single parameter, all the associated components automatically adjust in line with the new inputted data.\nAs aforementioned, in many ways, some 3D modelling software, such as Tekla Structures, already have inherent parametric capabilities, with this same idea of a continuous data transfer link at its core. For example, should you change something in a Tekla Structures model, the associated schedules and 2D drawings are also automatically updated. By incorporating a parametric tool, such as Grasshopper, into the workflow, you are essentially adding a new link at the start of this data chain, enabling data to be efficiently transferred and remain connected throughout the whole design process, from model through to drawings.\nIntegration\nThis idea of an integrated process is central to parametric design. Not just in terms of the integration within parametric design itself but also the position of parametric tools in the structural design process, with data being transferred from the computer script to the model and even on to the schedules and drawings. In fact, it perhaps even goes beyond this, enabling greater integration between disciplines in the context of the wider project and a more controlled flow of data through the project\u2019s overall Digital Plan of Works.\nUsed by both architects and engineers, parametric modelling has the potential to be the driving force behind architectural and structural design, making digital processes more feasible and allowing design data to be better connected throughout a project. However, this ideal does not come without its challenges, for in order to maximise the greater levels of integration that collaborative parametric design can deliver, the same level of quality and accuracy of data is required throughout the construction process, from the architect through to the engineer and beyond.\nFuture engineers\nIt is clear that the benefits of parametric and data-driven design are significant, automating traditionally repetitive workflows and enabling engineers to instead focus more time and attention on delivering added value on a project, whether in terms of design optimisation or a structure\u2019s geometric complexity. That said, it is important to note that such technological advancements cannot replace engineers themselves. Often a concern within BIM and the wider construction industry, there is a tendency to fear that, as technology develops, traditional skills will become null and void, with people instead relying solely on technology.\nYet, contrary to this opinion, as advanced as our modern-day computer and software scripts are, they are of no value in isolation. The successful incorporation of parametric design is more involved than simply running a script and letting the computer generate its own outputs, with no boundaries or controls. In order to use parametric design tools both correctly and successfully, the knowledge and expertise of an experienced engineer is essential. Only then can you control and have an understanding of the required parameters you are inputting, the variables you are analysing and validate the outputs from the script.\nMore recently, this increased focus on computational design is resulting in new graduates entering into the engineering industry who now possess greater computational programming skills. These new skills, technology and software represent an exciting step forward within the BIM and engineering industry and, when used correctly, can further expand and augment \u2013 not replace \u2013 an engineer\u2019s knowledge, perhaps in the same way that the introduction of the calculator would have done many years ago.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/video-nxt-bld-2019-mikolaj-bazaczek-herzog-de-meuron\/","title":"Video: NXT BLD 2019 \u2013 Mikolaj Bazaczek, Herzog & De Meuron","date":1565049600000,"text":"VR+ARCH: workflows in past, present and futuren \u2013 NXT BLD London, June 2019\nVirtual Reality at Herzog & de Meuron \u2013 first used for serious project purposes in 2015. Nowadays, reaching more and more users, VR hardware and software is evolving rapidly, constantly forcing us to rework our workflows and approaches. We are experiencing a full spectrum of curses and blessings that VR brings upon architects and their clients. In this talk, we\u2019ll focus on evolution of VR workflows and how VR can provide additional value to architect\u2019s everyday work. Used both for design development and client presentations, VR provides an unmatched platform for communicating architectural design. This will be a journey through time, different requirements and presentation flavors, various techniques and technologies.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/reality-modelling-contextcapture-and-the-pope\/","title":"Reality Modelling, ContextCapture and the Pope","date":1449446400000,"text":"A technology that can create mm accurate \u2018reality meshes\u2019 from photographs looks set to give laser scanning some serious competition. By Greg Corke and Martyn Day\nIn early 2015 Bentley acquired a French developer called Acute3D, which created a product called \u2018Smart3DCapture\u2019. The software essentially converted high resolution photographs into accurate, bitmapped 3D meshed models through photogrammetry.\nA similar technology was championed by Autodesk under the 123D Catch brand. This was actually a past licence deal between Autodesk and Acute3D. So Bentley also appears to have pulled off a bit of a coup.\nWhile laser scanners remain high ticket items, it seems lots of humble photographs when combined with good algorithms can produce up to 1mm accuracy meshed surveys of buildings, landscapes or cityscapes, combining aerial photos from drones of helicopters, together with terrestrial photographs.\nIt is possible to rapidly create lifelike 3D models out of photographs. At YII in London we were shown real-world uses of Acute3D\u2019s impact on civil, city, power, and planning, all within a seamless Bentley ecosystem. It was incredibly impressive.\nGreg Bentley explained that at the beginning of 2015 Bentley, while demonstrating MicroStation Connect Edition to an existing client, ESM Productions, also showed Acute3D\u2019s technology.\nIt turns out ESM Productions had been tasked with planning the Pope\u2019s visit to Philadelphia in September 2015. At 60 acres, the size of the public venue was huge so it was a Herculean task.\nBetween Bentley and ESM, a plan was hatched and Bentley took thousands of photos of Philadelphia from ground level and from helicopter. A reality model was then created for the design team to work off.\nConsidering Acute3D had only just been acquired by Bentley and it had yet to be properly integrated into MicroStation this also put some pressure on the Bentley development team.\nThe net result was 28,000 photographs (28GB of data) turned into one reality mesh, from which one square mile was refined.\nESM Logistics imported the mesh into MicroStation and the company\u2019s designers set-to adding in the 56,400 temporary facilities in detail using MicroStation\u2019s standard 3D and 2D commands. The resulting models can be easily shared via a browser or tablet.\nThe success of the use of reality modelling for ESM Productions meant that the company now wants to use it on all of its events, irrespective of size. Having sweated alongside its customer on this major job, Bentley has also speedily integrated the technology into MicroStation and produced a flavour called ContextCapture.\nScott Mirkin, co-founder and executive producer, ESM Productions, said of Bentley ContextCapture, \u201cIn the end, we experienced dramatic risk reduction, better decision making, exceptional timeliness, and greater efficiency. The goal we set with Bentley to test the applicability of reality modelling as a mission-critical event planning technology was completely validated, and we are now planning to offer this new value to our clients going forward. In fact, we were so impressed that we are creating a documentary highlighting our use and the outcomes we achieved.\u201d\nThe photographs for this project were all taken in one day and processed overnight, which would have been impossible to achieve with traditional laser scanning methods.\nIt seems that laser scanning \/ point clouds has some serious competition.\nThere are a number of interesting applications from as-built modelling to site progress monitoring (it is a whole lot easier to mount a consumer camera on a drone than a laser scanner). However, photogrammetry\u2019s key requirement is that it needs light, so not ideal for overnight surveys or when down a mineshaft!\nAt the recent Bentley Year In Infrastructure event in London, Greg Bentley took the Philadelphia reality model one stage further, showing one of Foster and Partner\u2019s designs for Comcast\u2019s new HQ in context within an Acute3D generated city model.\nWe were then taken inside the building and using LumenRT various lighting and shading options were demonstrated, as seen throughout the year with views out over Philadelphia. The potential for conceptual design and checking design options \/ impacts is truly mind-blowing.\nContextCapture uses GPUs to process the hundreds or thousands of photos needed to create a reality mesh. To speed processing time for very large models (typically involving more than 30 gigapixels of imagery) users can use the dedicated ContextCapture Center in the cloud, which is architected for grid computing with multiple GPUs.\nThis article is part of an in depth technology feature on Bentley Systems.\nRead Bentley merges design with reality.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-uav-to-cloud-solution-forged-with-autodesk-platform\/","title":"NEWS: UAV-to-cloud solution forged with Autodesk platform","date":1457395200000,"text":"3D Robotics uses Autodesk Forge to develop inspection, survey and site scanning solution for unmanned aerial vehicles\nLeading drone maker 3D Robotics (3DR) is using Autodesk\u2019s Forge platform to develop a new UAV-to-cloud solution that enables construction professionals to perform inspections, surveys and scans of sites from its unmanned aerial vehicles.\n3DR is an early adopter of the Forge platform, which consists of a set of Autodesk cloud services, APIs, and SDKs for developers to quickly create data, apps, experiences, and services.\nThe company has used Autodesk\u2019s ReCap Photo Web application programming interface (API) to develop its reality capture solution. 3DR\u2019s SOLO drone captures data and it is then processed in the cloud by the Autodesk ReCap engine. The resulting reality data (3D point cloud, 2D orthographic views, 3D mesh) is made available on Autodesk A360, from where it can be downloaded to be consumed by Autodesk software to support various use cases such as monitoring sites, measuring stock piles, getting the as-built context, survey and mapping, etc.\n\u201cWe are excited to collaborate with 3D Robotics to help field service professionals collect information and make insightful decisions across a variety of industries,\u201d said Amar Hanspal, senior vice president, Products at Autodesk. \u201cFrom the integration of Site Scan and Autodesk cloud services, to their use of the Forge platform, 3D Robotics is opening up new ways of working for our architecture, engineering and construction customers.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/school-children-inspired-by-aec-technology-at-learning-day\/","title":"AEC technology inspires school children at learning day","date":1551744000000,"text":"11 to 14 year olds from the North East educated in exciting new technologies for the construction sector\nLast week in Newcastle, local school children aged 11 to 14 were given a taste of what it\u2019s like to have a career in the construction industry working with new and emerging technologies. The learning day, hosted by design firm Space Group and Gateshead College, aimed to educate young children in careers available to them when they leave school and, importantly, help address the technological skills shortage in the construction sector.\nRob Charlton, CEO of Space Group and the person behind the Inspiration day, said: \u201cWe have found that there is little understanding amongst young people of what opportunities are available within the built environment professions other than the typical out-door builder roles, and as such few school children are aspiring to follow to a career in this important sector of the British and global economy. Which is why we set up the Inspiration Day, to work with schools in educating future construction professionals and change the lives of young people.\n\u201cThanks to the extraordinary support of industry leaders, professional bodies and progressive education, in the North East, we have delivered an inspiring programme that shows young people what construction is really like, it\u2019s fun and exciting, and uses only the very best technology has to offer today.\u201d\nSeven schools from across the region engaged in exercises in architecture, engineering, virtual reality, robotics and 3D modelling. delivered by Autodesk, FARO, Gateshead College, Northumbria University, NBS and the George Clarke charity MOBIE.\nJen Brown, a teaching assistant from Monkseaton Middle School said: \u201cWe selected children from years 7 and 8 who are studying STEM courses in school and who have already identified they are interested in construction when they leave school, they just lack clarity on what is exactly available to them. It\u2019s days like today that make it very clear that technology is front and centre of the way we build and that is what interests the children most.\u201d\nAutodesk began its session with the school children by asking them to build houses from Lego; they then partnered with Northumbria University, using their own BIM 360 software, to scan the Lego houses, creating 3D models which they could then manipulate on the iPads provided.\nMartin McHugh, head of department for design and technology at Washington Academy said at the event: \u201cAll of the pupils we have brought today are studying Level 1 and 2 Design in the Built Environment, an architecture based vocational course. The children have a keen interest in architecture and the exercises today on house building have been the perfect choice in sparking their imagination. Many of the children understand the concept of architecture but not the technologies behind it.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/24-from-the-show-floor-aitodesk-university\/","title":"24 from the show floor","date":1580774400000,"text":"Greg Corke reports on 24 firms (and even more technologies) that caught his eye at the Autodesk University Expo\nAutodesk University is much more than just a conference. The expo provides an incredible opportunity to explore some of the very latest technologies. This year AEC firms took the lion\u2019s share of the show floor, with Autodesk\u2019s new focus on construction attracting many exciting newcomers to Las Vegas.\nAvail\nMany Revit users share the frustration of searching for Revit family objects. Structuring data in folders can be good for storage but terrible for retrieval. Avail is a content management system designed to help firms take control of their digital assets. The software sits as a layer on top of your existing file network and extracts metadata from Revit and other applications to make it easier for teams to find content. Using a plug-in users can search for content within Revit, then drag and drop the content into the Revit model. There are some neat management tools that let users add comments to content to flag issues with a particular asset so they can be fixed later. Managers can also use the system to track asset usage.\nAvail can work with any asset but currently has deep integration with Revit, AutoCAD and Rhino. Next year this will extend to Civil 3D, 3ds Max, MicroStation and (hopefully) Sketchup too.\nAt AU, Avail announced an agreement with collaborative file and data management specialist Panzura so its technology can be used more effectively by distributed design teams.\nCADbox\nCADbox offers a simple solution for sharing AutoCAD and Revit files with other users over the Internet. It works a bit like Dropbox or Box insofar as it creates \u2018Sync folders\u2019 on the Windows PCs of collaborators and files are replicated in everyone\u2019s sync folders.\nWhen a user wants to make changes to a shared file, CADbox acquires a \u2018File Lock\u2019 for the user, so only their changes are synced to the CADbox cloudserver. There\u2019s also a CADbox marker iPad app that lets you view and markup CAD drawings.\nChaos Group\nChaos Group didn\u2019t have its own stand at AU, but on the Nvidia booth it was showing how V-Ray Next GPU is now able to use the dedicated ray-tracing hardware within Nvidia\u2019s RTX GPUs to speed up production rendering. The first applications with this capability are V-Ray Next for 3ds Max and Maya. Support for other V-Ray products is coming in the future. Also on show was Project Lavina (soon to be in beta). This will be a separate Chaos Group product for exploring and manipulating V-Ray scenes within a \u2018100% ray-traced environment in real-time\u2019 using Nvidia RTX GPUs. C\narlos Cristerna, RadLab Director at creative agency Neoscape has been trying out the technology and told AEC Magazine that it will be a game changer for his company\u2019s review process with its clients. \u201cThe review process today has to come to a screeching halt, to gather all the assets, put them together, then review it, so it\u2019s not effective. I think Lavina is going to allow us to not have to do that.\n\u201cOn top of that, it\u2019s also going to allow us to review the actual look and feel [of the project],\u201d he said, adding that current reviews often need to be done at low res and with lighting approximation.\nCupix\nIn the AEC industry there\u2019s currently a massive focus on laser scanning, but for some workflows a 360\u00b0 photo is enough \u2013 and the data is much easier to capture with an affordable 360\u00b0 camera like the Ricoh Theta.\nCupix offers a cloud-based service that enables construction firms to create a photographic record to document site conditions or to monitor construction progress. The resulting \u20183D tour\u2019 can then be aligned to a BIM co-ordinate system and compared to the as-designed BIM model.\nThe comparison feature is very slick and uses a split screen where the 360\u00b0 Tour and BIM model are synced together, so when the user changes the view in one window, it automatically adjusts in the other. Measurements can also be taken off both the BIM model and the 3D tour.\nCupix supports Revit, Navisworks and SketchUp files, as well as IFCs.\nFaro\nFaro showed a whole raft of new innovations, from mobile reality capture and autonomous laser scanning to design verification and Scan-To-BIM.\nThe Faro Swift is a new mobile scanning solution that combines its Faro Focus laser scanner with its Faro ScanPlan 2D mapper. Both devices are mounted on a tripod, along with a phone, tablet or laptop, and the system is then wheeled around at normal walking pace to capture the reality on site.\nA Faro spokesperson described the technology as being SLAM(ish), referring to the Simultaneous Localisation and Mapping technology used in handheld mapping devices, self-driving cars and Boston Dynamics\u2019 Spot robot. Accuracy is quoted to be under 2cm and depends on how fast the device is wheeled along. At the moment the output is monochrome, but colour scans are \u2018on the table\u2019 for the future, as is an integrated, more compact device.\nFaro reckons the Faro Swift enables kinematic 3D scans to be completed up to 7x faster than a series of traditional, fixed point scans over comparable areas. Additionally, as the mobile scanner includes a fully functioning Faro Focus, it can be used as a fixed scanner. Users can switch to \u2018high-fidelity scan mode\u2019 in real time for \u2018seamless integration\u2019 with mobile scan data.\nFaro also gave an update on where it\u2019s at with Boston Dynamics\u2019 Spot robot with a view to delivering a solution for fully autonomous laser scanning. It\u2019s still early days but the basic idea is to use the robot\u2019s mission function to first walk it through the construction site to create an initial 3D map, then define waypoints for it to go through and locations for it stop and scan. In the future, Spot could potentally be used for continual scanning using Faro Swift which is something Faro\u2019s lab team is working on.\nMichael Perry of Boston Dynamics explained that Spot will also be able to conduct autonomous missions on a changing job site using SLAM, \u201cWe\u2019ve been through sites where, one day, the drywall framing is up, the next day, the drywall is installed, and we\u2019re trying to make the robot\u2019s ability to navigate these spaces as robust as possible, so you don\u2019t have to continuously remap. And that\u2019s one of the efforts that we\u2019re doing, together with Faro,\u201d he said.\nSpot stands at thigh height, and while a Faro spokesperson admitted that laser scanners can benefit from a higher viewpoint, he reckoned an elevation of around a foot should be sufficient for most scans. Height becomes more important if the scanner needs to see above obstacles like a table or desk, but Spot could stand on tip toes to achieve this. Scissor lifts could also offer a solution.\nSpot is generally very stable and can walk over rugged terrain, but it has been known to fall over. To protect the laser scanner, Faro is working on a roll cage that will be attached to the frame of the robot. If Spot does take a tumble, it\u2019s able to get back on its feet automatically. At AU we saw many examples of how mixed reality can be used for design layout and construction verification using a headset like the Microsoft HoloLens. Faro is taking a different approach with its Faro Tracer, which projects a laser outline of a design onto a physical 3D surface or object so everyone can see it. The idea is that these \u2018virtual templates\u2019 can be used by workers in a pre-fab factory or on an actual construction site to quickly and accurately sequence work and position components. With reinforced concrete, for example, it could help workers precisely place formwork and rebar.\nIt looks like an exciting technology and one that could lead to construction layout and construction verification being done at the same time.\nFaro also demonstrated As-Built Modeler, a new Scan-To-CAD or Scan-To-BIM software that is designed to minimise the effort and time required to create as-built documentation. The software can import as-built reality data as point clouds or meshes from Faro and other scanning solutions.\nHoloBuilder\nHoloBuilder has been working with robotic manufacturer Boston Dynamics for some time now but at AU formalised the partnership and marked the launch of SpotWalk, a robotic construction capture solution.\nSpotWalk equips Boston Dynamics\u2019 Spot robot with \u2018autonomous\u2019 360\u00b0 image capturing technology, allowing customers to remotely walk a site and \u2018instantaneously\u2019 create a \u2018living digital record\u2019 of their construction projects. HoloBuilder says the technology is incredibly easy to use.\nThe SpotWalk app has two general modes. The first enables project teams to teach Spot the capture route simply by driving the robot via a smartphone interface. The second mode drives Spot autonomously on its trained path, taking pictures along the way at defined capture locations. Images captured by SpotWalk are automatically analysed by HoloBuilder\u2019s machine learning engine, SiteAI.\nGeneral contractor Hensel Phelps has already begun testing SpotWalk on the $1.2 billion San Francisco Harvey Milk Terminal 1 Airport project.\nLeica\nLeica had a couple of major announcements surrounding its 3D handheld imager, the BLK3D, that allows users to take precise 3D measurements from any 2D image it captures.\nFirst up, the company launched BLK3D Web, an online collaboration workflow that allows users to share the 3D measurable images online. It means anyone with access to the shared link can measure and mark-up the image files without requiring additional software.\nLeica also announced a BLK3D integration with Autodesk BIM 360 Docs which will enable BIM 360 users to use BLK3D measurable images in the issue creation and resolution workflow.\nOpenSpace\nThere\u2019s some incredible technology out there for capturing construction sites but many require specialist skills. The approach of San Francisco-based OpenSpace is to make things so easy that anyone can use its technology.\nOpenSpace uses a 360\u00b0 camera fitted on top of a standard construction site hard hat and then lets its Vision Engine technology do the rest, capturing 360\u00b0 video and photos \u2018passively\u2019. All the user does is hit record on their smartphone, then walk the site. Photos are captured every half-second and are \u2018automatically\u2019 tied to project plans using AI.\nThe technology can be used to document site conditions over time. There\u2019s also a BIM Viewer that can help improve coordination by enabling side-by-side photo-to-model comparisons.\nOpenSpace reckons the technology can reduce physical sites visits by up to 50%.\nAt AU, OpenSpace announced a new integration with Autodesk BIM 360, which is designed to streamline RFI and QA\/QC processes for construction teams. The software is also integrated with Procore and PlanGrid.\nParacosm\nThe Paracosm PX-80 is a handheld SLAM-based 3D mapping solution that incorporates LiDAR, colour imagery, and IMU (inertial measurement unit) data to capture hi-res point clouds in colour. It\u2019s particularly well suited to complex indoor environments but with a range of 80m can also be used outdoors.\nThe PX-80 comes with Capture, an iOS scanning application that runs on an iPad attached to the scanner. Through the app, users can see points being collected in real time, with a heat map showing where there\u2019s good coverage and where more data is required, so you don\u2019t leave the site with incomplete data. The scanner can collect 300,000 points per second and quoted accuracy is 1cm to 3cm.\nParacosm is putting a lot of resources into software development and is currently working on a solution that will allow captured data to be tied into ground control points, and then post processed to increase accuracy.\nMost of Paracosm\u2019s customers are in the US, but UK general contractor nmcn is using the PX-80 to capture detailed 3D site geometry on brownfield, greenfield, and refurbishment sites \u2014 both at the beginning of each project and regularly throughout the construction process for daily QA checks and as-built models.\nparacosm.io\nProject Frog\nProject Frog had some great exposure at AU with Autodesk CEO Andrew Anagnost highlighting the use of its new KitConnect software on the modular AC Marriott New York Hotel in Manhattan.\nAutodesk owns part of the company and its industrialised construction team actually helped develop KitConnect on top of Autodesk Forge.\nKitConnect is an integrated Revit to web application that is designed to manage configurable, modular \u2018Kit-of-Parts\u2019 content for scalable prefabricated building systems. Each component is designed for manufacturing and ease of assembly on site \u2014 better known as DfMA.\nThe software has two main roles. First it helps teams define components for their building systems with manufacturing in mind, capturing the rules and DfMA logic. Those components are then utilised during building design, with the cloud-based software acting as a Revit configurator to aid the design team.\nKitConnect can also be used to verify that standard components are available to purchase in the supply chain and, if they\u2019re not, swap them out for an alternative. This is designed to cut out the traditional iterative loop where a component goes back through pre-construction, gets validated, and then goes back to procurement.\nKitConnect isn\u2019t just focused on Revit. Project Frog is also developing workflows with Autodesk\u2019s Project Refinery for generative design, Dynamo for design automation, as well as manufacturing solutions Inventor and Fusion. We expect to hear a lot more from Project Frog in 2020 and think the company could play a key role in Autodesk\u2019s ambitions for industrialised construction.\nReconstruct\nThere are plenty of apps and services out there that use 360\u00b0 images to help track construction progress, but Reconstruct\u2019s Visual Command Center goes one step further, generating point clouds from 360\u00b0 videos. The resulting reality model can then be aligned to the BIM model to provide a 360\u00b0 \u2018augmented reality experience\u2019.\nEverything can be tied into a schedule so firms can simulate how a building will be constructed. With frequent walks of the construction site, firms can track progress or build up a past, present and future record of how a building was constructed. The SaaS (Software-as-a-Service) platform is built on Autodesk Forge.\nSpectar\nSpectar is a cloud and software platform that can bring Revit, AutoCAD or Navisworks models into Microsoft HoloLens so on-site teams can view them at 1:1 scale. The technology can be used for pre-construction or for actual installation work, such as stud walls or for complex geometry that might be hard to install using 2D plans, especially if being done by apprentices. The thing that caught our eye about the technology was the built-in repositioning tool that helps users deal with drift, a phenomenon often experienced with HoloLens where the model becomes misaligned with the real world over distance. When this happens, Spectar\u2019s software essentially lets you grab the model and manually realign it to a grid line.\nTrimble\nBoston Dynamics\u2019 Spot robot made many appearances at AU, not least during the AEC keynote presentation. Trimble used the event to promote a new partnership with Hilti and Boston Dynamics. The companies are working on a \u201cproof-of-concept\u2019 solution that integrates Trimble\u2019s and Hilti\u2019s construction management software solutions, GNSS technology and reality capture devices with Boston Dynamics\u2019 Spot robot platform.\nThe thing that stood out about this news is that the reality capture devices carried by the robot will directly communicate with a cloud-based construction management application. Speeding up and automating data acquisition is one thing, but automatically pushing the data to the cloud means it can be used in a timely manner by whoever needs it.\nVIM\nMicrosoft HoloLens has been the dominant mixed reality headset in the AEC sector for some time, but Magic Leap offers an interesting alternative for blending the digital and physical worlds.\nVIM, short for virtual information modelling, is Magic Leap\u2019s exclusive AEC partner. At AU the company invited delegates to check out its mixed reality demo in collaboration with Chicago-based general contractor, Skender Construction.\nSkender is using VIM\u2019s technology in its modular manufacturing facility to help its clients visualise what their modular buildings will look like before manufacturing and construction begins. By doing this in mixed reality, rather than virtual reality, Skender reckons it allows for a much more natural augmented collaboration.\nVIM isn\u2019t just about mixed reality. The company says its technology can be used to put large and fully coordinated Revit projects and other BIM model data into the hands of stakeholders and construction professionals, on a range of devices and platforms.\nWorkstation technology at AU\nAMD\nThe big news from AMD was the launch of the AMD Radeon Pro W5700, a new professional GPU that is designed for 3D CAD, Virtual Reality (VR), real time viz and GPU rendering. The double height board features a USB-C port so it can connect to new generation VR headsets, but AMD was pushing its wireless VR capabilities with the HTC Vive Focus Plus.\nFor AEC firms, this looks to be a powerful feature. On the show floor at AU, those trying out the technology were able to roam freely without having to worry about trailing cables. Multiple users can also collaborate in the same space without getting tied up in knots.\nThe big advantage of the AMD Radeon Pro \/ HTC Vive Focus Plus combination is model size, as data is streamed from a powerful workstation. This is in contrast to the Oculus Quest, which was seen on several stands at AU, where all the processing is done on the device and BIM models typically have to be broken down into manageable chunks.\nAt AUs gone by, AMD\u2019s presence would have started and finished with professional graphics but these days the company offers much more for users of Autodesk software.\nAMD new CPUs \u2013 3rd Gen Ryzen and 3rd Gen Ryzen Threadripper \u2013 are giving Intel a serious run for its money in the workstation sector and machines based on both processors were seen crunching their way through ray trace renders.\nAMD was also promoting the new Microsoft Azure NVv4 cloud workstation instance, which uses AMD GPUs and CPUs to stream 3D applications to virtually any device. The beauty of this new instance is that it\u2019s highly scalable. Users can use a quarter of a GPU to run Revit, for example, making it much more cost effective than other GPU-accelerated Azure instances, then scale up GPU resources when using a more demanding 3D application like Enscape. Billing is done by the hour.\nWe review the AMD Radeon Pro W5700 here and a BOXX workstation with the new 16-core AMD Ryzen 9 3950X CPU here.\nBIM Box\nBIM Box is a specialist manufacturer of high-performance workstations for the AEC sector. I\u2019m a bit of a workstation geek, as regular readers of AEC might know, and I had some great discussions with CEO Buck Davis on the show floor. I was particularly impressed with his extensive knowledge of Revit and other AEC applications including Leica Cyclone for point cloud processing and Enscape for real time viz, which means the company is able to match workstations accordingly.\nBIM Box\u2019s bespoke, highly tuned workstations (pictured below) use liquid cooling to hit CPU frequencies up to 5.4GHz. The company is based in the US but it has plans to enter the UK market.\nDell\nThe big news coming out of Dell was that it has re-engineered its Precision 7540 mobile workstation so it can take the more powerful Nvidia Quadro RTX 5000 (16GB) GPU. P\nreviously, Dell\u2019s mainstream 15-inch mobile workstation was limited to the Quadro RTX 3000 (6GB), meaning those who needed a GPU with more power (or more memory) for real time viz, GPU rendering or VR had to shop elsewhere, or choose the significantly larger 17-inch Dell Precision 7740.\nThis new update to the Precision 7540 took everyone by surprise, not least Lenovo who previously was the only major manufacturer that was able to offer the Quadro RTX 5000 in a 15-inch mobile workstation.\nWith Dell\u2019s updated Precision 7540, Lenovo has not only lost its market differentiator, but Dell reckons its re-engineered cooling system means it can run the Quadro RTX 5000 at 80W, 10W more than the ThinkPad P53.\nHP\nHP\u2019s big workstation announcement wasn\u2019t a new piece of hardware. Instead the company launched a software suite designed specifically for remote workstations. HP ZCentral allows individuals to connect remotely to a dedicated HP Z workstation and IT managers to create pools of HP Z workstations for groups to share.\nHP ZCentral is a new product, but it\u2019s not an entirely new technology. The remoting software, HP ZCentral Remote Boost Software, is really just an evolution and rebranding of HP RGS (Remote Graphics Software) with a bigger focus on collaboration whereby users can \u2018easily share\u2019 screens. What is new, is HP ZCentral Connect, a software broker that HP says allows IT departments to easily assign workstations, monitor connections and logins all through one simple interface. Users can access the next available workstation within an assigned pool of centralised workstations.\nLenovo\nLenovo often lets its customers do the talking and at AU we met with Neoscape\u2019s Carlos Cristerna who explained how his Lenovo ThinkPad P53 mobile workstation, which features the powerful Nvidia Quadro RTX 5000 GPU, is changing workflows at the AEC-focused creative agency.\nCristerna first spoke about his experiences of real time ray tracing with Unreal Engine at NXT BLD in June. He explained that in a matter of minutes, he can do as many renderings as he wants for a customer presentation. But it\u2019s not just about getting renders back quicker. He\u2019s since been looking at how the technology can help change the creative process within his team.\nAs an experiment he got together a programmer, art director and artist (himself) to work on an animation project. Cristerna explained that as soon as the art director had given feedback, he started walking away, as that\u2019s what he\u2019d always done. But Cristerna called him back and the changes were done on the fly, dramatically accelerating the creative process.\nOn the news front, Lenovo announced a new partnership with industrial VR\/XR head mounted display (HMD) manufacturer Varjo whereby the two companies will create \u201cCertified for Varjo\u201d pairings on Lenovo desktop and mobile workstations for all Varjo HMDs.\nThe certification process includes testing of ports, adapters, firmware, Nvidia Quadro drivers, Windows, etc and recommendations go down to CPU, GPU and memory level on fixed workstation models.\nLenovo was also showing its new rack-mounted workstation, the ThinkStation P920 Rack, which is based on the ThinkStation P920 desktop workstation and supports up to two Nvidia Quadro RTX 6000 GPUs. The machine can be used as a 1:1 high-performance workstation or, through virtualisation, can support multiple users concurrently.\nNvidia\nNvidia RTX took centre stage with the real-time ray tracing technology being shown in several new applications including Chaos Group V-Ray Next GPU and Autodesk VRED. Immersive VR was also on the agenda with Nvidia\u2019s variable rate shading (VRS) technology being demonstrated using Autodesk VRED 2020.2 beta and the HTC Vive Pro Eye, an enterprise VR headset with built-in eye tracking, running on a Dell Precision 7820 tower workstation with two Nvidia Quadro RTX 6000 GPUs.\nVRS (also known as foveated rendering) works by only rendering at the highest quality the part of the scene where the user is looking. It means GPU resources can be targeted where it matters and not wasted rendering pixels at unnecessarily high quality in your peripheral vision where it won\u2019t be noticed. The demo on Dell\u2019s stand featured a McLaren 720S but this type of technology could be used for high-end VR experiences in architecture where quality is of paramount importance.\nNvidia also presented its new CloudXR software development kit, which is designed for low-latency AR\/VR streaming over 5G networks to \u2018any device\u2019, such handheld tablets, VR headsets and AR glasses. CloudXR runs in the cloud but, for firms that have their own 5G networks, also in on-premise data centres. Automotive is always the go to industry for new technologies like this but we think there could be real value for AEC firms as well.\nSilverdraft\nSilverdraft was showing off its Devil GPU, an ultra-powerful server that can be kitted out with up to ten (yes, ten) Quadro RTX 6000 or RTX 8000 GPUs. Considering that Nvidia normally does demos on a server with three or four GPUs, this is a phenomenal amount of processing power for real time ray tracing in applications like V-Ray GPU. And if ten GPUs isn\u2019t enough then you can even connect up multiple Devil GPU servers over Infiniband. The Devil GPU is not just about real time ray tracing. It can also be used for multi-user VR in a single system. Silverdraft was also showing off its range of \u2018Demon\u2019 desktop workstations.\nWorkspot\nWorkspot offers cloud desktops and GPU-accelerated cloud workstations exclusively on Microsoft Azure. The company delivers what it describes as turnkey SAS solutions that make it very easy for admins to deploy virtual desktops.\nWith multiple regions of Azure, Workspot reckons latency is often less than 25ms wherever you are in the world. For all cloud workstations, low latency is essential if you want an experience that is comparable to a desktop workstation. However, the company also understands the importance of data sovereignty.\nAt AU, a Workspot spokesperson told us about a UK customer that has contractors and employees in India that connect to an Azure datacentre in the UK. In this example, Workspot admits that the latency is not as good as you would want it to be, but the customer doesn\u2019t have any versioning problems. It\u2019s all about what trade off you are willing to accept.\nOn a different deployment in the US, Workspot set up one AEC firm on two different Azure datacentres and replicated data between the two using Panzura technology.\nWorkspot currently uses GPU instances powered by Nvidia Tesla M60 GPUs over a 1:1 connection, but in the future will be looking to use the new AMD powered Azure GPU instances which will allow customer to scale down GPU resources. As it stands, we reckon one Tesla M60 is overkill for Revit users, meaning customers might be paying for resources they don\u2019t need.\nWorkspot is not the only cloud workstation provider offering virtual desktops on Microsoft Azure. UK-based IMSCAD also has an Azure offering through IMS Cloud.\nVirtual reality comes to Autodesk BIM 360\nAt AU we saw several examples of how collaborative VR tools are now linking to Autodesk BIM 360 to help support more efficient workflows and improve issue resolution. Check out the latest from InSite VR, The Wild and IrisVR in this article\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-v-ray-3-2-for-3ds-max-embraces-vr\/","title":"NEWS: V-Ray 3.2 for 3ds Max embraces VR","date":1435190400000,"text":"Two new VR camera types added to rendering toolkit, so scenes can be viewed on Oculus Rift and Samsung Gear VR.\nV-Ray 3.2 for 3ds Max, the latest release of the physically-based lighting, shading, and rendering toolkit from Chaos Group, introduces VR rendering capabilities and multiple V-Ray RT GPU enhancements. The software also includes improved volume rendering, distributed rendering, and global illumination.\nV-Ray 3.2 includes two new VR camera types, so users can now render stereo cube maps and spherical stereo images. Popular VR formats like Oculus Rift and Samsung Gear VR are supported.\nFor distributed rendering, users can now add or remove nodes on the fly, while, for Global Illumination an updated Light Cache algorithm is designed to remove light leaks and improve animations.\nV-Ray 3.2 is fully compatible with 3ds Max 2016 and provides full support for the new Physical Camera \u2013 co-developed with 3ds Max team \u2013 and the new Physical Camera Exposure Control.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-michael-perry-boston-dynamics\/","title":"Video: NXT BLD 2019 \u2013 Michael Perry, Boston Dynamics","date":1565049600000,"text":"What if human-like mobility could be added to automation on construction sites? \u2013 NXT BLD London, June 2019\nBoston Dynamics is looking to answer that question as it begins commercializing its highly mobile and dexterous robots. Vice President of Business Development will share an overview of Boston Dynamics\u2019 vision for applying robots to sensing and manipulation challenges in the robotics industry.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction.\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/philippe-pare-and-akshay-sethi-gensler\/","title":"Video: NXT BLD London conference \u2013 Philippe Par\u00e9 and Akshay Sethi, Gensler","date":1502323200000,"text":"Seeing is believing: using game-changing tools to discover the soul of design \u2013 NXT BLD London, June 2017\nUntil recently, digital tools could portray how an idea would look, but how an environment feels were left to the imagination, explains Philippe and Akshay. Colour, materials, scale and daylighting in renderings were no more than an illustrative approximation.\nHowever, by using the latest visualisation, simulation and VR technology, architects have increasingly more tools in their arsenal to communicate the certainty of their conceptual ideas through immersive means.\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Paul Nichols, Skanska\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Arthur Mamou-Mani, Mamou-Mani\nConstructing (and deconstructing) buildings with cable robots\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/qcoherent-software-set-to-release-lidar-software-extension-for-arcgis\/","title":"QCoherent Software set to release LIDAR software extension for ArcGIS","date":1178496000000,"text":"Colorado, USA, 04 May 2007: QCoherent Software announced that LP360 Classify, an extension of the LIDAR software tools for ArcGIS will be released at the ASPRS 2007 Annual Conference in Tampa, Florida.\nQCoherent software tools provide the data integration one needs while maintaining software performance when working with vast LIDAR datasets.\nLP360 Classify runs on top of LP360 and ArcGIS enabling users to classify\/edit LIDAR point clouds. With LP360 Classify, LIDAR points can be interactively driven to LAS predefined or reserved classifications. Classification tools are available in the ArcGIS data window and in LP360 cross-section, providing the ability to leverage ArcGIS layers in point cloud classification operations. LP360 Classify expands LPObjects for the customized programming of complex filters and extraction algorithms.\nQCoherent Software is a provider of integrated LIDAR software tools which is headquartered in Colorado Springs, CO, USA. . For more information visit QCoherent at www.QCoherent.com","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-tim-geurtjens-mx3d\/","title":"Video: NXT BLD London conference - Tim Geurtjens, MX3D","date":1502150400000,"text":"To print a steel bridge in Amsterdam \u2013 NXT BLD London, June 2017\nIn this inspirational talk, Tim shares his thoughts on the latest developments in Digital Fabrication and where he thinks things will go in the future. He explains how he was drawn into advanced robotics to develop the MX3D printers that can print metal objects of theoretically unlimited size, and presents MX3D\u2019s plan to \u2018print\u2019 the first steel bridge in the world over one of the oldest canals in Amsterdam. The ambitious project is set for completion in 2018. Essential viewing.\nView the other NXT BLD presentations\nTom Greaves, DotProduct\nReality modelling with phones and tablets (linked)\nSpeaker Name, company name Link to video on AECmag.com.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/kapio-cloud-construction-platform-is-bim-ai-and-big-data-ready\/","title":"Kapio Cloud construction platform is \u2018BIM, AI and Big Data ready\u2019","date":1566172800000,"text":"Platform helps construction teams to connect, report, collaborate and manage International engineering projects\nKapio Cloud is a new digital project management software from Copenhagen-based Optimise-International. The encrypted cloud-based platform is said to give construction companies secure access to analyse the reams of data generated from robotics, drones, BIM and industrial IoT sensors.\nThe system uses Artificial Intelligence (AI) and machine learning to analyse data. According to the company, this helps construction companies make more accurate decisions \u2013 an area which has traditionally been reactive, rather than predictive.\nDynamic dashboards and a multi-language interface are designed to make collaboration and analysis easy. Each project team can view a unique set of dashboards, specific to their KPIs and contract deliverables.\nKapio Cloud is a pay-as-you-go, SaaS model, where construction companies, contractors and clients only pay for what they use.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nvidia-introduces-mid-range-quadro-rtx-4000-gpu-for-ray-tracing\/","title":"Nvidia introduces mid-range Quadro RTX 4000 GPU for ray tracing","date":1542067200000,"text":"Powerful single slot GPU will also be VR Ready and feature 8GB of GDDR6 memory\nNvidia has introduced the Quadro RTX 4000, a new single slot, \u2018VR Ready\u2019 mid-range professional GPU powered by the Nvidia Turing architecture and the Nvidia RTX platform.\nLike the high-end Quadro RTX 5000, 6000 and 8000 announced earlier this year, the RTX 4000 is heavily focused on ray trace rendering.\nIt features dedicated RT (ray tracing) Cores and next-gen Tensor Cores for AI (Artificial Intelligence) inferencing. The RT cores trace the rays of light as they pass through a scene, reflecting off and refracting through objects, while the Tensor cores are used for AI-based de-noising, which Nvidia CEO Jensen Huang describes as \u2018filling in all the spots that the rays haven\u2019t reached yet\u2019 and is essentially a short cut to full ray tracing.\nWith Quadro RTX, Nvidia says designers can iterate a product model or building and see accurate lighting, shadows and reflections in real time. We imagine \u2018real time\u2019 in the true sense will be left to one or more high Quadro RTX 6000 or 8000s GPUs while the Quadro RTX 4000 will just do it quickly, but this will obviously depend on the complexity of the scene and the display resolution.\nNvidia compares the Quadro RTX 4000 to the previous generation Quadro P4000 and says there are significant performance improvements. It has 8GB of GDDR6 memory, delivering over 40 percent more memory bandwidth than the P4000 which has 8GB of GDDR5 memory.\nIt features 36 RT Cores for real-time ray tracing of objects and environments with physically accurate shadows, reflections, refractions and global illumination and 288 Turing Tensor Cores for 57 TFLOPS of deep learning performance. In comparison, the high-end Quadro RTX 5000 has 48 RT cores and 384 Tensor Cores and the ultra-high-end Quadro RTX 6000 and 8000 have 72 RT cores and 576 Tensor Cores.\nCommercial software that supports Nvidia RTX is not yet available but global architectural firm CannonDesign has early access to the technology. Ernesto Pacheco, director of visualization at the company, talks about his experience, \u201cOur designers need tools that unleash their creative freedom to design amazing buildings. Real-time rendering with the new Quadro RTX 4000 is unbelievably fast and smooth right out of the gate \u2014 no latency and the quality and accuracy of the lighting is outstanding. It will enable us to accelerate our workflow and let our designers focus on the design process without the technology slowing them down.\u201d\nThe Quadro RTX 4000 also includes hardware support for VirtualLink, a new open industry standard that is designed to meet the power, display and bandwidth demands of next-generation VR headsets through a single USB-C connector.\nThe GPU will be available in December on nvidia.com and from leading workstation manufacturers, including Dell, HP and Lenovo, and authorized distribution partners, including PNY Technologies Europe. The estimated street price is $900.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/iso-19650-going-global-part-ii\/","title":"ISO 19650: going global part II","date":1553644800000,"text":"In the second and final installment of this article on ISO 19650, Rebecca De Cicco explains why the UK is benefiting most from the practical application of the new standard\nWe have seen much momentum since the release of both the international and British versions of ISO 19650. This is an exciting time for our industry as finally a global solution for BIM is now available.\nRegions with little or no guidance are beginning to utilise new BIM processes and adopt a common language. With the British standards being used as a foundation for the development of the ISO, there is much to look forward to when it comes to its future use in the UK.\nWhat excites me the most, is that we have a world of opportunity ahead of us as a global industry, where we\u2019ll see regions with little to no standards trying to adopt new processes they know very little about.\nThe uptake on the use of PAS 1192-2 and 3 in regions such as Australia and China for example, was an excellent start, which can only grow further now we have the ISO. However, even though there is an understanding of the importance of the standards, no official mandate exists in these other countries like it does in the UK therefore gaining mass buy-in will be challenging \u2013 but we\u2019re ready to take on that challenge!\nIt will be an interesting journey to see the implementation of ISO 19650 across the world and although we don\u2019t have clear visibility across every region when it comes to BIM implementation, we still understand that consistency in language, process and information delivered intelligently is crucial.\nHere is my observation of where we are globally and why the disadvantages of other regions are proving to be a massive advantage to the UK, with expertise export opportunities now on our doorstep for UK BIM professionals to support other regions in their BIM adoption journey.\nRebecca De Cicco is the director and founder of Digital Node, a BIM-based consultancy working with clients all over the world to educate, manage and support the implementation of a clearly defined process, underpinned by technology.\nUK domestic strength\nIt is known that the UK is more advanced than many other regions of the world in its BIM delivery thanks to the BIM mandate. Therefore, due to our understanding and regular use of the 1192 suite of standards, the transition to ISO (it is presumed) will be a smooth one. This is also supported by the fact that the UK is the first region to release the British Standards \/ CEN version of the ISO and to develop the very first regional annex.\nThe same ease of ISO implementation sadly cannot be said for a global roll-out. Much of the regional development in BIM outside of the UK is being haltered by jurisdiction as well as historical context and we\u2019re seeing a very disjointed approach toward the uptake and use, as well as understanding of BIM according to ISO 19650. There are small pockets of BIM enthusiasts driving the BIM agenda in many different regions, however a formalised incentive is needed.\nISO 19650 international adoption\nBIM knowledge is extremely varied from region to region with a distinct lack of consistency in process. However, appetite is growing, and the ISO is just the very beginning.\nFor us, the key was to start promoting the use of the ISO even before it had been released. Our work with both private clients and government in regions such as Australia and China has been governed and supported not only by British Standards but also the knowledge that the ISO was on its way. Although much of the work outside of the UK in terms of BIM process and standardisation is specific to a region, the opportunity for global trade in regions such as Australia and China is enormous.\nThere are also many organisations with offices all over the world and in particular in Australia that have been pushing and mandating PAS 1192-2 as a solution. Therefore, these companies are already open to new ways of working.\nWe are seeing that local standards are hindering the development and use of a consistent process; therefore, the adoption of the ISO will have the ability to wipe this out. For example, in the US and Canada local BIM standards are very much unique to the language, history and processes defined and developed in these regions. This is why we believe uptake in the US will initially be slow when it comes to the use of the ISO but for Canada things are a little more positive. Canada is already using UK methods and products such as the NBS BIM Library and this is very much true for Australia too.\nThere are also huge opportunities in China, as we see Hong Kong leading the way nationally with the development of local BIM standards and certification for BIM professionals. These projects have been influenced by the local authorities (Construction Industry Council in Hong Kong) which have taken much of the guidance from British Standards. Much work is now being undertaken to internationalise these products with the release of ISO 19650. UK businesses will begin to see the benefits of the ISO being implemented globally as specialised knowledge and skill will be required to support these types of initiatives.\nNew opportunities\nThe construction industry is currently in a state of development and change. Not only due to the implementation of consistently applied methods for BIM and Digital Engineering, but also the way technology is influencing every part of the way we design, build and operate. An international standard for BIM creates great opportunities for global trade and translation of skills and resources across the world. We will see the UK still act as global leader when it comes to BIM according to ISO 19650 purely based on historical context and the whole BIM journey which has provided advancement in a region where innovation leads the way.\nWhich is why other regions will be looking to the UK for leadership and guidance.\nThis is a great time for the construction industry, and we are certainly in a state of positive change globally. ISO 19650 has been long awaited by all those paving the way for a future built environment where information flows freely and is accessible when required. Now it\u2019s time to use the new tools we have been given to make it work.\nRead the first article in this series \u2013 ISO 19650: going global \u2013 here\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-navvis-enhances-indoor-mapping-precision\/","title":"NEWS: NavVis enhances indoor mapping precision","date":1518739200000,"text":"Precision SLAM technology said to \u2018significantly reduce\u2019 drift error and improve SLAM accuracy\nNavVis has announced a new mapping software that it says significantly improves the accuracy of simultaneous localisation and mapping (SLAM) technology in indoor environments, such as long corridors. The Precision SLAM software update will be available for users of the mobile mapping system, the NavVis M3 Trolley, and is said to \u2018significantly improve\u2019 the accuracy of the resulting maps and point clouds.\nThe NavVis M3 Trolley is designed to build upon SLAM to increase speed and efficiency when scanning buildings. SLAM is a technique originally developed by the robotics industry that is now increasingly being used in surveying and autonomous driving technologies. It was designed to solve a core problem that long plagued robotics engineers by enabling a device to determine its location while simultaneously mapping an unknown environment. This is done by chaining millions of measurements into a trajectory estimate.\nHowever, according to NavVis, even when a device captures highly accurate individual measurements, chaining them will result in an accumulation of noise and tiny measurement uncertainties. Over time, the estimated motion will start to deviate from the true motion, which is known as \u201cdrift error\u201d. This can often be observed as a slight bending of long corridors that are actually straight. NavVis says that all available SLAM systems \u2013 regardless of whether they use LIDARs or other sensors \u2013 are inherently affected by this phenomenon.\nNavVis claims that its Precision SLAM technology significantly reduces drift error and improves SLAM accuracy.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/autodesk-adds-generative-design-capabilities-to-revit-2021\/","title":"Autodesk adds generative design capabilities to Revit 2021","date":1587427200000,"text":"2021 release also includes support for slanted walls, higher quality real time visualisation and the ability to create custom workspaces\nHaving started out in Project Refinery, generative design has now made its way into Revit 2021, but is only available to subscribers of the AEC Collection.\nGenerative Design in Revit 2021 is designed to make generative design much easier to use. Users simply load up a model, select the appropriate design study and define the goals. The software then returns a range of options which can be visualised via 2D lists and interactive 3D thumbnails and ranked by certain criteria. Once the best option is chosen it is directly populated as elements into the Revit model.\nGenerative Design in Revit currently includes three sample design studies:\n\u2022 Maximize Window Views helps score the quality of views to the outside from any position in a room.\n\u2022 Three Box Massing creates simple massing models and calculates areas.\n\u2022 Workspace Layout enables more effective placement of desks in a room based on user-selected criteria\nAutodesk will add more sample files in the future and on the Generative Design Primer. The Dynamo visual programming environment can also be used to create custom design studies.\nOther features for Revit 2021 include support for slanted walls that can \u2018instantly handle\u2019 slanted doors, windows, and other wall-hosted geometry; the ability to create custom Revit workspaces cased on discipline and job role; and improved interoperability between Revit and Inventor to offer better support for design-to-fabrication workflows.\nRevit 2021 also includes higher quality real time visualisation with more realistic materials and lighting. Autodesk claims view navigation is much faster and smoother, performing over 10 times faster than previous versions.\nThere are also a whole host of enhancements to steel modelling, rebar detailing and electrical design. Full details of what\u2019s in Revit 2021 can be found here.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/ai-and-iot-tech-to-drive-energy-efficiency-in-buildings\/","title":"AI and IoT tech to drive energy efficiency in buildings","date":1581379200000,"text":"Collaborative R&D \u2018i-REAP\u2019 research project could lead to energy efficiency advice service for building owners\nA system to determine the energy and cost efficiency of commercial buildings is being developed by researchers from the University of the West of England (UWE Bristol) and other partners. Data collected from a network of small sensors will initially help experts paint an accurate picture of energy consumption in a number of test sites with a view to setting up a service to offer energy efficiency advice to businesses.\nCalled i-REAP, which stands for IoT-enabled Real-time Energy Analytics Platform, the two-year \u00a31.5m collaborative R&D project is funded by the Department for Business, Energy and Industrial Strategy. It is led by engineering firm TerOpta, which is developing Internet of Things (IoT) enabled sensors for i-REAP.\nCostain is also one of the project partners and is providing five test sites across the UK in buildings belonging to the firm or its subcontractors.\nResearchers from UWE Bristol\u2019s Big Data Lab will initially carry out a feasibility study in the buildings, assessing the heating layout, staff sitting arrangements, office structure, orientation of buildings and building fa\u00e7ade, materials, as well as insulation. They will then install up to 80 IoT sensors inside and four outside each of the buildings. The sensors deployed inside will measure temperature, humidity and ambient light intensity, and externally temperature, humidity, wind speed and solar radiance.\nBy collecting data over a period of six months, the aim is that researchers will gather enough intelligence on the building to then give client advice on how the current building systems are functioning and how they could be improved by retrofitting the premises to make them more energy efficient and cost-effective. For instance, they may recommend making partitions double-glazed, or improving certain heating systems.\nProfessor Lukumon Oyedele is the principal researcher at UWE Bristol on the project and is Assistant Vice-Chancellor and Chair Professor of Enterprise and Project Management. He said: \u201cThis project contributes to fast-forwarding the adoption of AI and IoT for energy savings and looks to help the building sector to move from \u2018reactive\u2019 to \u2018predictive\u2019 approaches in developing guidelines for ideal retrofitting actions and low carbon heating.\n\u201cWhat makes it unique is also that we are able to analyse energy efficiencies in different sections of the building, at various times of the day and ultimately we want to see how commercial buildings can contribute to carbon neutrality.\u201d\nIn the long term, the overall aim of i-REAP is to contribute to UK\u2019s 2050 net zero strategy and provide enough information that will feed into policy formulation for commercial buildings.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/going-underground\/","title":"Going underground","date":1553558400000,"text":"Bentley Systems is developing new software that merges the engineering worlds of structural and geotechnical, from design all the way through to operations. The aim is to streamline workflows, improve accuracy and deliver digital twins that can even predict failures before they happen, writes Greg Corke\n3D modelling in structural design is widespread but the same can\u2019t be said for geotechnical engineering, which is still dominated by 2D. This presents a big challenge when it comes to data exchange as the two disciplines don\u2019t play together as well as they should.\nOne application that does embrace the third dimension is Plaxis 3D, an advanced geotechnical design, simulation, and engineering tool, which Bentley Systems acquired last year.\nPlaxis 3D uses finite element methods for analysing complex geotechnical projects such as metro stations, bridges, airports and tall buildings. It\u2019s particularly relevant in inner cities where new projects are often built in and around existing infrastructure, presenting much bigger geotechnical design challenges.\nImportantly, Plaxis 3D is not just about soil mechanics; it also handles soil-structure interaction so engineers can predict what will happen to the soil and how it will react to the structure once it is built. Structural geometry can be imported from CAD and analysis applications. Elements like piles and anchors can also be defined. But now Bentley has big plans to expand the software\u2019s capabilities much further.\nWhen two worlds meet\nBentley is well known for its extensive range of structural analysis applications, including Staad and RAM. Adding Plaxis into the mix not only fills a geotechnical gap in its portfolio but gives it the foundation on which to develop an integrated analysis application that can handle both linear structures and non-linear soils at the same time.\nWhen the development work is complete it should give Bentley a unique capability for geotechnical engineering, not dissimilar to a multi- physics system used in mechanical engineering that combines structural analysis with fluid dynamics.\nCurrently, a typical workflow between a structural and geotechnical engineer is very disjointed and involves a lot of estimation and back and forth, as Raoul Karp, Bentley Systems VP design engineering analysis, explains. \u201cGenerally, what happens is, the structural [engineer] will go to the geotech [engineer] and the geotech will say, \u2018you can use these kinds of foundations with these kinds of loads and get these kinds of estimates.\u2019\nThen the engineer takes it and tries to mimic that through some kind of simple spring or something in their structural model and then they come back and they say, \u2018well, the real forces that I got are this; does it still work with what you told me?\u2019\u201d\nBentley has already developed an initial workflow through its synchronisation software Integrated Structural Modelling (ISM). It allows an engineer to bring a Staad or RAM model directly into Plaxis 3D. \u201cYou can start to see the interaction between the soil, the stress and displacement and the structural model,\u201d says Karp.\nBut this is just the beginning. The aim is to fully integrate the two and create a single simulation environment that includes both the superstructure and the soil. So, if you start moving your geotechnical design, Staad will start changing the design of the structure on top, or vice versa.\n\u201cWe want to get to a point where we can consider everything together in a single model,\u201d says Karp. \u201cThat will be truly ground breaking.\u201d\nDigital Twin\nSolving workflow challenges for the design phase and increasing the speed and accuracy of the simulation is not the only reason that Bentley acquired Plaxis. It also has big plans for geotechnical engineering as part of its digital twin strategy \u2013 maintaining a digital representation of the infrastructure and of its geotechnical context over the lifetime of the asset.\n\u201cWhen you think about the concept of the [digital] twin, it\u2019s really going to come into its own in the operations phase of any asset,\u201d says Jan-Willem Koutstaal, Bentley vice president of local account initiatives, and the former CEO of Plaxis.\n\u201cFundamental to any decision you\u2019re going to make on a change in that asset is going to be the geotechnical engineering.\n\u201cYou use software to assess the initial ground conditions, to understand what you are going to be building on top of, but then throughout the lifecycle of the building you are doing regular observation through sensors in the foundations or building, or surveying with drones.\u201d\nThe idea here, is that by embedding sensors into the asset or creating reality models through ContextCapture, data can be continuously fed into the software so it can automatically re-analyse the asset based on its changing conditions over time. Such changes could be dictated by subsidence, seismic activity or weather, but there could also be changes in owner requirements or structural strategies.\nImportantly, the digital twin could also be used to help predict what might happen in the future. Koutstaal explains that in such a system a green light could denote \u201cyou\u2019re OK at the moment,\u201d but if the movement carries on at a certain rate you\u2019ll get orange, a critical state.\nThis type of predictive analysis could be augmented by deep learning, based on what has happened in the past, either on the same structure or on different structures. The software could then put forward multiple \u201cwhat if\u201d scenarios, leading to recommendations for pre-emptive action to be taken. In the most extreme cases this could help prevent catastrophic failure.\nConclusion\nBentley is long established in the engineering of structures above ground. Now, with its acquisition of Plaxis, it\u2019s looking to do the same below, introducing advanced geotechnical engineering technology to its global customer base.\nMarrying the two, previously independent worlds of simulation, is an obvious path for Bentley to take, to streamline workflows and to deliver more accurate results, but arguably the biggest prize will be in applying geotechnical engineering to digital twins.\nThe concept here is that the asset owner will not only have a digital twin of the infrastructure itself, but of the geotechnical conditions that surround it. And over time, the owner will be able to assess the impact of the changing environment and predict well in advance when maintenance or critical action might be required. It\u2019s a bold vision, and one that becomes even more compelling when you consider that it could save lives.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/dr-robert-aish-joins-autodesk\/","title":"Dr. Robert Aish joins Autodesk","date":1202428800000,"text":"It would appear as though Autodesk has pulled off a major acquisition from AEC competitor, Bentley Systems. Dr. Robert Aish, Bentley\u00dds director of research and \u00d9father\u00dd of its Generative Components (GC) technology, is set to join Autodesk\u00dds Building Solutions division this month.\nAish was core to the development and promotion of Bentley\u00dds parametric form-generating platform, Generative Components, which has been developed on top of MicroStation. Generative Components provides an environment in which geometry (lines, arcs, circles, solids, surfaces) can be given associated relationships, and transformed, generated and manipulated all within a user-defined framework. The product had caused a considerable buzz within the more adventurous architectural design firms, as well as in the structural engineering market \u2013 and all while the product was in Beta as it only officially shipped recently.\nWhile the technology was impressive, Aish actually took to the road to promote the technology, assisting in the establishment the Smart Geometry group, which provided training for architectural students and key practice designers who required more complex form-generating solutions.\nFrom my dealings with Bentley there was always some reticence when it came to talk about GC\u00dds future within the company, as many I talked to didn\u00ddt understand the impact and traction the technology was creating in clients that had advanced geometry problems. GC was seen as a niche technology for one or two signature architects. However, with the subsequent success of Smart Geometry events and larger than expected take up of the early beta, Bentley started to throw more resources behind GC\u00dds development. In fact, there are buildings and bridges in existence and many going through planning that were developed using these early beta code. Aish\u00dds departure will undoubtedly come as a major blow, perhaps not in the technology\u00dds development, but certainly in the technology\u00dds promotion.\nAutodesk is a company that concentrates on selling volume products. Under relatively new CEO, Carl Bass, the company is expanding to target Signature architects and high-end industries like Automotive design. However, these are very much low-volume industries. The potential of what Aish had done at Bentley was not lost on Bass. At Autodesk University 2006 there was one customer presentation that included the use of Generative Components in finding a solution to designing a complex bridge structure and Autodesk employees were starting to come to Smart Geometry events to see what was going on. Then just after Christmas 2006, Bass emailed me to say that he had learnt how to use Generative Components and from experimentation thought the software was \u00d9delightful\u00dd. Perhaps then, the seed was sown that Autodesk would attempt to recruit Aish or develop something similar.\nAish is widely respected in the construction industry from years of developing innovative CAD systems and has an enviable set of contacts in all the leading architectural and construction firms. It\u00dds certain that anything he would develop at Autodesk, would be eagerly anticipated and tested by Autodesk\u00dds key target firms.\nFrom talking to Aish, there is no doubt about the passion that he has for generative design, where the computer is the true amplifier of the mind, instead of a mere documentation device. A move to Autodesk would give him the possibility of spreading his technology to millions of designers, in a number of possible applications \u2013 AutoCAD, Revit, 3D Studio, Maya or even Inventor. At Autodesk University in Las Vegas in November last year, I talked with Jay Bhatt, VP of Autodesk\u00dds Building Solutions Division on the subject of what Robert Aish will do for Autodesk. Bhatt was sure that Aish wouldn\u00ddt be copying the work that he did at Bentley and making GC for AutoCAD. Future development plans seemed to be very much in the air, with Aish only just having officially joined Autodesk but they do know that he has significant respect within most of the major architectural practices and there\u00dds a bunch of very popular applications that he can now choose to develop on top of. Having had many attempts to develop GC at Bentley (the MicroStation platform kept changing), I would hazard a guess that Aish has not given up on what he has been working on for decades. I am sure he will develop some kind of generative technology for Autodesk, although the target platform is as yet undecided. One only has to look at the latest release of McNeel\u00dds Rhino to see that form generation is becoming mainstream and Autodesk is aware that it needs to develop that capability. Bentley meanwhile has a head start and will need to develop GC at an accelerated pace to get wider adoption and fend off any future threat from what will be a well financed and accelerated Autodesk development.\nIn fact, Autodesk and Bentley appear to be on collision courses in more than one vertical, now that Autodesk has expanded into developing vertical applications for Geo, Civil Engineering, Power and Process Plant. Autodesk has reverse engineered its own DGN (MicroStation) file format and appears to be recruiting aggressively from Bentley\u00dds various vertical sales and product management teams. While in the past Autodesk has appeared a less aggressive industry giant, playing more of a defensive role, the company\u00dds recent product expansion and outstanding revenue performance, comes with a new competitive attitude and strategies to single out Dassault Systemes in the engineering space and Bentley in the building and infrastructure for focus. The bottom line of all this development activity and increased competition is that the designer is going to get better, smarter tools, more choice and competitive deals, either being an existing or new customer.\nwww.bentley.com\nwww.autodesk.com\nwww.smartgeometry.com","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/smartgeometry-2011\/","title":"SmartGeometry 2011","date":1305158400000,"text":"Now In its eighth year, SmartGeometry 2011, took place over six days this Spring, hosted in Copenhagen by the Royal Academy of Fine Arts School of Architecture\u2019s Centre for IT and Architecture (CITA).\nFrom its beginnings as a test bed for Bentley Systems\u2019 GenerativeComponents (GC), the event has grown in stature and complexity adding an increasing amount of physical experimentation to the screen-based activities that one would expect at a computing conference.\nIn contrast to the near heavy industrial atmosphere of last year\u2019s event, electronics were at the forefront in Copenhagen. The heart of the event is the Workshop and CITA provided a magnificent former warehouse accommodating all of the workshop clusters in one space. As well as filling this space with laptops and other familiar hardware, mysterious perspex boxes full of brightly coloured wiring, crocodile clips and electronic components were visible throughout the workshop. One SmartGeometry veteran remarked, \u201cSG 2011 was a shock to the system, for everybody who was there, but we have come to expect that, although we still marvel at it.\u201d\n\u2018Smart\u2019 was more to the fore than \u2018Geometry\u2019 but that should be expected as being clever with geometry has become mainstream. Mere parametric manipulation of geometry is just part of the base skill set for this group.\nNo one could leave the workshop without jumping around on the Jedi-inspired \u2018Use the Force\u2019 sensory floor. The sensor data fed the Kangaroo physics plug-in for McNeel\u2019s Grasshopper and GC generating real-time virtual geometry driven by the movement and pressure on the floor. This was projected alongside the installation showing participants how their movements modified the geometry. While this was great fun, serious applications could be to use similar sensor and software combinations to give real-time loading feedback of real or mock-up structures.\nCollaboration\nSince joining the directors of SmartGeometry, Xavier De Kestelier of Foster + Partners, and Shane Burger of Grimshaw have led a change in direction to a looser organisation and a reorganised event format, first seen last year in Barcelona where working in larger collaborative groups has become central to the workshop phase.\nDuring 2010 an open invitation was issued for workshop topics and leaders. The responses overwhelmingly included \u201cusing real-world data\u201d. Over 50 applications were whittled down to 12. A second invitation was then issued for workshop participants who were required to bring a high level of knowledge and ability to the cluster. This resulted in a carefully selected mix of 120 academics, practitioners and students, largely from an architectural or engineering background but also including representatives of other disciplines including software, maths and statistics, grouped into 12 \u2018clusters\u2019 working over a four-day period.\nBy day four the activity had moved from data gathering, analysis and processing to construction of installations that would be exhibited. To achieve this, work had been carrying on well into the small hours every night.\nBuilding the invisible\nOn the face of it this year\u2019s \u2018Building the invisible\u2019 theme could hardly be more different from SmartGeometry 2010, which was, literally, a more concrete event pushing the boundaries of direct fabrication in AEC. Despite the inclusion of \u2018invisible\u2019 in the event title the workshop participants were all required to produce physical installations for the end of workshop exhibition presenting the results of their activities. These displays ranged from a large-scale mock-up of a Gaudi inspired, acoustically diffusive ceiling, via knitting, to Xbox Kinect-driven interactive environmental simulations.\n\u2018Building the invisible\u2019 focused on gathering various kinds of data and exploring the possibilities offered by making this data integral to the design process. These ranged from evidently practical environmental and structural data to more abstract inputs, such as behavioural activities and the electrical sensitivity of building fabric. While there are a number of simulation tools in the market, many tend to be a step or two away from the heart of the design process and can take time to return results. One of the goals at this event was seeking simulation feedback in real-time to make it a more interactive part of the design process.\nTalkshop\nIn addition to the exhibition installations, the knowledge gathered by the clusters contributed to the Talkshop day. This was structured around four roundtable sessions, an engaging format where cluster members presented topics to seed each session\u2019s discussion. Common themes emerged as the day proceeded, the discussion being enlivened by questions and comments from the conference Twitter feed.\nIn session one, Data by Design, Bruno Moser of Foster + Partners, talked about data presentation and highlighted how distortions of projection or visualisation, whether intentional or otherwise affect how is data perceived, citing examples like the famously euro-centric Mercator projection.\nOle Sigmund of the Technical University of Denmark reviewed developments in optimisation, a subject that has featured in previous SmartGeometry events and is now an accepted part of design processes in many fields. The example shown \u2014 minimising the weight of titanium in an A380 wing rib \u2014 is clearly a highly appropriate application, however \u201coptimal is not necessarily beautiful.\u201d Conventionally ribs are lightened by punching circular holes, however these punches can cause stress concentrations, the optimised ribs have a seemingly random arrangement of struts and spaces.\nAt present, optimisation can only assess a limited number of variables. Optimising for stress or other single variables is relatively straightforward. Participants were asked whether optimisation on a wider scale involving many variables should be sought. Some thought that the value of multi-variable optimisation may be questionable; its results would be heavily dependent on the starting assumptions and the relationships between them. The notion that optimisation could replace the judgement of designers provoked lively responses from the floor. One participant was heard to exclaim: \u201cArchitecture based on raw data is unlikely to happen!\u201d Others argued that we must question the assumptions made by optimisation tools or other simulations.\nIn his presentation, Mr Sigmund had anticipated this response, showing a modified optimisation flowchart that included \u201cArchitect Satisfied?\u201d as the final gate in the process. It seems the human designer is not being replaced quite yet.\nAn interesting distinction was made between simply gathering larger quantities of data and the improvement of data gathering and instrumentation techniques. Improved techniques increase the resolution of our data, a subtle but important difference. When all this numerical data is available will we be distracted by precision? The answer came \u201cbe as precise as you need to be but not over precise, sometimes knowing whether the result is 50 or 100 is enough\u201d.\nSession Two, entitled Form Follows Data, featured simulation methods currently in practice. Foster + Partners Giovanni Betti\u2019s main theme was that a pretty image may not communicate any information. His comment that \u201cdata is worth nothing unless it makes its way to your gut feeling,\u201d was echoed throughout the event. Many participants agreed with Mr Betti that \u201cdata does not equal information\u201d and \u201cdata is worth nothing unless it makes its way to your gut feeling.\u201d\nMr Betti posed the question: \u201cHow do we visualise\u201d? In contrast to some of the previous colourful but uninformative illustrations, described by one audience member as \u2018data porn\u2019, he showed the Beaufort Scale website as an example of data visualisation that communicates.\nThe Beaufort Scale names and numbers are illustrated with images of actual conditions making a direct connection with the human experience calibrating gut feeling.\nSession three picked up some of the earlier themes exploring the nature and value of data and provoked the most lively discussion of the day. Performative Data featured Daniel Piker and Robert Cervellione \u2014 developers of the Kangaroo plug-in for Bentley\u2019s GC and McNeel\u2019s Grasshopper, the generative modelling tool for Rhino.\nInspired by analogue form finding carried out by Gaudi, Frei Otto and others, Kangaroo was written to provide the kind of real-time interaction found in real world models, while seeking to be playful as well as useful. They highlighted the point that to be really useful simulation needs to offer results to designers in real-time.\nRevisiting the earlier statement that data is not information, Flora Salim of RMIT Melbourne, talked of the importance of interpreting data to produce information that can be used by designers. She wondered whether the growth of data might produce career statisticians in the design world? Ms Salim concluded, \u201cexpert knowledge and intuition is needed when interpreting data. A statistician would produce stats but would they be useful?\u201d\nJonathan Rabagliati, chairing the discussion remarked \u201cit\u2019s all about optimising gut feeling!\u201d\nThe final session of the day entitled The Data Promise was anti-climactic and failed to put forward a cogent exposition of this \u2018promise\u2019. During discussion Volker Mueller, Research Director for Computational Design at Bentley Systems, asked: \u201cWhy do we want data?\u201d It seems that the answer to this question varies according to context. In some cases more and better-interpreted data can clearly be of great value. In others data and the processes surrounding it, while interesting in themselves, may actually be a distraction.\nThere was a clear conclusion that all of these things are still only tools and that human designers and their ability to make decisions on \u2018gut reaction\u2019 will remain in charge.\nSymposium\nThe Symposium held on the final day contained a series of keynotes and reports from all of the workshop clusters. Our host opened with examples of the work being carried out at CITA. GC has been extensively used in design projects from initial form finding to fabrication.\nThe experience gathered during these projects suggested that the interfaces to complex parametric tools need to have options for manual overrides to be applied allowing designs to be tweaked \u201cby eye\u201d in the traditional manner. Never mind the ghost in the machine, let\u2019s have a designer!\nMost interesting is CITA\u2019s investigation of the use of craft techniques and traditions supported by computing technology and component fabrication, where the parametric model was not required to communicate construction detail, only to generate a visual model and component sizes, the detail being implicit in the chosen craft technique. A lesson for those involved in Building Information Modelling (BIM) processes: The most severe limitation of current research is that it generally only addresses single materials and working with the reality of mixed materials presents a higher level of complexity yet to be fully explored.\nIn other keynotes, Lisa Amini of IBM\u2019s Smarter City Lab in Dublin highlighted how the Earth\u2019s increasingly urban populations underlie many of the current issues affecting world cities. She showcased IBM projects where sensor-based data gathering is used to inform urban information networks to make a wide range of services smarter. Examples shown included studying energy performance management in over 1,400 New York City buildings and monitoring environmental conditions, pollution levels and local marine life in Galway Bay, Ireland\nShowing a series of innovative projects Ben van Berkel of UN Studio described the extensive range of computing techniques used in his studio\u2019s design processes as being at the stage of \u201ca handcrafted digital project\u201d and thinks \u201cwe will have to become programmers in the future.\u201d\nUsman Haque, CEO of Connected Environments Ltd showed a series of inspiring technology-driven public art projects and gave us the background to Pachube.com. Describing experimental projects where site micro-climate and building environmental monitoring had been carried out by the firm\u2019s trans-disciplinary research group, Billie Faircloth of Kieran Timberlake warned us to look carefully at the results.\nIn one project a room\u2019s temperature readings were way out of range, investigation revealed a steam main running directly below that the site investigation had missed.\nCraig Schwitter of Buro Happold reviewed a series of projects in collaboration with Chuck Hoberman featuring sensor driven moveable elements that, unlike most conventional solar shading, respond to solar position and intensity. In the atrium pictured left, the internal facades are shaded but the atrium floor receives dappled sunlight adding an aesthetic dimension to the shading solution.\nConclusion\nThe most used word at SG 2011 was \u2018data\u2019. It is a word with a very specific meaning but also many casual and misleading usages. In common speech \u2018data\u2019 is often interchangeable with \u2018information\u2019.\nOne key lesson to take away from SG 2011 is that data does not equal information. We can gather as much data as we like but that data only becomes information once it has been processed, interpreted and then presented in a form that is comprehensible to the recipient.\nThis applies as equally to a pencil drawing on A4 paper as a seductive infographic.\nThe other key lesson was the importance of connections. SG 2011 utilised a rich mix of hardware sensors and scanners, parametric software, plug-ins, custom programming, linking\/interpreting applications and output hardware. Without the connections to enable these to communicate together much of the workshop\u2019s activity would have been impossible.\nIn the less esoteric world of AEC production both of these lessons are vitally important. We should not be distracted or seduced by the existence of massive amounts of data \u2014 we actually need the processed comprehensible information that arises from data. We also need to avoid being trapped in data silos and be able to utilise information from many diverse sources.\nSmartGeometry is always an exciting event. Don\u2019t be surprised if some of the experiments provoke technological developments that could be seen in the workplace soon. www.smartgeometry.org\nClusters\nStarting with the 2010 event, the workshop is organised around Clusters of 12 people to focus and apply more energy to each of the themes. At previous events participants worked in smaller teams or even individually, resulting in a more diffuse output.\nPerforming Skins\nThe Performing Skins cluster has taken some steps in researching the application of parametric techniques to multiple materials. Building on the active research of the cluster leaders, knitted fabric skins were shaped to match underlying timber structures, producing one of the most striking workshop installations.\nHaving resolved the challenge of interfacing the parametric design via floppy disk (just where do you buy a floppy disk these days?) with a 1980\u2019s CNC knitting machine, and explored pattern making within the knit, electrically conductive threads were introduced into the fabrics. Responding via electromagnetic induction to human touch or changes in ambient humidity, these could react in various ways, changing the configuration of the fabric.\nKnitting many separate circuits into the fabric allows spatial response\/control. The once rare fabric structures that have now become ubiquitous largely follow a narrow range of forms dictated by the cutting and stitching possibilities available to flat woven fabric.\nKnitting allows 3D form to be an integral part of the material\u2019s creation. Could a future application for CNC knitting be the production of large scale fitted membranes or composite components containing wiring and smart sensors or active components?\nUrban Feeds\nThe Urban Feeds cluster chose to explore the ability of handheld devices to gather city scale data.\nProprietary devices such as the Libellium WASP node are commercially available, however they are preconfigured \u2018black boxes\u2019 and thus limited. The group built their own devices based on the open source Arduino processer. Branded \u2018Ambient Sensor Kit\u2019 (ASK) their units were assembled during the first day of the workshop from standard electronic parts and pre-cut clear acrylic enclosure kits, all of the electronics are intended to be tinkered with giving more control of the data capture. However ASK still contains assumptions that have to be understood and modified by feedback. When trying to understand what data is in this type of context, it is useful to realise that the data gathered by the ASK devices was a long list of resistances logged as a voltage. How these voltages are then interpreted is critical to the outcome.\nParticipants chose to capture four data streams: temperature; light levels; motion and CO2 levels, with each entry spatially logged by a GPS sensor. Data capture was carried out by the team walking around Copenhagen carrying the ASK units (with a pocket full of 9v batteries, power consumption was considerable). A real world application could involve many units in fixed locations gathering real-time data over longer periods. Back at the workshop the data was processed into 2D mapping and 3D forms overlaid on Google Earth and Streetmap via Pachube.com, a web-based service used to store, share and discover realtime sensor, energy and environment data from objects, devices and buildings around the world.\nA step further in data visualisation was the generation of sculptural forms that dynamically visualised the data streams to direct people to parts of the city with the appropriate conditions for their needs \u2014 somewhere quiet and shady for hangover sufferers for instance.\nInteracting with the city\nIn a darkened corner of the workshop this cluster built a scaffold frame to support a series of linked data projector and Microsoft\u2019s Kinect sensor combinations. More Arduino processors linked some existing research threads to these sensory display installations.\n\u2018Hacking the Kinect\u2019 has been a recent techie headline, so use of the Kinect is attention grabbing. In reality \u2018hacking\u2019 fundamentally consisted of writing drivers to connect the Kinect to ordinary PCs instead of the Xbox. Once connected the point cloud data gathered by the Kinect can be accessed and used. Following the quest for real-time simulation the Kinect input enabled interaction with applications by hand gesture or in the case of \u2018Ophelia\u2019s Beach\u2019 by placing block models into the display area.\nThe possibilities for physically interactive user interfaces demonstrated here should lead to products that we may see very soon. We are all becoming used to gesture-based interfaces on the small scale of our smartphone screens, the extension of this into our CAD systems is surely inevitable.\nAbout the author: Marc Thomas is an independent project technology consultant (www.isisst.co.uk).","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/boston-live\/","title":"EVENT: DEVELOP3D LIVE Boston","date":1475539200000,"text":"On Sept 26, AEC Magazine\u2019s sister publication, DEVELOP3D, hosted its first ever conference and exhibition on US soil for a wonderfully intimate celebration of design, engineering and technology\nDEVELOP3D LIVE UK has become renowned for its eclectic conference program, with over 40 speakers sharing insights on all aspects of design, engineering and manufacturing \u2013 even the occasional superhero and dragon.\nDelivering this same format for an intimate 14 speaker event in Boston, USA was a daunting task, but we felt it was important to do so. DEVELOP3D LIVE is not only about education \u2014 through presentations and live demonstrations of the very latest in product development technology \u2014 but also about creating an environment that encourages the sharing of ideas to foster creativity. Networking should not only be between direct peers, but across disciplines.\nArchitect Jorge Barrero, HKS, whose 3D printed building presentation captivated an audience of industrial designers, product designers, engineers, manufacturers and venture capitalists found this melting pot particularly stimulating. Many of the leading CAD software developers and 3D print manufacturers were seen discussing the future of our industry over a friendly drink.\nCollaboration on a larger scale was explored by William Annal, Scotrenewables, whose distributed team relies on cloud-based CAD to design its tidal energy machines.\nWe also had some amazing designers share their stories. Gustavo Fontana, Bose, detailed the audio equipment manufacturer\u2019s advanced industrial design process, which gives designers freedom to choose from a smorgasbord of product development technology. Jon Friedman, Freight Farms, wowed the audience with his story of how a standard shipping container can be turned into an automated vertical hydroponic farm. John DePiano, explored the future of product development \u2013 from robotics to products \u2013 highlighting the BIOSwimmer, an underwater vehicle that replicates the dynamics of a fish.\nIndustrial grade 3D printing for end-use parts \u2014 of all sizes \u2014 also took centre stage with Rize, Stratasys, Markforged and Formlabs all presenting the very latest tech.\nDEVELOP3D LIVE Boston concluded with a networking drinks reception on the exhibition show floor. Then it was all on to the new Autodesk BUILD space for more fun, drinks, food, robots and inappropriate headwear!\nWe all had a truly wonderful time. So, from the DEVELOP3D Editorial team we\u2019d like to say a huge thanks to everyone \u2014 delegates, speakers, and sponsors \u2014 for helping make it such a memorable event. Bring on 2017.\nPlans are already underway for DEVELOP3D LIVE USA 2017. To be kept informed register your interest at d3dliveUSA.com\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/review-fujitsu-celsius-j550\/","title":"Review: Fujitsu Celsius J550","date":1454544000000,"text":"Designed and manufactured in Augsburg, Fujitsu has used its German engineering know-how to make a compact workstation with no compromise graphics. By Greg Corke\nWhile most workstation manufacturers already have several generations of Small Form Factor (SFF) workstations under their belts, Fujitsu has only just rolled out its first. But for designers seeking a compact desktop with almost no compromises the Fujitsu Celsius J550 has been worth the wait.\nThe Celsius J550 stands out for its 3D graphics capabilities. Unlike other machines in its class, it can host a full height graphics card, up to the mid-range Nvidia Quadro K2200 (4GB) or AMD FirePro W5100 (4GB).\nThis is big news for 3D CAD users as they can now get levels of 3D performance that were previously only available in a tower.\nIn comparison, SFF workstations from Dell, HP and Lenovo rely on a low profile graphics card, such as the AMD FirePro W2100 or Nvidia Quadro K1200, to fit inside their slimline chassis. For more demanding 3D CAD users, these GPUs are likely be a bit underpowered.\nThe Celsius J550 manages to squeeze in its full height graphics card by making it sit parallel, rather than perpendicular, to the motherboard. The riser card that makes this possible is a simple, but impressive piece of German engineering. It can be quickly removed from the motherboard making upgrades or repairs easy.\nSimilar attention has been paid to storage, with a drive cage that hinges through 90 degrees to give easy access to up to four 2.5-inch Solid State Drives (SSDs) or two 3.5-inch Hard Disk Drives (HDDs).\nFor workflows with high I\/O requirements, such as point cloud processing, video editing or simulation, high performance PCIe storage is also on the menu. There is support for up to three M.2 NVMe SSDs (one on the motherboard and two on a PCIe add-in card) and it is even possible to configure two of these tiny drives in a RAID 0 array. Such high speed storage used to be a hallmark of high-end workstations, so this is quite exceptional for a machine of this class.\nDespite the high-end storage capabilities, most CAD users will be more than adequately served by one or two drives.\nOur test machine\u2019s 512GB 2.5-inch SSD gives ample space for OS, applications and current datasets, but a secondary 1TB or 2TB Hard disk Drive (HDD) would not go amiss if you have a lot of data. Those on a budget may consider dropping down to a 256GB SSD, which will save \u00a3120.\nDespite packing in the graphics and storage, the Celsius J550 still compares favourably in terms of size. At 332mm (w) x 338mm (d) x 89mm (h) it is only slightly bigger than the Dell Precision Tower 3420 (290mm x 292mm x 93mm) and is actually smaller than both the HP Z240 SFF (338mm x 381mm x 100mm) and Lenovo ThinkStation P310 (338mm x 394.5mm x 102mm).\nThe chassis is reassuringly solid, weighing close to 9kg. It can sit horizontally or vertically on the desk, with detachable feet giving added stability. There\u2019s no shortage of USB 3.0 ports, with two at the front and six at the rear.\nFor everyday CAD work, performance is excellent and there is virtually nothing to separate it from the significantly larger HP Z240 Tower (see page 30) although the Z240 Tower does have optional higher end graphics, up to the AMD FirePro W7100 or Nvidia Quadro M4000).\nWith four cores and a clock speed of 3.6GHz, the Intel Xeon E3-1275 v5 is ideal for performance CAD, though the built-in Intel HD Graphics P530 is somewhat redundant with the add-in AMD FirePro W5100.\nDepending on budget, there are over 10 other CPUs to choose from, including a range of quad core Xeon E3-1200 v5 and Intel Core i3, Core i5 and Core i7. Our test machine felt very responsive when working inside Revit and MicroStation Connect, even when working with large models.\nWith only 8GB DDR4 RAM we did hit the memory limits at times so would definitely recommend an upgrade to 16GB, adding \u00a350 to the cost of the machine. With a maximum capacity of 64GB there is plenty of scope for future upgrades.\nThe one downside to the Celsius J550 is fan noise. While there was only a gentle hum in day-to-day CAD work, noise became more noticeable when rendering or hammering both CPU and GPU in our graphics benchmarks. It is not loud by any means, but there are certainly quieter machines out there.\nConclusion\nFujitsu may be late to the party with its SFF workstation but it certainly blows the competition out of the water when it comes to 3D graphics. By offering a choice of full height mid-range GPUs inside the Celsius J550, power CAD users can now have it all: the performance of a tower in a chassis less than half the size. For this reason alone Fujitsu\u2019s machine demands close attention.\nAdded to that, with a huge choice of drives and CPUs, and a well engineered chassis, there is plenty more to like about the Celsius J550. Unless your workflow demands more CPU cores for simulation or rendering, or higher end graphics for design viz, the SFF workstation now presents a truly tempting alternative to the deskspace-hungry tower.\nSpecifications\n\u25a0 Intel Xeon E3-1275 v5 (3.6GHz up to 4.0GHz) (Quad Core) processor\n\u25a0 8GB (2 x 4GB)DDR4 2,133MHz ECC memory\n\u25a0 512GB SSD\n\u25a0 AMD FirePro W5100 (4GB GDDR5) GPU (15.201 driver)\n\u25a0 Microsoft Windows 7 Professional 64-bit\n\u25a0 332 x 338 x 89mm\n\u25a0 3 years bring-In\/ onsite service warranty\n\u25a0 \u00a31,450\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/bentley-appoints-anthony-flynn-chief-marketing-officer\/","title":"Bentley appoints Anthony Flynn Chief Marketing Officer","date":1048118400000,"text":"Bentley Systems, Incorporated today announced the appointment of Anthony Flynn to the position of chief marketing officer (CMO).\nIn this newly created position, Flynn will manage the worldwide corporate marketing activities of the company and report to COO Malcolm Walter.\nThis appointment follows the campaign launch of \u201cmanaged environment\u201d for users, made possible by Bentley\u2019s V8 Generation of products. It also comes as the company has now formed vertical teams to serve its four vertical markets: building, plant, civil, and geospatial.\nFlynn was moved to CMO from his position of director of marketing for Bentley\u2019s building team, where he led the recent launch of that group\u2019s successful building information modeling (BIM) campaign. Previously, he served Bentley as president of Boston Communications, where for five years he served as senior marketing consultant to Bentley.\nFlynn\u2019s previous executive and management roles include heading corporate and product marketing for Viewlogic Systems Inc., a pioneer in electronic design automation software tools, and other positions at Bricsnet and Mentor Graphics.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/trimble-releases-more-details-of-trimble-xr10-with-hololens\/","title":"Trimble releases more details of Trimble XR10 with HoloLens","date":1574812800000,"text":"New solution includes mixed reality hardhat and a new version of Trimble Connect for HoloLens, which handles larger models much better\nTrimble is taking orders for its Trimble XR10 with HoloLens 2 system, a site-ready mixed-reality solution that enables workers to visualize 3D data on project sites. The XR10 is more than just a HoloLens 2 integrated into an industry-standard hardhat; it also includes Trimble Connect for HoloLens, a cloud-based software optimised for the mixed reality headset.\nThe XR10 device includes all of the new functionality of HoloLens 2, including a flip-up-visor, wider field-of-view and more instinctive interaction enabled through hand-tracking. It can be used to overlay Constructible Building Information Models (BIM) and other digital project data onto the physical context of the site.\nTrimble Connect for HoloLens is an application-specific extension to Trimble Connect, the cloud-based collaboration platform. New additions include step-by-step guidance for complex tasks and assigning to-do tasks and RFIs to other stakeholders.\nThe software currently relies on file-based transfer between the Connect platform and the various extensions (mobile, desktop, web, and HoloLens), but a spokesperson told AEC Magazine that Trimble is actively exploring data streaming options, such as Azure Remote Rendering. This should allow models with an incredibly large number of polygons to be streamed directly to the device. (We cover Azure Remote Rendering in more detail in this AEC article)\nFor now, Trimble is handling models of \u2018any size\u2019 in a different way. In Trimble Connect for HoloLens v3.0, which will be released on December 2, upon opening a large model in tabletop view, the app uses an \u2018innovative algorithm\u2019 to prioritise visualisation of those features on the model that are most important. The user is then provided with a section box tool where they can section the model down to certain areas of interest. Upon doing so, the app will optimise and re-render the volume inside the section box. The further narrowed down the section box is, the higher fidelity the assets in the box will be.\nOnce aligned on-site (1:1 scale), the application automatically prioritises rendering polygons that are most important to the user (largest, closest, within the field-of-view, etc.). As the user moves, this prioritisation \u201cbubble\u201d will move with them, very similar to how it\u2019s done in video games. Users also have a new Fishbowl tool that allows them to manually set a radius of visualisation. This will section a sphere around the user past which polygons are hidden. As the user moves, as will their fishbowl. Trimble\nexplained that these new tools are based on the workflow expectation that users are generally only concerned with a certain area of their construction site at any given time. The company says it can prioritize that area and turn off (or lower the LoD of) other important areas so they are always seen in high fidelity.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/a-q-a-with-andrew-anagnost\/","title":"A Q&A with Andrew Anagnost","date":1606262400000,"text":"As part of Autodesk University, CEO Andrew Anagnost held a virtual briefing with members of the press over Zoom\nEach year, as part of Autodesk University, Autodesk CEO, Andrew Anagnost, sits down with the global press to take questions. In this year\u2019s virtual event, the format was a little different, but it still gave a fascinating insight into the direction the company is taking. Topics included digital twins, construction, data exchange through APIs, thin clients, open standards, the future of Revit and lots more.\nQuestion: Can you tell us about the Spacemaker acquisition?\nAndrew Anagnost: The user is getting incredibly valuable information that would have been incredibly laborious for them to create previously, presented to them instantaneously and in real time as they\u2019re trying to work through constraints and decision processes. So, it\u2019s fully aligned with a philosophy we\u2019ve had for a while. We\u2019ve been watching them for a couple of years now.\nAnd it\u2019s our opinion, and it\u2019s the opinion of our customers too, that they built the best Artificial Intelligence cloud-based platform out there in the market right now. And, more importantly, they\u2019re actually solving real-world problems with it. That\u2019s what some of our biggest customers told us, that this stuff works. And that\u2019s one of the reasons why we acquired it.\nSo, it\u2019s really fully aligned with where we were going strategically. It\u2019s a great team. I mean, this is an absolutely fabulous team of machine learning specialists, data scientists, and engineers and I think they\u2019re going to make a fantastic addition not only to Autodesk, but to our portfolio.\nQuestion: How are customers responding to Covid, what has the impact been, and what is Autodesk doing to respond and help customers in this climate?\nAnagnost: The impacts are broadly felt, we feel them as a software company. One of the things we\u2019re seeing a lot from customers, and we\u2019re hearing a lot, is they\u2019re still working on their processes for working remotely and managing constricted environments.\nI think this is one of the places where we\u2019re working very closely with a broad set of customers, we\u2019ve tried early on to make a lot of things free, so customers could access some of these things. We\u2019ve transitioned more to paying. But there\u2019s still things out there that we\u2019re doing for customers in terms of training and other types of things that are absolutely intended to assist customers through this.\nBut every customer I talk to is learning how to work with new types of tools and new types of processes. And the interesting thing is almost all of them say, \u201cwe\u2019re not going to go back to the way we were before\u201d.\nAnother interesting thing that I will just talk about one customer generically, but it\u2019s a large customer that that builds large facilities. And one of the things they found is, that by managing the restrictions on a construction site digitally \u2013 because, you know, they can only put so many workers on the site at a time, and they had to be spaced a lot more than before \u2013 they actually have found that their throughput on the construction site is more efficient. So, they\u2019re actually building faster \u2013 in this case by adopting some of the digital tools. Even with the constrained people on the site, they\u2019re moving faster.\nBut let\u2019s be clear, this is tough on everyone. Every one of our customers across the board has seen their cash flows decrease, their buildings decrease, their pipeline of new business has been impacted. The good thing that\u2019s great about a lot of customers, our industry, is they\u2019ve proven themselves to be extremely resilient and their existing projects have continued to carry them through a lot of what\u2019s going on. But it\u2019s tough and a lot of people are nervous about how next year is going to shape up.\nQuestion: There have been attempts to use generative design for space planning, like in the Autodesk Toronto office, for example. Will Spacemaker technology displace the generative design Autodesk has itself developed for AEC?\nAnagnost: It\u2019s interesting. They\u2019re tackling different parts of the problem. And I think it\u2019s important to know that we have multiple vectors on how some of this technology works. The work that we did with generative, and Toronto, was actually working on space within a space. So, for instance, when we use generative design to design the Toronto office, we were looking at the alignment of the office within the physical space that had already been selected. We were moving walls around, moving windows around, creating sloped surfaces, and all things like that.\nSpacemaker has started a lot of its efforts at the kind of upfront urban planning stage. So Spacemaker is trying to assist design in highly constrained urban settings. And one of the things it\u2019s trying to do, and one of the things I think is really exciting about the way they position themselves, is they\u2019re trying to maximise return for all their stakeholders \u2013 the investors, the developers who are developing the properties, the local cities, and the environment that exists around that, as well as the people that live in the buildings they\u2019re producing.\nWhat Spacemaker does is it says \u2018Look, I have a highly constrained urban setting, I want to maximise the return to the developer, I want to minimise the impact on the local city and I want to minimise the impact on the local environment while maximising the utility and liveability of the space for the people who use it\u2019.\nSo, they will slice the space up into large chunks of modules. What we\u2019ve done with generative design, is when you get inside of a module, like a building or an office, we can configure that inside there using some of our generative tools. There\u2019s a great overlap between the envelope of a building or a multi-family residence or a multi-office commercial property, and what we\u2019ve done with generative design inside Autodesk.\nQuestion: Can you discuss Autodesk licensing trends, enterprise business agreements and the move from concurrent subscriptions to named user subscriptions?\nAnagnost: The enterprise licensing stuff is a very popular method that our largest customers use for accessing our entire world. So, remember, it\u2019s a consumption-based model they get. They only pay for what they use and, for instance, we just acquired Spacemaker, so we\u2019re going to be working to integrate Spacemaker with some of those offerings.\nIt\u2019s a great way for our customers to get access to technology pretty quickly, and use a broad swathe of our portfolio. Now, the move to named users \u2013 I want to make sure that we understand exactly what\u2019s going on here. The move to named users is just part and parcel of our move to a fully SaaS model.\nIf we name every user, we\u2019re able to deliver more value, more capability and more insights to every user. However, that doesn\u2019t mean we\u2019re going to abandon the idea of occasional users and concurrent use models in the cloud. We want to name every user, we\u2019re going to definitely replace the old multi-user paradigm. But what our customers have not yet seen or been able to embrace is the new paradigm that will allow them to take some of the best things we\u2019ve learned in enterprise business agreements and use them further down the market to manage occasional use inside their companies.\nSo, we will be naming every user because that\u2019s the paradigm of the future. It allows us to deliver the most personalised experience to each user, the best insights to each user, the best information to the companies that manage all the users. But we\u2019re also going to be delivering the ability to manage how much work an occasional user does, versus how you pay for a full-time user. And that will be coming too as time goes on.\nQuestion: How will Autodesk Pay Per Use model will work?\nAnagnost: It will work in a very similar [way] to the current multi-user paradigm, but with a cloud-based deployment and a cloud-based monitoring system. So, if you think about how our EBAs [Enterprise Business Agreements] work, they give access to the entire portfolio, but the customer only pays if somebody uses something for a day.\nYou can imagine an environment where a smaller customer doesn\u2019t have to buy an EBA or that kind of \u2018all-in thing\u2019, when a smaller customer owns a few, purely named-user licences for high use individuals, and then purchase a capacity pack or a plan that allows them to access the whole portfolio for a certain number of bytes over several months or a year.\nAnd they can use that for the occasional use user and use that to manage their capacity in such a way that they\u2019re not putting full licences, full named user licences on the desktop of an occasional user.\nYou will see customers blend the two just like they blended multi-user and single-user licences. But it\u2019s all going to be on a single cloud infrastructure in the cloud. Maybe a lot of people might not realise, but our existing multi-user licences were just our old licensing system, deployed in the customers\u2019 environments. We couldn\u2019t deliver named-user value, we couldn\u2019t manage it in the cloud. All of its going to be managed in one place in the cloud, and operate in this kind of hybrid mode, where people have both types of capabilities in their accounts.\nQuestion: You talked about how Autodesk is building pipelines around open data standards at the very end of your talk. Can you provide a little bit more information about this? What open standards does Autodesk see as critical for your pipeline plans? Both in AEC and in manufacturing?\nAnagnost: So, like we did with IFC, we continue to support open standards within manufacturing and the standards that allow people to collaborate information. We embrace the standards in the media and entertainment space.\nBut one of the things I hope people understand about these standards is, it\u2019s not so much about the file format standards anymore. And I want to impress upon people that the future of our industries is not files, OK? Files are what I\u2019ve classically liked to call one of those dead things walking. Everybody works on files. Everybody uses files today. But what\u2019s really powerful is, can you communicate about the information that\u2019s critical to a particular design at more of a sub-element level, not just a file level?\nSo, file exchange is great, and you can see us embracing all of these file formats, so that we can bring them all together to make sure that people can use all the data they need. But what\u2019s more important is data APIs that allow you to pull information that\u2019s been defined by standards, and you can talk to the APIs and exchange information behind the scenes. You might dump or save a file from time to time but, in the future, most of what our customers do, is going to happen without a file ever being opened by us.\nAnd that\u2019s the exciting thing about the future, it\u2019s these data APIs. The open standards are just a way for us to make sure that we\u2019re all communicating about the right things. And we\u2019re totally committed to these. But in the end, the future is going to be data APIs.\nQuestion: What\u2019s your vision for these new [Construction Cloud] products and for construction in general?\nAnagnost: At the high-level, our vision for construction is nothing less than to connect everything from the very early conceptual design phases, all the way to the final handoff to the owner, and everything that goes in between. What you\u2019ve seen with Construction Cloud, especially with the new rollout of Autodesk Build, and you\u2019ve seen us take all the great technology we have and we\u2019ve actually now built it off on a single platform, Autodesk Docs. Everything hangs off that platform now.\nOur best-in-class mobile capabilities are basically from the PlanGrid team. The PlanGrid team was a best-in-class mobile application organisation. And the PlanGrid technology is showing up in this new platform as our best-in-class mobile tools. Nobody\u2019s got anything better for mobile devices than we do.\nOne of the things that\u2019s front and centre of what we are trying to do is empowering the site with high performance, high utility mobile tools connected to a single platform. But I think what you also saw with the notion of Quantify, and the other things associated with that is a continued focus on where the money actually gets committed. And that\u2019s in the planning phase of construction, the pre-construction phase, and all those things associated with it.\nNo one is going to have better tools for pre-construction planning than Autodesk. We\u2019re going to be deep, it\u2019s all on a single platform, you can see that right now. And you can see all we\u2019ve taken from every piece of technology we\u2019ve learned over time. And if you go and you see the keynotes on this, you\u2019ll get a real sense for how you can see the best of everything we\u2019ve ever built integrated into this new offering. It\u2019s designed to help save money, not only in the planning stages, but also execute better on the construction site phases.\nYou\u2019ll see us touch every aspect of the process and I really encourage you to go back and watch some of those demonstrations of what the Construction Cloud can do now what Autodesk Build really is so that you get a really deep understanding of how it works and what problems it\u2019s targeting.\nQuestion: Is there any change in your indirect sales channel strategy? How are partners included in the company\u2019s strategy?\nAnagnost: There\u2019s really no change right now. More and more of our business comes direct. But I\u2019ve always said that partners are an important part of our business. If there\u2019s a really important change that I think is worth talking about it\u2019s the roles our partners play in helping our customers integrate all these technologies, more and more.\nThe partners are front and centre with helping the customers understand our cloud APIs, integrate our cloud APIs into their processes, and build custom solutions on top of those APIs to help the customers build out complete and stitch together their solutions.\nThe emerging role of partners, as system integrators for our customers, as trusted partners to build this and put it all together for them, is something that I think you\u2019ll see as an emerging change and an important thrust for how we work with our partners.\nThe partners will continue to sell the software; they\u2019re going to continue to get access to all of this software. Over time, the mix of how much of our business comes from direct and indirect will change. You\u2019ve heard me talk about that many times. But the amount of money our partners are making will continue to go up and up and up over time.\nQuestion: How do you see the Forge ecosystem and capabilities evolving?\nAnagnost: You remember we\u2019ve had Forge for a while, it\u2019s existed for many years now. What you\u2019re seeing now is additional aspects of Forge being exposed publicly. Things that we had behind the scenes, but nobody saw before. Like, for instance, what we\u2019re doing with Forge for manufacturing and the data disaggregation that allows people to talk to specific slices of data within the manufacturing process and light up some of these collaborative processes.\nOur goal for Forge is to be the best third-party development environment for design and making in the industry. And we think we are well on our way and we believe that by powering this industry with open standards, open ways of communicating web-based APIs and accessing the data, and also the workflows that are involved, we\u2019re going to see an explosion of innovation on top of these platforms. People building all sorts of new custom tools.\nIt happened ages ago, for instance, when AutoCAD came out. AutoCAD was a platform in its original form. And a lot of AutoCAD\u2019s big impacts early on were from third-party developers. The Forge ecosystem we\u2019re building is going to have an even more significant impact because of the way you can stitch the processes together in the cloud. Look for us to continue to add more and more to Forge but, more importantly, to improve the developer experience that a third-party sees with Forge.\nQuestion: Will we see more partnerships between Autodesk and construction product suppliers like Schneider Electric?\nAnagnost: I think partnerships in all of these spaces right now, given the digitisation that\u2019s happening in our industries, is an inevitable outcome of the rise of all these new ways of working. So yes, you\u2019ll see partnerships from us of all types. And there\u2019ll be partnerships where we overlap a little bit, where we\u2019re fully complementary, but it makes sense in a connected world like this, especially when you\u2019re trying to use open APIs and data APIs to connect things together to be partnering more closely with a broader collection of individuals. I would fully expect to see more partnerships like that in the future.\nQuestion: Autodesk has gotten a lot of feedback from customers on the future of Revit. What have you learned from that feedback? And when will some of the investments that you\u2019ve said that the company is making really bear fruit for our architecture solutions?\nAnagnost: I\u2019ve published numerous commentaries on this. I want to make sure that I point people back to the things we\u2019ve said in the past, about where we were at with our architecture customers.\nWe made deliberate choices to invest in some of the guts of Revit, and in other areas of Revit. And we knew that some of the architecture requests and preferences were not necessarily being met at the same velocity. So, this wasn\u2019t a surprise to us, in terms of some of the concerns we heard from architecture customers. We take them very seriously.\nAs a matter of fact, one of the things we took incredibly seriously, and I think you can see it in some of Amy\u2019s [Bunszel] comments, and some of the blog posts that Amy\u2019s done is that making sure we stay connected to some of our architecture customers that are further down in the market, lower down in the mid-market, where their voice may not have been heard as much as the larger architecture customers, and some of the larger architecture customers in the mid-market.\nWe\u2019ve definitely set up capabilities and facilities to make sure that those individuals have line of sight, so to speak, to Autodesk and we are engaging with lots of them in listening sessions.\nWe\u2019ve gotten a lot of great information, in terms of when they\u2019re going to see some of these investments. We actually started investing well ahead of any discussions that happened recently. You\u2019re going to see capabilities, functionality and tools for architects that are on the roadmap showing up within the next releases coming in the next year and beyond.\nThe investment in Spacemaker, $240 million, kind of represents a fundamental investment in technology that will ultimately show up in other parts of the architecture pipeline. Spacemaker was founded by architects to build solutions for architects and what they\u2019ve done is incredibly elegant, and that technology will show up in other places.\nThe other thing I want to make sure that you\u2019re aware of, is that one of the big requests we\u2019ve had from architects is \u2018how can you help us make better decisions about embedded carbon and total carbon associated with some of my designs?\u2019 I think you\u2019re going to see some exciting solutions over the next year from Autodesk that helps architecture customers make more informed decisions about the total carbon footprints of the designs they\u2019re working on.\nQuestion: What\u2019s the deal with Omniverse? Will you be selling Nvidia services and customers pay Autodesk in tokens? Or will it be handled by Nvidia, a bit like the Unity partnership?\nAnagnost: We integrated with Omniverse. We think it\u2019s the right way to get multiple data formats collaborating in virtual worlds. We\u2019re going to follow Nvidia\u2019s lead with how they want to bring all that technology to market. The real news for us is, we\u2019re all in, we\u2019re going to support this incredibly tightly, we\u2019re going to make sure we evolve and grow with it. We want customers to be able to bring together data from multiple applications, multiple types of tools, and engage in an immersive dialogues and conversations.\nHow that evolves over time is going to be directly related to how Nvidia wants to evolve their model with regard to Autodesk and Omniverse over time. We\u2019re absolutely going to follow their lead and engage with them on some of these things.\nOur goal is to make technologies like this available to our customers, because we think they\u2019re important. And open technologies like this are particularly important in our space.\nQuestion: We see Microsoft being a bit more upfront and focused in their cloud services for specific sectors like Azure Maps, Azure Digital Twins. Is Autodesk working for closer integration with these? Or is it best to be more cloud or platform agnostic?\nAnagnost: So, one of the reasons we engaged in the Digital Twin Consortium at the level we did is to work with vendors like this, to make sure that we\u2019re all solving problems together. There\u2019s going to be multiple players involved, bringing multiple types of capability to the problem. Azure\u2019s Digital Twin technology is completely different than what we bring to Digital Twins and it\u2019s not connected to BIM in the same way. However, if we work together with what Microsoft is doing, we will provide much more powerful solutions to the industry and that\u2019s one of the reasons why we\u2019re involved in some of these discussions and some of these engagements.\nI believe all of our customers are going to be getting tools from multiple places and it\u2019s the responsibility of Autodesk and companies like ours to make sure they all work together. And that they\u2019re able to stitch information together and pass information back and forth from each other because each one of these tools will solve a unique and different problem for a particular persona in a particular space. The more these things work together, the better it is for the entire ecosystem.\nQuestion: Down the road, do you see a business impact from a COVID-induced slowdown in manufacturing, construction, energy, education and other industries that have taken a big hit in the revenues because of this?\nAnagnost: The future is uncertain. The good news is there\u2019s two vaccines out there on the horizon. They won\u2019t show up anytime soon, but that will affect people\u2019s investment decisions in various industries. What we see is people\u2019s pipelines are building back up again. They\u2019re just building back up differently than they were building up before the crisis. And this has been indicative of just about every downturn we\u2019ve ever seen in our industry. The demand shifts, it shifts to other things.\nI think you\u2019re going to see over the next year, more spending on infrastructure, potentially more spending on multi-family housing, you might see some pullback and commercial, and things associated with that.\nIn manufacturing, I think you\u2019re going to see a shift to different types of products and preferences \u2013 tools that are more aligned with some of the digital workflows. You\u2019re going to see a shift to more electrified types of applications.\nWhat happens in a crisis like this is economies come back, but they come back different. And the great thing is that our customers are super resilient and they\u2019re going to adapt to these different shifting patterns in demand. There are going to be different patterns in demand, but we see on the other side of this, more people making more things more digitally connected with better decision-making tools.\nWhile certainly the near term is uncertain, the long term is absolutely not. Probably some of you read all these books about the Spanish flu pandemic, from the early 1900s. I\u2019m sure you know, it was popular reading, riveting stuff, by the way! I read a couple of them myself and you know, one of the things that came after the Spanish flu epidemic was the roaring 20s. Hopefully, we don\u2019t get a repeat of the roaring 20s, because a lot of strange stuff happened in the roaring 20s. But people came back and they came back strong.\nIf we can come back as strong and as powerful on the other side of this, heightened with more digital processes, I think all of our customers are going to be able to adapt and do new things in incredibly interesting ways.\nI guess that short term is uncertain. Long term, I\u2019m highly optimistic.\nQuestion: What established workflows are there from Spacemaker to Revit? And how will you seek to improve them?\nAnagnost: Spacemaker will incorporate BIM information right now. It consumes that information. But actually, there\u2019s lots of exciting opportunities. This is one of the places where we\u2019re very interested in building out connections and capability.\nI think that\u2019s one of the areas that we\u2019ll be talking about more in the future. How we\u2019re building connections from what Spacemaker is doing way up front in the process, to further downstream in the process and building a bridge between the general design capabilities of Revit and the generative design capabilities inside of Spacemaker. Stay tuned on some of that.\nQuestion: Can you talk a little bit more about the Digital Twin Foundation and the areas we expect to see standards? Also, can you talk about the Rockwell relationship in terms of digital twin for manufacturing appointments?\nAnagnost: Talking about core Digital Twin technology. What is a Digital Twin? It aggregates information that comes from lots of different places. It is essentially built as a database from the get-go. It\u2019s got a very fluid way of aggregating the information. It can sit in the middle of a lot of different information sources and a lot of BIM models that are surrounding it.\nThe fundamental technology you\u2019re seeing there is akin to some of the things that I showed on AU mainstage a couple of years ago, with regards to dynamically adapting views of a design, and the final stage being this representation of what was actually built. It\u2019s pretty powerful technology. It works very differently from how Revit works, for instance, and how it actually responds to the role.\nPartnerships with companies like Rockwell, for instance, on the manufacturing side, allow us to incorporate some of the information from how they instrument a factory, and ensure that it actually works with how we represent a factory or something like that. Just like how the partnership with Schneider helps us represent more effectively how a building incorporates electronic systems.\nAll of this is very connected; the hardware that actually powers the factory, the hardware that actually keeps the building running, are all things that we want to be represented accurately and captured meaningfully inside the Digital Twin.\nQuestion: Fusion is a thick client architecture. How long before AutoCAD, Revit and other desktop applications become thick clients?\nAnagnost: Fusion is a medium client architecture. Here\u2019s what I\u2019ll say. Fusion is going to get thinner, we believe in the app model, we believe the future of this space is not browsers, though browsers are going to be a major part of it, everything\u2019s going to work in a browser, everything\u2019s going to have a browser-based version. Fusion has a browser-based version, TinkerCAD, a fully browser-based application. But\u2026 the world is app based, and the apps are going to get thinner, Fusion gets thinner. And yes, you\u2019re already seeing the rise of an app-based model in the AutoCAD world.\nThe question that I think you might be asking is, are we going to see the same kind of apps build up in every one of our spaces? The answer is both Yes and No. In some cases, what you will see is a new form of application that shows up, that replaces existing applications over time \u2013 like, for instance, you know there\u2019s plenty of room for Spacemaker as a bigger client as time goes on, and there\u2019s plenty of room for Fusion to get thinner. And there\u2019s plenty of room in the Media & Entertainment space to have thinner client solutions.\nThe key thing that you\u2019ve got to recognise about Autodesk is we believe in the app model; we believe in some of these powerful new devices that are going to be able to do amazing things. And we want apps running on them with a fully multi-tenant, powerful cloud-based architecture behind it. That\u2019s what we built and that\u2019s the future we\u2019re betting on.\nWe will have solutions, and we do have solutions, that support browser-only models \u2013 AutoCAD does. There\u2019s an AutoCAD version which is completely browser-based. It\u2019s beautiful. It\u2019s been featured on mainstage in numerous forums because of its power and capability. It has an app model too and then it has a heavily thick desktop version. Look for that world to show up everywhere. But app first, it\u2019s going to be app first, everywhere.\nFinal words from Andrew Anagnost: We are living through unparalleled times right now. They\u2019re incredibly different, difficult. They\u2019re challenging from a business perspective. They\u2019re challenging from a human perspective. We are all learning to adapt and change and respond in ways that we really never wanted to. Nobody wanted to be in the current situation we\u2019re in right now. But here\u2019s what\u2019s amazing about what\u2019s happened \u2013 whole entire industries, thanks to the rise of a lot of digital infrastructure, have been able to adapt the way they work to an unprecedented situation.\nIf this situation hit twenty years ago, the impact on all of us would have been significantly more adverse, even if it hit ten years ago, it would have been more adverse. It\u2019s amazing, actually, how much we\u2019ve been able to adapt, even with all the struggles that are going on right now. If you look out on the other side of this, people have now realised that integrating and digitising is not optional. It\u2019s the way to create a more resilient and more productive business model. I think what you\u2019re going to see on the other side of this is people using technology in all sorts of new ways.\nWe\u2019ve learned that even in the software business, we don\u2019t all have to be in the office every day to get our job done. What kind of productivity? Is that going to open up long term? I spent three hours of my day, every day, maybe a little less, commuting back and forth to the office on the Bart train, OK. What have I done with that time? I\u2019ve given some of it back to my family. I\u2019ve given it some of it back to work. And all I know is that I don\u2019t miss being stuck on the train during that time. We\u2019re all going to be adapting to these new models of working, we\u2019re not all going to be on the train every day. I think the other side of this is something to be potentially excited about and to be optimistic about.\nThese are tough times. We will get through these tough times. And on the other side of this, there are going to be better times. And what we\u2019ve learned through all of this, we are not going to forget. I\u2019m looking forward to seeing how all of our customers take what they\u2019ve learned from this and turned it into an amazing new set of possibilities.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-grasshopper-archicad-live-connection-now-available-for-mac-os\/","title":"NEWS: Grasshopper-Archicad Live Connection now available for Mac OS","date":1481673600000,"text":"Powerful tool links early stage design to BIM\nGraphisoft\u2019s Grasshopper\u2013ArchiCAD Live Connection, a bi-directional \u201clive\u201d connection between Rhino and Grasshopper to ArchiCAD is now available for both the Windows and Mac operating systems.\nThe software is designed to fill a gap in the design process between early stage design and BIM by allowing basic geometrical shapes created in Rhino to be translated into full BIM elements in ArchiCAD, while maintaining algorithmic editing functionality.\nUsers can download the free Grasshopper-ARCHICAD Live Connection plug-in for ARCHICAD 20 on Mac OS here.\nTo learn more about the software, read our review.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nAutodesk NavisWorks\nNEWS: Trimble releases Trimble MEPdesigner for SketchUp 2.0.\nNEWS: Dell slims down mobile workstations for Intel Skylake\nGrand Central - a glimpse into the future of FM\nHP Mars Home Planet competition enters rendering phase\nThe pillars of Fabrication Integrated Modelling (FIM)\nRevit: Shared Parameters\nNEWS: AMD makes aggressive play for professional VR\nAdvertisement\nAdvertisement\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/building-regulations-part-l\/","title":"Building Regulations, Part L","date":1165449600000,"text":"Part L of the latest Building Regulations, which came into force in April 2006, is designed to cut carbon emissions by at least 20 per cent in dwellings and 27 per cent in other buildings, compared to the previous 2002 regulations. AEC Magazine discusses the implications for the building industry with CADline\u00dds David Payne, product manager for Cymap.\nHow big an impact will the new Part L have on architects and building services engineers?\nDavid Payne: This drive to improve energy efficiency in new building design provides a real challenge for the whole building industry. In particular, it will have a profound effect on the traditionally \u00d9arms length\u00dd relationship between architects and engineers.\nHistorically, individual products supporting the industry have tended to be developed autonomously. In meeting the demands of this new market however, we have recognised the need to bring together the best technologies to provide an integrated, end-to-end solution designed to speed up the design process.\nHow would you describe the architect\/engineer relationship?\nDP: in the past, the architect has been able to complete a design in isolation and then pass it to the building services engineer to design and add in the required services. Where a design has been less than efficient, the engineer has been able to solve the problem at this late stage by, in effect \u00d9throwing bigger or better services at it\u00dd in order to ensure compliance. Under the new more stringent regulations however, engineers will no longer be able to correct an inherently non-compliant design in this way and will need to be involved earlier in the design stage. Whilst architects will have the greatest impact on whether or not a building meets the demands of Part L, the prominence of the engineer will be raised along with the need for better communication between the architect and engineer.\nIn future, architects will need to work much more closely with their engineering counterparts from the initial concept development of a project to avoid the risk of months of abortive design work with potentially disastrous implications on building costs and the imposition of penalty clauses for late completion.\nBut does the industry recognise the need for change?\nDP: As a provider to the industry, we are convinced of the need for an integrated approach. It is encouraging to see, therefore, that closer collaboration is starting to happen in practice, with positive results. Indeed, we have already seen some architectural practices working closely with consultant engineers to check building services and Part L compliance and we are working hard to help bridge this gap from both a technology and support service perspective.\nYet in order for the two groups of professionals to communicate effectively, their respective design software must also be able to \u00d9talk to each other\u00dd. In order to make this happen, we are using our experience of both architectural design and building services automation software to develop a fully-integrated, end-to-end software solution.\nWhat does such an integrated solution look like in practice?\nDP: Our goal is to bring together three key software products in Cymap, Tas Building Designer and Autodesk Building Systems (ABS). CADline\u00dds Cymap building services design solution, for example, offers mechanical and electrical modules and can be used for all forms of commercial & domestic property, both new builds and refurbishments.\nIt provides comprehensive solutions for heating, cooling and energy consumption, piped and ducted services, and low voltage wiring and lighting design and can be used at all stages of the design process, from initial conception through scheme design to the production of working and as-built drawings. By enabling users to perform both routine and complex calculations associated with heat loss, heat gain and energy consumption at each stage, Cymap allows engineers to improve both productivity and design accuracy.\nNext, Tas is a suite of software products, which simulate the dynamic thermal performance of buildings and their systems, enabling users to compare alternative heating\/cooling strategies, energy demand and check designs for Part L compliance. The main module is Tas Building Designer, which performs dynamic building simulation with integrated natural and forced airflow.\nAnd finally, ABS provides a building design and construction documentation application for mechanical, engineering and plumbing design and construction professionals based on the AutoCAD software platform \u00b1 and in looking ahead to 2007, Revit Systems \u00b1 enabling designers and drafters to improve accuracy, productivity and co-ordination.\nHow important is improved communication in making this happen?\nDP: In a word, essential. As stand-alone solutions, Cymap, Tas and ABS each deliver powerful productivity benefits, yet by looking at the bigger picture and bringing them together, we aim to make compliance easier and project delivery smoother and more cost-effective.\nTo achieve this, Cymap is already linked with Autodesk\u00dds ABS software and we are well underway to achieving similar collaboration with Tas. The underlying objective is an industry first \u2013 to provide intelligent, two-way dataflow throughout the design process, so removing the need to input data more than once which in turn will improve accuracy as well as saving time and money.\nBy communicating in this way, not only will it ensure compliance but also make design intent communication between the architect and engineer easier and speed project delivery. In short, an integrated approach will make the whole life-cycle of project management more efficient.\nAs ever, it\u00dds good to talk.\nCADline is a leading building services and design solutions consultancy and Autodesk UK Premier Solutions Partner","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-smartglasses-used-for-bim-integration\/","title":"NEWS: Smartglasses used for BIM integration","date":1422489600000,"text":"3Squared and Autodesk to pioneer new concepts for wearable technology in the construction industry.\n3Squared, a specialist in web based software and mobile apps for the construction industry, is working with Autodesk in Sheffield and San Francisco, to demonstrate how wearable devices could deliver tangible benefits on construction sites. This includes looking at ways to improve information delivery, enhance on-site communication between engineers and enhance health and safety.\nThe first pilot project will integrate Smartglasses with BIM (Building information Modelling) 360 Field, a Construction Management application developed by Autodesk. It will explore different ways and opportunities to capture, present and display information on site.\n3Squared and Autodesk will also investigate how wearable devices could be used to deliver project documentation directly to an engineer on-site, to capture video and pictures and other field data during the commissioning and handover of a building project. In addition, they will look at how to provide access to drawings and plans and how technology such as iBeacon could be used to deliver contextual-based information on site.\nTim Jones, Managing Director of 3Squared, said, \u201cIt made sense to partner with Autodesk on this project, as both our companies have extensive experience in providing technology solutions to the construction industry. We firmly believe that wearable technology could have a significant impact in the construction industry. There are endless opportunities where these technologies could be used to improve information delivery, health & safety and much more.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nStructural BIM\nArtificial Intelligence (AI) frontiers in construction\nNEWS: Leica simplifies reality capture with BLK3D\nTrimble adds bridge design functionality to Tekla Structures\nISO 19650: going global\nPreoptima introduces Carbon Twins\nnima virtual conference to focus on data\nAsite publishes Digital Twin report\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/revit-2021-1-extends-focus-on-automation-and-computation\/","title":"Revit 2021.1 extends focus on automation and computation","date":1596672000000,"text":"Autodesk enhances Dynamo for Revit and adds new sample studies to Generative Design for Revit to optimise room layouts\nAutodesk has concentrated on automation and computation for the new Revit 2021.1 release, adding new sample studies for Generative Design for Revit, and introducing a new 2.6 version of Dynamo for Revit along with new Dynamo nodes.\nThe software also offers better interoperability with Autodesk Infraworks and Autodesk FormIt Pro and enhanced data exchange with third party products, including import for SAT, Rhino, and Sketchup 2020 files.\nFor project teams, there\u2019s a new \u2018suite of features\u2019 for better sharing and model coordination, including the user request to reset shared coordinates, and a new way to share 3D project views. Design for Revit was first introduced in Revit 2021. The new Grid, Stepped Grid, and Randomize Object Placement sample studies can be used to create and validate room layout design alternatives, and help address social distancing guidelines in response to Covid-19.\nThe 2.6 release of the visual programming platform Dynamo for Revit introduces new features like the Documentation Browser, as well as faster performance. New nodes include Path of Travel, Generative Design, and new platform categories for \u2018automating design across all disciplines\u2019.\nTo improve workflow at early-stage design, complex curved geometry and material overrides can now be transferred \u2018seamlessly and accurately\u2019 from FormIt Pro to Revit.\nFor civils, interoperability has been improved between InfraWorks and Revit, and it\u2019s now possible to import and document bridge designs \u2018more reliably\u2019, although users will have to wait for a new version of InfraWorks to be able to do this.\nFor collaboration and coordination, users can now \u2018quickly create and share 3D views\u2019 with all project stakeholders without having to share the Revit model. BIM 360 integration has also been enhanced with \u2018more service resilience\u2019 and up to 30% faster download times when opening, syncing, and reloading Revit models in the cloud.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/bentley-empowered-2008-conference-report\/","title":"Bentley Empowered: 2008 conference report","date":1214870400000,"text":"Bentley\u00dds annual customer event was held in Baltimore this year. Called Bentley Empowered, the main focus was on Sustainable Infrastructure; environmental challenges and opportunities ahead for construction. Martyn Day reports.\nStarting last year at Autodesk University and moving doggedly onwards, the Green bandwagon has quickly made its way across the prairie of every CAD vendor\u00dds conference agenda. That is not a complaint, I am really happy that this is the case, as talking about the problem is perhaps the start of addressing the global issues that we face\nGoing \u00d9Green\u00dd is a great opportunity for all software vendors to add great analytical functionality to their products. After having acquired so many of the leading analytical developers Bentley has great potential to provide essential products to assess design performance and environmental impact.\nBentley is also an important CAD vendor to get involved in this messaging, due to the kind of clients it has. While it may have substantially less seats sold than the volume empire of Autodesk, Bentley\u00dds corporate customers get involved in huge multi-billion dollar markets and projects. This represents Big business reacting to the market conditions and the implications of \u00d9sustainable design\u00dd, even before there\u00dds much of a legal infrastructure to enforce it.\nThis was the second time that Bentley had brought its customer event to Baltimore and this year the company worked hard to do its best to earn its green credentials. It turns out that 65% of local Bentley colleagues travelled to Baltimore via bus or carpool and only hotels within walking distance of the Baltimore Convention Centre were contracted for attendees.\nThere was the onsite recycling name badges and lanyards at BE Conference and the brochure printed on Forest Stewardship Council (FSC) certified paper. The registration counters were seeing their third year of usage and water jugs and recycled paper cups were everywhere! Bentley has committed to reducing its carbon footprint by 15% before the end of 2009, based on its 2006 levels.\nSustainable keynotes\nThe first morning\u00dds keynote addresses were an interesting mix of Bentley execs and a Green business guru, Andrew Winston, author of \u00d9Green to Gold \u00be How smart companies use environmental strategy to innovate, create value and build competitive advantage\u00dd.\nCEO Greg Bentley was up first to report on the company\u00dds performance, outlook on sustainability and the challenges the company faces. Greg Bentley said that Sustaining Infrastructure was not just a buzz-term or new mission but an enduring commitment for the company.\nBentley talked of how he believed that sustaining our environment is not incompatible with sustaining our economies, given the necessary infrastructure investments. He added, \u00fdI\u00ddm a believer in global markets, so I personally believe that the financial returns on infrastructure investment \u00b1 exemplified by the opportunities in clean energy self-sufficiency, efficiency, and conservation \u00b1 are sufficiently high that these investments will inexorably be forthcoming.\n\u00fdI even believe that these superior returns will motivate continued institutional reforms to encourage public-private partnerships in those infrastructure domains that have been traditionally funded only through taxation. I have no doubt of the multiplied opportunities that lie ahead for all of us, as a result.\u00af\nOn the company\u00dds performance, Bentley Systems has generated continuous revenue growth, at a compound annual rate of 14% over the past 15 years. In 2007, it had revenue growth of nearly 16%, to $450 million.\nAs Bentley is the largest privately-owned software company in the USA it does not have to report to the markets, so this yearly report is the main source of Bentley\u00dds annual performance. The company has 2,800 employees working in 51 countries, with the majority of its business and employees being outside North America. Greg Bentley also said that over the past 15 years the company has invested over one billion dollars in R&D and acquisitions. Following on from Greg Bentley\u00dds belief that sustainability can be achieved without impacting business, Andrew Winston came on stage to highlight companies that are doing just that.\nMaybe it\u00dds my European sensibilities that found the whole profit from going green a little distasteful but Winston\u00dds presentation did slowly win me round, or at least explain to me how a capitalist mindset can justify what needs to be done and how business must adapt.\nIn Europe we are used to Government intervention and legislation, while America still prefers the free market and less interference with business.\nIn the UK we have Part L of the building regulations driving environmental design, while the US has LEED accreditation which is completely optional.\nBuilding regulations in the US also vary from city to city. That said, the US Federal Government and some cities (3,500 mayors) are stipulation LEED compliance on tax-payer funded projects. Winston was especially scathing of companies that marketed green but were in fact brown \u00be highlighting one example of a garbage (rubbish) bag company that claimed to have bio-degradable plastic sacks.\nTo degrade they required sunlight, so in a landfill they had little chance of degrading! Winston was all about building an \u00d9Eco-advantage mindset\u00dd \u00b1 this may well be business-guru babble but I figure anything to get the pure-business minded on-board is a good thing. We were given a copy of the book and I\u00ddve started to read it, despite my early scepticism. As if looking for a reward for recycling the use of the word \u00d9sustainable\u00dd to new levels, Keith Bentley, CTO, took to the stage and talked about Sustainable Software. I thought that this was going to be a hammer-hit too far on the sustainable nail, but hey, this is Keith Bentley and Keith doesn\u00ddt do marketing waffle!\nIn fact, Keith Bentley made a very interesting pitch explaining the long-term view to development that Bentley takes versus other companies that were not mentioned. Bentley rarely gets the recognition that it deserves for its technical prowess in limiting customer pain by rarely altering the file format, actively supporting other vendors\u00dd file formats as native and generally providing stability of its platform.\nThis is absolutely essential given that the kind of projects that MicroStation gets used on can last years, if not decades. Keith Bentley recommitted to continued development without the pain. As is usual with Keith\u00dds addresses he gave some insight into future hardware requirements: dual if not multi-core machines, Windows Vista and 64-bit.\nAthens\nThe next release of MicroStation, due Q4 2008, got another airing. I\u00ddm sorry there\u00dds no pictures of it here, I did ask, but I was told Bentley were not promoting it yet, despite showing it to 6,000 people last month and 6,000 the previous year in BE, Los Angeles. Athens will be mainly an under-the-hood release with re-architecting for consistency of big datasets, federated distribution, complex workflows, user interface, breadth and depth of services and functionality.\nThe big feature focus remains on conceptual design tools, dynamic views, distributed projects management, and built-in geo-coordination.\nGenerative Components\nThis wouldn\u00ddt be a Bentley report without something on Generative Components (GC). The big news on GC wasn\u00ddt launched at BE but a few weeks prior at the American Institute of Architects (IAI). Called the Discovery Subscription programme, for $250 (if you are in the US), you can download MicroStation and GC and learn all about the innovative parametric design application that everyone is talking about.\nSince the departure of Bentley\u00dds chief scientist, Dr. Robert Aish to Autodesk, Bentley\u00dds GC marketing has gone into overdrive and it wants to spread the GC word everywhere. Although you get a full copy of MicroStation, it has to be used in GC mode.\nBE Communities\nThe new BE Communities website is Bentley\u00dds attempt to create a web destination for infrastructure community members to connect, communicate, and learn from each other. There are Forums, Blogs, a Wiki and Resources. With a corporate culture akin to Intergraph or IBM, this is a positive move on behalf of Bentley. Many of the bloggers have interesting opinions and there are some personalities worth knowing. Unfortunately, the look and feel of the site isn\u00ddt perhaps as relaxed as I would have hoped but it\u00dds a start to establishing a more personal relationship with its clients.\nThe last point to mention here but the first thing I noticed at the even was Bentley\u00dds new logo, colours and marketing. After a long hiatus, Bentley is back into marketing itself to the outside world, with some new executive appointments and a company revamp.\nThis literally happened a few months before the BE event and so it\u00dds not fair to judge what direction or vision Bentley will be aligning with. After years of concentrating on its installed base, it seems that the company is now keen to promote its image and products in a more aggressive way, which is good news for those customers frustrated with Bentley\u00dds self-imposed lack of visibility.\nWhile much can be said of jumping on the green bandwagon, it is Bentley\u00dds massive armoury of analysis tools that stand the company in good stead to deliver probably the most comprehensive suite of compliance tools to the building and infrastructure markets. If we take BE 2008 as the call to arms, next year I expect to see the company start to better-integrate its suite of analytical tools across its product set.\nIn the next issue of AEC Magazine we will look more in depth at Bentley\u00dds BIM and Beyond vision, which was presented in a break-out industry session.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/regenerating-london-king-s-cross\/","title":"Regenerating London King\u2019s Cross","date":1401148800000,"text":"Architectural practice John McAslan + Partners relied on MicroStation to develop this iconic station redevelopment that melds a Grade 1 listed building with state-of-the-art design.\nBuilt in 1852, King\u2019s Cross railway station is one of the main transportation hubs in London. As with the rest of the United Kingdom\u2019s rail network infrastructure, the station has struggled in recent years to manage increasing passenger numbers and to provide the space and amenities commuters expect.\nThe \u00a3547 million redevelopment of King\u2019s Cross station is at the heart of one of the largest regeneration projects in Europe, with 67 acres of brownfield land being redeveloped to create eight million square feet of offices, retail space and housing.\nThis complex urban regeneration project faced many technical challenges, including the potential impact on the mainline station, a number of underground transport connections, and the nearby St Pancras International railway station. The redevelopment also had to maintain the historic Grade 1 listed building, which required continual verification from English Heritage and other statutory bodies.\n\u201cKing\u2019s Cross station is not just a building; it is a major transportation node in a highly sensitive part of the city,\u201d explains Cliff Green, project technology manager at John McAslan + Partners. \u201cThe way the station connects with the city and supports the urban regeneration project is hugely complex.\u201d\nFor a project of this size and complexity, architects John McAslan + Partners needed equally sophisticated models and modelling tools. MicroStation\u2019s federated model structure allowed the large design team to share common files between multiple disciplines. Using Bentley tools they were able to merge and organise numerous elements of the project including heritage and conservation work on current structures, as well as existing services and transportation infrastructure.\nRemaking a London icon\nThe King\u2019s Cross redevelopment project is operated by Network Rail and sponsored by the Department for Transport. The project required a new design and construction, reusing existing structures and restoring historic building elements, including the train shed and the station\u2019s previously obscured historic faade.\nGiven the number of organisations involved in this complex project, from design through fabrication and construction, it was essential to enable efficient collaboration and information sharing. MicroStation\u2019s federated model structure ensured everyone on this large design team had access to high-quality, consistent, validated data.\nThe collaborative process enabled the project team to avoid disruption to London Underground passengers and allowed Network Rail to meet its promise to TfL that no mainline train service would be cancelled due to redevelopment.\n\u201cWith so much information to share with so many different parties, the speed and effectiveness of information capture and collaboration was a key aspect of this project,\u201d Mr Green said.\n3D modelling\nThe western concourse semi-circle diagrid roof, which rises 20 metres and spans 150 metres, involved engineering and co-ordination challenges.\nThe new concourse, which covers 7,500 square metres, and is the largest single-span structure in Western Europe, could only accommodate supports at a limited number of points, due to various subway and services tunnels. The roof also had to be constructed in a way that did not touch the historical structures. Accurate 3D modelling of the roof and surrounding structures was vital to successful co-ordination between an international team of architects, structural designers and fabricators.\nDesign verification\nBecause the station is an important historical structure, one of the key challenges for this project was the need for on-going design verification during the extended heritage and conservation negotiations. High-quality, accurate 3D models were vital to the restoration of the historic south facade, and the refurbishment of the vaulted main train shed (including incorporation of photo-voltaic arrays).\nUsing detailed survey data along with MicroStation\u2019s power to handle complex infrastructure project data, John McAslan + Partners was able to weave historical structures into a modern project. \u201cWe were able to speed up the design process and could accurately predict the outcome of works to the existing historical fabric,\u201d explains Mr Green. \u201cIt allowed savings during the construction phase through the reduction of errors in documentation and also for savings at scheme level by allowing rapid communication of the design to the client, consultative organisations and to project collaborators.\u201d\nThe firm also used MicroStation to verify design concepts, for example assessing the way new buildings would work with the existing site and how the building\u2019s flow would work for passengers, including everything from concourse layout to signage.\nIn addition, the concourse is now clad with 5.2 million tiny ceramic tiles that had to be laid over a two-way curve with construction joints. John McAslan + Partners used MicroStation to model each tile to assess how they could be best installed \u2014 a process that simply could not have been carried out manually.\nSupportive technology\nAs the project progressed, John McAslan extended its use of Bentley software from 2D and 3D, to building information modelling (BIM) and GenerativeComponents, without the need to step outside the MicroStation interface. \u201cThis is a really smart way to work,\u201d says Mr Green. \u201cWe can work in large teams and use 2D, 3D, and BIM to suit the state of the project, the type of project, and the skills of our team. The flexibility of MicroStation offers a real advantage and allows us to work to the best of our ability.\u201d\nThe station remodel has enhanced passenger amenities, rationalised operational activities, and significantly increased retail space.\nIn addition, John McAslan + Partners played a key role in the wider transformation of the King\u2019s Cross area. This included improved infrastructure, social, and commercial changes that now connect the station with the substantial King\u2019s Cross Central scheme to the north, as well as improved interchange links with the London Underground, St Pancras station, Thameslink services, taxis and buses.\nImages Courtesy of John McAslan + Partners\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/reality-capture-modelling\/spot-in-construction-boston-dynamics\/","title":"Spot in construction","date":1606003200000,"text":"Covid-19 has not stopped Boston Dynamics from working with AEC hardware and software developers like Trimble, together with advanced users like Foster + Partners. Martyn Day explores the impact the quadruped robot is having on construction\nWhen Boston Dynamics agreed to come and talk at AEC Magazine\u2019s NXT BLD event in 2019, the company was just about finalising the design of its quadruped robot Spot before it moved to mass production. This was a major move for Boston Dynamics as it had spent decades developing robot technology with no obvious plan to productise its innovations, funded by bodies such as the US Defense Advanced Research Projects Agency (DARPA), then Google, which helped it create Atlas, Petman, BigDog, Wildcat and Spot robotic platforms. Founded in 1992, Boston Dynamics was bringing its first robot to the mass market.\nWhile talking with Boston Dynamics, it was clear that while they had designed Spot, it was certainly a technology in search of an application. As a quadruped robot, Spot should be able to handle construction sites, areas of danger, repetitive tasks like site survey, so the company was reaching out to firms in that space to see what they could do with the platform. Fortunately, at NXT BLD, the laser scanning companies sent over their development teams and they had even asked for the payload bay specifications to 3D print platforms for their scanners. Both Topcon and Faro were carrying out live scanning tests on the show floor!\nRoll forward a year and a half and Boston Dynamics has just announced a major partnership with Trimble and Foster + Partners, with more to follow. The irony here is that the chances of seeing robots in the wild has increased dramatically, while the chances of seeing humans outside their houses has dropped dramatically. Thank you Covid.\nTalking with Brian Ringley, construction technology manager at Boston Dynamics, it was interesting to hear how Spot has developed to become a construction worker.\nThe chassis design has been continually refined to offer better protection and weather sealing and firms like Tesla are using Spot to help with the rapid construction of its \u2018Giga factories\u2019 for batteries. The need for extended operation, beyond the 90 mins operation of the onboard battery, has led to an update coming soon, which will allow Spot to automatically head to recharge itself. At the same time, the much-seen articulating arm will be available next year, allowing new interactive applications of the robot.\nWith that, Boston Dynamics has been working on the controller software APIs allowing developers to interface with Spot\u2019s sensors and even tap into its power in the payload bay. After a bunch of refinements, Spot is ready for duty.\nThe Trimble partnership\nThe first major developer to announce a partnership is Trimble. The company has extensive experience in construction robotic automation featuring ultra-precise GNSS (Global Navigation Satellite System) and machine control to full automated vehicles, such as grading systems.\nWe talked with Martin Holmgren, the company\u2019s general manager for buildings field solutions. He explained what Trimble brings to the party, \u201cWhat makes Trimble unique and I think the preferred partner for Boston Dynamics is that in the construction world we already have a very heavy investment and focus on autonomy. And whether it\u2019s an excavator, or a 70 pound Spot robot, for us it\u2019s another automated vehicle in the mix.\n\u201cIn a Trimble world we have the full 3D model of what should be built, as well as information coming out of say, a laser scanner or other sensor tool, that can rapidly augment the understanding for a robot of what\u2019s around it \u2014 how to best navigate it, but also things like how it should position itself to optimise the value of this scan. So there\u2019s a lot of AI and kind of next level intelligence that we can bring to the workforce and we intend to bring Spot both to the civil construction sites, the earthmoving sites, but also to indoor environments. With Spot\u2019s size and its agility, it\u2019s is a really good fit for us both in and outdoors.\u201d\nAfter hours\nAutomated anything on a construction site comes with health and safety risks which, at the moment, limit the mixing of people and heavy automated devices, but there will come a time when that will be the norm. I asked Holmgren if Spot changes that concern, or if it was going to be a limiting factor in its deployment? He replied, \u201cIf you had a laser scanner on Spot\u2019s back, when everyone\u2019s left for the day, there\u2019s no human obstructions. You could have Spot walk every floor of your building, during the off hours of the construction side, collect a complete set of point clouds that are ready to go, available in the computer of the project manager when he arrives at the site in the morning.\n\u201cIt actually doesn\u2019t require anyone to be there. Digital design, digitise the construction site, that\u2019s where we add value.\u201d\nThe earth moves\nLooking at outdoor examples, Holmgren outlined one concerning the digital terrain model, which needs to be monitored each day, or every three or five days, so they can know how much material has been moved and how much material needs to be moved to achieve the features of a specific site. \u201cThis ties back to cash flow for these companies, because the subcontractor wants to be paid quickly as possible. So, there\u2019s tension in the construction process between determining the right time for an investment and for payments.\n\u201cOn smaller construction sites, you may need to achieve a certain terrain model using a total station and Spot with a Prism or GNSS, we can use it for layout, essentially meaning we could even place stakes in the ground with Spot\u2019s arm.\u201d\nPower in reserve\nTrimble\u2019s equipment already comes with onboard batteries that outlast Spot\u2019s 90 minute duration so it doesn\u2019t need to tap into the robot\u2019s power supply, maximising it\u2019s longevity. Trimble is looking at perhaps tapping into Spot to get a charge boost when it goes back to the recharge station.\nI asked Holmgren about reliability, as this is Boston Dynamics\u2019 first prosumer product and construction sites are exposed and a potentially unfriendly place for precision electronics and servos. \u201cWe have, of course worked with Boston Dynamics for quite a while now we\u2019ve seen the evolution. While Spot looks identical to what you saw in 2019, performance and capabilities have evolved tremendously.\n\u201cThe reliability is not at the same level as you would expect from mainstream construction technology but I would say they\u2019re getting there. Trimble is very, very careful with putting our brand and our reputation behind a product. We have units in North America, New Zealand, India, testing them in different climates, different temperatures. Like any brand new technology you bring onto a construction site, whether that\u2019s robotics or something else, it will be improving over time, but we have determined that we\u2019re comfortable bringing this to market and that the benefits by far outweigh the challenges.\n\u201cOf course, we offer Trimble support and we\u2019re comfortable with all of European Union, US, Canada, Australia, Japan, Singapore, as starting markets and we will have local service centres at each of these locations, with a level of service capability to provide continuity of operation for users.\u201d\nPricing for Spot through Trimble has yet to be set but they are looking at both outright purchase and monthly hire.\nFoster + Partners\nAs part of Boston Dynamics\u2019 Early Adopter Program, Foster + Partners Applied Research + Development group (ARD), has been exploring the use of Spot on construction sites, by laser scanning on a regular basis to easily compare the \u2018as-designed\u2019 models against the \u2018as-built\u2019.\nThe team used its Battersea Roof Gardens mixed use project (the third phase of the Battersea Power Station development, as a testbed). Spot followed a designated path on a weekly basis, scanning as it went, which generated a sequence of highly comparable, consistent models.\nMartha Tsigkari, partner, Foster + Partners explained, \u201cThe ability of Spot to repeatedly and effortlessly complete routine scans, in an ever-changing environment was invaluable, not only in terms of the consistency but also the large amount of high-quality data collected.\n\u201cThrough this process we developed a sequence of scans that may help us track the project progress against timeframes as well as facilitate regular comparisons against the BIM model.\n\u201cOur scans can ensure that very quick and accurate changes to the newly designed system could be made to accommodate the differences captured by the scans \u2013 all in a matter of days. This could result in savings both in terms of time and money.\u201d\nFoster + Partners has also used Spot to construct a digital twin of its own campus in London. The team built up a fourdimensional model, showing how the space changes over time. Adam Davis, partner, Foster + Partners, said, \u201cCombining temporal and spatial information with data from sensors that read environmental conditions and occupancy, we can construct an intricate model of how people, furnishings and environmental conditions interact. This, in turn, helps us to operate our premises more efficiently and to anticipate how new designs will perform.\u201d\nConclusion\nWhile Spot may be a novelty as it\u2019s the first roaming robot in construction, the reality is that it brings real business cases with it and will expand and drive further digitisation of the profession. It will drive Digital Twins and deployment of 4D and will also help in spotting errors on site quickly.\nWhile talking with Holmgren, we discussed the development of swarming technology and the interaction of robots on site. It\u2019s too easy to just wonder at Spot as a singular device but imagine a construction site as a network of machines, each carrying out their task, all feeding back to a centralised system, as they sculpt, position and place everything in the right place, in the optimum sequence, pausing only to recharge.\nThere will be robots surveying the site to watch over the work of other robots and add additional layers of geometric feedback and data to the cloud, where AI can auto recognise building components, ensuring the next ones arrive on site just in time. Construction sites of the near future are going to be very different places, if we ever manage to leave home again.\nSPOT will cost $74,500 for a base model and obviously increases with payload and selfcharging stations, articulated arm.\n\u25a0 bostondynamics.com \u25a0 construction.trimble.com\/spot\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/workstations\/reviews-boxx-apexx-a3-3rd-gen-amd-ryzen\/","title":"Review: BOXX Apexx A3 [3rd Gen AMD Ryzen]","date":1575936000000,"text":"BOXX is one of the first to offer the new 16-core AMD Ryzen 9 3950X, delivering impressive performance for rendering with no compromise for CAD, writes Greg Corke\n2019 will go down as the year that AMD really took the fight to Intel. The chipmaker\u2019s 3rd Gen AMD Ryzen CPUs (the 3000 series) not only delivered excellent multithreaded performance (thanks to more cores than Intel at similar price points) but impressive single-threaded performance as well. F\nor designers, engineers and architects, the benefits for ray trace rendering were clear but, more importantly, there was very little compromise in CAD or BIM software where single-threaded performance is king, something that could not be said for 1st or 2nd Gen AMD Ryzen.\nAMD launched its first Ryzen 3000 CPUs this summer, with five models ranging from the 6-core Ryzen 5 3600 (3.6GHz, 4.2GHz Turbo) up to the 12-core Ryzen 9 3900X (3.8GHz, 4.6GHz Turbo), which we reviewed in August. Now there\u2019s a new 16-core model, the Ryzen 9 3950X (3.5GHz, 4.7GHz Turbo), that boosts multithreaded performance even higher. And at \u00a3625*, it\u2019s exceptionally good value. A few years back, a processor with that kind of spec was almost unheard of and would have set you back several thousand pounds.\nBOXX is one of the first workstation manufacturers out of the blocks, making the AMD Ryzen 9 3950X available in its Apexx A3 workstation.\nTo see what the 16-core CPU was capable of, our first port of call was the push-button ray trace rendering tool KeyShot. Here, the Apexx A3 completed our 4K, 128-pass rendering in 135 secs, nearly twice as fast as an 8-core Intel Core i9 9900K overclocked to 4.9GHz.\nBut what does this really mean to your average CAD user, who also uses ray trace rendering software? Rendering the same scene with 64 passes at FHD resolution took a mere 18 secs and with 128 passes, only 35 secs. At this speed, you really can render and adjust without interrupting your flow.\nAt the other end of the spectrum, rendering a top-quality 256 pass, 7,680 x 4,758 resolution scene took just over 18 minutes. In short, you no longer need to leave jobs like these to run overnight.\nSingle-threaded performance is equally impressive. In the past, it was unthinkable to have so many cores in a CPU without negatively impacting performance in CAD, but with the AMD Ryzen 9 3950X, you really can have your cake and eat it too.\nIt completed our single-threaded Solidworks IGES export test in a mere 80 secs. Of all the workstations we\u2019ve tested for this magazine, only the 8-core Intel Core i9 9900K overclocked to 4.9GHz beat this with a time of 75 secs.\nWith 128GB memory spread across four 32GB DDR4 DIMMs, our test machine is fully loaded and set up to handle some very large datasets. Most CAD-centric workflows should be fine with 64GB, which would considerably bring down the overall cost. of what is quite an expensive machine.\nGraphics is courtesy of the excellent Nvidia Quadro RTX 4000 (8GB), which is a great match for mainstream real-time viz and VR workflows. In the arch-viz focused Enscape, for example, we got a decent, if not silky-smooth 19 frames per second at 4K resolution using the very complex museum model. The Quadro RTX 4000 is also good for GPU rendering, but if you\u2019re considering a CPU with 16 cores, you probably have established CPU-centric rendering workflows anyway. The other benefit of going down the CPU route is having a massive 128GB to play with for loading in complex models, HDRi environments and materials. With the Quadro RTX 4000, you have to shoehorn everything into 8GB.\nWhen it comes to storage, there\u2019s potentially some room for improvement. Tested with a single 500GB Samsung 960 Pro M.2 SSD, BOXX UK concedes that the machine would usually be configured with more modern Samsung SSDs and the as-reviewed machine is priced with a 1TB Samsung 970 Evo Plus. However, this drive is still PCIe 3.0, so can\u2019t take advantage of the increased bandwidth on offer in the PCIe 4.0 Gigabyte AMD Ryzen X570 Aorus Ultra motherboard.\nTo put some figures behind this, the Scan 3XS WA6000 Viz and its Corsair MP600 PCIe 4.0 SSD was 16% faster at copying 4.6GB of 3ds max data (comprising 60 large scene files and 4,400 smaller materials) and 6% faster at copying 2.1GB of Solidworks data (comprising 3,400 parts and assemblies). Of course, it\u2019s important to note that not all workflows will benefit from PCIe 4.0 and reliability is equally, if not more, critical when it comes to workstations \u2013 and we\u2019ve never had a Samsung SSD fail on us.\nThe machine also comes with a 2TB 3.5-inch Hard Disk Drive (HDD), although this wasn\u2019t fitted in our test machine. A second HDD can be added if required.\nAs we\u2019ve come to expect from BOXX, the Apexx A3 is very well-built, made from \u2018aircraft-grade\u2019 aluminium, and with a strength and rigidity beyond that of off-the-shelf cases. And at 174 x 388 x 452mm, it\u2019s also very compact, significantly smaller than Scan\u2019s 3rd Gen AMD Ryzen Threadripper workstation.\nThe CPU is liquid-cooled, connected to a large radiator with two fans at the front of the machine. In extended rendering tests, the CPU maintained an impressive 4.0GHz (0.5GHz over its base clock speed), but fan noise was noticeable. In fact, we found the fans to be a little erratic on the whole, revving up and down from time to time, even when idle. It wasn\u2019t particularly loud \u2013 but the change in tone could be distracting to those who work in quiet offices.\nConclusion\nIt\u2019s telling that the two workstations we have in for review this month both feature AMD CPUs. This would have been unthinkable a few years back, given Intel\u2019s long-standing dominance in the workstation market, but the tide seems to be turning.\nArchitects, engineers and designers who are 100% focused on CAD or BIM may still fare better with Intel but, if you\u2019re into CPU rendering or other multi-threaded workflows like point cloud processing or simulation, which can take full advantage of the many fast cores on offer, there\u2019s now a compelling argument for AMD Ryzen, or indeed AMD Ryzen Threadripper. And as an agile independent workstation manufacturer, BOXX offers both, giving it a distinct advantage over Dell, Fujitsu, HP and Lenovo, who are still 100% focused on Intel.\nWith the Apexx A3, the execution is good and the build quality of the compact custom chassis superb. The only downside is the erratic fan noise which some may find distracting.\nDuring January and February 2020, the BOXX Apexx A3, as reviewed, is available for a special bundle price of \u00a34,400. This includes a 24\u201d Asus Pro Art 1920 x 1200 PA24AC display, a Cherry DW9000 BT\/RF rose gold Wireless Keyboard and mouse and delivery.\nProduct specifications\n\u25a0 AMD Ryzen 9 3950X CPU (3.5GHz, 4.7GHz Turbo) (16 cores)\n\u25a0 Nvidia Quadro RTX 4000 GPU (8GB GDDR6 memory)\n\u25a0 128GB (4 x 32GB) DDR4 2,666MHz memory\n\u25a0 1TB Samsung 970 Evo Plus SSD\n\u25a0 2TB 3.5-inch Hard Disk Drive (HDD)\n\u25a0 174mm (w) x 388mm (h) x 452mm (d)\n\u25a0 Microsoft Windows 10 Pro 64-bit\n\u25a0 36 month return to base (RTB) warranty as standard\n\u25a0 24\u201d Asus Pro Art 1920 x 1200 PA24AC display\n\u25a0 Cherry DW9000 BT\/RF rose gold Wireless Keyboard and mouse\n\u25a0 \u00a34,400 (Ex VAT) special price Jan\/Feb 2020)\nCPU benchmarks (secs \u2013 smaller is better)\nCAD\n(Solidworks 2019 IGES export) \u2013 80 secs (smaller is better)\nBIM\n(Revit 2019 \u2013 RFO Benchmark v3.2 (model creation) \u2013 105.9 secs\nRendering\n(KeyShot 8.1) \u2013 135 secs (smaller is better)\n(V-Ray Next Benchmark) \u2013 23,714 kSamples (bigger is better)\nGraphics benchmarks ((frames per second @ 4K res) (bigger is better)\nVIZ (Enscape)\nMuseum \u2013 19\nVIZ (Lumen RT)\nRoundabout \u2013 20\nGPU rendering benchmarks\nGPU rendering\n(Solidworks Visualize 2020) 1969 Camaro car @ 4K\n\u2022 1,000 passes \u2013 316 secs\n\u2022 100 passes + AI denoising \u2013 37 secs (smaller is better)\nGPU rendering\n(V-Ray Next Benchmark)\n262 mpaths (bigger is better)\n*CPU prices taken from scan.co.uk on 4\/12\/19 (Ex VAT)\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/the-cde-conundrum\/","title":"The CDE conundrum","date":1519776000000,"text":"Current common data environments have a number of shortcomings. What needs to happen for them to better support efficient information-sharing in our industry, asks Paul Wilkinson?\nWhen it comes to defining a common data environment, or a CDE, confusion abounds. If we want an \u2018official\u2019 definition, we might look to the UK Avanti Project, which ran between 2001 and 2005 and drove the development of BS 1192:2007. This describes a CDE as, \u201ca repository, [\u2026] a project extranet or electronic data management system.\n\u201d Two years later, PAS 1192-2:2013 defined a CDE thus: \u201cA single source of information for any given project, used to collect, manage and disseminate all relevant approved project documents for multidisciplinary teams in a managed process.\u201d This could take the form of \u201ca project server, an extranet, a file-based retrieval system or other suitable toolset.\u201d\nIn other words, at different times, the industry has talked about documents and file repositories on one hand and data management systems on the other, as if they were the same thing.\n\u201cIn looking to move away from clunky reliance on emails and attachments, which was simply a digitisation of a clunky paper process, we seem to have confused our aspirations to share data and manage BIM processes, with specifications about exchanging files,\u201d says Turner & Townsend BIM consultant Shaun Farrell. \u201cAs a result, our processes often tend to focus on manually exchaning, checking and approving file-based deliverables.\u201d\nFarrell believes that CDEs should be more integrated with the authoring platforms used to create information \u2013 not just BIM tools, but also other digital tools including Microsoft Office, Oracle Primavera and costing and estimating platforms, giving seamless support for time-based simulations, cost and asset\/ operational management data.\nHowever, he recognises that industry inertia, current information management practices and the widespread familiarity of managing documents and folders make this a challenge.\nEvolution, not revolution\nIn an earlier article, I outlined how, since 2011, the UK industry focus on BIM has offered a wealth of opportunities to software-as-a-service (SaaS) providers specialising in construction collaboration. Most of these now provide a CDE as part of this service, to enable the sharing of modelbased information and related workflows by teams looking to comply with the UK BIM Level 2 requirements. These include Aconex, Asite, Autodesk BIM360, Bentley ProjectWise, GroupBC and Viewpoint. But the shift to providing CDEs has been an evolution, not a revolution.\nWhen Farrell digs into the detail of CDE use, he says, compliance with the BS 1192:2007 process (see box below, The BS 1192 process in a nutshell) is pretty patchy, often breaking down at the very first \u2018check, review, approve\u2019 gateway.\nThe BS 1192 process in a nutshell\nBS 1192:2007 outlined a four-step process following the life history of information. This starts with initial design \u2018Work in Progress\u2019 (WIP), then progresses through \u2018Shared\u2019 to \u2018Published\u2019, with any outdated or superseded information going to \u2018Archive\u2019.\nTypically, WIP-stage information is only viewable by those involved within a company, but it still needs to be captured in the CDE as it will be subject to checks and approvals that might need to be audited later.\nOnce WIP is \u2018Shared\u2019, other members of the design and construction team can see the information \u2013 it might be used for coordination and clash detection, for example \u2013 and once these processes are completed, it may be \u2018Published\u2019. This means the work now constitutes a specific client deliverable: a planning application, a construction status document, model or drawing are common examples.\nA lot of information is also given \u2018Client Published\u2019 status, meaning it becomes part of the client\u2019s asset information system.\nMost of the current crop of CDE platforms do not handle the transition from \u2018Work In Progress\u2019 (WIP) to \u2018Shared\u2019 particularly well, he says, due to lack of integration with the authoring platform and the fact that few authoring platforms integrate with collaboration platforms, except when they are vendor-specific solutions designed to do exactly that.\n\u201cDesigners are making some key early decisions here, and yet we are often only capturing the final output from their decision-making, then uploading that deliverable to the CDE. I\u2019ve yet to see many CDEs that allow source information and references to be tracked and linked back to WIP. Clear and auditable change management between information exchanges is still a human activity, or specialist platform-based activities based on file-by-file comparison, rather than information progress over time.\u201d\nOne solution that potentially bridges the gap is Opentree\u2019s Cabinet. Teessidebased Opentree provides an internal document management application that connects via APIs to several CDE platforms (Viewpoint, Aconex and Viewpoint are among early partners), allowing the sharing of CAD, BIM, MS Office and other files. Importantly, Cabinet enables sharing of both files and their related metadata, and then ensures metadata is synchronised if, for example, a Revit model\u2019s suitability status is changed.\nNottinghamshire-based contractor North Midland Construction recently turned to Viewpoint and Opentree to help it demonstrate PAS1192 compliance processes to its clients. \u201cAs NM Group provides both design and project delivery services, it is doubly important for us to have a strong platform to connect our backoffice information management to the daily needs of our project teams,\u201d explains Gary Ross, head of BIM at the company. \u201cCabinet will demonstrate to our customers in highways, civil engineering and utilities that we satisfy the BIM process requirements they and their supply chain partners increasingly demand.\u201d\nClient ownership\nBut should a CDE be something provided by a contractor or by a project manager? At Turner & Townsend, Farrell believes not:\n\u201cVery few of our clients are actually in the construction industry \u2013 to them, the industry is often a necessary evil that they have to deal with in order to procure the products produced by that industry,\u201d he says. While the CDE focus has been mainly around PAS 1192-2, project delivery and multi-party information creation, this often isn\u2019t key to the company\u2019s clients, he explains.\n\u201cTheir \u2018business as usual\u2019 tends to focus on asset operations, including minor and major works, recognising that operating and maintaining built assets can represent up to 85% of their whole-life cost. So we need to be thinking more about, and supporting, their long-term information and data needs \u2013 or the needs of the people undertaking facilities management for them.\u201d\nIn an ideal world, a CDE should be owned by the owner-operator, he says, with suppliers invited into it for the duration of their inputs. \u201cHowever the reality is that we often have multiple CDEs, duplicating data for different businesses and for different parts of the process \u2014 design, procurement, construction, operation \u2014 and little agreement on how long these environments need to be set up, secured, stable and accessible.\u201d\nThis is a good point, because there has already been at least one court case relating to a dispute involving one party blocking a contractor\u2019s access to a CDE, namely Trant Engineering v Mott MacDonald.\nFarrell is not the only observer frustrated by over-concentration on the delivery phase. Anand Mecheri, CEO of BIM data platform provider Invicara, says the full potential of BIM has not yet been fully realised: \u201cNobody was building the platforms, and no platforms will succeed if the base data is bad. Garbage in, garbage out. BIM, particularly at Level 1, has been too focused on drawing deliverables rather than data.\u201d\nMecheri continues: \u201cData still isn\u2019t a high priority for everybody; supply chain interest in data only materialises when the client demands delivery of data to agreed standards. We need more aware owners and more mature practitioners. So it\u2019s early days for BIM. It\u2019s a marathon not a sprint.\u201d\nAccording to Mecheri, Invicara provides visibility of how products are incorporated into designs, assembled into systems, and operated in buildings. It provides a data connection from manufacturer right through to the asset owner-operators\u2019 data needs. In January 2018, the company announced a $10m investment from construction products giant Kingspan, which will be helping Invicara pilot solutions.\nMecheri claims that Invicara takes a different approach to the current generation of CDE vendors. \u201cThey come from an EDMS background; we come from a world of data,\u201d he says. \u201cWe start from validated data and then enable clients to put good data on top of that, and to then get actionable intelligence from their built assets. As our entry point is different, so our approach to APIs is different.\u201d\nInvicara provides application programming interfaces (APIs) to allow data to flow between the systems of all the participants in project design, delivery and operation. Its first product, BIM Assure, created a plugin for Autodesk\u2019s Revit to share data to its cloudbased platform. \u201cWe are the only vendor providing rules-based BIM data validation in the cloud. Solibri does BIM validation but on the desktop and the future is all about the cloud,\u201d says Mecheri.\nConnecting data\nA focus on data could help towards the achievement of the \u2018Digital Built Britain\u2019 vision. First articulated in a February 2015 report, this gained new impetus from the establishment in November 2018 of a new \u00a35.4m Centre for Digital Built Britain at the University of Cambridge.\nThe centre is intended to lead the combination of BIM with the Internet of Things (IoT), advanced data analytics, data-driven manufacturing and the digital economy, in order to enable the UK to plan new infrastructure more effectively, build it at lower cost and operate and maintain it more efficiently.\nMark Bew, former chair of the UK BIM Task Group, is a strategic advisor to the CDBB and says: \u201cLevel 2 was all about doing the existing model better. The future is about doing something different. There is going to be a lot of inertia. The status quo \u2013 the \u2018we\u2019ve always done it like this around here\u2019 \u2013 is the first hurdle to overcome. There is a general lack of understanding of why we need to change and this must be addressed.\u201d\nSteps have already been taken; for example, connecting BIM and smart cities thinking to create a Level 2 \u2018City package\u2019 of guidance. \u201cWhat we will be doing over the next year,\u201d says Bew, \u201cis gluing our BIM and City standards together. The alignment of end-to-end data is not perfect by a long way, but we\u2019ve taken the first step in standardising data services for the entire life cycle of assets, including service provision.\u201d\nBew\u2019s own company, PCSG, for instance, has already helped develop a platform that enables teams to rapidly access open and licensed third-party GIS data and combine it with projectspecific BIM data to improve planning and decision-making (see box below, Introducing GeoConnect+)\nHowever, such examples are currently rare. As Invicara\u2019s Anand Mecheri says: \u201cConstruction is the biggest, most complex and least digitised industry on the planet. But data will grow in value, particularly when asset owners start to analyse their business operational metrics and see how better economic, environmental and social performance is related to how their built assets perform.\u201d\nMeanwhile, CDEs are, it seems, a c lass of technology compromised by industry processes still based on the exchange of files and on version and status control of those files.\nTechnology providers could then be creating applications that exploit such data services. Shaun Farrell says: \u201cChange management is the biggest challenge \u2013 we still can\u2019t track changes easily in something a ubiquitous as Excel for example \u2013 as evolution of our technologies has been slow. We still insist on uploading and downloading files when data synchronisation is where we need to be.\u201d\nSome CDE vendors are trying to facilitate industry change by integrating with other tools to cut data duplication and speed up processes. Aconex and US vendor Procore, for example, are building communities of technology partners connected by APIs. Mecheri welcomes this: \u201cA \u2018network of systems\u2019 must replace the idea of a single CDE application. Duplication of data is a huge problem.\u201d\nIntroducing GeoConnect+\nConsultancy PCSG worked with Reading-based GroupBC and Ordnance Survey to develop a service called GeoConnect+, launched at the GeoBusiness event in London in May 2017.\nGeoConnect+ connects BIM information with geospatial data in a way that helps large asset owners and operators manage large, disparate estates better. Datasets include OS open data, OS mapping data, land and property data, flood, river and road network data.\nThe map or satellite imagebased interface is accompanied by a menu of \u2018layers\u2019 that can be switched on, singly or in multiple combinations, as required. Users can, for example, view all listed buildings or sites of special scientific interest, in a specific area, and even identify the number of households, complete with addresses and postcode data, that may potentially be affected by a development.\nSussex-based regional contractor Mackley undertakes a wide range of civil engineering projects in marine and river environments. Technical services manager Trevor Mossop, said: \u201cGeoConnect+ simplifies document access and ensures that all of the available documents, pertinent to a geotagged location, are bound together and returned from any device without reliance on understanding the search criteria. This is a massive step forward in the usability of the CDE, where accessibility and ease of use dictate buyin by staff on any project.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-autodesk-robot-gets-dynamo-plug-in-for-form-generation\/","title":"NEWS: Autodesk Robot gets Dynamo plug-in for form generation","date":1416528000000,"text":"Autodesk\u2019s computational geometry creation tool to drive structural analysis in latest tech preview on Autodesk Labs\nThe Dynamo Plug-in for Robot Structural Analysis is the latest technology preview to land in Autodesk Labs. The software allows designers and engineers to drive Robot with Dynamo, Autodesk\u2019s visual programming interface for computational-based geometry creation.\nDesigners can create parametric and complex structural frames models in Dynamo, then submit it to Autodesk Robot Structural Analysis for simulation, and review the results returned from the analysis.\nAutodesk Dynamo is free and the Structural Analysis for Dynamo package is free, but you will need a license of Robot Structural Analysis 2015 to participate in what sounds to be one of the most interesting technology previews in a while. However, Autodesk points out that if you don\u2019t have a license, you can still obtain a free 30-day trial of Robot.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/the-generation-game\/","title":"The generation game","date":1570579200000,"text":"If there\u2019s anything to learn from living in the 21st-century, it\u2019s that change is inevitable. Technology evolutions are now planned in by the quarter but occasionally they can come from left field. The technology to design and fabricate high-quality engineered buildings is entering a disruptive phase. How will large AEC firms cope?\nIt may come as a surprise, but in the late 1970s and early 1980s there were many architectural and construction firms in London using 3D computer modelling systems to generate plans, sections, elevations and drawings of their buildings.\nSoftware such as BDS, RUCAPS, Sonata and GABLE, running on mainframe and minicomputers, were the first BIM solutions. Compared to the tools of today they may well appear rudimentary, and they were certainly very expensive, but they were actually very advanced for the time. It goes against the stereotype that the industry is not very progressive in adopting new technologies and applying them in practice.\nThen came the relatively low-cost IBM PCs in the 1980s and, together with cheap drafting software, such as AutoCAD, most of these mainframe dinosaurs were relegated to the scrapheap, and the architectural design world went \u2018a little flatter\u2019. However, none of the original concepts died, living on in ArchiCAD, AutoCAD AEC, Architectural Desktop and then culminating in the arrival of Revit in 2000.\nIn those early days of Computer Aided Design (CAD), technology changed at a snail\u2019s pace. We would literally have years in between software releases and meeting someone with a graphics card capable of a massive 256 colours was once a truly rare event! Today\u2019s AEC firms are facing a very different landscape: evolution is built in, software upgrades and updates can come every six months or less, hardware can now mean VR headsets and software can be online as a service.\nDesign IT directors of large AEC firms have to stay up to speed with what\u2019s coming, while managing what they have and preparing for what they will have in the future in order to remain competitive.\nOf course, all this CAD stuff is not just technology for technology\u2019s sake; like in any business today, technology is a weapon. For instance, in 2003, Foster + Partners\u2019 Swiss Re (aka the Gherkin) was a grand display of Sir Norman Foster\u2019s trademark diagrid glass fa\u00e7ade, adding to London\u2019s and the world\u2019s architectural vocabulary. The building was designed and modelled in MicroStation, with Arups modelling the steel.\nFoster + Partners\u2019 in-house team of geometry experts, the infamous \u2018specialist modelling group\u2019, wrote a program to produce take-offs from the model to automate the creation of pannelisation drawings. At the time, I knew of several firms which begged Bentley Systems to write a similar script for them, so they too could compete in the new vernacular with automated pannelisation take-offs, which they duly did. It\u2019s now not unusual to find programmers in design practice and, to this day, Foster + Partners has the biggest in-house specialist team (but still less than 1% of total staff), producing tools for their designers, solutions for geometrical problems and developing in-house applications.\nTechnology stacks\nToday\u2019s design IT directors live in a complex world. For most, budgets are tight, but they still have to develop their BIM technology stacks for multi-CAD environments, connecting and managing distributed teams and data, in different countries, with varying skill sets, complying to common standards and conforming to local standards. All this while working on projects and collaborating with other firms who might take a completely different approach to just about everything.\nDrilling down further, there are decisions to make on which platforms to use for design, document management, document distribution, common data environment (CDE), collaboration hub, security, visualisation, virtual reality (VR), augmented reality (AR), 3D printing and not to forget the workstation, mobile hardware and cloud services. With so many choices and so many moving parts, the day-to-day job involves the spinning of a lot of plates.\nWith the increased update cycles, new software needs to be checked. All too often the technology stack can be undermined by just one or two components updating automatically and breaking essential links. Like painting the Forth Bridge, it takes so long to build a stack, that you need to start again when you have finished. It\u2019s the pain that comes with building these systems which leads to the stickiness of solutions that \u2018just work\u2019 and is, in some way the attraction of SaaS services, which run off-site and take some of that trouble away.\nHowever, with change being inevitable, today\u2019s generation of design IT directors are constantly scanning the market for technology to solve some of their current problems, improve workflows or prepare for any possible dynamic technology shift, such as 2D to BIM or desktop to cloud.\nSome of the larger firms might have a hand-picked R&D team, but most do not, and the task might befall to one individual. In times of significant change that responsibility can hang heavily. Process change can come at a productivity loss and an increase in risk in project delivery \u2013 the full buy in of the management team is essential. It\u2019s another reason why the industry has tended to change slowly. But there are some changes, which simply cannot be ignored.\nHaving a Fab time\nFrom talking to many AEC firms over the last year, it\u2019s clear that a number of changes are coming together, which could have a fundamental impact on the way buildings are designed and fabricated. Whatever you want to call it, Digital fabrication, Design for Manufacture and Assembly (DfMA), Prefab, Modular or Precision Manufactured Housing, there is a general consensus that architectural design systems need to be able to better integrate with automated off-site construction machines.\nThe bad news is that the BIM modellers which we have all standardised on, even though they produce 3D models, were never intended to produce accurate geometry to drive CNC (Computer Numerical Control) machines. They would also likely struggle to handle the size of models required for a level of detail that is needed to generate assemblies with detailed bill of materials (BOMs).\nAll BIM modellers currently available were designed to automate the production of co-ordinated 2D drawing sets to be handed to builders to make. This isn\u2019t the future. The UK already has a number of building factories being constructed, or up and working \u2013 Urban Splash, Legal & General and Berkeley Homes, to name but a few.\nDigital Fabrication is coming at a time when market dominating products like Revit are 20 years old and are only receiving minor updates each year. Revit still mainly runs on one CPU core, hardly makes use of GPU acceleration, and gets very unwieldy for large models often requiring them to be cut up. The net result is we are seeing a lot more activity in firms testing out alternative BIM products and even looking to software solutions which are popular in aerospace and automotive, such as Dassault Syst\u00e8mes\u2019 3D Experience platform (better known previously as Catia) and predominantly used by Frank Gehry and Zaha Hadid Architects (ZHA). While Catia is not used on every project at ZHA, it is used to send explicit 3D models to fabricators, bypassing 2D drawings for feedback on manufacturability during the design phase. It\u2019s something that other firms are also buying into, such as SHoP Architects in New York, who has worked on a number of digital fabrication projects.\nMany other \u2018signature\u2019 architectural firms are looking to re-evaluate everything from conceptual design to fabrication. McNeel Rhino and Grasshopper seem to be the weapon of choice for firms that design freeform architecture and Rhino now works in SketchUp, AutoCAD, ArchiCAD, Unity, BricsCAD and Revit. This new development is called Rhino Inside (whcih we review here).\nRhino Inside could potentially be seen as a liberating technology, as conceptual geometry created in Rhino with Grasshopper could be pulled out or injected into pretty much any of the main design and documentation tools. It would mean that Rhino becomes a common data environment of sorts, but the geometry is essentially \u2018dumb\u2019, maintaining little of the \u2019I\u2019 in BIM data.\nHowever, when it comes to \u2018dumb geometry\u2019, machine learning advances from companies like Bricsys have tools that could bring order out of the chaos. Its AEC modeller, BricsCAD BIM, offers a unique post-rationalisation capability, called \u2018BIMify\u2019, which can parse a dumb mesh model and intelligently work out what the BIM objects are, applying the correct IFC tags to elements it recognises, such as doors, walls floors, slabs etc. Dumb geometry need not stay dumb for very long. This could be game changing as you could pull models of cities out of Unity and set BIMify loose in BricsCAD! I\u2019m calling this \u2018R\u00e9chauff\u00e9 BIM\u2019 as it\u2019s possible to heat up any geometry, back to BIM.\nComputational design or generative design has become a core design tool for many large firms, so if the base geometry stayed in Rhino, G-code could be easily generated from and it could also feed the model and drawing creation, a role for which BIM is slowly becoming relegated to.\nProducts like ArchiCAD have built-in links to Grasshopper to drive BIM objects from scripts. This workflow is being evaluated by many architects wishing to automate the building of BIM models from geometry definitions. Rhino Inside also brings that possibility to Revit (and any other 64-bit Windows application, which would include Unity and Blender).\nFoster + Partners is a huge Rhino and Grasshopper customer. Francis Aish, Head of Applied Research and Development and a partner at Foster + Partners explained how they keep the geometry in Rhino for as long as possible to enable dynamic changes to the building definition before handing it over to Revit for detailed documentation.\nThe company uses scripting to develop tools for its architects, as well as providing automation for laborious processes. Recently in Denmark, Aish demonstrated a real-time design evaluation VR program which his team had created on top of Unity. In our discussion Aish mused on the fact that many AEC firms were developing code to do the same things, reinvent the wheel and that perhaps there should be a more open approach to sharing some of the tools developed, especially when it comes to assisting collaborative working.\nWhile Rhino is a surface- only based modeller, HOK has just announced it will be looking at BricsCAD BIM as an interesting Revit alternative and has become a BIM Alliance Partner of Bricsys. BricsCAD started life as an AutoCAD clone but now has a full ACIS solids-based BIM tool on top it and the interesting BIMify machine learning capability.\nRise of programming\nThere are many ways to start a concept model but, to be honest, it\u2019s probably the weakest part of the architectural software market. Common tools are Grasshopper, Dynamo, Rhino, SketchUp, Maya, massing in Revit or another BIM tool.\nSome firms want to jump straight into BIM, some sketch and folks like Gehry crumple up paper. CAD does not like imprecision and conceptual design needs freedom, unless you know you are going to build a bunch of rectangles \u2013 in which case go straight to massing. BIM tools really need to improve at this phase, to capture artistic input.\nThe leading firms typically use scripting and programming early on to allow iterative form finding and the easy creation of complex geometry.\nThe popularity of Rhino, Grasshopper, Dynamo and Python programming has really created a different type of architect within practices. The computational designer can either develop tools for the rest of the team, or be the key driver in defining complex geometrical forms.\nThe visual programming style of wiring up mathematical \u2018effects\u2019 and \u2018generators\u2019 is an easy route into geometry-generating code and is frequently used at the core of project teams.\nBefore computational generators, 3D models would have to have beeen hand crafted and were complex and slow to edit. With the computer in control of generating forms and managing the complexity, the architectural vocabulary has again been expanded.\nIt has also created a divide between scripted geometry definitions and traditional BIM tools, which operate on higher-level components, such as walls, doors and windows. In geometry-led practices, the Rhino definition drives the BIM, which essentially becomes a documentation tool, not a design tool. For many, this documentation and 2D drawing phase is seen as a necessary evil, mainly for contract compliance and for communication with clients, but advanced fabricators and construction firms would sooner have the 3D model to check for manufacturability and accurate fabrication quotes.\nNot all architectural firms need Rhino or complex NURBS curves, but still employ programming teams. Rob Charlton, chief executive of Space Group, and serial BIM entrepreneur (BIM Show Live, BIM Store, BIM Technologies) is based in Newcastle and has a team of four dedicated programmers in his practice.\nCharlton explained that in the North East fees are lower and customers are more conservative and are focussed on value. He might not produce buildings with too many curves, but he uses his programming team to deliver on the opportunities he sees in extending the BIM data he creates or captures for clients by developing services.\nHis current focus is on developing a digital twin platform for facilities management (FM), built on Autodesk Forge tools, to enable the BIM model to drive efficiencies post build and get the data out of Revit and into a tool that non-CAD folks can use. In short, Space uses programmers to drive additional services revenues and occasionally to fill in gaps for functionality that Revit doesn\u2019t provide.\nSome large firms have no dedicated programming or scripting resources at all and rely on the skills of their talented design teams. They usually teach themselves or are active in communities.\nVisualisation revolution\nAnother case in point, for a generational change which currently faces design IT directors, comes in the shape of our need for beautiful visuals. In this area we see possibly the most regular yearly changes. GPU and CPU dev\\elopment continue at breakneck speed. Every year it seems the previous chips are being bettered, making workstation choice critical to match software with machines of appropriate specifications. Do you invest in multi core CPUs for traditional CPU rendering, or high performance GPUs that can now be used for many different tasks, including real-time visualisation, ray tracing and VR? While the established base applications, 3ds Max and Maya, are again on low velocity development, we are seeing rapid advances from the key game engine players, Unity and Unreal. Promising instant visualisation and deep integrations into today\u2019s BIM modellers, the future of viz seems to be in different applications.\nWith a product like Enscape, the real time visuals you can get, essentially as a bi-product of the design system, are quite stunning. Talking with Kohn Pedersen Fox (KPF) Associates\u2019 Cobus Bothma, director of applied research, he explained how viz was moving from just being the presentation tool to becoming a valuable asset in analysing designs, for feedback into the project teams. Currently the KPF product stack means every designer has a copy of Enscape and some have Lumion. Botha is also looking at deploying Twinmotion for producing images with really rich environments. Ultimately these Twinmotion models will directly feed into Unreal for the full viz and VR treatment. KPF then uses remote workstation technology to stream pixels to iPads for in-house design meetings.\nVR and AR are set to be extremely important in future design workflows. While a lot of the current use cases are around customer presentations, Unreal and Unity, as well as whole host of other AEC-focused real-time viz and VR tools, are all about expanding their appeal to designers.\nBIM models, especially in Revit, are heavy, so game engines could also offer parallel workflows. By extracting the geometry as a lightweight mesh with multiple Levels of Detail (LoD), together with the metadata, they could be used for all sorts of downstream functions \u2013 clash detection, collaboration, design review, sections, elevations, quality assurance, and all types of VR and AR experiences, even on site. Importantly, as Unity and Unreal keep up with the very latest hardware, BIM data could instantly be streamed out to multiple devices \u2013 mobile, desktop, web, VR or AR. This clearly doesn\u2019t leave a lot for the original BIM model and, even less if that BIM definition can\u2019t drive digital fabrication. Food for thought.\nIn my conversations with Aish from Foster + Partners, he explained how architects really provide experiences and sell these to clients. In many respects, BIM asks architects to make deep structure-based decisions in the concept phase, when all this could be worked out later. Perhaps returning to a time where sketches and artist renditions were enough to get the client\u2019s buy-in; the digital equivalent will be placing the customer in an augmented environment.\nFinancial drivers and locks\nWith increasing costs for enterprise licences from Autodesk which, let\u2019s face it, has the lion\u2019s share of UK and US markets, the cost of software is also becoming a major driver. Firms are unable to leave subscription without losing access to all licences, but are very aware of how much they are paying and how little development their core BIM tool, Revit, has received. When one software vendor can dictate tough contract terms, AEC firms are made painfully aware every three years, of how little leverage they have. This is another reason for the current interest in evaluating what else is on the market.\nAutodesk is busy moving to the cloud and has decided the next generation of BIM modeller will reside there. Project Plasma is the start of this core technology but has suffered delays and is probably years away. This is happening at the same time that digital fabrication is becoming a near-future consideration. This is probably not a good time to have your development \u2018gap\u2019.\nThe industry is looking to liberate itself from proprietary file formats, which came with BIM modellers. In a federated working environment, poor interoperability leads to data wrangling, a potential to lose valuable information, and it impedes collaboration. Architectural practices have consciously expanded their toolsets to use best in class, so the search for industry standard CDEs goes on.\nDo It Yourself\nWe find ourselves at a crossroads for AEC development. We have an old generation of desktop applications, with increasing costs and a lack of interoperability, while looking at the oncoming requirements and opportunities of digital fabrication, artificial intelligence (AI), generative design, and defining new workflows.\nThe large AEC firms have always tended to use commonly available platforms in which to do their work, but some have put forward the notion that the needs of the top 20% will rarely get a look in when a software vendor is aiming to deliver features for the 80%. Some have even mused that with BIM being a drawing process and manufacturing CAD systems being overkill, they should collectively design their own concept to fabrication system. There are enough components and technologies available to get a good head start, but the fundamental question would remain; should they be architects or software developers?\nIt\u2019s not lost on me, the irony that the first new BIM tool on the market in 20 years, BricsCAD, started off life as an AutoCAD clone. I wonder if any software house is brave enough to throw everything away and start from scratch, looking at the future needs, based on the inputs and outputs? But Revit is so entrenched, and multi- disciplinary workflows rely on its ability to hold all this data, will the benefits of digital fabrication drive a fundamental change in process?\nConclusion\nLooking at the technology landscape and the commercial pressures to drive efficiencies, compress timelines, reduce cost and reduce carbon, the combination of digital fabrication with off-site factories seems inevitable. We are already seeing significant investment in the residential sector in the UK. In America and Japan this is being combined with an adoption of mass timber. This will scale to office blocks and high rises as processes are refined.\nChanges in materials, fabrication and automation will mean fundamental changes to design workflows and ultimately key deliverables. While in automation and aerospace 2D is still commonly used, the model definition is the key driver throughout its lifecycle. AEC has much to learn from lean manufacturing methodologies.\nWe should all also be aware that there are giants of the tech world looking to disrupt this space. Ikea, Google and Amazon have their eyes focussed on designing, fabricating and delivering manufactured buildings. Even Elon Musk is trying to redefine construction, and looking to automate the creation of his Giga factories. Firms like Katerra have already started the process in the States).\nIt is time to re-evaluate our tools, our processes and deliverables to encompass a different future, where many buildings are designed for automated manufacture and assembly and an integrated approach to disciplines becomes essential. However, the barriers are not just technical. People and culture is often mentioned when talking about the challenges of changing the AEC industry\u2019s workflows and practices for the better. There are feedback mechanisms from all aspects of the industry, which also make it difficult to move forward.\nFor me, one of the biggest barriers to the AEC industry progressing are its tool makers, the software developers, which approach solving every new problem or customer requirement with the same old software they have been \u2018evolving\u2019 for years to replace 2D CAD. If BIM won\u2019t get us to manufacturing, it could be seen as a convoluted route to just creating 2D DWGs. In many respects, the core concept of BIM has failed, with multiple BIM models being created to suit the needs of each part of the design and build process; a construction BIM model is different to an architectural one.\nAt some point soon there will be a dramatic change in workflow or output, that software developed decades ago can no longer address just by adding some new features. Catering to legacy systems impinges on innovation. Sometimes it takes new eyes to analyse the needs and trajectory of the market. With a focus on factory-based manufacture, they would likely come up with a very different solution to BIM as we know it.\nAfter a summer of talking to some of the biggest AEC firms it\u2019s clear to me that some of the leading BIM tools have failed as concept design tools, but succeeded as design checking + documentation loops \u2013 although there are still some challenges surrounding large models, complicated geometry and the fact they don\u2019t naturally benefit automated off site construction.\nWhile AEC firms will carry on refining their existing BIM processes and participating in the status quo, there is a common understanding in the top 20% of firms, at least, that things are changing rapidly and that new design tools are needed to define directly manufacturable definitions.\nThis fact is not lost on the major software companies; Autodesk, for example, is scrambling to develop a new system but it seems to have been caught out on the rapid industry-focus on digital fabrication adoption and existing software will not easily evolve to bridge the chasm.\nThe net result is we are seeing CAD systems that were designed for automotive and aerospace being trialled within some architectural practices and construction firms, trying to bridge the gap, connecting designers with the factory through 1:1 accurate assembly models, BOMs and G-code.\nCAD software companies rarely look forward to generational changes of the core products. It\u2019s a time when they might get it wrong and open themselves up to the competition as effectively the playing field gets levelled. The big play of the last five years has been the race to get existing apps and data to the cloud; digital fabrication was not on the radar. And here, I fear, instead of taking a fresh look and starting from scratch, developers will hamstring themselves by working within self-imposed constraints of catering to their legacy system and users, when there are already manufacturing-centric modelling systems like 3DX Catia to bridge the chasm.\nWe are at the start of a generational change and design IT directors of leading firms have started to rethink their technology stacks, processes, costs and deliverables, and are much more likely to evaluate alternative and new technologies and with that the market is opening up.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/vr-mr\/review-from-revit-to-vr\/","title":"Review: From Revit to VR","date":1486080000000,"text":"Greg Corke gets hands-on with three Virtual Reality (VR) applications that work seamlessly with Autodesk Revit, weighing up their capabilities and assessing how well they combine with the HTC Vive and workstation GPUs\nVirtual Reality (VR) is one of the most exciting technologies to hit the AEC market in years. Architects, engineers and clients alike can experience a realistic virtual prototype of a building long before it is built.\nA fully immersive VR experience gives you a sense of scale, depth and spatial awareness that simply cannot be matched by a rendering, walkthrough or physicalscale model. The feeling of presence \u2013 of existing inside the 3D model \u2013 is quite incredible. Users have the freedom to explore a building at their own pace, to understand how it will feel and function. Walking across rooms, teleporting through doors, peering around corners \u2013 it\u2019s all possible with a fully tracked roomscale experience.\nThe impact on the design process can be huge \u2013 but only if VR can be used at the precise moments where it adds most value.\nIt\u2019s fine to wait days or weeks for an expert to produce a polished VR experience for communicating proposals to the client or the team. But for VR to truly influence design, it is the user of the CAD\/BIM software that must have full control over when to enter the virtual environment. This gives them the opportunity to explore alternatives, evaluate the impact of modifications and detect errors early. Discovering issues weeks after a design has progressed can create delays and ramp up costs.\nThis presents a huge opportunity for software developers to create push-button workflows to move quickly from CAD\/BIM to VR. Such tools not only bring in the model geometry but also materials and lighting, so nothing needs to be done post-import.\nMany of the current developments centre on Autodesk Revit, with a number of applications now supporting both the Oculus Rift and HTC Vive Head Mounted Displays (HMDs). It is these two HMDs that currently give the most immersive VR experience. The Samsung Gear VR and Google Cardboard deliver a more basic VR experience without positional head tracking.\nVR software\nFor the purpose of this article, we tried out three of the main \u2018Revit to VR\u2019 applications using Revit 2017. All of the tools work with both the Oculus Rift and HTC Vive, but we only tested with an HTC Vive, using two different workstations and GPUs.\nEach application works on the premise that whatever you have in Revit \u2013 model, sun settings, lighting, materials and entourage \u2013 is automatically transferred into VR, simply by pressing a button, or a few buttons. This makes all three applications well suited to non-expert users.\nProspect from New York-based IrisVR can be used to create fully navigable VR environments in minutes (even under a minute) using a Revit plug-in. It also works with other CAD\/BIM tools, including SketchUp and Rhino. The software places a big emphasis on performance rather than visual fidelity. It comes in two versions \u2013 a free version, Prospect Basic, for simple navigation; and a paid-for version, Prospect Pro, which adds more functionality and costs $200 per person, per month.\nEnscape is a real-time rendering plugin, providing a live link from Revit to a desktop 3D game engine that can also be used for VR. It can take minutes (even under a minute) to go from Revit to VR and, uniquely, any change made in Revit, automatically updates in Enscape. The software places a big emphasis on visual fidelity. Prices start at $449 per year.\nAutodesk Live is a cloud service and application that turns Revit models into a fully navigable desktop game engine experience where users can also enter VR at the click of a button. Files are uploaded to the cloud using the Autodesk LIVE Revit plug-in. Once processed, the game engine experience is downloaded to the desktop. This process takes around 10 to 15 mins with small to medium-sized models. Autodesk LIVE experiences can also be enhanced with Autodesk Stingray. Prices start at $30 per month, although this is an introductory offer.\nVR headsets\nEntry-level smartphone VR headsets such as Google Cardboard can deliver impressive results, but for a fully immersive, navigable room-scale VR experience, you will need an Oculus Rift or HTC Vive. Both HMDs are tethered to a powerful 3D workstation via a long cable.\nRoom Scale, as the name suggests, means the VR experience takes place in a space the size of a typical room. For architecture, this can be a big plus, as users can explore virtual buildings on foot, taking small steps, with larger distances covered by teleporting. While room-scale VR is great for presentations and design review, a seated experience often works better when using VR as a design aid. This is simply because of the ease which with one can flip between Revit and VR.\nThe Oculus Rift started out life as a seated\/ standing experience, where the user could only move small distances and navigation was performed with a gamepad. This type of experience still comes with the base package, but the Facebook-owned HMD now offers two optional hand-tracking Touch controllers and additional VR tracking sensors for a full room-scale experience. A total of three sensors are recommended. One comes with the standard headset, another with the Touch controllers and a third can be bought separately\nThe HTC Vive offers both a room-scale VR experience and a standing\/seated experience, in spaces up to 5m x 5m. The HMD comes with two wireless controllers, each with four control buttons (touchpad and menu on the front, trigger on the back, and grip on the side). The kit includes two tracking base stations.\nHTC also offers a Business Edition of the Vive, which includes dedicated phone support and better peripherals, including a 5m (16-foot) headset extension kit, and four face cushions, which are particularly useful if the HMD will be shared among many people. (Faces often get sweaty in VR). In the future it will also come with Vive Enterprise Software, an enterprise-friendly package that does not require users to install the games-focused Steam store.\nImportantly, the Business Edition comes with a commercial warranty. If you use the standard consumer version for commercial use, your warranty is null and void.\nSetting up the HTC Vive With a little bit of guidance and the right equipment, the HTC Vive is fairly easy to set up. Position the tracking base stations at opposite corners of a room, at a height of 2m, angled down to between 30 degrees and 45 degrees with an unobstructed view of each other. For a permanent or semi-permanent VR room, mount the base stations on the wall. For a portable set-up, use camera tripods.\nIdeally, both base stations should be connected to each other using the sync cable, but it\u2019s not essential. Both base stations plug into mains power. They are not connected to the HTC Vive or workstation.\nThe HTC Vive headset features a long, chunky cable that connects to a dedicated link box. The link box then connects to power, and to your workstation via USB 3.0 and a video cable (HDMi if you have a gaming GPU or DisplayPort if you have a pro GPU). When turning on your workstation, you may need to pull out the video cable or Windows will boot to the HTC Vive, rather than to your monitor.\nInstallation is very straightforward and the HTC Vive set-up software guides the user through the process step by step. Depending on the size of the room and the requirements of the user, the HMD can be set up for a full room-scale experience or a standing experience.\nDetailed instructions can be found here.\nAutodesk (Revit) LIVE 1.6\nAutodesk LIVE has many different components \u2013 a Revit plug-in, a cloud service that creates .LVMD files and a standalone Autodesk LIVE Editor that reads\/writes .LVMD files. There\u2019s also a free viewer, Autodesk LIVE viewer, which is available for Windows or the iPad. This reads .LIVE files published by the Autodesk LIVE editor. The VR capability is only available on Windows.\nTo prepare a model, simply create a 3D view in Revit, then go to the Revit add-ins tab and click the \u2018Go LIVE\u2019 button. The software then does some pre-flight checks on the model and alerts you to any aspects that need your attention. This includes textures that can\u2019t be found, section boxes that you might want to turn off and levels of detail that you might want to enable so that you can see all the model geometry.\nMissing textures need be re-mapped manually. If they aren\u2019t, the objects will just appear plain white. All other issues can be sorted out at the click of a button without leaving the dialogue box.\nHitting \u2018Go\u2019 will upload your file to the cloud for processing. As Revit files often stretch to hundreds of MBs, upload speed is important but, when on a decent connection, it is processing time that takes up the bulk of the process. On average, with 0.5Mb\/sec upload, we found our 100MB to 200MB Revit test files took around 15 mins to upload, process and download the resulting .LVMD file to our desktop workstation. Larger files can take a lot longer. Also, depending on how busy the Autodesk LIVE servers are, you can end up in a queue, which can add a significant timelag.\n.LVMD files can be opened in the Autodesk LIVE Editor, a real-time desktop game engine design viz environment that also has a VR capability. For this article, we\u2019ll start off by looking at what can be done on the standard desktop application and then share our experiences of VR.\nFor navigation, use the mouse to orbit around a building or select a 3D viewpoint inherited from Revit. Alternatively, with \u2018tap and go\u2019, click on any location and the software will walk you there, automatically navigating stairs and doors. You can set the view height so it\u2019s possible to see what an adult, toddler or wheelchair user might experience when interacting with the space.\nModels can be viewed in different display styles, including a clay type render mode, which is useful for early-stage design, when the focus is on form and volumes. The default fully rendered setting shows you fully baked materials and \u2018realistic\u2019 RPC content, including people and trees. Overall, the render quality is very good.\nAutodesk LIVE also includes tools to explore daylighting. Simply move time and date sliders and shadows adjust in real time. Revit lights can be set to turn on automatically at night.\nOne of the most powerful capabilities of Autodesk LIVE is the ability to view the underlying BIM data within the model. Simply click on any object and the data appears in a dialogue box.\nMoving into VR is easy. Simply click the VR button in the bottom right hand corner of the screen, put on the VR headset and you\u2019ll see a bird\u2019s-eye view of the model \u2013 the so-called Mini Map. The model is placed on a hoop, which the user can grab, spin and pull closer to the face to inspect from any angle. It\u2019s a really good way to get an overall view of the project and to quickly zoom in on details. It\u2019s possible to return to the Mini Map at any time, simply by pressing the Vive\u2019s trackpad, which can be really useful for getting your bearings.\nMost of the navigation in VR is done by teleporting. Simply use the HTC Vive trigger controller to point and click. In a flash, the model fades in and out and you\u2019ll find yourself in the new location. Depending on the size of your room-scale set-up, you can also walk short distances.\nDoors open simply by looking at the them. This adds to the realism and your understanding of the building, but can be a little annoying if you simply want to see the detail of a door \u2013 a glass door, for example, which you can see through anyway.\nUnfortunately, most of the functionality from the Autodesk LIVE game engine experience is currently not available in VR. You can\u2019t interrogate the model, nor change the time of day in real time without pressing ESC, taking off your headset and swapping VR controllers for a mouse. Autodesk LIVE VR is currently very much a navigation and viewing experience \u2013 albeit a very good one. However, it\u2019s likely that Autodesk will add more functionality to VR as it works out the best way for users to access more advanced capabilities.\nThe visual experience in Autodesk LIVE is very good. This can be further enhanced with Autodesk Stingray, the underlying game engine technology on which Autodesk LIVE is built.\nAutodesk LIVE models taken into Stingray can also be turned into fully interactive experiences (think light switches, running taps, TVs or adding mechanisms to flip between different design configurations). This level of customisation is complex and requires a design viz specialist or game developer. Moving forward, it\u2019s likely that the Autodesk LIVE Editor will inherit some of Stingray\u2019s capabilities, while maintaining the user-friendly interface.\nIn terms of workstation hardware, Autodesk LIVE VR has pretty high-end requirements, with the recommended spec being an Nvidia GeForce GTX 1080.\nIn terms of professional GPUs, all of our test scenes ran fine in VR mode on the high-end Nvidia Quadro P6000. We would also expect the Quadro P5000 to give a decent experience, but we didn\u2019t test this.\nThe AMD Radeon Pro WX 7100 worked fine in desktop display mode, but in VR, we found it to be underpowered. Even with relatively simple models, the scene jumped around in front of our eyes, which made it disorientating and unusable.\nThe good news is Autodesk is currently looking at ways to reduce the hardware requirements so that Autodesk LIVE VR will also run on less powerful GPUs. The company told AEC Magazine that this could be achieved by tuning down some software capabilities. This would mean that the visuals would not be so compelling, but at least the experience would be fluid.\nIn summary, Autodesk LIVE offers a simple workflow from Revit to VR, good navigation and high-quality visuals with realistic materials. The big downside is the time it takes to enter VR. While 15+ minutes might not seem that long in the grand scheme of things, it does put a big barrier in place for true iterative design workflows.\nPredictably, Autodesk is aiming to bring this time down. Currently, a simple change to the Revit model would mean a completely new upload, but Autodesk is exploring new methods so only deltas (changes) need to be uploaded and processed. It is also looking to reduce processing time by developing more computeintensive algorithms (think multiple CPU cores or highend GPUs for light baking) to better harness the scalability of the cloud.\nAutodesk LIVE is available on a 30-day free trial, which allows ten jobs to be processed. A subscription costs $30 per month for unlimited use of the cloud and the app, but this is an introductory offer so is likely to change.\nUpdate \u2013 Nov 2017\nSince this article was written, Autodesk LIVE has been rebranded to Autodesk Revit Live.\nThere have also been a couple of main features added.\nFirst up is Level of Detail geometry generation and material instancing, which Autodesk says results in a 20% performance improvement for users when working with large models [e.g. a hospital complex] and VR. Autodesk Revit Live now also suggests a recommended optimization setting based on your hardware.\nThe second is that Revit Live now suports Revit LT as well as full-blown Revit.\nAutodesk is gearing up for a new release any day now, so expect a raft of new features in what we presume will be Autodesk Revit Live 2.0.\nEnscape 1.8.3.3\nEnscape is a real-time visualisation tool designed to work specifically with Revit. The software can be used on a standard display or in VR. It offers a push-button workflow from Revit to VR and, uniquely, a live link between the two applications.\nOnce a link is established, any changes made in Revit \u2013 be it geometry, materials, layers or lighting \u2013 will automatically appear in Enscape seconds later. With other Revit to VR applications, the entire RVT file must be processed again.\nThis excellent feature makes Enscape incredibly well-suited to iterative workflows. Make a change in Revit, then, in less than the time it takes to put on your HMD, assess those changes in VR. Should you so desire, live updates can be paused.\nEnscape\u2019s unique workflow is possible because of the way the software works. Rather than exporting geometry from Revit, it only sends graphics information to Enscape, which is then rebuilt as proxies.\nThe software is installed as a plug-in inside Revit and is given its own tab. To enter VR, simply click the \u2018VR\u2019 Button, pick a predefined Revit view from the pulldown list, then hit run. In less than a minute, you\u2019ll be able to see your Revit model in VR.\nThe best way to enter VR is usually from a bird\u2019s-eye view, as this gives a good overall picture of the project. However, this initial view must be set up properly in Revit. Enscape has a maximum teleport distance, so be careful not to be too far way. If you are, the only way to get closer to the building is to fly there using the Vive\u2019s trackpad controls (the left controller moves you left and right, the right controller moves you up and down) and this can take some time. You also need to ensure your model has some surrounding terrain.\nOnce you are close enough, navigation is intuitive. You can teleport through doors \u2013 both solid and transparent \u2013 which is great, but you can\u2019t teleport through certain types of windows. This can be annoying if you quickly want to jump outside to view the exterior of a building. To get around this, Enscape lets you fly through any solid object \u2013 windows, walls and all. Flying is very useful for viewing buildings from above, from any angle. However, it did make us feel a bit sick and, at times, gave us vertigo.\nEnscape offers limited functionality inside the VR environment. You can change the time of day using the grip buttons on the side of the Vive controller. Shadows will change in real time and lights will come on at night. Unfortunately, Enscape does not display a clock in VR, so the wearer of the HMD cannot assess lighting at specific times of the day. A clock is visible when viewing the model on a monitor, so the wearer can momentarily lift the HMD, but it\u2019s a bit of a fudge.\nIf you want to capture certain elements of your building to jog your memory or for basic design review, you can also take s c r e e n sh o t s with a custom keyboard hot key. This is pretty tricky with a standing, room-scale experience (think one leg, big toes, small keys). Ours is the voice of experience as we used this exact method to produce the screen shots for this review.\nFor more control, you will need to take off the HMD and use a mouse in Revit. Through the Enscape settings dialogue box, users have full control over contrast, colour saturation and colour temperature, which is great for getting the desired look. There\u2019s a papermodel mode, which strips out all materials but retains light and shadows. This is useful when you might not have correct materials assigned or simply want to dumb down the view to emphasise form. A polystyrol mode gives the effect of your building being made from Styrofoam. Both modes can be turned on at the same time. Line thickness can also be changed to enhance the edgfes of objects.\nTo bring scenes to life, Enscape can automatically replace planar ArchVision RPC models in Revit with more realistic 3D entourage. This includes 3D people and trees with individual leaves. The software can also directly support all types of RPC content, including premium human models from RPC content partner AXYZ.\nRender quality, in general, is excellent. The software supports Global Illumination, simulating how light bounces off objects onto other surfaces. This can be turned on and off.\nEnscape offers plenty of scope to take visual realism to the next level, by tweaking lighting and materials or adding custom Entourage. Phil Read from Enscape reseller Read | Thomas, who supported us during this review, has some great tips on self-illuminating objects, material bump maps, reflectivity and transparency.\nEnscape offers more than just high-quality visuals. To deliver an even more realistic experience, it allows you embed sound sources within your Revit model. Here WAV files are added to Revit Family Components, such as a stereo playing music or birdsong from a tree. The volume of the sound increases as you get nearer and it also responds to the acoustic qualities of the objects within the space, which really adds to the realism.\nWith all its visual quality, Enscape has pretty high GPU requirements, with the developers recommending at least an Nvidia GTX 980 or Quadro M6000 GPU. The software performed very well with our Nvidia Quadro P6000. It did not work with the AMD Radeon Pro WX 7100.\nIn summary, Enscape is an excellent choice for Revit to VR, offering a seamless workflow with near-instant updates. This is great for architects or engineers who want to jump between Revit and VR to assess different options or for presentations where clients can immediately see the results of their feedback. Enscape also has the added benefit of offering a desktop game engine experience using a standard 2D display. And models can be distributed as a fully contained .EXE, which can be viewed without any additional software.\nThe visual quality is excellent, encouraging exploration of materials and finishes, as well as form, fixtures and fittings.\nHowever, some of this is lost in VR, simply because of the display resolution of the HTC Vive and other HMDs (you can still see the pixels).\nIt would be great to see more control given to the wearer of the HMD for things like mark-up, but with Revit driving the application in real time, this is less of an issue.\nEnscape is available on a free 14-day trial. Prices start at $449 per year.\n\u25a0 enscape3d.com \u25a0 readthomas.com\nUpdate \u2013 Nov 2017\nSince this article was written, there have been a number of features added to Enscape.\nThe major emphasis has been on increasing visual quality and realism. Key enhancements are detailed below.\nRealistic Lighting: Reflections and indirect lighting are now done using real-time path tracing, which is more faithful to reality.\nRealistic grass: Enscape can now draw real geometry grass blades for more realism.\nTrees have been reworked to better resemble an architectural rendering look.\nDecals such as posters, can now be added to scenes.\nPolystyrol Mode has been made more realistic with physically correct subsurface light scattering.\nPerformance has been improved to better support larger projects and slower workstations.\nIrisVR Prospect 1.1.0\nOut of the three Revit to VR tools featured in this article, Iris VR Prospect is the only one to concentrate solely on VR (there\u2019s no formal desktop experience, although spectators can look at the monitor to see what the wearer of the HMD is seeing). This focus helps make it the most mature in terms of what you can do inside VR, including real-time daylighting, layer management, annotation and screen capture.\nIrisVR Prospect also has the added benefit of being able to work with other applications, including SketchUp and Rhino. There are plans to add support for ArchiCAD, MicroStation and Navisworks. This expansion will help extend the focus beyond architecture and into infrastructure.\nHaving this breadth of functionality calls for a higher price tag of $200 per user, per month. However, there\u2019s also a free version, Prospect Basic, that strips back all the extended functionality, but still includes the core push-button workflow for file conversion and VR viewing. With Prospect Basic, it\u2019s not possible to save projects as native Prospect files (.IVZ). The VR experience always has to start in the CAD or BIM software or with a neutral 3D file.\nModels inside IrisVR Prospect appear more clinical than in Autodesk LIVE and Enscape. Textures are less realistic and RPC content is represented by outlines, but the wearer of the VR headset still gets an excellent sense of presence and scale. Materials and outlines can be toggled on and off by hitting M and O on the keyboard. The resulting \u2018white model\u2019 gives a schematic feel and is great for conceptual design.\nBy offering a less visually rich experience and by performing heavy geometry optimisation, Prospect is able to run on more modest workstation hardware. We found that the entry-level VR graphics card, the AMD Radeon Pro WX 7100, delivered an excellent experience with all our test models.\nProspect installs as a plugin inside Revit, which can be accessed via the add-ins tab. Simply select your 3D view, then click \u2018View in VR\u2019. Once your file is ready, click \u2018launch\u2019 and you\u2019re straight into VR. Most of our files took a minute or less to process.\nIn VR, you first enter Scale Model Mode, where you\u2019ll see your project on top of a plinth. From this bird\u2019s-eye perspective, the model can be rotated using the HTC Vive trackpad. Lean forward to get a closer view of the building \u2013 even stick your head inside.\nTo view the building at human scale, simply point your cursor at a surface on the model then click the trigger button. You can teleport anywhere where it shows an avatar. To stand outside the building, the Revit model must include some form of surrounding topography.\nFor navigation, take short steps in your VR \u2018room space\u2019, then travel larger distances by teleporting. A circle denotes places where you can teleport. A cross shows where you can\u2019t.\nNavigation is pretty easy, although there are some limitations. You can teleport through transparent walls, doors and windows, but not through solid doors or walls.\nThere are a few ways to get around this. One option is to move close to a door, reach through until your control disappears, then click the trigger to teleport to the other side. Another is to make all doors transparent in Revit. A third is to turn off all door layers, leaving the model with door frames. The developers are looking at ways to improve this, including X-Ray modes or the ability to select an element in VR, and turn off all similar objects.\nOne of the Vive\u2019s controllers is used for navigation, the other to hold the tool palette, which includes Home, Screen Capture, Annotation & Callouts, Daylight and Layers. To access tools, simply use the other controller to point and click at the icons.\nThe home button puts you back where you started, in Scale Model Mode. Screen capture takes a snapshot of where you are looking.\nThere are very simple annotation tools, which allow you to redline the 3D file with a freehand marker or draw a fixed circle callout at the centre of your view. Once you\u2019ve completed the markups, these can be saved with the screen capture tool. IrisVR admits that these tools are quite basic and will be improved in subsequent releases. In the future, there may be the ability to assign custom text and symbols to objects.\nOne of the most powerful capabilities of Prospect is the daylighting tool, which allows you to see how light and shadows change with time or date, based on the geolocation of your Revit model. Simply use your Vive controller to move the appropriate sliders on the tool palette and see the light and shadows update in real time. The sliders can get a little frustrating if you move your cursor slightly off centre as it then stops scrolling. Some leeway would be great here, just like when you\u2019re in Windows and it still stays engaged even when you move your cursor off the scroll bar. IrisVR told AEC Magazine it will be addressing this in the next release.\nLayers can be a powerful way of viewing model data; for example, you can strip back everything to the steel frame, view different design phases, toggle between materials, or explore different furniture arrangements. To get the most out of this feature, users will need to do some work in Revit upfront.\nLayers are listed alphabetically, so it can be a bit of a pain if the layer you most frequently want to turn off lands at the end of the list. Renaming layers is the obvious solution, but this could impact company layer conventions. Layers suffer from the same scrolling issue as daylighting.\nIrisVR already has lots of ideas on how to improve the software. It is exploring ways to view metadata, simply by clicking on an object. IrisVR admits that the only way to do this would be to load all the objects individually, which would impact performance, so there is still some work to do here. (One of the reasons Prospect delivers such good performance in VR is because objects are currently grouped to reduce draw calls.)\nCollaboration is also going to be a big focus moving forward. Ideas currently being explored include project libraries, an easy way for clients to view IrisVR projects, and shared sessions, where multiple users can exist in the same project, with each participant represented by an avatar.\nIn the more immediate future, the next release of Prospect will be able to load in perspective cameras from Revit, to allow users to jump between waypoints at the click of a button. IrisVR sees this as a useful way to quickly get a bird\u2019s-eye view of the project or to guide clients through a building, rather than letting them wander off on their own.\nIn summary, IrisVR stands out for its expansive toolkit. The daylighting and layering tools, in particular, are very impressive. The push-button workflow where models can be processed very quickly makes it well-suited to design iteration workflows. The relatively low GPU requirements and free version will also make it exceedingly attractive to smaller AEC firms with tight budgets. With the prospect of BIM data being exposed and better collaborative tools, we\u2019re really looking forward to seeing how this impressive tool develops.\nIrisVR Prospect Pro is available on a free 21-day trial. Licenses cost $200 per user, per month. Iris VR Prospect Basic is free.\nUpdate \u2013 Nov 2017\nSince this article was written, there have been a number of features added to IrisVR Prospect.\nMeasuring in VR: a new measuring tool allows users to verify sizes, distances, and clearances in real scale in Virtual Reality.\nTraveling between viewpoints: 3D views from Revit can now create viewpoints within Prospect. These are camera locations that you can use to walk a client\/colleague through a project to present the flow of space more effectively.\nSectioning\/modifying models in Scale Model Mode: In Scale Model Mode user can now lift\/move the model around space, rotate it, zoom in, and section it.\n360\u00b0 Panorama: A new 360\u00b0 Capture tool allows users to instantly capture a 360\u00b0 panorama from within Prospect.\nMulti-user collaboration: Earlier this year, IrisVR previewed a multi-user mode that will allow users anywhere in the world to collaborate on AEC projects in VR. The technology will be available by the end of the year.\nRevit to VR \u2013 workstations and GPUs\nVirtual Reality demands extremely powerful workstation hardware. While most modern CAD workstations should satisfy the m i n i \u2013 mum requirements for CPU, memory (3.30GHz Intel Core i5 4590\/8GB RAM or above), and USB 3.0, they will likely fall well short on graphics (GPU). Both the Oculus Rift and HTC Vive require a GPU capable of sustaining a minimum frame rate of 90 FPS while in VR. Anything below this and the model can jump and the user can experience nausea or motion sickness. This is because what the user sees on the HMD is not what the brain expects to see, based on head movements.\nTo make things easier for those buying VR workstation hardware, both AMD and Nvidia brand their GPUs \u2018VR Ready\u2019. This stamp of approval works well in the games market, as games contain fixed datasets designed to run on specific GPUs. However, just because a professional GPU is labelled \u2018VR Ready\u2019, it does not mean it will work with all professional VR applications out of the box.\nPerformance is both applicationand model-dependent and is influenced by the size of the da taset, the complexity of the geometry (number of polygons), how well the application optimises geometry for VR, as well as lighting and textures. Custom Revit objects, particularly those with curves, can eat up GPU resources, especially if there are hundreds of them within your model.\nFor example, the AMD Radeon Pro WX 7100, a low-cost ($799) professional \u2018VR Ready\u2019 GPU, gave us a comfortable experience with IrisVR Prospect out of the box. But it stuttered with Autodesk LIVE and didn\u2019t work at all with Enscape. For these more demanding applications, we needed the considerably more expensive Nvidia Quadro P6000 ($6,999). We imagine the Nvidia Quadro P5000 ($2,499) would probably be powerful enough to deliver a good experience in Autodesk LIVE and Enscape, but we didn\u2019t get to try this. Ideally, you should try before you buy, using your own datasets.\nIn many cases, a simple graphics card upgrade can turn your desktop CAD workstation into one capable of running VR. However, this depends on the type of workstation you have; all the aforementioned GPUs need an auxiliary power connector and between 150W and 250W of available power. And all of them, bar the AMD Radeon Pro WX 7100, take up two PCIe slots. You need to make sure your CAD workstation can satisfy these demands. You may need to upgrade your Power Supply Unit (PSU) as well.\nOf course, there are plenty of pre-configured \u2018VR Ready\u2019 desktop workstations. These are available from HP, Dell, Lenovo, and Fujitsu, as well as custom system builders like Scan, Workstation Specialists and BOXX. The Armari V25 is a slimline, custom-built VR workstation with a chassis designed to house the HTC Vive\u2019s Link box.\nVR-ready workstations don\u2019t have to come in tower form factors. Dell\u2019s new All-in-One, the Precision 5720, features a 27-inch screen and an integrated AMD Radeon Pro WX 7100 GPU. Also coming soon are a whole range of VR-ready mobile workstations, featuring AMD Radeon Pro WX 7100, Nvidia Quadro P4000 and P5000 GPUs. We expect there to be a lot of interest around VR-ready mobile workstations for those wishing to take VR to client offices.\nVR is not limited to professional GPUs. There are number of consumerfocused AMD Radeon and Nvidia GeForce GPUs that meet or surpass the minimum requirements for both the HTC Vive and the Oculus Rift.\nThese GPUs should work perfectly fine with all the Revit to VR applications mentioned in this article. However, they are not certified for Autodesk Revit or other CAD \/BIM applications, which will be very important for some firms.\nRevit to VR \u2013 conclusion\nIt\u2019s still very early days for VR but we are already starting to see the extensive benefits that this exciting technology can bring to architecture and engineering \u2013 from functional and aesthetic evaluation of projects to daylighting studies, markup and client communication.\nWe\u2019re excited to see how these capabilities grow over the coming years. Applications like Enscape will likely keep their strong focus on using VR as an extension to desktop design visualisation, but it will be interesting to see how products like IrisVR Prospect evolve to better support design\/review workflows.\nVR is great for identifying issues with buildings, but it\u2019s very much a one-way street. The process would benefit greatly from being able to capture this information and feed it back into BIM authoring tools. Technologies such as the BIM Collaboration Format (BCF) or voice recognition could play important roles here.\nSince this article was written in February 2017 all of the tested VR software products have received new features. We have listed some of these features at the end of each review.\nAutodesk LIVE has also been rebranded to Autodesk Revit Live.\nIn addition, Nvidia has released a single slot, professional VR Ready GPU called the Nvidia Quadro P4000. At $999 it is considerably cheaper than the Nvidia Quadro P5000 and P6000. We have tested the Nvidia Quadro P4000 with IrisVR and Autodesk Revit Live and it performs well. Check out our full review at tinyurl.com\/P4000-AEC\nAMD has also released two new dual slot pro VR capable GPUs, the AMD Radeon Vega Frontier Edition ($999) and AMD Radeon Pro WX 9100 ($2,200), both of which are considerably more powerful than the AMD Radeon Pro WX 7100.\nMore VR tools for Revit\nRevizto is a standalone real-time viz tool with a focus on collaboration, co-ordination and issue resolution. It works with Autodesk Revit, ArchiCAD, SketchUp, AutoCAD Civil 3D, Navisworks and AutoCAD, and also supports FBX, IFC, BCF and PDF file formats. Revizto runs on PC, Mac, iOS and Android, as well as supporting the Oculus Rift and HTC Vive.\nLumenRT from Bentley Systems is a game engine design viz application that plugs directly into Autodesk Revit, ArchiCAD, Bentley MicroStation, Sketchup and others. It can also import models from many more applications. Users then bring the model to life by adding entourage, including realistic trees, people, water, wind and moving vehicles. LumenRT can currently create VR panoramas but will add support for the HTC Vive and Oculus Rift later this year.\nClick here to read our Beginner\u2019s Guide to VR for Architecture\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/studio-style-architecture-design-cad\/","title":"The role of tech at Studio Libeskind","date":1502064000000,"text":"Greg Corke visited Studio Libeskind\u2019s New York offices, a stone\u2019s throw from the World Trade Centre site, to explore the role that technology plays in the practice\u2019s world-renowned architecture\nBy his own admission, Daniel Libeskind was a late bloomer. Having started out in architecture in the late 1960s, he formed his own practice in 1989 when he won a competition to build the Jewish Museum in Berlin. Technology also came late to the table and there was no role for CAD when designing the bold, zig-zagged building At the time, Berlin was still a divided city, so technology trickled in very slowly.\n\u201cThe building was done completely by hand,\u201d Mr Libeskind explains. \u201cTrigonometry, logarithms, calculating angles \u2013 sine, cosine, tangent. There were more than a thousand windows. Each one had a different shape and they were all calculated by hand.\u201d\nFollowing the completion of the project in 2001, exhibition designers asked Libeskind for the model. \u201cI rolled up a bunch of drawings \u2013 plan, section and elevation \u2013 and I sent it to them and he said, \u2018no, no, no, give us the 3D [CAD] model\u2019.\n\u201cI said there is no 3D model and there was just a silence, a long silence and a gasp,\u201d he smiles.\nStudio Libeskind has come a long way since and CAD and BIM now play a critical role in all of its projects. \u201cWe\u2019ve been able to build things that probably would never have been fathomable,\u201d says Carla Swickerath, CEO\/Principal.\nBut Studio Libeskind still relies heavily on traditional techniques. Hand-crafted working models adorn every corner of its prestigious Manhattan headquarters. Concept models, hand drawings, even simple gestures, continue to be the creative inspiration for projects.\n\u201cIt always originates in drawings. I still draw by hand,\u201d says Mr Libeskind. \u201cYou can involve the viewer or client with ideas much more powerfully just by drawing something in front of them.\u201d\nProjects often then move into CAD for further exploration. \u201cDepending on the sketch, there are millions of interpretations of that sketch, so then we try to think about different possibilities and those usually start to be generated in a computer,\u201d explains Ms Swickerath.\n\u201cI think that\u2019s kind of miraculous,\u201d adds Mr Libeskind \u201cbecause it would have taken a very long time to speculate creatively with a pencil and paper.\u201d\nStudio Libeskind\u2019s design process is nonlinear. In other words, projects can move from sketches to CAD to physical models and back into hand drawings. There is no set formula, explains Studio Libeskind partner Yama Karim, adding that it\u2019s all about staying nimble and using every resource, every medium that is possible.\nMr Libeskind himself does not use CAD, but admits that most of his buildings could not have been built without technology, and certainly not completed on time or on budget. \u201cI am not a parametric designer, I don\u2019t use these tools, just as a service mechanism, but it makes possible the designs which we initiate,\u201d he says.\n\u201cBeing able to translate that drawing into a [computer] program that allows one to make it very, very precise, measurable, scientifically rational, very accurate\u2026 that whole matrix of possibilities is to me really the wonder of practicing architecture.\n\u201cAnd I can hardly conceive, 1990 \u2013 it\u2019s not that long ago \u2013 to sit down and do a building by hand. It\u2019s like going back to a primeval time. It\u2019s amazing the progress that has been made.\u201d\nDigital toolkit\nStudio Libeskind has benefited from a huge range of CAD tools over the years, from Form Z to AutoCAD. Every project centres on the 3D model.\n\u201cWe use Revit, we use Rhino, we use 3ds Max, we use whatever we need to use and we are often beating them into submission,\u201d says Ms Swickerath. \u201cBecause something you need to do in Revit \u2013 and Revit is a fantastic program that does amazing things \u2013 but sometimes it\u2019s not the right application for what we\u2019re doing or the right phase to use it.\u201d\nFor design development, for example, Rhino is often preferred, as the NURBS-based modelling tool can be both freeform and precise at the same time. \u201cOur forms tend to work well with that process,\u201d says Ms Swickerath.\n\u201cIn our office in general, Rhino is probably one of our favourite tools,\u201d adds principal Michael Ashley. \u201cIt\u2019s nice and fast and then we\u2019ll move into Revit when the project is more standard.\u201d\nGrasshopper, the generative design add-on for Rhino, is used on some projects, but only to solve specific design problems rather than as a driver for form.\n\u201cSome elements of the project might be scripted,\u201d explains Mr Karim. \u201cSo, tiling a pavilion where it\u2019s thousands of pieces that need to tessellate in a certain way, we script that.\u201d\n\u201cWe don\u2019t do generating shapes out of the computer,\u201d adds Ms Swickerath. \u201cWe use the computer to articulate the shapes we are trying to build. To make something more rational, to try to find the right solution.\u201d\nFor design visualisation, the firm relies on 3ds Max with V-Ray. But the modelling and rendering tool is not just used to create stunning visuals and animations. In one recent project, it was also used to optimise the design of a fa\u00e7ade by simulating light.\nThe Libeskind Tower is one of three high-rise skyscrapers at CityLife, a new residential, commercial and business district in Milan. Its curved glass fa\u00e7ade is designed to vertically extend the piazza it borders, so it becomes a lens into the public space.\nStudio Libeskind was keen to avoid negative aspects of a lens, which could potentially fry people on the plaza, so each and every single facade was broken down into tiers for a feathering effect to spread the concentration of light.\n\u201cWe did some [internal] testing in 3ds Max just to see the light, but we have a great team of engineers that actually do a series of analyses and say, \u2018OK, well we need to tweak something this way or that way or else this person at this time of day is really going to get sun burn\u2019,\u201d says Mr Ashley.\n\u201cSome of it is intuitive, also. You\u2019ve got to understand that if you create a lens, you could come out with magnifying glasses over ants,\u201d jokes Mr Karim.\nMicro machine\nStudio Libeskind runs its CAD software on workstations and recently adopted the HP Z2 Mini, a tiny desktop machine that dwarfs standard workstation towers (see our hands-on review here).\nDespite its diminutive form factor, quad core Intel CPU and entry-level Nvidia Quadro GPU, Studio Libeskind has found the HP Z2 Mini to be more than capable of supporting its core design workflows.\n\u201c[Even with some of our heaviest users], we haven\u2019t had an issue with any of the programs that we use, any of the complicated geometry, any of the difficult dense models that we use,\u201d says Ms Swickerath. \u201cSo, for us, it\u2019s been like \u2018give us more of these\u2019. We need lots of these because they are actually space saving and they are quiet and they have a performance that even surprised us.\n\u201cWe do a lot of complex models that have a lot of input from multiple engineers. [The HP Z2 Mini] is yet to be tripped up.\u201d\nMr Ashley adds that the HP Z2 Mini could serve the requirements of all 50 architects at the New York office, adding that Studio Libeskind would then probably need three or four higher-end workstations to support more demanding graphics and rendering workflows.\n\u201cBut even so, rendering is often done in the cloud, with cloud services, so you just set the scene and send it off somewhere else to render,\u201d he adds.\nLet\u2019s get physical\nDespite its reliance on digital workflows, physical models continue to play a critical role at all project stages. Models are very important to the design process, so that the team can see how ideas are working out. To this end, the practice still has a fully equipped model shop at its Manhattan office.\n\u201cEverything we make in the computer \u2013 if we\u2019re designing a project, exploring options and making things work \u2013 we\u2019re building models at the same time, because we always like to stand back and look at it in space, because the computer can be very deceiving,\u201d says Ms Swickerath.\nWhen it comes to physical models, Studio Libeskind takes the same approach as it does for CAD and uses the best tool for the job. Models can be hand crafted with wood, 3D printed or sculpted with paper, scissors and glue.\n\u201cSometimes, if it\u2019s a presentation model of a complex piece, we\u2019ll get some pieces made [3D printed] and build it together and make it part of our hand-crafted model,\u201d explains Ms Swickerath.\n\u201cSometimes, it\u2019s a study thing, where you can do 3D models quickly. It [3D printing] is just one more tool in our kit.\u201d\nTraditional methods are not to be underestimated, says Mr Libeskind, adding that the practice sometimes makes huge models \u2013 large enough to stick your head inside. \u201cI remember we did a model for the staircase at the Denver Art Museum, which was almost as big as this room,\u201d he says, indicating the small meeting room in which he\u2019s speaking. \u201cIt was just a very small part of it. It was a very complex shape but we couldn\u2019t explain it [with just drawings and CAD models].\u201d\nAll together now\nAt Studio Libeskind, projects are seen through from beginning to end, with success hinging on close collaboration with its partners. With projects taking place all over the world, there is much to learn when working with local firms.\n\u201cWe aren\u2019t dogmatic, like \u2018Here\u2019s a design, take it or leave it,\u201d explains Ms Swickerath. \u201cIt\u2019s like, \u2018Here\u2019s the big idea. It\u2019s complicated, it\u2019s interesting, let\u2019s make it work.\u2019 Then we take the input [from our partners].\n\u201cWe have this great collaboration with everybody that we work with that allows us to build these buildings.\u201d\nThe 3D model forms the lynchpin to this process, and data is regularly shared with partners. \u201cWe communicate with our clients, our engineers, all of our consultants. They are able to put their information into our models and work very quickly.\n\u201cWe work with our engineers about structure \u2013 how to make it more efficient, how to make it cheaper.\nMs Swickerath explains how Studio Libeskind has great confidence in its models and also shares its data with contractors. \u201cIt\u2019s a tool to help them do their job better,\u201d she says. \u201cWe try to make that collaboration so that they can see the model and understand things better.\u201d\n\u201cWe always feel the better we communicate, especially when things are difficult to understand with one drawing or one perspective or one model, then the more successful we are in getting what we want built,\u201d she says.\nMr Libeskind thinks the 3D model has also helped change the dynamic when working with clients. \u201cClients are more engaged because they can see more,\u201d he says. \u201cIt used to be that you presented a drawing or a [physical] model. The client was relatively innocent, not really fully understanding everything. But, with a 3D model, for example, a client can see everything. It\u2019s like an X-ray \u2013 you can see every flaw.\n\u201cThere are clients who are very, very peculiar, very particular \u2013 not just the square metres, the square footage, but aesthetic questions, functional questions. [A 3D model] makes the work more interactive, it makes the work, I think, more intelligent, because more people are aware of what is going on. There is less opacity, becoming more transparent and I think more democratic.\u201d\nBut Mr Libeskind also acknowledges the limitations of technology. \u201cThe computer is a means to an end,\u201d he says. \u201cI\u2019ve never been seduced to think that the computer could solve the problems of architecture, because architecture is an art.\n\u201cDesign originates in creativity, in innovation. It doesn\u2019t originate with a tool. In fact, probably, creative design originates with your eyes closed,\u201d he says.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-university-2020\/","title":"Autodesk University 2020","date":1605830400000,"text":"From AI-based conceptual design and digital twins to the evolution of Construction Cloud and a new \u2018open\u2019 approach, there was plenty for AEC professionals to get their teeth into at this virtual event, writes Martyn Day\nBeing in the middle of a Covid-19 lockdown is probably the furthest away from being in Las Vegas for Autodesk University (AU) that I could ever imagine. This year, no cacophony of slot machines while dashing about to catch key sessions. No corridor chats or bar hopping, just a global audience of designers and builders, sat at home in front of a computer, wondering if they have enough coffee, toilet paper and saying \u2018you\u2019re on mute\u2019 a lot to colleagues. \u2018What happens in Covid stays in Covid\u2019, doesn\u2019t quite have the same ring as the Vegas equivalent.\nThe good news is the Autodesk event team has done an excellent job of maintaining the breadth and mix of talks, classes and activities as best as is possible online and you can dip in and out at will, to catch-up on literally hundreds of hours of videos given by topic experts.\nFor the past few years, while the cloud, automation and machine learning have become key topics for AU keynotes, this has tended to be focussed on future promises that the technologies hold, as opposed to an actual connected and integrated deliverable.\nThis AU saw Autodesk double down on last year\u2019s launch of \u2018Construction Cloud\u2019 and start to expand capabilities significantly beyond just storage, model sharing and document distribution workflows. As you will read, the AEC division has added a lot of meat to its bones this year.\nCEO keynote\nThe main keynote was delivered by Autodesk CEO, Andrew Anagnost. In a wide-ranging talk, Anagnost commented on the challenging year we have just had. With firms thrown into turmoil by a global pandemic, we were forced to work from home, adapt and create a new normal. All at once we realised how fragile our economies, processes, supply chains and ecosystems were. Practices had to think differently to meet the demand changes and be able to suddenly become virtual organisations.\nWhile the near-term outcomes are unknown, Anagnost had a positive and optimistic view on what will happen on the other side of the pandemic. By rolling with the punches and adopting increasingly digital workflows to cope with remote working, he feels that the construction industry will not go back to its old processes and workflows. Instead, it will be permanently digitised providing productivity benefits.\nObviously a mass digital construction migration is good for Autodesk\u2019s business. By giving away extended access to Autodesk BIM 360, the company both helped firms deal with going virtual during Covid, as well as giving them a free taste of what was possible by relying on the cloud as a backbone. Anagnost explained how Autodesk was building on the services which continued to be fleshed out within its Construction Cloud platform from design, planning, building and through to operations.\nThis year, Autodesk announced three new platforms, Build, Quantify and Coordinate, as well as a new Digital Twin platform called Tandem. With Autodesk Docs underpinning most of this extended functionality, Autodesk announced that it will be giving Docs away as part of its Collections next year. In many respects, Docs is the gateway drug to a lot of the new functionality of Autodesk Construction Cloud.\nDuring the keynote, Anagnost gave several real-world examples of firms adapting to the new normal. Of special interest was James Hepburn, engineering principle of BDP, who explained how his team rapidly evolved the plans for the first Nightingale hospital in London, using Revit and team collaboration to pull off a remarkable turnaround of converting the Excel Conference Centre into a 4,000 bed Covid centre. If only we had the same level of planning and foresight in Government to get enough trained staff to run it.\nAnagnost also announced that they had acquired Spacemaker for $240 million. The Norway-based software company develops a cloud-based urban planning and site design tool, which uses artificial intelligence (AI) to aid rapid conceptual design.\nTo date, the firm has tended to find its biggest traction in the developer community, as opposed to architectural design practices. Autodesk hopes this will prove popular with design firms. It also inherits a 110 strong development team, which has been deploying AI to aid design decisions. More on this later.\nThere was a lot of talk about openness and industry data standards. Anagnost explained that he was a massive fan of breaking down traditional interoperability problems. Autodesk\u2019s new membership of the Open Design Alliance (ODA), taking a seat at the table of BuildingSMART for IFCs and becoming a founding member of the US-based Digital Twins Foundation, were commitments from Autodesk to enable data to flow within federated workflows. Anagnost also specifically stated the company\u2019s commitment to better support IFC and to certifying ISO 19650 workflows in its Construction Cloud.\nAutodesk has also teamed up with Nvidia to support Omniverse, which uses Pixar\u2019s USD \u2018collaboration\u2019 format for models, texture and environments. While Nvidia provides the GPU backbone on the cloud, Autodesk is adding support to its AEC and Media and Entertainment products and platforms. More on this later.\nAEC keynote\nSince 2015, Autodesk has been preoccupied with building its cloud offerings and SaaS business. Much of the early work was spent creating Forge, a technology layer which would enable it, its third-party developers and customers, to rapidly develop design tools in the cloud.\nTo do this it created modules of capability, such as a DWG engine, a model viewer etc., which could be used as building blocks to create new on-demand applications, such as BIM 360. This methodology would also allow Autodesk to quickly absorb and re-purpose acquisitions to add to its cloud platform. And here, Autodesk has acquired a lot of technology in the last three years. It\u2019s been on quite a journey and has ultimately led to the Autodesk Construction Cloud portfolio, which continues to be augmented with further additions.\nAs Anagnost announced in his keynote, there are three new modules: Build, Quantify and BIM Collaborate.\nBuild unites PlanGrid and BIM 360 with new functionality to create Autodesk\u2019s field and project management solution. Within Build, teams can handle workflows such project management, quality, safety, cost and project closeout.\nBuild also includes the PlanGrid Build mobile app with additional new capabilities for field workers. PlanGrid Build mobile app offers features such as RFI creation, issue tracking, always up-to-date drawings and a markup capability. As Autodesk acquires new technologies, sometimes there is duplication and here, while BIM 360 was first, the data backbone of PlanGrid was deemed to be the most adaptable for mobile, which is essential for the capabilities Build provides.\nQuantify does what you would expect and enables estimators to automate 2D and 3D quantification from BIM models in a single platform. Features include: 2D take-off count and area take-off, together with automated 3D take-off from BIM components. The platform supports predefined or custom classification systems with customised formulas. Built-in document version control provides notifications of changes to models or drawings.\nBIM Collaborate: Enables project teams to work together by handling the whole design collaboration and coordination workflow. This features a broad tool set for model review, markups, issue management, change analysis, and clash detection. Non-authoring collaborators are also now invited, making it easy to see in-progress changes on desktop and mobile devices without being a subscriber to Autodesk products.\nBIM 360 Design is being rebranded BIM Collaborate Pro and will also offer users Revit Cloud Worksharing, Collaboration for Civil 3D and Collaboration for Plant 3D. Customers will have access to model coordination and Insights and connections to the Autodesk Construction Cloud.\nAll these capabilities are underpinned by Autodesk Docs, which provides the underlying common data environment, delivering an integrated platform and familiar interface across workflows. Autodesk Insight also delivers analytics from the models, as well as the ability to export that data. It encompasses Construction IQ artificial intelligence to identify and mitigate risk to data added to workflows.\nAutodesk also announced a Docs plug-in for AutoCAD to allow CAD drawings to be published as a PDF directly from AutoCAD through Autodesk Docs or BIM 360.\nAutodesk\u2019s AEC Collection is not only getting Docs added but also project dashboards, reports all built from the Construction Cloud platform. There will be a new \u2018bring-your-own subscription\u2019 model to allow project owners to invite other subscribers (internal or external to the company) to join a project.\nAutodesk is including Autodesk Docs as part of the AEC Collection next year and all these new modules will be available in early 2021.\nAutodesk\u2019s AEC Collection is not only getting Docs added but also project dashboards, reports all built from the Construction Cloud platform. There will be a new \u2018bring-your-own subscription\u2019 model to allow project owners to invite other subscribers (internal or external to the company) to join a project.\nAutodesk Tandem\nAutodesk is making its first foray into the world of Digital Twins with the ongoing beta of the cloud-based Autodesk Tandem. Here, Autodesk has used the Forge API to develop an environment in which BIM project data can be loaded from many sources via multiple formats to create a rich data model to be used in multiple phases of a building or infrastructure asset\u2019s lifecycle.\nDigital Twins provide an opportunity for the designers, fabricators and owners to share all the project data (electrical, cooling, escalators, anything) from design to decommission.\nTandem does not yet support laser scan data to capture the as-built but this is in development, together with connectivity to IoT sensors \u2013 which is the bedrock of a true Digital Twin. Tandem is due to finish its beta testing early in 2021.\nIt\u2019s well worth looking at Project Dasher which is an Autodesk research project that analyses data from live sensor inputs in the actual asset and displays them in a dynamic and live way.\nTandem provides Dasher with the Digital Twin geometry and live paged data from sensors, and runs real time analysis across all sorts of use cases and gives really beautiful rendered feedback live within the Digital Twin model.\nIn anticipation of entering the market, Autodesk became a Founding Member of the Digital Twin Consortium in October 2020, a US-based organisation whose members are committed to establishing digital twin best practices. Here, the UK seems to be a bit ahead of the US, with our own National Digital Twin programme, run by the Centre for Digital Built Britain (University of Cambridge), based on the founding Gemini Principles.\nThe Digital Twins market is set to be huge and with Revit having a dominant role in the creation of structured asset data, a cloud-based system that could automate the collation of all project data, for handover to clients, is a very exciting prospect.\nThere are whole new business models to be had for architectural firms who create consistent, detailed models of their projects. The data that was generated during the BIM process, augmented with as built information and IoT sensors could easily extend revenue streams from the data maintenance of old projects. Till now, Digital Twins have tended to be in the realms of large portfolio owners, like oil refineries, or universities. With Autodesk Tandem functionality being located in the same place as all of the Revit project data, this could potentially democratise Digital Twins. It will all come down to price and educating clients as to the benefits of post-construction data usage.\nOn openness\nThroughout this whole Autodesk University, the majority of company executives talked about Autodesk\u2019s openness, the need for openness in AEC and about the company\u2019s commitment to being open. As cases in point, Autodesk has joined the board of buildingSMART, an organisation it help set up (then known as the International Alliance for Interoperability (IAI)), as well as signing up with the ODA for its IFC toolkit (a format initiated by Autodesk) to improve its IFC fidelity. Autodesk has also joined forces with Nvidia to connect its design and rendering products to Nvidia\u2019s Omniverse which leverages Pixar\u2019s USD (Universal Scene Description) format.\nAutodesk watchers would at least raise an eyebrow at this new-found conversion to promoting interoperability. Despite setting up the IAI in 1994, and being a driving force to produce IFC coming out in 1996, Autodesk\u2019s IFC capability has left a lot to be desired, probably aided by Revit\u2019s native RVT file format dominating the BIM landscape.\nThe poor quality of IFC in Revit was one of the many complaints raised by the Open Letters Group. Autodesk\u2019s commitment has certainly been lacking vs the rest of the industry.\nAutodesk has also had a longstanding battle over the years with the ODA which essentially only existed to reverse engineer DWG for Autodesk\u2019s competitors to open and save native DWG. In the past Autodesk successfully sued the ODA over its emulation of \u2018Trusted DWG\u2019.\nToday, the ODA not only reverse engineers DWG but also Revit\u2019s RVT format and provides the industry standard IFC toolkit. Autodesk\u2019s membership of the ODA only applies to IFC and my understanding is that this negotiation started before the Open Letter was released.\nAutodesk\u2019s newfound passion for supporting open formats may well come from CEO Andrew Anagnost and the company\u2019s focus on cloud, as well as services such as Digital Twins, which need to be able to pull in data from multiple sources.\nCustomers also need assurance that they are not trapped in any one vendor\u2019s cloud. However long-term, files are on the way out and openness will be defined by API access to cloud repositories, as files will not be emailed or transacted and it will all happen through the interconnectedness of the cloud.\nPerhaps with the death of files being inevitable, it makes doubling down on open file formats at this point an interesting, if very late, decision. One could argue Autodesk is just outsourcing its IFC development but it is certainly turning that into a marketing opportunity.\nNvidia Omniverse is a cloud platform, powered by Nvidia\u2019s GPUs which aims for universal interoperability using Pixar\u2019s USD format, across most of the major 3D design platforms, from BIM to rendering and VR. The platform is designed to act as a hub, enabling live collaboration between users of different applications with real time ray tracing and offering capabilities such as virtual simulation. Autodesk has bought into this 100% and USD will appear in Maya, Max and Revit, as well as other Autodesk applications. It\u2019s currently in beta and we don\u2019t yet know what the subscription cost will be, or if there is a token usage for processing. Read Greg Corke\u2019s write up on KPF\u2019s usage of the technology here.\nSpacemaker acquisition\nThe big surprise of Autodesk University was the announcement that Autodesk was purchasing Norwegian developer, Spacemaker. The cloud-based AI software helps urban designers, real estate developers and architects make more informed design decisions. The software generates many iterations of workable alternatives for the best sustainability and quality of life outcomes. The software has a great interface and looks easy to use. It\u2019s currently limited to rectilinear geometry, but more than adequate for simple massing and data projections.\nIn the example given, if you\u2019re interested in a plot of land for new apartments Spacemaker can explore in real time what the rentable area is, how many apartments are possible and what the lines of sight might look like. Users have the ability to assess as many criteria as they need right in the cloud, combining generative design to analyse factors like noise, wind, and sightlines, just to name a few.\nIt has been said that Autodesk plans to keep Spacemaker as a separate entity and not to interfere, which would be great, as Autodesk is \u2018early in\u2019 to this formative site analysis market, which features other great developers such as Hypar.io, Testfit.io and Digital Blue Foam. Gensler has also developed its own conceptual generative site\/urban planning solution called Blox. Other AEC firms have done the same.\nWhile hailed as a big boost to architecture and design, it is true that Spacemaker has tended to focus on developers and other areas of traditional prop tech. Autodesk will need to assist in the development of features and marketing of Spacemaker and its solution, and broaden its appeal to architectural firms. It will also need to be rewritten using Forge to play nicely with the Autodesk cloud ecosystem. With 110 employees, Autodesk also gets a great team of AI\/ML and generative programmers in with the deal.\nSchneider Electric\nAutodesk and Schneider Electric are working to develop a new cloud-based service for electrical designers, leveraging Autodesk Forge APIs to provide users with an environment which connects designs from concept, through schematic design, and into detailed design. The new solution is aimed to fill gaps in BIM-based workflows enabling load distribution mapping, power balancing, equipment sizing, and single line diagramming.\nCertification\nFollowing up on a previous keynote promise of Anagnost\u2019s (2017) to offer training to customers to adapt to future changes in working, Autodesk has updated its Certification Program. It has added eight new industry-aligned, role-based, self-paced certifications. The company saw an 800% increase in traffic to its learning resources during Covid-19 lock-down. It\u2019s great to see some people put that time to good use.\nThe Open Letter response\nAn article about Autodesk in 2020 would not be complete without an update on the Open Letters group and Autodesk\u2019s response. There was precious little acknowledgement within the keynotes, but the topic of the open letter came up a number of times in Revit sessions and talks with the executives of Autodesk. The week before Autodesk University, Amy Bunszel published an update on where Autodesk was at, concerning their responses and activities related to Revit development and community engagement.\nThe first \u2018wins\u2019 for the Open Letter Group have been Autodesk\u2019s decision to allow access to software five versions back (up from three) and an extension of a year for Network licences before users have to move to named user licensing (this was attributed to Covid relief by Autodesk).\nAutodesk has also announced an acceleration of its Pay Per Use (PPU) licensing development, which will work a little like Network licences. This is expected to arrive mid next year.\nOn Revit functionality, Autodesk has started a mass engagement with Revit customers on hearing what features they would like to see and what needs fixing. It has been more open with its roadmap and has hired a person specifically to engage with users. We will have to wait and see what changes this makes to the Revit roadmap and at what velocity the development team can redress the woeful lack of development over many years. Some customers have pointed out to me that the acquisition of Spacemaker would typically equate to 2-4 years of Revit development.\nIt\u2019s interesting that Autodesk has opted to cast its net wide for feedback on Revit. Anagnost was proud that the team had connected with the silent middle-tier of Revit users. From what I can see, the real gripes are coming from Revit\u2019s most advanced users, who push Revit to the max. These include many signature architects.\nSurely, by addressing the needs of the most demanding users, everyone can benefit? The problem is, these demanding users want deeper redevelopment of Revit\u2019s code; they want what\u2019s already delivered to work better, while the general masses may be more interested in having additional new features. By casting the net wider, Autodesk dilutes the request of power users. While perhaps fairer to all customers, it also benefits Autodesk\u2019s aim to limit how much core redevelopment is actually requested. It\u2019s not easy having millions of customers or a mature product.\nWhat doesn\u2019t seem to be being addressed is the pricing and ratcheting up of cost of ownership, which Anagnost clearly dismissed in his various public statements on the issue. Also, Autodesk\u2019s aggressive approach to non-compliance, as well as what exactly is the long-term future of Revit and architectural design beyond the desktop. Revit is the on-ramp to Autodesk\u2019s Construction Cloud, with no architectural BIM to feed the process, no cloud business model. We will revisit this topic in the next edition.\nConclusion\nFor the last three or four years at AU, Autodesk has been building its cloud infrastructure and capabilities. A lot of these have come from acquisition, leading to integration work and a paced roll out of integrations. There has also been a fair amount of talk about collaborative design systems, such as Project Quantum\/Plasma. To date, this elusive next generation design system has proven to be vapourware. Anagnost was even quoted as saying Quantum was little more than a Powerpoint or is still deep in the software labs.\nEarlier in the year, Anagnost promised that AU would provide a sneak peek of what Plasma had become. This was not self-evident in the mainstage talks; it may, of course, have been shown in one of the many other sessions (at the time of writing AU is still in full swing).\nAutodesk seems to have accelerated development of its cloud offerings and in 2020 that means we have seen a bumper crop of new capabilities added to Autodesk\u2019s Construction Cloud. As the technology stack builds, it\u2019s looking more like an interesting destination. Autodesk cloud facilitates collaborative design, hosts and manages designs and drawings. It checks for errors, allows quantity take off, gives cost insights, distributes the right data to site and now is stepping into post construction with CAFM and Digital Twin \u2013 all with a lessening need to send and manage files in the messy federated way the industry has worked for years, despite digitisation.\nHowever, if you\u2019re a dedicated user of Autodesk\u2019s desktop applications, and not so cloudy in outlook, there were slim pickings on show in the keynotes. It\u2019s now evident that Autodesk sees desktop products as in transition that will, in time, become part of the cloud architecture. The big question is how long will that process take? I am already hearing rumours of Autodesk experimenting with delivering VDI instances of its desktop products. This is inevitable, we just don\u2019t fully know the timeframe.\nAn interview with CEO Andrew Anagnost will also available online soon and all the AU talks are available online. There are plenty of interesting talks to see there and to keep you occupied in the current lockdown.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/leica-geosystems-introduces-on-pole-rtk-gps-surveying-system\/","title":"Leica GeoSystems introduces On-Pole RTK GPS Surveying System","date":1137801600000,"text":"Leica Geosystems has announced the introduction of the SmartRover, a new, lightweight, high-performance, all-on-the-pole RTK GPS surveying instrument. Weighing just 2.8 kg (6.16 lb). Consisting of the Leica AT1230 SmartAntenna and Leica 1250 Controller, the SmartRover is designed to be fully compatible with the Leica SmartStation, the first total station with integrated GPS. The SmartRover features Leica Geosystems\u2019 exclusive SmartCheck and SmartTrack technology for high performance in the field. The SmartCheck algorithms provide centimeter-level position accuracy at update rates up to 20 Hz with 99.99 percent reliability for baselines up to 30 km. The SmartTrack engine acquires all visible satellites within seconds, tracks to low elevations, measures beneath trees and provides exceptional multipath mitigation and advanced anti-jamming capability. The SmartRover incorporates Windows CE and Bluetooth technologies. This facilitates direct contact to the office via the internet to upload and download data to enhance survey productivity. With its integrated CF card, data can be seamlessly exchanged with Leica System 1200 instruments to provide full X-Function capability. All System 1200 application software is also available on the new RX 1250 Controller.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-turbosite-drone-to-automate-aerial-reporting\/","title":"NEWS: TurboSite Drone to automate aerial reporting","date":1431302400000,"text":"New software uses the Dronekit SDK from 3D Robotics to add drone support to mobile site reporting tool.\nTurboSite Drone from IMSI\/Design is said to be the first ever automated aerial reporting app for UAV (Unmanned Aerial Vehicles). The software is based on the mobile site reporting tool TurboSite.\nWith TurboSite Drone users have the ability to set waypoints in advance of a flight. Doing so autonomously pilots a 3DR-powered vehicle to precise GPS coordinates, allowing users to take photos, videos, dictations, and text notes at that precise spot, as well as assign \u201cpunch list\u201d action items for future follow-up.\nUsing a range of markup and measurement tools, photos can be marked up even while the drone is still airborne. A custom PDF or HTML can be automatically generated and distributed to specific people on the design team before the UAV lands.\n\u201cThere\u2019s up to 2.5 hours of aerial report creation for every 15 minutes of flight time,\u201d stated Royal Farros, CEO of IMSI\/Design. \u201cTurboSite Drone will do away with that, making aerial report creation and distribution one-button simple.\u201d\nDoug Cochran, CTO of IMSI and a former practicing architect who worked on large projects such as The Venetian Hotel in Las Vegas added: \u201cDrone usage is one of the most exciting innovations to happen to AEC in a long time. The ability to have instant aerial data\u2026 and be able to instantly share that aerial data with team members\u2026 will change the very foundation of construction management going forward.\u201d\nThe software will be be available in Q3 2015 and will be available from $19.99 per month. A free fully-functioning download with a two waypoint limit is also available for evaluation purposes.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-rebecca-de-cicco-digital-node\/","title":"Video: NXT BLD 2018 London conference - Rebecca De Cicco, Digital Node","date":1530144000000,"text":"How Smart Cities, BIM and Digital Construction will alter future skill requirements. \u2013 NXT BLD London, June 2018\nRebecca De Cicco runs Digital Node consultancy and works with clients all over the world. In her talk, De Cicco looked at Smart City trends out to 2025 and how the skill dynamic will change looking forward. The UK is in good stead having jumped on BIM early and developed standards, processes and frameworks to deliver structured data. Many countries are taking the lead from these standards and applying them to their construction markets and Smart City initiatives.\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/consumers-energy-implements-large-scale-mobile-gis-with-1800-units-of-arcpad\/","title":"Consumers Energy Implements Large-Scale Mobile GIS with 1,800 Units of ArcPad","date":1020124800000,"text":"ESRI is pleased to announce that Consumers Energy (CE) will implement 1,800 seats of Tadpole-Cartesia\u2019s field information system solution, which was developed using ESRI\u2019s ArcPad software. Field employees in CE\u2019s service, distribution, and line groups will benefit from this large-scale deployment of a field geographic information system (GIS). Consumers Energy is one of the nation\u2019s largest utilities, providing electric and natural gas service to more than six million residents in all 68 counties of Michigan\u2019s Lower Peninsula.\nOne of the largest ESRI mobile GIS implementations of its kind, the field information system uses ArcPad mobile GIS software for remote GIS mapping and analysis. The contract was awarded to Tadpole-Cartesia, a software subsidiary of Tadpole Technology plc, after an extensive benchmark and review process.\nField engineers will be able to boost productivity and efficiency through streamlined data management and maintenance, enhanced data automation, and greater data access. The ultimate value of the implementation will be better customer service, whether it is for field service, new construction projects, or outage management.\nKey application areas for the field information system at CE include land base development, gas and electric distribution management, automated construction and as-built drawings, and outage maintenance. In addition, CE maintains an extensive database related to 1,700,000 underground gas and electric services not associated with electric and gas distribution data. Users can access, redline, and query these graphical records as well.\nThe Tadpole and ESRI relationship combines ESRI technology and Tadpole\u2019s value-added modules to facilitate optimised GIS in the field. The ArcPad software-based field information system integrates seamlessly with the utility\u2019s existing information technology infrastructure including ESRI\u2019s ArcGIS software suite and applications such as dispatch and work order management.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-abel-maciel-bartlett-school-of-architecture\/","title":"Video: NXT BLD 2018 London conference - Abel Maciel, Bartlett School of Architecture","date":1530144000000,"text":"Design Thinking, Teams and Disruptive Technologies. \u2013 NXT BLD London, June 2018\nDr. Abel Maciel explored some of the challenges we face today, especially the interoperability of data through advanced tools: cloud computing, concept simulation, collaborative working, \u2018assisted decision making\u2019, big data, generative design, reality capture, virtualisation and building management systems. He also gave an insight into emerging technologies \u2013 Blockchain, how game theory modelling could be applied to improve collaboration and combining IoT with machine learning and Blockchain to make data more machine readable and how that integrates with BIM processes.\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/leica-introduces-leica-tps1200\/","title":"Leica introduces Leica TPS1200+","date":1192060800000,"text":"Norcross, USA, October 10, 2007: Leica Geosystems introduces survey grade total station with enhancements in accuracy, productivity, and scalability in their new Leica TPS1200+ Total Station.\nUsers no longer have to trade pointing accuracy to gain the convenience of robotic instruments. In combination with the new positioning camera, the PinPoint reflectorless EDM range and accuracy has increased two fold. The improved features of the telescope and the new high contrast color display make this instrument the best in its class.\nTo improve instrument performance to reflectors, a new CMOS camera system has been developed. This camera now makes pointing accuracies of 1 sec possible. To improve the overall instrument distance accuracy, a new red-diode EDM has been designed to bring the base EDM accuracy to 1mm.\nBuilt in DXF file support makes sure that all drawing programs are supported both in the field and in the office. Improved Automatic Target Recognition (ATR) sensors in the TPS 1200+ follow the prism with precision. With Leica\u2019s SmartStation, SmartRover and SmartPole options, the user can customize their surveying equipment to their specific tasks, crew\u2019s needs and budget. Leica System 1200 combines TPS and GNSS in the most efficient and complete way. System 1200 is a complete solution that is ready to be expanded when you are.","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/esa-envisat-presents-detailed-portraits-of-the-earths-land-surface\/","title":"ESA ENVISAT presents detailed portraits of the Earth's land surface","date":1178841600000,"text":"Europe, 11 May 2007: As part of the ESA initiated GlobCover project it has created a detailed portraits of the Earth\u2019s land surface. The detailed portraits of Earth\u2019s land surface have been created with ESA\u2019s Envisat environmental satellite.\nAround 40 terabytes of image data was acquired between December 2004 and June 2006 and processed to generate the global composites. The composites will support the international community in modelling climate change extent and impacts, studying ecosystems and plotting worldwide land-use trends.\nThe United Nations Environment Programme\u2019s (UNEP) Ron Witt said: \u201cThe GlobCover data sets should allow UNEP to do frequent monitoring of environmentally-critical sites and known \u2018hot spots\u2019 in areas we have under examination around the globe, and to update our knowledge of such changing environmental conditions, in order to alert the global community to emerging problems before it is too late for decision-makers and civil society to take action in this regard.\u201d\nThe Food and Agriculture Organization of the United Nations (FAO) will use GlobCover products to support many of its activities. \u201cGlobCover products should constitute an important interpretation asset in support of more dynamic environmental parametres such as rainfall and vegetation condition for FAO\u2019s global and national food security early warning programmes on which ESA and FAO cooperate closely,\u201d FAO\u2019s Dr. John Latham explained. \u201cIt will also significantly contribute to the monitoring and assessment of global land cover and as such will support the contribution of FAO to the assessment of land degradation and the monitoring of global forest cover.\u201d\nData acquired from Envisat:\nThe products are based on Envisat\u2019s Medium Resolution Imaging Spectrometer (MERIS) instrument working in Full Resolution Mode to acquire images in polar orbit at an altitude of 800 km with a spatial resolution of 300 metres.\nThe global composites are produced by processing MERIS images together in a standardised way. Thirteen out of 15 MERIS spectral bands are processed with an upgraded algorithm including tools for ortho-rectification, cloud screening and full atmospheric correction, which accounts also for aerosol.\nThe global land cover map, which is approximately ten times sharper than previous global satellite maps, is derived by an automatic and regionally adjusted classification of the MERIS global composites. The 22 land cover classes are defined according to the UN Land Cover Classification System (LCCS).","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/bim\/the-future-of-aec-an-executive-perspective\/","title":"The future of AEC: an executive perspective","date":1612137600000,"text":"Recently, Ron Fritz, CEO of Tech Soft 3D, hosted a roundtable discussion with four other technology executives to discuss trends circling around the AEC industry and the impact of the Covid-19 pandemic\nHow is the Covid-19 pandemic forcing the AEC sector? to digitise and modernise faster than it otherwise would. How will the AEC industry change? Which areas will be first to evolve, and which will be resistant to transformation? And ten years from now, which aspects of AEC that we take for granted today will look like something out of the Stone Age?\nOffering their thoughts on these and other matters are:\n\u2022 Anand Mecheri, CEO of Invicara, a developer of digital twin solutions\n\u2022 Clifton Harness, CEO of TestFit, an automated building configurator\n\u2022 Hilmar Gunnarsson, CEO of Arkio, a provider of collaborative design tools for architecture\n\u2022 Richard Humphrey, Vice President of Product Strategy and Product Management at Bentley Systems, a provider of AEC software\nA lightly edited and condensed version of the conversation follows.\nQ: What will the medium- and longerterm impact of the pandemic be on construction software companies and the AEC space in general?\nMecheri: We can already see some of the medium-term impact. I was on a call with some clients in Singapore today, and construction there has nearly ground to a halt, simply because there are no construction workers. We see similar impact in India. Also, there\u2019s an overriding concern about demand contraction in the commercial real estate space. Retail, of course, already was contracting, and the pandemic has accentuated it like crazy.\nLong term, things will be fine and back to normal \u2013 but there will be some lessons learned. I think this pandemic will be a trigger for change. For starters, there\u2019ll be more investments in technology to optimise usage and management of built assets, ensure occupant wellness, and create more efficient buildings overall.\nHarness: Operationally, I think companies are learning to do things differently during the pandemic. You\u2019re starting to see various companies in the AEC ecosystem adopting new technologies like Zoom and Slack. But those tools are disrupting email, so we\u2019re still pretty far behind as far as investing in the right things to push everyone into the 21st century. For example, there are only around 500 prefab modular construction projects in the US. I think we\u2019re still really far behind in the means and methods of construction.\nGunnarsson: My company is pretty much a \u201cremote first\u201d company, with employees scattered across four different countries in Europe. We\u2019ve been working this way for a long time. What\u2019s interesting is that if I look at my customers \u2013 the architecture firms and engineering firms \u2013 they are in the same boat now. They have had to ask themselves: \u201cHow do we do business and get work done remotely? How do we collaborate if we\u2019re not in the same place?\u201d Because we develop VR and collaborative design tools, we have an answer to those questions. I\u2019m a big believer that in the long term, the idea of all of us having to physically be present in the same place to do something will start to diminish.\nHumphrey: The key to planning and executing construction projects used to be to get as many people on site and to collaborate in a common location where you\u2019re close to the project, because the project is the context by which you can have a discussion and make your plans and execute and make sure the project is in control. When that can\u2019t happen anymore \u2013 like during this pandemic \u2013 collaboration has to happen digitally.\nWe\u2019re seeing a lot more virtual design in construction, whether that\u2019s taking task-based workflows and capturing data in the field, or tracking performance metrics via cloud data repositories and analytics. 3D, 4D, or 5D model context provides a way to navigate that data and collaborate with teams that are no longer colocated on the project site, while making sure that the teams that are on the project site are working in the right areas. Covid- 19 has helped drive a lot more interest in those types of applications.\nQ: It seems that the Covid-19 pandemic has forced the construction industry to digitise faster than it otherwise would. What areas do you think will gain traction first, and which areas will be slow to change?\nHarness: I think this is the time to shine for technologies like VR. I think it helps solve the core problem with the Zoom meeting, which is that we\u2019re all still in a room somewhere. If you can use something like VR to basically hijack the visual system of your brain and transport you somewhere else, that\u2019s very valuable.\nTo return to remote working, I think that\u2019s only going to continue to gain traction. My father, who is obviously from a different generation than I am, said \u201cWow! That\u2019s amazing how productive people can be just on their laptop at home.\u201d I think we\u2019re finally starting to see the rigid, old-school mentality of how you manage people and run a business evaporate. That, in my mind, has got to be the biggest win from Covid-19, especially for knowledge workers like those in the software industry.\nHumphrey: Although Covid-19 has accelerated some digitisation and the desire to get into virtualisation of work, particularly around model context, the reality is that our industry still doesn\u2019t deliver a model as a contractual document. So, I think you\u2019re going to see the existing task- and form-based workflows accelerate faster than the model-based workflows.\nMecheri: A lot of what my company delivers \u2013 a digital twin \u2013 focuses on how to operate and optimise the built environment in the operations phase. Doing that requires a convergence of data from all the distinct data silos that sit inside buildings and in the operation space today. You have building management systems, energy management systems, space management systems, maintenance management systems, islands of IoT implementations, and so on. There are a lot of different silos.\nThe industry is realising that truly converged, data-driven contextual solutions can help them deliver things faster and more efficiently \u2013 and as a result, I think there\u2019s going to be a clear acceleration towards convergence.\nGunnarsson: In our case, what my company primarily focuses on is the early stages of the design phase. What I find most interesting is this idea that we can evolve beyond getting together in the same place, into meeting more virtually.\nWith VR, more and more people are starting to experience going into a space \u2013 and it might be a relatively simple space, it doesn\u2019t have to be photorealistic \u2013 and feeling present with somebody else during that early design phase. They see that you can use tools like this to truly understand the space before it\u2019s even been built and make better design decisions.\nQ: Ten years from now, what aspects of the way things get done in the AEC industry will have people scratching their heads and saying, \u201cWhy on earth did we ever do things that way?\u201d in the same way that we look back now on landlines or televisions without remote controls?\nHumphrey: We\u2019re already seeing initial momentum around prefab offsite manufacturing. Robotics will take that even further. Even though a lot of robotics is still in its early stages, it will bring in advancements around sensors and real-time data, to the point that a decade from now, many aspects of construction will be automated, and people will say, \u201cI can\u2019t believe you used to manually build a lot of this stuff!\u201d\nOn a similar note, the industry has been doing machine control automation for earthworks for a long time. There won\u2019t be any people driving that equipment 10 or 15 years from now \u2013 it will most likely be fully automated, thanks to real-time feedback from sensors and advances in robotic processes.\nGunnarsson: One of the things that somebody might find strange in 10 years\u2019 time is the idea that we do so much of our work on 2D screens. Not just architectural design, of course \u2013 nearly everything. That\u2019s your focus, and that\u2019s the thing that you work on and get information from: this 2D screen on a PC or mobile device.\nIn the future, people will look at the idea of designing a 3D object on a flat screen as something completely out of the Stone Age. I think that paradigm shift is going to probably happen faster than people think once AR\/VR technology is mature enough, and it\u2019s already taking rapid steps in that direction.\nMecheri: People have realised that with a tool like Google Maps, you can go into any city in the world, including ones that you\u2019ve never been to before, and find your way around perfectly fine. Whereas in a building, you still need that one guy who knows the building and knows how to fix things to be around if something goes wrong. That\u2019s one of the biggest changes that we believe will happen coming out of this pandemic. People will realise it\u2019s crazy not to have converged building data that institutionalises knowledge and brings together actionable information.\nHarness: I think the AEC industry is going to recognise that it can\u2019t have 15,000 point solutions, most of which don\u2019t talk to each other, solving really small problems. There needs to be an API approach, like in other industries \u2013 otherwise, you\u2019re just creating barriers to adoption. So, ten years from now, I think people will look back and say, \u201cWhat was that all about?\u201d\nI\u2019m a millennial, so I\u2019m a digital native. But the generation after me, Gen Z, are internet natives. So, their concept of work is going to be very different than even my concept of what work is. And their ability to wield technology is going to be far beyond anything that we can imagine right now. They\u2019re just going to bring a completely new perspective to the industry, and I\u2019m really excited about it.","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/us-government-sues-to-halt-kennedy-map-sale\/","title":"US Government sues to halt Kennedy map sale.","date":1019692800000,"text":"It is an intriguing document: A map of Cuba with former President Kennedy\u2019s handwritten notes apparently scrawled during the hectic early days of the Cuban missile crisis.\nThe map \u2014 along with civil rights documents \u2014 is at the centre of a legal tussle between a Web-based memorabilia collector and the federal government, which claims the documents were improperly removed by Kennedy\u2019s personal secretary.\nU.S. District Court Judge Robert Ward on Monday temporarily blocked the sale of the map until a hearing scheduled for next week. The collector had been seeking $750,000.\nGary J. Zimet, operator of the memorabilia site, has advertised being the exclusive seller of a map and its original envelope identified as, \u201cCuban Missile Crisis Map With JFK\u2019s Handwritten Annotations Indicating Locations of Russian Missile Sites October 16, 1962.\u201d\nIn its arguments, the government said Evelyn Lincoln \u2014 the personal secretary who worked for the White House on Kennedy\u2019s papers until July 1964 \u2014 also compiled annotated and handwritten notes for the President Kennedy Library Corp. until at least 1972.\nThe map and civil rights documents were donated to the United States in February 1965 for deposit in the John F. Kennedy Presidential Library, the government said.\n\u201cIt appears that Evelyn Lincoln improperly removed the map from the custody and control of the United States\u201d and later gave, sold or bequeathed it to Robert L. White, a private collector of Kennedy memorabilia, the lawsuit states. It did not suggest that Lincoln, who died in 1995, had done anything criminally wrong.\n\u201cWhatever path the map may have traveled, it nevertheless falls squarely within the deed of gift and rightfully belongs to the United States,\u201d the government wrote.\nIn February, Moments In Time Inc. began advertising the map, prepared by the CIA, as having been given to White by Lincoln. The document features rows of Xs indicating presumed missile sites.\nZimet posted on his Web site a copy of a letter signed by White that reads: \u201cThis was saved, in its original envelope, by the personal secretary to the president and my close friend Mrs. Evelyn Lincoln. I acquired it from her in 1995.\u201d\nThe lawsuit also demands the return of nine documents, six with notes by Kennedy, all related to the 1962 enrollment of James Meredith at the University of Mississippi. He was the first black student admitted into the school, sparking rioting in which two people were killed.","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/bearingpoint-esri-and-sas-introduce-leading-edge-development-planning-solution-for-commercial-retailers\/","title":"BearingPoint, ESRI and SAS introduce Leading-Edge Development Planning Solution for commercial retailers","date":1176249600000,"text":"Virginia, USA, 10 April 2007: ESRI, BearingPoint, one of the world\u2019s leading management and technology consulting firms and SAS, a business intelligence firm, announced today the availability of a unique, integrated market-planning solution for commercial retailers called Market Planning and Portfolio Optimization (MPPO).\nCommercial retailers are constantly faced with critical decisions on where to locate new stores, close underperforming stores or remodel stores to increase revenues. The MPPO solution is a leading-edge tool that combines best-in-class consulting, GIS, business intelligence and predictive analysis services and software solutions to provide retailers with detailed insight into the planning process. This insight helps them forecast sales of new locations, estimate impact on existing stores, model optimal return on invested capital, and improve visibility and control over every aspect of the corporation\u2019s combined retail real estate activities.\nThe integrated market-planning solution will reduce cycle times for new store development and help retailers with strategic planning by providing increased insight into market drivers and conditions. This insight will not only help retailers ensure that new stores achieve higher sales levels and existing stores experience lower sales transfer, but help them increase overall market share. In addition, the solution provides significant increases in efficiency over other planning tools by lowering costs associated with existing portfolios and allowing retailers to manage more sites with fewer resources.\n\u201cBearingPoint has a dedicated consulting practice focused on helping commercial retail clients develop great sites fast, and this solution is best-in-class,\u201d said Jack Thompson, a managing director in the BearingPoint consumer markets practice.\n\u201cThis solution establishes a new benchmark in market analytics by combining proven best practices in real-estate decision-making with leading technology for predictive statistical analysis and advanced spatial analysis,\u201d said Mike Johnson, director of commercial sales at ESRI. \u201cResults from this solution will provide new insights that will undoubtedly impact market-planning and site-location decisions.\u201d\nAccording to Alexi Sarnevitz, SAS senior director of global retail strategy, \u201cWith the demise of the mass market, today\u2019s most successful retailers are pursuing a consumer-centric approach to all elements of their business. Assortments, pricing, inventory, service offerings and even store designs and layout are being tailored to best serve dominant local-market consumer segments.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-tim-geurtjens-mx3d-2\/","title":"Video: NXT BLD London conference - Tim Geurtjens, MX3D","date":1502150400000,"text":"To print a steel bridge in Amsterdam \u2013 NXT BLD London, June 2017\n>\nIn this inspirational talk, Tim shares his thoughts on the latest developments in Digital Fabrication and where he thinks things will go in the future. He explains how he was drawn into advanced robotics to develop the MX3D printers that can print metal objects of theoretically unlimited size, and presents MX3D\u2019s plan to \u2018print\u2019 the first steel bridge in the world over one of the oldest canals in Amsterdam. The ambitious project is set for completion in 2018. Essential viewing\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Paul Nichols, Skanska\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Arthur Mamou-Mani, Mamou-Mani\nConstructing (and deconstructing) buildings with cable robots\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/fabsec-fbeam-2006\/","title":"Fabsec FBEAM 2006","date":1165449600000,"text":"The use of cellular steel beams is on the increase in the UK construction sector. But in order to make the most of this weight saving technology, you need a design tool that engineers find easy to use says Greg Corke.\nDemand for structural steel has risen steadily in the UK over the past few years and it is now estimated that over 70% of all multi-storey buildings use some form of steel framed construction. And while demand is pushing up material costs, clients are also expecting more for their money, which means structural engineers are continually looking for new construction technologies to keep that competitive edge.\nOne such technology that is grabbing the attention of engineers is cellular steel beams. One in three multi storey buildings are now constructed using this system, which boasts longer spans while using less steel (particularly when used as a composite beam) and can also lead to a reduction in overall building height as services can be fed through the cells in the web of the beam.\nOne company that has been pioneering the adoption of this type of construction is Fabsec, which since its inception in the year 2001 now has a production capacity in excess of 50,000 tonnes per year. However, unlike most cellular beam systems which have their webs peppered with holes, the cells in Fabsec\u00dds beams can be cut exactly where you want them prior to going on site. These can be circular, oval or rectangular and can be positioned and sized to coincide with specific building services requirements, or the web left intact so that secondary beams can be easily welded on.\nDesign files created in FBEAM can be utilised directly by Severfield-Rowen and William Hare subsidiary, Cellbeam for the automatic manufacture of Fabsec steel beams.\nNaturally with such flexibility in cell placement, the design and optimisation of such beams is more involved than with standard sections and there needs to be an efficient design system in place to deal with this. Fabsec\u00dds solution to this is a free software tool called FBEAM, which enables engineers to design both composite and non-composite cellular beams with ease, both in their ambient state and in fire conditions.\nBeam design and optimisation\nFBEAM uses a Wizard style process to guide the engineer through the design process. For simple beam design, the software offers up a number of standard templates. In the case of a custom cell composite floor beam, engineers must first enter data regarding the type of slab, the spans and arrangement of the floor. This is followed by a whole host of loading details, including imposed loads, point loads, and end moments.\nMoving onto the actual beam design, FBEAM initially suggests a \u00d9rule of thumb\u00dd starting point for the section size of the beam followed by flange sizes, but these can be edited by the engineer. Providing maximum flexibility in design, top and bottom flanges can also be of different sizes, as can depth along the section to create custom tapered beams.\nNext up, custom or standard cell arrangements can be generated using the built in web opening tools. These can be generated automatically along the section by spacing or \u00d9number of\u00dd or produced individually according to size, location and type. Naturally, FBEAM provides full editing functionality, to accommodate a large air conditioning duct for example, but the software will automatically flag up anything that does not comply with basic design rules, such as when hole sizes are too big, or if an opening is located below a point load applied to the top of the beam.\nOnce the beam is designed engineers are given the option to either analyse the beam as is or to optimise it to find the perfect balance between strength and weight\/cost. N.B. Cost is based on cost of steel plus cost of manufacturing processes such as amount of scrap and weld runs.\nThe system checks for typical design criteria, such as shear force, bending moments, web buckling, and deflection, and will warn the engineer if it fails any of these checks. For the optimisation stage, beam parameters such as depth, flange width or web thickness can be left totally free, fixed at a specific value or set within a range. The software then uses an iterative design algorithm to calculate which elements of the design are over and under utilised and adjusts them accordingly. Considering the amount of calculations the system is doing it\u00dds a very efficient process taking a matter of seconds, but beams can also be batch processed, so you can go off and make yourself a cup of tea if you want to optimise all the primary and secondary beams in a steel framed building in one go.\nFire engineering\nIn addition to designing and optimising at ambient temperatures, FBEAM can also be used to design fire engineered beams which provide up to two hours of fire resistance. Unfortunately this functionality is only available to selected customers and subject to an NDA (Non Disclosure Agreement) to protect Fabsec\u00dds intellectual property and commercial position. Leigh\u00dds Paints, a 25% stakeholder in Fabsec provides the design data for its Firetex FB120 intumescent paint, which can be applied off site up to 2.2mm thick and then, should it be subjected to excessive heat, swells to 50 times its applied thickness. Using FBEAM\u00dds Fire Engineering module users have potential for even more costs savings in the optimisation process. This is because the software looks at the effect of using different beam cross sections and coatings in conjunction with one another, rather than on an individual basis. For example, increasing the mass of steel by, say 10% (and hence increasing the fire hesitance of the steel itself) and using less paint could turn out to be cheaper than using less steel and more paint.\nProject capability\nNew for version 2006 is a project facility which enables users to create projects to encompass all the beam designs and organise them by groups such as floor levels or roofs. A Group Edit facility now makes it easy for last minute design changes to be incorporated, such as loading modifications. This enables beam parameters (e.g. size, loading, cell arrangement) to be changed globally, where a change in one beam can be propagated down to others. In previous versions of the software this had to be done on a beam by beam basis.\nStructural analysis links\nIntegration between different structural software is becoming more and more important across the industry and for this latest release Fabsec has followed this trend with a two way link between FBEAM and the RAM Structural System, a structural analysis solution from RAM International, the company recently acquired by Bentley Systems. Using the FBEAM plug-in inside RAM, engineers can select any standard Universal Beams which they would like to convert to Fabsec sections or alternatively the system can flag up any candidates which it thinks are likely to have any substantial mass or cost savings \u2013 in most cases these are long span sections.\nRAM will then export these sections, together with their loads, so they can be designed inside FBEAM. Beams are to be found inside a project navigator which enables engineers to easily see each beam type, where it is located inside the building and its analysis state. These can then be designed, analysed and optimised in the standard way, either individually or on a group by group basis, and once complete the data can then be imported back into RAM Structural System to coordinate the structural model.\nConclusion\nBy transforming standard Universal Beams into Fabsec sections typical weight savings are claimed to be in the order of 20-25%, and with this in mind it\u00dds easy to understand why Fabsec is now producing over 50,000 tonnes of beams under license every year. However, in addition to material savings, there are many additional benefits to be had from its cellular beams. Spans of twelve to 24 metres are eminently achievable, meaning bigger uninterrupted office spaces; there are fewer structural elements to erect on site; and because the services can be fed through the beam instead of under it, developers can lower the overall building height \u00b1 which can lead to a reduction in cladding costs, additional floors within the same building envelope.\nHowever, many of these benefits would be lost in the mix without an intuitive and cost effective way of designing these long span cellular beams and with FBEAM 2006, Fabsec looks to have just the tool to do this. FBEAM is an incredibly easy tool to use. It\u00dds a simple product designed for a specific task and with its Wizard-based approach and tutorials embedded in the free CD most engineers should be able to hit the ground running.\nThe link to the RAM Structural System is an interesting development for a product which until this latest release existed predominantly in its own sphere. However, this development work is not finished and workflow between the two products will be refined in future releases, plus Fabsec is also talking to other structural design and analysis software vendors to broaden its reach.\nIn FBEAM you won\u00ddt find an eye catching 3D modelling environment common to many mainstream structural software products, but for most users ease of use and simplicity is more important than bells and whistles. And when considered as part of an overall structural solution, Fabsec is delivering a very attractive proposition which could help structural engineers get an edge in an increasingly competitive marketplace.\nProduct: FBEAM 2006 Supplier: Fabsec Price: \u00faFREE Web: www.fabsec.com","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-moritz-luck-enscape\/","title":"Video: NXT BLD 2019 \u2013 Moritz Luck, Enscape","date":1565049600000,"text":"From real-time to realism \u2013 NXT BLD London, June 2019\nReal-time rendering and VR in a single click has transformed the way architects understand and present designs. Now real time ray tracing is taking things to new levels of realism.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/iot-tech-to-tackle-risk-of-catastrophic-bridge-failure\/","title":"IoT tech to tackle risk of catastrophic bridge failure","date":1552262400000,"text":"SGS and AIMSight solution uses smart crack sensors and data analytics to monitor bridge assets in real timeT\nInspection specialist SGS has teamed up with AIMSight to launch a new structural health monitoring solution for bridges that uses \u2018smart crack\u2019 monitoring sensors and data analytics to provide continuous, real time monitoring of assets.\nThe Internet of Things (IoT)-based technology serves as an early warning system, so asset owners can precisely anticipate and schedule on-site inspections and maintenance work, and respond to defects before they have a serious impact on their operation or finances.\n\u201cBridges built for a different time are creaking under the strain of traffic densification and heavier vehicles,\u201d said Thomas Meyer, Global Innovation Manager, Industrial at SGS. \u201cOur new technology continuously monitors critical infrastructure which will help keep our road network working safely.\u201d\nOver and above the crack monitoring sensors, which are attached to the asset and collect specific data on defects, additional parameters are also captured, such as local temperature and acceleration, which enables root-cause analysis of the defects\u2019 behaviour.\nAccording to SGS, in the UK there are 72,000 bridges on the local road network alone, of which more than 3,000 are categorised as \u2018substandard\u2019 (meaning unable to carry the heaviest vehicles seen on our roads) and in urgent need of repair at a cost of \u00a37billion.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-new-drone-system-delivers-sub-5mm-accurate-rail-surveys\/","title":"NEWS: New drone system delivers sub-5mm accurate rail surveys","date":1500940800000,"text":"Plowman Craven Vogel R3D system offers \u2018faster & more accurate\u2019 way to capture rail data\nPlowman Craven has launched a new system that uses unmanned aerial vehicles (drones) to survey rail infrastructure to sub-5mm accuracy. The Vogel R3D system has been successfully trialled by Network Rail and the survey accuracy results approved.\nFrom a working height of 25m, the Vogel R3D system can also capture busy and inaccessible areas of the rail network during traffic hours, removing the need for possessions and line blocks and reducing the exposure of workers to risk.\n\u201cThe application of the Vogel R3D system is a real game-changer for Network Rail and helps us to satisfy many of our survey requirements in a safe manner without the cost implications or potential programme delays associated with multiple possessions,\u201d said Chris Preston, senior engineer, Network Rail.\nMalcolm Donald, Director, Plowman Craven added, \u201cThe Vogel R3D system really does represent a step change in the surveying of the rail environment. Not only does it massively reduce risk, cost and timeframes compared to traditional surveying methods, but the data accuracy is also superior to traditional laser-scanning systems, making Vogel R3D a very attractive proposition with a wide range of applications. It can also be deployed on any infrastructure project that requires remotely-captured, engineering-grade data such as bridges or oil rigs.\u201d\nPlowman Craven\u2019s system uses a drone mounted 100-megapixel camera to capture overlapping aerial images of an entire site. Bespoke workflows and software algorithms are then used to convert the imagery into 3D point cloud data that can be used to create a range of client deliverables ranging from track alignment to interactive site visualisations. According to Plowman Craven, the accuracy is superior to any other method of data capture, including static laser scanning and train or trolley-based kinematic scanning systems.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/laser-technology-unveils-trupulse-360\/","title":"Laser Technology unveils TruPulse 360","date":1181779200000,"text":"Denver, USA, 12 June 2007: The Laser Technology, Inc. (LTI) has released TruPulse 360 which is based on its TruPulse 200 laser measuring device and combines an integrated compass, inclinometer and distance laser. It delivers mapping-grade accuracy and the ability to shoot from any angle.\nThe TruPulse 360 utilizes the electronic compass technology to overcome limitations on other devices. The internal circuitry provides 3-axis monitoring of the earth\u2019s magnetic field and uses LTI\u2019s TruVector compass technology and calibration algorithms to produce the azimuth accuracy, regardless of the instrument\u2019s orientation in physical space. The TruPulse 360 can be tilted, rolled or even used upside down, and it will still measure the correct azimuth in the direction that the user is viewing, allowing the user to \u201cshoot from any angle.\u201d\nThe feature is particularly useful to get distance, height or location measurements in difficult or impossible-to-reach areas, such as in mountainous terrain, wetlands or across dangerous snowfields.\nTruVector compass technology also allows for a simple field calibration procedure. It evaluates the local magnetic environment and provides user feedback on the quality of calibration. In addition, the instrument keeps a constant watch on its internal status via built-in system tests, which continually monitor the integrity of the compass calibration and alert a user if a re-calibration is required.\nLaser Technology Inc. is a Colorado-based company involved in the design and manufacture of laser-based speed and distance measurement instruments for traffic safety and professional measurement organizations and businesses.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-mike-leach-lenovo\/","title":"Video: NXT BLD 2019 \u2013 Mike Leach, Lenovo","date":1565049600000,"text":"Navigating challenges surrounding AR and VR hardware \u2013 NXT BLD London, June 2019\nAs the world of augmented and virtual reality technologies continue to evolve, the commercial use cases of AR & VR are changing daily. Fast becoming a pivotal part of most major AEC projects, this trend is visible from simple deployments with onsite health and safety training through to complex BIM workflows and photorealistic sales and marketing must-haves. With use cases exploding and the variety of HMDs on the market today, ranging in features and price tag, it\u2019s hard to know what solution works best for you, your budget and your workflow. Join Lenovo as we discuss the landscape of AR & VR devices and deployments. We\u2019ll also cover the impact computer workstations have on creating and experiencing this ever-changing of world of AR & VR, and beyond. Learn how many of the new workstation & graphics technology trends supporting the AEC community can help you and arm yourself with all that you need to know for 2019 and beyond.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/exactal-costx\/","title":"Exactal CostX","date":1219449600000,"text":"Moving from pencil and drawing board to CAD improved the productivity of designers overnight. However, with so much design information captured in digital form, even greater benefits are possible. Martyn Day reports on a costings application that\u00dds rapidly growing in popularity.\nThe main benefit of moving from paper to Computer Aided Design (CAD) is the ease of editing, while removing the need for rework. Now the majority of companies take their CAD systems for granted, it\u00dds amazing how many firms still look at CAD as being a better drawing board and not a totally different environment for creating and optimising designs. With the move to 3D, early adopters saw the opportunity to gain the benefits of Building Information Modelling (BIM) \u00be but it is not necessarily the death of 2D drawings, as the output is still the bread and butter of the construction world.\nSo, how about extending the use of 2D drawings or 3D models beyond mere symbolic representation for builders? With so much really useful, intelligent information included in the CAD file, there are a number of applications that give this data even better return on investment and improve productivity and budgeting of projects. Quantities and Costings seem like an obvious area for digital improvement but most of the tools are stand-alone, take data one way and essentially are \u00d9clunky\u00dd to use.\nExactal\nExactal is an Australian-based software developer now operating in the UK that produces CostX, an intelligent tool that assists cost estimates from 2D or 3D CAD data. Launched in 2004, the software is now sold all over the world, including Ireland, UAE, Malaysia, Singapore, Cyprus, New Zealand and across Africa. CostX supports bi-directional links to existing standard formats and industry formats (DWG, PDF, DXF, Revit and in October of this year DGN) and captures project dimensions and quantities using its own \u00d9intelligent cost geometry technology\u00dd to generate project estimates. Using a visual-like interface, measurements can be both automatically and manually extracted from CAD files. CAD experience isn\u00ddt necessary to operate the software and there doesn\u00ddt need any CAD software to be present on the PC to access the files. All measurements can be extracted to Excel.\nCostX supports live links, so changes to any project drawings will automatically be reflected in the costings and quantities of the whole project. The software also offers analysis tools to compare and contrast these changes. The software comes with a range of frequently updated Rate libraries, or users can update or add-in their own custom costings. Then with this data, CostX can generate a raft of reports to aid in the decision process. It also has beneficial applications for Quantity Surveyors, Builders, Developers, Project Managers, Sub-contractors, Valuers, Estimators and Architects.\nObviously for Quantity Surveyors, CostX looks like a powerful solution as it provides a single environment for capturing dimensions, developing feasibility studies and generating full Bills of Quantities. The automatic report writer takes the pain out of repetitive tasks and its output is to a highly-professional presentation quality.\nCostX Viewer\nThere is an easy way to share costing estimates with the CostX Viewer. Files can be distributed in a safe read-only, portable format, which can have live links back to the CAD data, offering real-time costings to all project participants or owners. Users of the Viewer can dig very deep into the project dimensions, quantities and spreadsheets, specifically to \u00d9ten levels\u00dd deep. The Viewing software also includes drawing interrogation tools. This is a great tool that helps project members to become acutely aware of the cost implications of any changes made.\nConclusion\nCostX offers new possibilities for re-using the CAD design data to accurately estimate quantities and costs, within a live environment. The automation of the process will dramatically improve productivity. Some customers have anecdotally reported that work that has typically taken three days to complete can now be done in half a day and the more changes to a project, the bigger the rewards for using the automatic bi-directional cost estimation.\nOne particularly interesting use of this kind of software would be at the conceptual level, where construction costs, maintenance costs and operational costs could be estimated much earlier in the design process, leading to better optimised buildings. With the push for sustainability and materials becoming increasingly important, CostX could have an important role to play in exploring the performance and cost of any design or proposed design changes.\nWe will have a full review of CostX in the next edition of AEC. Price \u00fa2,000 per seat.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/worcesters-civic-experiment\/","title":"Worcester","date":1285372800000,"text":"Architectural landmark strives to be carbon neutral while housing varied constituencies.\nThe Worcester Library and History Centre, in Worcester, England, is a bold civic experiment that brings together, in one building, a fully integrated public and university library, the Worcestershire Record Office, the Worcestershire Historic Environment and Archaeology Service, and the Worcestershire Hub Customer Service Centre. The notable architectural landmark also combines indoor and outdoor working areas with quiet spaces, and aspires to be carbon neutral in accord with strict sustainability guidelines set by Worcestershire and the University of Worcester.\nThe building is the first in Europe to house these varied functions and constituencies under one roof. As it turned out, getting the roof right proved to be the key element that made the rest of the building work, according to designers at Feilden Clegg Bradley Studios.\n\u201cThe design aspires to live up to the wonderful opportunities presented by the site and the community\u2019s needs,\u201d explained Feilden Clegg Bradley partner David Appel. \u201cThe distinctive roofscape echoes the undulations of the Malvern Hills, and the fractured form ensures it is in keeping with the relatively fine grain of historic Worcester. The funnel-like roof shapes, which evoke the Royal Worcester pottery kilns, are as fundamental to the building\u2019s interior as they are to its external appearance because they provide natural ventilation and natural lighting.\u201d\nFeilden Clegg Bradley aspires to be a school of architecture that implements research-led design. On this project, it used parametric modelling enabled by GenerativeComponents. The model allowed its experienced architects to apply environmental analysis techniques to their design and continue to refine sustainability aspects very late in the design process.\n\u201cWe produced an intricate yet very flexible representation of the design in GenerativeComponents,\u201d said Richard Priest, Feilden Clegg Bradley architectural software engineer. \u201cThis gave our team the power to alter many variables and produce options that responded to our brief while also providing real-time feedback that ensured we stuck to stringent sustainable design requirements.\u201d\nThe complex, non-organic forms of the roofscape raised issues that made conventional analysis difficult. \u201cWe found that the baffle surfaces under many of the design solutions tended to twist out of plane, which was not obvious on paper or even from some manual 3D modelling,\u201d Mr Priest explained. \u201cWithout the parametric model, we wouldn\u2019t have discovered this until construction.\u201d\nUsing the model also meant that functional and sustainability design factors could keep pace with aesthetic changes. \u201cThe component nature of the model allowed us to make aesthetic changes without adverse effects. We could simply swap in the newly designed component without remodelling the whole structure,\u201d he said. \u201cAt one stage, the layout of the entire roof changed, which would have been a major issue if we had been using manual modelling methods. In this case, we simply rearranged the structural centre points of the roof, and the model readjusted itself to the new layout.\u201d\nThe Feilden Clegg Bradley team spent a lot of time implementing the environmental and structural constraints of the GenerativeComponents model, but this meant that structural forms, such as the baffle buildups, were generated automatically in response to changes in form.\n\u201cWe saved time producing accurate construction information,\u201d Mr Priest said. \u201cWorking off the structural centres of the funnels and using variables, we could change the internal and external depths of the funnel walls to fit changes in the materials specifications.\n\u201cAs these changes were made to the funnel walls, the follow-on effects to the geometry of the baffles was automatically regenerated with no further user input, saving huge amounts of time. We could be comfortable that the regenerated form still satisfied all sustainability requirements.\u201d\nA balancing act\nEach rooftop funnel \u2014 there are seven in the completed design \u2014 serves as a skylight and ventilation shaft, with large glazed areas that let in natural light. Balancing the amount and placement of glass with the ventilation and natural light requirements proved to be one of the most useful aspects of the parametric modelling approach. Feilden Clegg Bradley designers could monitor and adjust funnel shapes and glazing layouts and immediately analyse the results. \u201cThe ability to do this with real-time feedback meant we didn\u2019t have to use a model-analyse-remodel system. Rather, it all happened as one process,\u201d said Priest.\nFeilden Clegg Bradley began work on the model during the bid stage of the project in order to produce visualisations. Front-loading the modelling effort meant that once the bid was won refined design could begin immediately, and the company used iterated versions of the original model throughout the project lifecycle. Mr Priest said: \u201cThis saved us many weeks while moving from winning the bid to final design.\u201d\nIn addition to the university, public, and civic functions, the centre includes retail space and public outdoor space and serves as a hub in downtown Worcester. Commendably, Worcestershire and the university used this major public investment as a way to support the local economy by sourcing local materials and as a proving ground for progressive sustainable architectural ideas. This fit very well with Feilden Clegg Bradley\u2019s design philosophy, as the firm was named the UK Sustainable Designer of the Year for 2009.\n\u201cCommunities thrive on a sense of belonging and shared cultural heritage, particularly in places like Worcester that have had a rich and varied history,\u201d Mr Appel said. \u201cSocial and environmental responsibility is at the heart of our practice, and we set out to provide an iconic landmark building that contributes to the quality and diversity of the city. Sustainability has been central to our design approach and has been a primary influence on all design decisions.\u201d\nThe architecture was used to address issues as varied as biodiversity, water conservation, energy efficiency, microclimate enhancement, and reduction of pollutants.\n\u201cA key feature of the scheme is two water meadow basins fed by rainwater collected from the building roof,\u201d said Mr Priest. We were able to streamline the environmental and structural requirements and create an efficient structure while also eliminating 250 tons of steel from the roof structure.\u201d\nOther sustainable features include biomass boilers for heating and the use of water from the River Severn for cooling. To verify the natural ventilation scheme\u2019s effectiveness, a physical model of the centre was tested in wind tunnels at Cardiff University. Since the building is near the river, flood mitigation measures were treated as an opportunity to create natural habitat in the Worcester urban area, encouraging plant, animal, and bird life characteristic of the Worcestershire region.\nBy applying cutting-edge design techniques to implement rigorous sustainability standards, Feilden Clegg Bradley was able to build a striking architectural landmark that will be efficient to operate and will serve its many constituencies for decades to come.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/cad\/generative-design-to-help-address-risks-of-covid-19\/","title":"Generative design to help address risks of COVID-19","date":1589760000000,"text":"Digital Blue Foam\u2019s Covid Space Planner assesses the threat of exposure to virus in interior spaces\nDigital Blue Foam is developing a tool to help address the risks associated with the use and occupancy of interior spaces due to COVID-19. Covid Space Planner analyses factors such as total occupancy and proximity, airflow and ventilation, and the duration of activity in the space. Based on these factors, the tool applies Digital Blue Foam\u2019s generative design capabilities to recommend spatial planning strategies, such as the location of entry points, work areas, furniture placement, and retrofits, to mitigate the risk.\n\u201cWith the gradual easing of lockdown restrictions in many countries, people will slowly begin to move back into communal spaces. Everyone will need to look at their environment and make critical judgments,\u201d says Camiel Weijenberg, Digital Blue Foam.\n\u201cWe are developing a tool which extends the capabilities of Digital Blue Foam to help business owners critically assess the risks based on factors of building volume, people, and airflow so they can design the safest and best strategy for their needs,\u201d adds Sayjel Patel, Digital Blue Foam.\nDBF is currently working on a free version which it intends to make available to small business owners during the pandemic. The Singapore-based company is in contact with experts in medicine, CFD analysis, and wayfinding to develop new capabilities around airflow simulation and risk assessment. The team is currently in discussions to develop a pilot project and looking to release a beta version for general use at a later date.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/digital-concept-design\/","title":"Digital Concept Design","date":1374019200000,"text":"2D CAD and BIM have both been sold on their benefits to the detailed documentation stage of a project. Ideation and conceptual design, at best, has fallen to simple mass modelling. Will the latest batch of tools change this?\nPen and paper, pencil sketches, cardboard, foam, scrunched up paper, lego, clay; whichever way you do it, it is still a highly analogue, iterative, exploration process. While the end design will need to be detailed in some form of Computer Aided Design (CAD) system, the limitations and explicit nature of CAD makes it far from useful at the formative stages of the design process.\nPhysical, real-world form finding and sketching will never disappear but many software firms are attempting to assist the ideation needs of architects through a variety of approaches.\nTo some degree, any significant move towards digital concept design is also going to be a generational issue, with younger minds more willing to experiment with the latest technology.\nI do not expect an architect like Frank Gehry will ever migrate from his pen and paper models but new recruits and future signature architects may well adopt a computer-aided approach.\nMightier than the pen?\nCAD software developers are looking to digitise the pen. Just as CAD replaced the drawing board, could the latest touch interfaces and styli replicate the drawing process?\nCompanies like Wacom led the charge with its pen tablets, pen displays and styli for mobile tablets. Products like the Cintiq moves the computer\u2019s monitor to a reclined drafting position, adds touch sensitivity and provides a powerful digital sketching tool when combined with industrial design sketching software and products like Alias and SketchBook Pro from Autodesk. These displays are very common in automotive and product design.\nHowever, the pen displays are not cheap and find themselves now competing with the new smaller tablets, such as the iPad.\nThere are many, many 2D sketching apps available. Autodesk has SketchBook Pro and SketchBook Ink, Adobe has Photoshop Touch. There is a great app called Paper and Wacom\u2019s Bamboo Stylus (I think it is the best one out there) comes with its own drawing application. Judging from some of the work done by customers, the quality of output that can be created rivals anything I have seen on paper, it is just a question of adapting to the tool set and having the time to experiment.\nThe advantage of having a digital sketch means it is easier to display and share wherever you are and possibly use as a start point for massing. The more advanced (expensive) PC-based sketching systems enable freeform sketching and will take these to create Class A surfaced models, ready for generating car bodies and beautifully sculpted forms.\nSimple 3D modelling\nWhile there are a number of established conceptual design tools, the new \u2018point of entry\u2019 for creating masses is Autodesk\u2019s FormIt, which is both an iPad\/tablet application, as well as a full \u2018in-browser\u2019, free modelling tool. The software both delights and frustrates but I have seen some amazingly detailed models created in it.\nAt this year\u2019s Council on Tall Buildings and Habitat, held in London, Martyn Day caught up with Robin Partington of Robin Partington Architects (RPA) and formerly of Foster&Partners, where he was most well know for his work on London\u2019s Swiss Re building, aka the \u2018Gherkin\u2019. I asked Robin about his practice\u2019s approach to the conceptual stage of design\n\u201cWe choose our tools on a project by project basis. Yes we use Rhino and others but the backbone of our company is Bentley Systems\u2019 tools. We don\u2019t tend to use SketchUp that much, as we tend to want to give meaning to the designs of our buildings at a pretty early stage.\n\u201cWhat\u2019s changed in the last two or three years is the way we are using CAD. Instead of the 3D team playing catch up with the design team using pencils or scalpels, we are now using 3D right up front, defining the parameters for the building, defining the logic.\n\u201cIt\u2019s not exclusive, we still sketch, model with cut foam but we now run in parallel as each one has a strength and appropriate at a particular time in the concept phase, each feeding off each other. However, none of the manual techniques capture the logic that\u2019s going to drive the process and feed into our backbone and refining process.\n\u201cIn the past we used to push the design really hard with \u2018scribbles\u2019, foamboard and sketches an the someone would \u2018CAD it up\u2019. That isn\u2019t helpful, that\u2019s a recording process.\n\u201cThe current range of projects we use all tools parallel but the next generation will lead with digital tools. We are working on a new 42 storey 200 apartment residential building with a complex shape, there is no repetition floor on floor. Every apartment on every floor is different and the amount of design per square foot is much higher than an office building. How do you manage such a complex design?\n\u201cBefore we go into detailed design before we let the architects loose with pens and foamboard, we are sorting out the rules, defining the schema in the computer that will allow us to control the process.\u201d\nSatellite maps can be pulled in for modelling \u2018in-situ\u2019 and sun studies can be carried out. These models can be taken through to Autodesk\u2019s analysis tool Vasari or into full Revit. As the software is \u2018cloud-enabled\u2019, whatever you design can be accessed from anywhere. While it remains a work in progress, Autodesk\u2019s development team has done some great work here.\nTrimble SketchUp Make and Pro, as they are now called (see pages 14 and 15), have been doing the design community a great service for over a decade by providing a free and easy to use 3D modelling tool. Many architects have cut their teeth on this software to generate both simple and complex designs, to examine volumes and create renders.\nAs the product has progressed, it is now possible to download powerful applications to assist in quick creation of walls, doors, windows, floors and roves, as well as run lightweight environmental analysis. The Pro version adds a layout tool to produce detail drawings, sections and elevations. Although SketchUp models can be exported and used in BIM solutions to define the production geometry.\nFor all its good features, SketchUp is geometrically limited and to get anything non-rectilinear requires some thought.\nA low cost tool that has made some impact for advanced users is bonsai3D from AutoDesSys (the folks behind FormZ). Bonzai3D is specifically aimed at sketching and conceptual modelling with a powerful NURBS geometry engine. It has lots of sexy capabilities like high-end rendering, the ability to unfold geometry, animation, 3D printer support, contour and terrain tools, documentation and fabrication links.\nProbably the king of the low-cost modellers is McNeel Rhino. Its industrial strength engine plus extreme popularity has made the product a bit of an industry enigma. With a powerful API, there is a huge third-party developer community and the product is used in everything from conceptual design to fully defining highly complex buildings that you may have seen at last year\u2019s London Olympics. Rhino is pretty much the de facto standard with young designers and heavily used in signature architects practices of the likes of Zaha Hadid.\nComputation and modelling\nTalking of Rhino, there is a free add-on called Grasshopper, which offers a visual scripting interface to drive the creation and definition of geometry. This was first done by Robert Aish at Bentley with its Generative Components application, which ran on top of MicroStation.\nHere the designer can create their own tools through programming. Would you like a roof that has beams like a sunflower Fibonacci series? Or a structure that automatically adapts to edits around the edges? Using these scripting tools its possible to use the power of the computer to find the best solution through computer-aided generation.\nAgain many of the leading architects have special modelling teams that assist in developing scripts and programs to allow the designers to experiment with complex geometry. However the arrival of plug and play visual interfaces has brought this powerful capability down to us non-programming mere-mortals.\nThis trend towards computation design has not been lost on Autodesk. It hired Dr Aish away from Bentley and now has DesignScript, a new parametric language for AutoCAD. It has also just released Dynamo, which is a visual scripting (plug and play) tool for Vasari and Revit, looking very similar to grasshopper for Rhino, enabling points to be placed, manipulated and geometry created based on variables and equations. This could be used to drive underlying geometry, such as a roof shape in Vasari and, with iterative changes and analysis, to find the optimal shape for solar radiation or wind load.\nIt is at this point that we can look back at the suite of conceptual design tools and see how merely replicating the sketching function really is such a small part of what computers could possibly offer the form-finding process.\nThe earlier in the process that inefficiencies and problems can be identified, the better the quality and performance of the deliverable.\nScripted form finding, optimisation and visual feedback of performance are incredibly powerful tools to have. A number of leading-edge architects are already benefiting from generative tools throughout the entire process.\nImagine being told by planner that a complex curved tower needs to be 50 metres lower and the amount of work that would involve in 2D or BIM in working out the pannelisation alone. Using parametric scripting tools some architectural practices can literally grab the top of a complex curved building and move it down, the software automating many of the necessary changes in the rest of the building.\nAs projects grow in complexity, computer-based conceptual tools such as Generative Components are already helping architects such as Robin Partington Architects (RPA) tackle daunting design tasks and connecting the creation tools to the documentation \u2018backbone\u2019. While many practices will not face the challenges of this extreme form of architecture, the process adopted by RPA is one that is being replicated in many design firms.\nConclusion\nCAD and BIM systems are not very good at dealing with imprecise abstractions. Sketching is always going to be a worthy skill to have and there will be many mediums through which to express more artistic abilities \u2014 and paper does not run out of battery power!\nThe challenge is to clearly define a complete internal process of taking concept through to delivery of detailed drawings identifying the best technology fit, offering maximum benefit to each stage. For many it starts with a pencil and always will, but beyond sketching, before detailed design there are now a number of powerful tools which provide clear early feedback as to how well a design meets the brief or identify a number of viable solutions to be considered. Inefficiencies discovered early on can be designed out, saving money on HVAC and other operational costs.\nLeading architects are already considering how a complete digital process could benefit their practices, from the basics, such as being able to capture and store concept ideas, to driving explicit model generation and regeneration through an iterative process.\nThere are a range of low-cost tools that can be adopted by any architectural practice to go digital earlier and use the computing power that is already on their desktops to increase the confidence that early concepts will meet, if not surpass client briefs.\nConceptual design: alternatives\nAutodesk Dynamo and Vasari\nVasari (pictured) is a conceptual massing design tool for creating building concepts. Offering integrated analysis for energy and carbon to help optimise design intent, models can be seamlessly passed on to Autodesk Revit. Vasari Beta 3 is now live, with improvements that went in to Revit 2014, use on Windows 8, and a license that is good till May 2014, according to Autodesk.\nDynamo is Autodesk\u2019s free visual scripting tool for Vasari and Revit which is similar to McNeel Grasshopper.\nPoints and geometry can be manipulated through a plug and play interface to assist in generative complex form finding and targeting the most efficient design to meet the customer\u2019s brief.\nMcNeel Rhinoceros and Grasshopper\nRhino is a de facto standard in advanced architectural practices. The NURBS based modeller is popular with graduates and supports a sizable ecosystem of plug-in products pushing Rhino\u2019s capabilities to match most higher-priced CAD systems.\nGrasshopper (pictured) is a free visual front-end to Rhino for generative scripts to create and manipulate complex geometry. Programs are created by dragging components onto a canvas and connecting them with \u2018wires\u2019 to create generative algorithms, which make and manipulate Rhino geometry. With Rhino\u2019s popularity, there are a lot of Grasshopper users and many modern buildings have benefitted from its use.\nAutodesk SketchBook Pro and Ink\nTablet or PC-based, Autodesk SketchBook Pro (pictured) is a sketch, paint and drawing application. Made for professional designers, artists and illustrators, SketchBook Pro is specially optimised to work with pen tablets such as Wacom Bamboo, Intuos, and Cintq products, to deliver an authentic drawing experience.\nIt has more than 100 illustration tools, custom colours, and do-it-yourself brushes.\nSketchBook Ink is a resolution-independent pen and ink drawing application. While not as powerful as SketchBook Pro, the software outputs high resolution images directly from a tablet computer. The software is only available in the iTunes store.\nautodesk.com\/products\/sketchbook-pro\nAutoDesSys bonzai3D\nBonzai3d comes from the developers of FormZ. It is a 3D modelling application aimed at delivering conceptual design and sketching.\nAccording to AutoDesSys, bonzai3d offers a powerful geometry engine over products like SketchUp and creates models that can be used for construction drawings, photorealistic rendering, and fabrication.\nThe lastest version, bonzai3d 3.0 adds 80 new features including 14 new tools that add functionality for fabrication, 3D printing, NURBS modelling, shape editing, texture mapping and component management.\nA Sun Position palette has been added to make creating shadow and sun studies easier.\nThe website hosts a number of user forums to help crack that niggling issue, and includes tutorials by users and a Maxwell render plugin.\nformz.com\/products\/bonzai3d.html\nBentley Systems AECOsim with GC\nAECOsim is Bentley\u2019s professional multi-disciplined BIM offering based on the company\u2019s core platform, MicroStation. Generative Components (GC) was the first generative scripting tool for conceptual design and works on plain geometry or BIM components.\nIt is possible to work through many \u2018what if\u2019 scenarios by directly manipulating geometry while capturing relationships among building elements. Generative Components is already enabling leading architects and engineers around the world including HOK , Arup, Foster+Partners, Grimshaw Architects, , Kohn Pedersen Fox, Morphosis, and many more.\nbentley.com\/en-GB\/Products\/AECOsim\/bentley.com\/en-US\/Promo\/Generative%20Components\/default.htm\nAutodesk FormIt This cloud-based conceptual modeller points the way to what could become a powerful free massing tool to compete with the likes of SketchUp. autodeskformit.com.\nTrimble SketchUp Make and Pro Trimble has reignited SketchUp within its Trimble Buildings strategy with its own ecosystem of vertical building applications and the Pro version bringing production quality drawing. sketchup.com","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/rhino-grasshopper\/","title":"Rhino Grasshopper","date":1243900800000,"text":"Popular among students and professionals, McNeel Associate\u2019s Rhino modelling tool is endemic in the architectural design world. The new Grasshopper environment provides an intuitive way to explore designs without having to learn to script, writes Martyn Day.\nGenerative modelling is undoubtedly becoming one of the most exciting CAD developments adopted by the industry. While architectural practices lagged mechanical designer\u2019s appetite for 3D by about 20 years, there has been a sharp increase in the use of 3D and advanced form-creation tools and Rhino is one of the more popular solutions.\nRhino has played a predominant role within that move to 3D because of its low cost, ease of use and powerful feature set. Bob McNeel, the man behind McNeel and Associates, developer of Rhino, estimates that it possibly has around 50,000 architectural users worldwide. However, Rhino is developed to be a non-industry specific surface modelling tool, at home designing a yacht, a ring, a shoe or a skyscraper it produces surfaces that are useful for all designers.\nMcNeel has developed a number of Rhino add-ons and plug-ins, mainly offering additional broad-functionality for rendering and animation in the guise of other animals \u2018Penguin\u2019 and \u2018Flamingo\u2019, as well as \u2018Bongo\u2019 and \u2018Brazil\u2019. The latest enhancement is called Grasshopper and comes free of charge while it is in development. Aimed at the emerging generative shape designers, Grasshopper is tightly integrated into Rhino and allows the user to interactively drive geometry via a plug and play interface, removing the need for learning the RhinoScript language.\nBob McNeel said that Grasshopper was developed as an attempt to make scripting more accessible to users that wanted generative modelling tools. \u201cDuring the design process, designers set-up sophisticated relationships between the parts of the design problem. Before Grasshopper, Scripting, .NET, or C++ code was the only way to do that in Rhino. Writing code is not something designers really want to get their head into. It seemed like most bigger firms have a few \u2018scripting geeks\u2019 that could not keep up with the designers\u2019 demands. So more and more designers were asking for scripting training\u2026 but then they hated it once they figured out how tedious coding was.\n\u201cGrasshopper is a way for designers to look at design problems as a set of sophisticated relationships and to map those relationships graphically and programmatically into a system that allows them to interactively play with alternatives. At first Grasshopper was very simple but, based on user feedback, it now allows for very complete systems, including the ability for expert users to extend the system with C# and Visual Basic components.\u201d\nGrasshopper works within Rhino and uses standard Rhino geometry but has its own slick interface window. Algorithms and manipulators are dragged, dropped and connected, as if they were being wired together like effects pedals. It is about as easy as it gets to use but still requires a methodology and understanding of geometry to get a desired result.\nRhino in London\nRhino is particularly popular with expressive London-based architects, such as Zaha Hadid, Buro Happold, HOK Sport and Foster + Partners. Fostering Grasshopper\u2019s usage in London is SimplyRhino, the largest Rhino reseller.\nThe company runs the annual Shape to Fabrication event which focuses on the use of Rhino and Grasshopper in modelling forms and shapes, through to complex engineering analysis and final manufacturing. McNeel programmers and even Bob McNeel usually make an appearance and are very accessible. The Simply Rhino Shape to Fabrication events are always complete sell outs and well worth attending.\nGlobally, Bob McNeel knows of 12,000 active Grasshopper customers, 90% of which are architects but admits there may well be more as users do not have to register to download. This liberal attitude permeates through McNeel\u2019s business model and means the company is very customer focussed, leading to a very active user community.\nOne of the stand-out messages from Grasshopper was McNeel linking the modelling to fabrication. While other CAD vendors seem to only concentrate on the modelling aspect in creating the 3D forms, McNeel has always talked about what happens once the design is complete. \u201cOur assumption is that Rhino is all about \u2018design for digital fabrication\u2019. Rhino has always been about free-form shapes that are accurate enough to manufacture. Architecture is the only market we are in that still requires complete 2-D documentation. In all of the other markets, the Rhino 3-D model is used in all phases of design through to fabrication. In many cases without any 2-D documentation.\n\u201cAEC is only beginning to catch up. Many of the limitations are not related to the CAD technology, instead the problem is with the AEC business model where everyone is trying to protect themselves from being sued by the other members in the process. Lucky for us, free-form architecture has become very fashionable and it is not possible to fabricate those buildings from 2-D drawings alone. In general, I would guess that more than half of all Rhino users are on the fabrication side rather than the design side.\u201d\nGrasshopper is being used and talked about by the same people that had advanced geometry needs and bought into Bentley Systems\u2019 Generative Components (GC). However, GC is script-based and requires training. It is also based on MicroStation, which has a parametric modeller, while Grasshopper uses a very visual plug and play interface to automate the scripting and is based on Rhino, which is a non-parametric surface modeller.\nBob McNeel admitted that the company does not know much about GC, \u201cexcept that people tell us that it is harder to learn and use than Grasshopper. Since Grasshopper is very flexible, users can set up most any kind of relationship they like, so I guess you could say some of those relationships are parametric. But if the user wants to organise their generative model more like a script, it is more script-like. We are trying not to limit anyone\u2019s shape generation process by forcing them to think about it in a certain way. In most cases, Grasshopper is instantly interactive when you change an input (geometry or parameter) or when you change the definition.\u201d\nPerformance\nOne of the biggest limitations of all parametric modelling tools is performance, it is very easy to create a script that forces the computer to make thousands of calculations and slow down. The shipping 32 bit version of Rhino suffers from the 2GB RAM limit. To access 64 bit it is suggested moving to the \u2018work in progress\u2019 Rhino 5 builds that are available. Bob McNeel explained the strategies to limit performance degradation: \u201cOur goal is for the generative process to be completely interactive. If you make any change to the Grasshopper definition or an input, you see the change instantly. Of course, as the definition gets more complex and the model larger, it slows down. There are options to not regenerate every time you make a change. Also, it is easy to \u2018disconnect\u2019 part of a definition while you are working on others.\u201d\nThe Mac\nMcNeel has said that Rhino is available as a \u2018work in progress\u2019 for the Apple Macintosh and it may be some time before Mac customers will be able to use it. \u201cGrasshopper is a .NET application. It is not clear how we will be able to get Grasshopper over to OSX. The Rhino for OSX is still in development and we haven\u2019t addressed any of the issues related to plug-ins yet,\u201d said Bob McNeel.\nConclusion\nRhino has always been an impressive modelling tool but with Grasshopper, it becomes a very powerful design exploration conceptualiser. The interface for developing the generative designs is worthy of an \u2018ease of use\u2019 prize and shames established products like Bentley\u2019s Generative Components. However, users still need to know what they are doing and how to get what they want from the geometry mathematically. The amazing work of Grasshopper users speaks volumes.\nWhile Grasshopper is currently free, it may incur a cost in future. \u201cGrasshopper is still in development. It will be free to all Rhino users as long as it is in development\u2026 at least another year. We are not sure yet if at some point it will be an option, or included with Rhino, or a special version of Rhino, or there is a basic version with Rhino and a full version option! In any case, it will not be a financial burden to anyone that wants to use it,\u201d said Bob McNeel.\nMcNeel has recently launched a new website for the Grasshopper community, which offers tutorials, a gallery and an active forum.\nPrice: Rhino \u00a3890","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-aecosim-building-designer-to-read-native-revit-rvt-files\/","title":"NEWS: AECOsim Building Designer to read native Revit RVT files","date":1480636800000,"text":"Bentley currently testing the ODA\u2019s Teigha BIM Kernel\nBentley Systems is currently testing Revit (RVT) file read capabilities inside Bentley AECOsim Building Designer.\nThe enabling technology, the Teigha BIM Kernel, is currently being developed by the Open Design Alliance (ODA).\nBentley told AEC Magazine that the RVT read capability is not yet scheduled for an AECOsim Building Designer release, adding that the ODA does not currently offer an RVT write capability. AECOsim Building Designer can currently read RFA family files.\nMeanwhile, to learn more about the Teigha BIM Kernel, read our story.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nNEWS: Allegorithmic launches Substance Designer 6\nFaro launches WebShare Enterprise for reality data\nBringing design to life\nNEWS: Bentley Systems delivers big advances in Reality Modelling\nNEWS: Autodesk adds IFC export to Inventor\nNEWS: Cidon Construction halves estimating costs\nNEWS: BIM Show Live announces 2018 date and new awards\nMotif introduces 'single click' AI rendering\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-2013\/","title":"Autodesk 2013","date":1338336000000,"text":"As reliable as the sunrise, Autodesk released the updates to its family of CAD and Building Information Modelling (BIM) tools this Spring. Martyn Day visited Autodesk\u2019s San Francisco HQ to see the latest design technology.\nLast year\u2019s big news from Autodesk was the introduction of \u2018Suites\u2019, which collated its vertical applications into three levels of workflow-driven collections: Standard, Premium and Ultimate. If you use multiple applications or yearned for something like 3ds Max, then these Autodesk Suites offer a lot of bang for your buck. According to Autodesk dealers, the suites have proven popular with customers looking to increase the use of their digital data, as well as in marketing and in the field.\nAt the 2012 launch the workflows for suites were rudimentary at best, with some having more solid \u2018serving suggestions\u2019 as to how data could flow between them to get the real benefit of multiple applications. The 2013 launch gave the company another year to improve the compatibility between and enhance data flow throughout the products to offer greater benefits.\nCloud\nIt is clear that Autodesk is still refining its approach to cloud technology. Autodesk Cloud, launched late last year, has already been rebranded as \u2018Autodesk 360\u2019. The service provides a free 3GB of storage as a collaboration workspace and a place to store design files that can be accessed and viewed while out on the road, through products such as AutoCAD WS and Autodesk Design Review for iOS and Android.\nThe service has been updated to support 2D and 3D DWGs and DWFs. There are extra benefits for subscribers with powerful online capabilities such as rendering design optimisation and energy analysis, and structural analysis, all of which are done quickly on cloud servers, freeing up machines to carry on with other work until the results come back. Subscribers are entitled to 25GB of storage space.\nHearing Autodesk talk about the cloud, one gets the feeling that it is more like a religion than a service. The company sees cloud delivery changing the way we all do computing, with CEO Carl Bass telling us that within three years cloud would be offering significant benefits to how designers work.\nAutodesk is leading the charge with commercially available cloud applications and what appears to be a holistic document management and data distribution system. It is possible to create a model, save it to Autodesk Vault, have a DWF sent to the cloud and remotely accessed on a mobile device anywhere in the world. The company has also enabled 2D drawings to be saved, distributed and edited via mobile devices anywhere in the world. This is significant with customers uploading over 300,000 drawings per week.\nAutodesk has created a joined up backbone of distribution management, together with number crunching specialist applications. Applications over cloud will take longer to be achieved.\nAutodesk is experimenting with Revit and Inventor running on iPads over the Internet. Mr Bass is convinced customers will want and use this over typical PC workstations but the transition may take some time.\nCloud delivery could spell the end of annual releases; and sooner than we might think. This is because cloud delivery will enable all users to have access to the latest releases, which will be delivered automatically from the cloud servers.\nInnovation\nAutodesk is looking to expand Project Butterfly, rebranded as 123D Catch, from Autodesk Labs, which combines multiple photographs to build 3D models.\nBrian Mathews, former head of Labs is now vice-president of Reality Capture at Autodesk and he is passionate about the technology\u2019s possibilities. At the launch event Mr Mathews showed how the company was using Octocopters to fly cameras over landscapes and cities, using the video they captured to turn into accurate 3D models.\nAutodesk has spent a lot of time developing this technology to enable it to compete with expensive laser-based survey equipment. In future, a camera attached to a toy helicopter will be used to capture huge sites and produce detailed 3D bitmapped models.\nMr Mathews also talked about the gap between building use simulation and real world use. Human actions are difficult to predict and often sit outside the \u2018ideal\u2019 parameters software needs to be effective. Autodesk is experimenting with placing thousands of sensors in buildings to capture human intereactions with design, which could be fed back into analysis modelling. Unfortunately recent feedback on analysis software from CFD experts found that the results are not to be trusted.\nRevit and Suites\nRevit 2013 Architecture\/MEP and Structures have been merged into a single product called Revit 2013, with full functionality and no data silos. Users can customise the ribbon to the tools that they use most frequently and turn off the those that are not needed (Architecture, Structure, Mechanical, Electrical, Piping). This move has been made to support multidiscipline workflows.\nRendering in Revit has been substantially improved with anti-aliasing available in all views, enhanced sky backgrounds, gradient backgrounds and improvements to isometric and perspective views. Autodesk has also included a progressive real-time ray tracer.\nRevit 2013 finally gets MicroStation V8 DGN capability; DWG, DWF and IFC capability have also been improved. There is another new stair tool that is a significant improvement for making complex stairs and a new materials database, which includes thermal performance capabilities.\nConstruction\nLike all other Building Information Modelling (BIM) software developers, Autodesk is now changing its development focus to make Revit appeal to the construction industry, which needs a different type of BIM model to design. New tools enable models to be sliced, diced, merged and excluded, supporting parts and assemblies for construction. Overall, a solid balance between enhancements and new capabilities to Revit. The \u2018Suite\u2019 variant, Autodesk\u2019s Building Design Suite 2013, includes additions to the Premium and Ultimate editions; Autodesk Navisworks Simulate 2013 software for the Premium edition; and Autodesk Infrastructure Modeler 2013 and Autodesk Robot Structural Analysis Professional 2013 software for the Ultimate edition.\nConclusion\nWith its move to greater diversity, could Autodesk run the risk of having too many products. Carl Bass disagrees. He felt that Autodesk had created \u201cOffice on the cloud\u201d, which he compared with Google, which he said had done \u201chalf a job\u201d with GoogleDocs.\nBy way of explanation, Mr Bass said that GoogleDocs did not envision a cloud-centered experience, but replicated the file systems and software silos that we are bound to with last decade\u2019s computing methodology \u2014 when a word processor created words, spreadsheets did numbers, etc.\nApplications no longer work the same way in the cloud. Mr Bass envisaged a system that did not have demarcations and would automatically bring up the functionality that was required to transform, edit or create.\nPerhaps in future there will be one system with all of Autodesk\u2019s know-how and functionality, which will provide the right interface for the task. If anything, Autodesk is getting more radical in its old age.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/letter-to-autodesk-aec-customers-demand-better-value\/","title":"Autodesk AEC customers demand better value","date":1595635200000,"text":"Today, a substantial number of leading AEC firms wrote an open letter to Autodesk CEO, Andrew Anagnost, highlighting a range of concerns, but specifically pointing out years of price increases with a lack of development of Revit, a tool on which they have come to rely. Martyn Day explores the reasons why\nIn the thirty years we have been following the AEC technology market and the 18 years in which AEC Magazine has been dedicated to the BIM process, we have never seen the likes of this \u2014 an open letter from a community of national and international design practices venting their angst at a technology supplier. These are sizeable, mature BIM practices which include Zaha Hadid Architects, Grimshaw, Rogers, Stirk, Harbour and Partners and many more.\nThe letter to Autodesk CEO, Andrew Anagnost succinctly details how they:\n- Have had to negotiate their way through five different licence models in four years.\n- Have seen a 70%+ increase in cost of ownership of Revit licences up to the end of 2019 (with more coming).\n- Are perceiving low levels of product development to the software most critical to their businesses, while experiencing negligible improvements in productivity.\n- Have been forced to subscribe to \u2018collections\u2019 of non-integrated products, which are explained as high value, but in reality, do not fit with the needs of their businesses.\n- Have been frustrated with Revit\u2019s geometry capabilities for manufacture and construction, forcing them to spend even more on third-party software to fill the gaps.\n- Have seen poor commitment to open interoperability in a collaborative industry, which makes them feel trapped.\n- Have no clarity on the roadmap for next generation BIM technology, and have lost trust in the annually repeated promises of significant new innovations.\n- Have an urgent need for cost stability and flexibility in the troubled times ahead.\nPrior to writing the letter to Autodesk, the firms carried out a questionnaire on various aspects of Autodesk software development and business practices. There were nine questions in total, which the firms rated on a scale of 1 to 10 (1 being strongly disagree and 10 being strongly agree \u2013 Net promoter score).\nOn average, each answer scored between two to three out of ten. The answers were used to define the letter to Autodesk and agreed at director level at all firms. Some firms that took part did not want to openly sign, for fear of \u2018retribution from their software supplier\u2019 expecting dreaded \u2018licence audits\u2019 \u2013 which in itself is a damning reflection of the relationships that Autodesk has managed to foster with some of its paying customer base.\nFrom our experience of talking with customers, all the views raised here are widely held within the BIM community and have been bubbling away for years. Autodesk customers, even the closest of competitors, talk openly to one another in business forums and have taken collective action.\nTo fully understand what led these firms to take such drastic action, we have to take a step back and look at the history of Autodesk\u2019s evolving business model and its BIM product evolution. We will see that this has been a long time coming and we are at a point where mature BIM users are frustrated.\nRevit, collaboration & next generation\nAutodesk used to think the answer to all design problems could be found in applications based on its flagship AutoCAD drawing tool. The company developed model-based architectural design tools: Autodesk Architecture, then Architectural Desktop but these were soon eclipsed by a new kid on the block \u2013 a technology startup called Revit.\nRevit was launched on April 5, 2000 and, with Autodesk\u2019s competitors circling to buy it, Autodesk acquired the company on 2002. While Revit got off to a slow start, its message of BIM and coordinated 2D drawings eventually came to dominate in architectural practices in many geographies.\nFast forward to 2020, and Revit is now over 20 years old. In the software industry, an application is deemed old at ten years and is usually rewritten to try and make use of operating system and hardware changes.\nWhile Autodesk has certainly added to the product\u2019s capability and fleshed it out over 18 years, its core engine still predominantly only uses a single CPU core. This is an issue when modern day processors tend to boost performance by adding cores, not by increasing clock speed.\nWhile a select group of more recently added features will utilise multiple cores and threads, the Revit engine has been showing its age for many years now. Similarly, in the world of graphics, basic features like occlusion culling were only added in 2017.\nThe main oxygen for Revit users is to load up on as much RAM as possible and cut up models before they become too much of a drain on performance.\nIn the past, releases that have managed to give more performance, have quickly been eaten up with larger models, or more detail. Here, in this letter to Autodesk, the most mature and BIM-capable practices are expressing what we see as industry-wide frustration in the lack of an underlying modern software architecture.\nIn moving from big \u2018R\u2019 releases to bi-annual subscription feature updates, Autodesk has also not helped itself by dissipating the delivery of its development work. Updates appear in the March and September time frames and the drip feed of features adds to the feeling that Revit is in something akin to maintenance mode.\nThe key point here is Revit should have had a fundamental bottom-up rewrite a long time ago, either in stages (such as with products like Nemetschek\u2019s ArchiCAD) or as a whole new codebase. If any software firm were to develop a BIM tool from scratch today, or even five years ago, it would not have the underlying software architecture of Revit \u2013 CPU, GPU optimisation or database structure.\nWe don\u2019t think Autodesk has been oblivious to this fact and in years gone by we had conversations with Autodesk team members about whether Autodesk\u2019s next generation BIM tool would be based on Fusion 360 (Autodesk\u2019s manufacturing CAD tool) and cloud.\nAt the time, and still true today, the cloud was seen as the future. One Autodesk exec explained to us several years ago that the idea of writing a Mac version would be very short lived, as any future rewrite would leverage the cloud and if it worked in a browser, it would run on any operating system or machine. But five\/six years on, that cloud version is not yet available, and the reality is, that mature users continue to push Revit to its limits and are spending time and effort working around its inherent limitations.\nThe letter to Autodesk highlights the fact that cost of ownership for enterprises has gone up considerably. Development has not matched expectation of expert users and practices want a BIM tool that utilises the multi-core CPU workstations they have been buying for the last five years, to get better productivity, create bigger models, add more detail and require less workarounds.\nCollaboration in the AEC industry isn\u2019t just important; it\u2019s essential as there is not one product or platform that can do everything. Despite inventing the Industry Foundation Class (IFC) methodology for data interchange, which is now an open ISO standard, Revit\u2019s coherence in IFC import\/export is widely ridiculed. Customers are also fed up with incompatibilities in data exchange within Autodesk\u2019s own portfolio of products, as well as with other important industry players.\nThe letter to Autodesk mentions lack of clarity on a roadmap to deliver next generation tools. In 2016 Autodesk did announce it was developing a cloud-based BIM design tool for the AEC industry, codenamed Project Quantum but that vanished and then reappeared as Project Plasma in 2019. However, nothing has been heard of that publicly for over a year. It does appear that Autodesk\u2019s reluctance to invest in re-developing Revit\u2019s core could be because a new generation is in development.\nAEC magazine has championed the coverage of this exciting development but there seems to have been significant delays and we aren\u2019t 100% convinced that the Revit team is actually doing the development work. We will return later to this topic within the article.\nSales, prices and licences\nThe letter to Autodesk clearly lays out the pressures practices are currently experiencing. Since the move to subscription, Autodesk has been ratcheting up the cost of ownership considerably. In the last five years, the signatories of the letter have assessed that a seat of Revit has gone up 70%+ and with Autodesk\u2019s planned licence changes this year will increase further.\nThis move to a pure subscription business has been inevitable since 2013 when Autodesk offered its first subscription alternative after it had long admired Adobe\u2019s move to online subscription, removing distribution and channel margin, and connecting directly to each customer.\nWith the move to pure subscription, in 2016 Autodesk stopped selling perpetual licences. Over the last three years Autodesk has been increasing the cost of maintenance to perpetual user base \u2013 5%, 10% and then 20% per annum with incentives to switch to subscription. Now in 2020 Autodesk is delivering a coup de gr\u00e2ce beyond price coercion to drive companies away from their perpetual licences, stopping maintenance support completely for perpetual software.\nWhile not withdrawing their right to use the software in perpetuity, as per the EULA (End user Licence Agreement), it is looking to render their licences useless in the long term with no future updates or patches. In August 2020, the company will also be removing network sharable licences and insisting on firms having a copy per user (named user), culling multi-year discounts and putting an additional price on CAD manager-level insights to user metrics.\nWe have heard repeated stories of aggressive Enterprise Business Agreement (EBA) sales, which harness the fact that Revit is a de facto standard in many countries, is a proprietary format, and is not backwards compatible between releases. Autodesk is also confident that the barrier to move to one of the few other BIM tools is high.\nWhen firms move onto EBAs they give up their perpetual licences, making it impossible to go back. After being encouraged into an EBA, possibly on a three year discount, the customer has a lot less negotiating capability next time around, as all the seats would need to be repurchased (re-subscribed) at fixed non-discounted prices. Some firms like HOK walked away from their EBA renewal.\nAnother contributing factor to the writing of this letter would have undoubtedly been recent changes to future licensing. Autodesk is removing the multi-user network (concurrent) licences and instituting named licences. Every user must have their own license, and sharing is not allowed under the EULA. So, you cannot have a single machine running a licence that a number of people might use throughout the day or week.\nAutodesk believes networked licences are typically shared between two users \u2013 but from talking to AEC firms we feel the reality might be more like four users to one licence.\nIf a user chooses to trade in their network licence in Autodesk\u2019s time frame, they get two licences for the price of one under subscription for eight years. However, if a customer decides to not trade in their network licence, they have to pay an additional 20% in maintenance in order to renew their licence for another year. After that, Autodesk will not renew the maintenance and customers are faced with buying a new subscription for every user at normal prices or use the software without any future maintenance. This is clearly a way to force more subscription seats into customers and there appears to be no obvious benefit to customers in the removal of network licences.\nLicensing is probably an article in itself. It\u2019s a convoluted mess. Perpetual, subscription, Suites, Collections, single user, network, named, maintenance, Token Flex, and crossgrades. The individual circumstances of any Autodesk customer will be a licensing voyage in its own right, together with Autodesk\u2019s policies in seemingly never-ending evolution.\nCustomers are currently running serial licences and named licenses as Autodesk again implements another transition. This makes licensing difficult to manage for any large practice. Indeed, in the Q4 FY 2019\/20 call to investors, Autodesk CEO, Andrew Anagnost admitted, \u201cThey\u2019re [customers are] living in what we\u2019ve affectionately call [sic] hybrid hell inside the company where they\u2019re trying to manage two types of different system [licence systems].\u201d\nAutodesk customers will be more than aware that Autodesk is hot on anti-piracy and has a team devoted to \u2018non-compliance\u2019, yet the products themselves come with very little in the way of management tools to help firms be in complete control across an extended enterprise.\nThe Autodesk software audit is feared amongst both software pirates and the IT directors of paying customers. A whole industry has grown up around licence management tools to try and maintain control and protect firms from aggressive audits from their software providers.\nIn fact, some firms told us that they chose not to sign the letter for fear of \u2018commercial reprisals\u2019, namely audits looking for infringements and associated fines. We would suggest that Autodesk in its valid search for true piracy has damaged customer relationships in the process.\nThe forthcoming changes to licensing, i.e. the move to individual named licences, which come with basic management tools are all coming this year (the \u2018Pro\u2019 level named licence analytics are available for an additional fee per seat). Autodesk states that this will help sort out the \u2018hybrid hell\u2019 which they created for customers. However, all of this comes at additional cost, which the letter points out is already an issue, plus a loss of network licencing.\nThe additional upside for Autodesk is that named user licencing also gives Autodesk access directly to the names and desktops of every user, in every company, and information about how they use the software.\nData ownership and cloud\nGood faith is important between a client and a customer. It\u2019s even more important in subscription and SaaS models. When business practices that include price-coerced licensing upgrades and crossgrades result in rapidly increased cost of ownership, IT directors feel helpless. In addition, when all project data is held in a proprietary format, on servers owned by a company with a poor record of open standards, epic levels of trust are required.\nThe letter also highlights the fear of what will happen to customers\u2019 data and IP when on Autodesk\u2019s cloud, as well as some general concerns on performance and reliability of BIM 360.\nAnagnost, Vision and R&D\nThe open letter is addressed to Andrew Anagnost, CEO of Autodesk since June 2017. Prior to the CEO role, Anagnost was instrumental in developing and rolling out the move from perpetual to subscription model and was a core executive team member in Autodesk\u2019s Oregon mechanical CAD team.\nIn changing the business model, Autodesk has gone from a typical stock price of $40 to $60 per share under previous CEOs to around an incredible $240 a share (62% up from March 2020, at the time of writing this article) a move that has taken the company from $2 billion in annual revenues to almost $3.3 billion.\nUnder Carl Bass (Autodesk CEO, 2006 \u2013 2017) Autodesk had somewhat of a \u2018Cretaceous period\u2019 with lots of new product development and code streams. On taking over in 2017, Anagnost immediately started culling a lot of software development that was not core to the business or was not making money.\nAutodesk\u2019s product development was streamlined by priority, to software that was currently selling and mature (e.g. Revit, Inventor), emerging software (e.g. Fusion 360 and BIM 360), and then next generation technology such as Project Quantum. This changed the company\u2019s R&D dynamics and we saw a huge investment in cloud and construction.\nWhen Anagnost got the CEO role, AEC Magazine interviewed him at Autodesk University London in June 2017 over a video link. We pointed out that with Autodesk subscriptions, there is an issue. Products like Revit have not met the expectation of companies that have invested in them.\nWith the announcement that Autodesk was no longer selling perpetual licences and was requiring enterprise customers to exchange them for multi-year Enterprise Business Agreements (EBAs) we suggested that Autodesk was asking customers to pay more for something they were already unhappy with the value of. \u201cThe price hikes have been so big that they (customers) can\u2019t quite believe it,\u201d we said.\nIn response, Anagnost explained that Revit still had a long way to go in terms of possible development and that Quantum (next generation collaborative BIM) was in the same continuum. He agreed that the maintenance base was struggling with pricing, explaining that users were asking \u201cwhat\u2019s going on?\u201d and \u201cwhere is the value?\u201d\n\u201cI talk to them and I even call the one-man shops, as well as the big guys, and I say this consistently: I understand and I want you to give us a year to prove to you that we are delivering value, don\u2019t leave us,\u201d he said.\nIn the interview, Anagnost conflated the AEC industry with happy customers from the manufacturing (MCAD) industry, especially those using the new code stream product Fusion. However, at the time Fusion was a $50 per month wannabe product which aimed to compete with the market leader Solidworks, after Autodesk had failed to dominate with Inventor. In contrast, Autodesk was dominant in AEC and BIM but customers had their data in Autodesk\u2019s proprietary BIM formats.\nIn 2018, following continued conversations with readers, we used the Autodesk University press conference to call out the unhappiness within Autodesk\u2019s enterprise AEC customers. Anagnost jokingly replied \u201cFake News\u201d.\nWe then had a meeting in which Anagnost explained that some customers were unhappy because it had not been properly explained that big discounts had been applied to their previous three year deals because they had surrendered their perpetual licences for subscription licences.\nWe relayed that firms were telling us of sales meetings with Autodesk in which no negotiation on price was being entertained, consultancy inclusion was mandated and they were warned that not signing the EBA would involve firms having to subscribe all the licenses at the market rate.\nIn 2019, things had not progressed and we raised the issue again in a direct communication with Anagnost. In our exchange, Anagnost explained that Revit\u2019s development was still ongoing and that MEP and generative design will be key development areas (this came to pass in the recent Revit 2021 release which included generative design (although only for subscribers of the AEC Collection).\nAnagnost admitted that the company\u2019s investment in development had shifted from \u2018A\u2019 (Architecture) to \u2018E\u2019 (Engineering), \u2018C\u2019 (Construction) and \u2018M\u2019 (Manufacturing), with the majority of the money going to the \u201cC\u201d and the \u201cM\u201d.\nIn 2018, Autodesk spent $875m on Plangrid, $275m on BuildingConnected, as well as an undisclosed amount on Assemble Systems (amongst others). It was the biggest acquisition year in Autodesk\u2019s history. In the last two months, Autodesk has acquired Pype, a US firm that uses AI to automate construction project management workflows, and made a $7m investment into Bridgit, a workforce planning software firm.\nThe construction space and cloud (SaaS) have become an obsession for Autodesk as it fights Procore, Trimble, Oracle and seemingly Uncle Tom Cobley for the right to be the data platform for (initially) the US construction sector. Anagnost told AEC Magazine that architecture development was not being ignored but had been pushed out to a multi-year development effort.\nThe next generation of BIM tool, Project Quantum, was announced in 2016 by the then senior VP of products at Autodesk, Amar Hanspal. It was renamed Project Plasma in 2018, and now has a third unknown name.\nAnagnost was obviously a big fan of Quantum, having highlighted the concept in his keynotes at Autodesk University in 2017 and 2018. But, in terms of development, he told AEC Magazine last year that it was most certainly not where he wanted it to be. Anagnost explained that he found out it had been announced before Autodesk had a defined and functioning data layer. From the Autodesk Q1 FY 2020\/21 investor call, it now looks like we might see something more solid at the virtual Autodesk University in November 2020.\nThe letter might not come as too much of a shock to Anagnost, as he has openly admitted that architecture development has seen a reduction in investment, and has been stretched out over a number of years on his watch.\nHowever, the cost of Revit ownership to Autodesk\u2019s AEC base has seen a 70%+ price increase per seat in five years, for limited development. In our discussions with Autodesk customers, they feel the Collections (Suites) are stuffed with applications they don\u2019t want or use. Instead they just want development to focus on their core BIM tool Revit. This November, it will have been four years since the idea of Quantum was originally shared publicly, and the deployment of a full next-gen BIM tool is still seemingly a number of years away.\nConclusion\nIn the world of business, looking at the current Autodesk share price and the increase in revenues (up 61% FY 2017 to FY 2020), Anagnost is a great CEO. There will not be an unhappy investor on the planet with those kinds of returns. Sales up, income per user up, margins up, with an aggressive laser-focussed acquisition strategy for organic growth.\nIn many ways, in terms of business execution, it\u2019s back to the days of Carol Bartz (Autodesk CEO 1992 \u2013 2006) who put shareholder value at the top of the line and grew the company to its first $ billion, although that was somewhat painful with AutoCAD R13 happening soon after she joined.\nThe problem is that Autodesk\u2019s short-term business objectives have seemingly taken priority over the long-term software development of the AEC flagship tool, Revit, while customers have seen significant increases in their cost per seat for little productivity benefit.\nWith almost annual coercion to move some element of their licences to please Autodesk\u2019s business model, large customers are fed up with being herded around and feel like hostages in a Kafkaesque relationship.\nAutodesk is undoubtedly \u2018squeezing the lemon\u2019 hard for its mature BIM customers and in this new subscription world, positive ongoing customer relationships are key. The letter states it clearly: customers want price stability, licence model stability, significant development of their core tool and they don\u2019t want to feel like hostages.\nAudits make them feel like software pirates and non-compliance is a growth division for Autodesk revenue. There is also a lesson here for customers, especially those at board level, in that not telling visiting VPs and CEOs of software providers exactly how you feel about development and costs on any visits, gives a false impression. If a company is using the business language of \u2018partnership\u2019 in its marketing, explain what reciprocal effort is required on their behalf.\nThe reality is that when Anagnost was promoted to the position of CEO, Revit was already overdue significant rework. The transition to cloud in AEC has also not been as straightforward as anyone in the software industry had originally envisaged.\nKicking Revit\u2019s re-development into the long grass and focussing on spending a vast sum on cloud for building contractors seems somewhat of a parochial retrograde vision for a future where a model-centric BIM could drive digital manufacturing and off-site processes.\nIncreasingly, leading BIM-based architectural design firms are seeing Revit as a painful way to get 2D drawings. It\u2019s not the front-end conceptual design tool of choice for complex geometry projects, just part of the documentation process. When they ask for new features or specific capabilities many of these firms are tired of hearing, \u2018use [computational design software] Dynamo to develop it yourself\u2019.\nAutodesk is much more likely to add in capabilities to please all users, than cater to the needs of architectural practices that are really pushing the boundaries of form and looking towards digital manufacturing.\nThe open letter is certainly born out of frustration, from firms that have invested so much time, energy and money into Revit and Autodesk products but now feel limited by these tools. These firms should be the key cheerleaders\/fan boys and marketing partners to promote industry change to model-based design and onto digital fabrication.\nIt will be interesting to see how Autodesk reacts and if it makes any cultural changes to address its now vocal, frustrated, advanced BIM user base. We are certainly seeing much more experimentation with other CAD and BIM authoring tools such as ArchiCAD, Catia, BricsCAD, Rhino\/Grasshopper, Rhino Inside, BlenderBIM and others, driven by a number of factors. Practices are seeking better value for money and not just as alternatives to Revit but also to advance their digital design and fabrication workflows. Mature BIM practices want to progress onwards and not feel like they have gone into a BIM technology cul-de-sac.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/moving-beyond-conceptual-design-with-simulation\/","title":"Moving beyond conceptual design with simulation","date":1445299200000,"text":"Using multiphysics simulation to understand the interaction between environmental, geometrical, and structural variables, engineers at Newtecnic ensure that innovative building fa\u00e7ades are both beautiful and buildable\nSimulation software has become a key tool for Newtecnic\u2019s fa\u00e7ade design work, which includes Federation Square in Melbourne, Australia, and more recently, the Heydar Aliyev Cultural Centre in Baku, Azerbaijan.\nBespoke is the norm for these iconic building projects and each one requires thinking that goes beyond conceptual design alone. Architects provide an artistic view and perhaps surface modelling of a building design, then engineers build up the fa\u00e7ade in layers, making sure that the design retains its creativity while also ensuring its structural integrity.\n\u201cA key aspect of our work is to describe the physical behaviour of the fa\u00e7ade systems we design,\u201d explains Carmelo Galante, head of research and development at Newtecnic.\nFrom solar studies that allow optimisation of the shading design in order to reduce cooling loads and maximise visual comfort, to the way in which fixing brackets for rainscreen cladding affect the integrity of the insulation, there are numerous challenges that can be resolved with the help of simulation.\nNewtecnic uses Comsol Multiphysics for its fa\u00e7ade design and analysis work. It is used to study the 3D thermal bridging effect \u2014 the way in which highly conductive materials penetrate insulation \u2014 on the overall energy efficiency of the build-up, evaluate the maximum temperature of components, and suggest the most suitable product or material. \u201cI can evaluate cladding pressures on the building structure for schematic design stages and study more complex fa\u00e7ades in which mechanical and natural ventilation are present at the same time. I can evaluate how different design configurations would affect the hygrothermal performance of a fa\u00e7ade system,\u201d says Mr Galante.\nMr Galante imports complex geometries from Autodesk AutoCAD and Rhino. Autodesk Revit is being used more and more at Newtecnic, including a new add-on that allows users to interface Comsol simulations with the Revit environment. Galante also combines the use of simulation with parametric design tools, such as the programming language Grasshopper, which is used to build and analyse complex geometries by means of generative algorithms.\nA current project involves designing the fa\u00e7ade system for a high-profile private building comprising a series of self-supporting concrete shells ranging from 10m to 80m in length and reaching heights of up to 30m.\nThe concrete shells are clad with a rainscreen fa\u00e7ade system made out of ceramic panels that are doubly curved in order to accurately reproduce the building geometry. Each panel is supported at its corners by adjustable fixing brackets made out of stainless steel. The brackets are attached to the concrete structure through four post-drilled anchorages. As the brackets penetrate the insulation layer and have a much higher thermal conductivity than the concrete structure, they create thermal bridges through the fa\u00e7ade envelope, significantly reducing its thermal performance.\nBy conducting a simplified 2D simulation study, engineers were able to look at how the thermal bridge effect created by the brackets influenced the temperature distribution in the fa\u00e7ade. The results from the simulation were entered into a Grasshopper script to evaluate three areas of interest: those influenced by one bracket, those influenced by two or more brackets, and those not influenced at all.\nEngineers were then able to prepare an accurate geometry for the system, including all the components of the build-up. \u201cIt is a real advantage to be able to combine two tools,\u201d says Mr Galante. \u201cGrasshopper allows me to investigate the geometry on a very large scale \u2014 that of the entire building \u2014 then I move back to the simulation software with this information and create a detailed 3D model to capture the real physics of the system.\u201d\nUsing this approach, engineers could conduct a 3D analysis to study the thermal bridge effect in the bracket and surrounding building and compute the global heat transfer coefficient (U-value) of the fa\u00e7ade.\n\u201cUsing multiphysics simulation allows me to develop a better understanding of the real situation,\u201d explains Mr Galante. \u201cI can combine fluid flow with heat transfer by conduction, convection, and radiation, meaning that I can thoroughly evaluate the interplay of different physical effects and confirm the performance of different structures and materials.\u201d\nResponding to design changes\nNewtecnic\u2019s engineers have to respond to continually changing designs, and need to be able to validate any design updates. \u201cSimulation allows us to do that,\u201d observes Galante. \u201cWe can demonstrate exactly what effect a design change will have, whether it relates, for example, to energy efficiency, structural performance, corrosion, or the lifecycle of a component.\u201d\nFor Newtecnic director Andrew Watts, it is all about answering questions such as \u201cIs it worth changing this to make something else work?\u201d or \u201cIf we have to change it, how much do we have to change it by?\u201d He comments: \u201cWith simulation, we can move away from the traditional building philosophy of studying individual components that only perform one function and can instead think in terms of multi-functional components, and of the building as a whole.\u201d\nSimulation is used to conduct analyses for every component of a building, and these results are fully integrated with drawings so that budget estimates are both clear and comprehensive. Fabio Micoli, associate director at Newtecnic, notes the value of delivering live feedback to clients.\n\u201cSimulation minimises construction costs by allowing contractors to see exactly what they are required to build, thereby reducing the need for contingency budgets or time for unresolved design issues and allowing the construction team to concentrate on meeting project deadlines.\u201d\nContinuing improvement\n\u201cThe digital tools that we use, such as simulation software, enable us to explore new possibilities and improve our design processes,\u201d Mr Galante says. He can see potential for expanding the use of simulation in the organisation, including using the new Application Builder, part of Comsol Multiphysics version 5.0.\nAs Mr Micoli notes, \u201cWe could, for example, enhance communication with clients by creating an application that allows an architect to modify different parametres and see exactly how changes would affect their design without knowing the underlying multiphysics simulation details.\u201d\nWith simulation, a better understanding of building performance can be delivered, ensuring that an innovative architectural design puts its best face forward.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-launches-construction-cloud\/","title":"Autodesk launches Construction Cloud","date":1574035200000,"text":"New integrated construction management offering features a wide range of acquired and in-house developed web services\nAt Autodesk University in Las Vegas today, Autodesk announced the availability of the Autodesk Construction Cloud, which appears to be the aggregation and repackaging of all of its acquired and in-house developed web services.\nThe company cited Assemble Systems, BuildingConnected, BIM 360, Construction IQ and PlanGrid as the core components, which connect AutoCAD, Revit and Civil 3D data to the construction management, bidding and contracting process.\nThe tech stack has over 50 enhancements, including new Artificial Intelligence (AI) features which look for drawing and model issues pre-construction. We explore this in more detail in this dedicated AI article. Autodesk has identified that a significant number of errors in documentation lead to on site problems and are now looking to check for errors in Autodesk BIM 360 submissions.\nCloud services and tech stacks, mainly aimed at connecting owners, designers, contractors and sub-contractors are the new battle ground for software developers in the AEC market, as Autodesk goes head to head with Procore, mainly in the North American market. There are many other big players on this area too, such as Trimble and Oracle.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/ai-eleven-start-ups-to-watch-in-aec\/","title":"AI - eleven start-ups to watch in AEC","date":1574640000000,"text":"We explore some of the emerging AEC focused software tools that are harnessing the power of Artificial Intelligence (AI)\nArtificial Intelligence (AI) is coming to all aspects of the AEC industry. At the moment many are early start-ups but there is quite a range of new products and services using AI to enhance AEC workflows.. While a few mentioned here are still in open beta testing, many are commercially available. Here is the AEC Magazine guide to the emerging developers\nTestFit offers AI-enhanced site feasibility study and building configuration. The target market is architects, developers, and general contractors.\nModels generated by the software can be tweaked in real-time using generative design technology for instant review of options. The user can specify a wide variety of parameters to guide option generation. A set of configuration utilities can spatially layout units, stairs, amenity spaces, elevators, corridors, and balconies. A parking configurator solves stalls, drive aisles and ramps. Monthly subscription start at $375. \u25a0 testfit.io\nSpacemaker provides a breakdown of each proposed project, allowing teams to explore multiple options in-depth. It is designed for collaborative workflow among architects, engineers, real estate builders, urban planners, and local authorities.\nThe software lets architects and developers weight various parameters, such as natural light, noise regulations, and the number of rooms a plot can fit.\nThe software uses machine learning to add user choice to its knowledge base, gaining additional expertise with use. Named customers include Skanska, Obos, AF Gruppen, and NREP. The platform is available on monthly subscription. \u25a0 spacemaker.ai\nBeing in the browser, allows Hypar Explore to bypass uploading of heavy BIM data sets by reviewing content and uploading results from user inquires. The service supports IFC standards, allowing it to be BIM platform independent. A designer can provide a set of requirements and ask for ten options, for example, to be generated from the data.\nCurrently users execute Python or C++ scripts to interact with BIM data; the company is working on more user-friendly interaction.\nThe developers, led by two Autodesk AEC alumni, Ian Keough and Anthony Hauck, are aiming for Hypar Explore to become an open platform where users buy and sell custom scripts and data. Hypar Explore currently works in free preview mode. \u25a0 hypar.io\nThe software includes a Safety Suite for observation, safety monitoring, and predictive analysis.\nThe AI engine, named Vinnie, is trained to recognise construction risks in photos and other project data. The observation utility offers risk scoring and workflow analysis, while the predictive analysis solution creates prioritised project rankings.\nThe company claims its predictive-based approach to construction site monitoring has led to safety incident rate reductions of up to 30%.\nNamed customers include Barton Malow, Shawmut, AECOM, Suffolk, and Skanska. Pricing is by inquiry. \u25a0 smartvid.io\nOpenSpace offers AI-powered photo site documentation, automatically mapped to plans. The company says its proprietary Vision Engine platform is the first \u201cfully automated reality capture system, enabling builders and owners to capture 360 video and photos without any manual input and in a fraction of the time of traditional tools.\u201d\nThe software allows real-time data gathering that assembles reports in a few minutes. The software uses the $800 Garmin VIRB 360 camera, which builders or site managers strap to their hardhat to document site development. The software then uploads the data to a cloud service, where photos are organised, stitched and mapped to project plans automatically. Objects including windows and moulding are automatically identified.\nNamed users include Beck Group, DPR Construction, Hathaway Dinwiddie, NOVO, and Suffolk. Contact the developer for pricing. \u25a0 openspace.ai\nAlice Technologies is an AI-based construction scheduling and management platform. It collects data including crew size, expected quantity of cranes and other heavy equipment, materials, design models, and production rates. The software then runs millions of simulations to arrive at schedules for user review, modification, and bidding.\nThe company claims the result is shorter projects, lower operating costs, and increased site safety. As the user adjusts a suggested simulation, all changes ripple through the plan to provide a real-time update of cost and time. The selected simulation automatically generates a full 4D cost-loaded schedule for tracking and visualisation. Alice can also be used once the project is underway to find alternatives if changes or problems occur.\nThe AI algorithm in Alice has been trained by mapping out the relationship between building sequence and schedule, by analysing thousands of existing projects. Named users include Hawaiian Dredging Construction Co, AF Gruppen, and Parsons. \u25a0 alicetechnologies.com\nAPE Mobile is a mobile app (iOS, Android, Windows) for capturing field data and generating real-time updates. It offers checklists, daily logs, and automates paperwork to deliver PDF reports that conform to a company\u2019s existing workflow.\nSite assistance technology uses AI to explore existing databases and to understand natural language enquiries. Users can make queries like \u201cShow all inspections for Phase 3\u201d or \u201cHow much concrete was poured today?\u201d Data reports can be exported to more than 800 existing construction management solutions or to popular cloud storage and data sharing sites.\nNamed users include DM Civil, TCD, GSE Engineering, Croker Construction, Kypreos Group, and COLAS WA. The product is available on monthly subscription. \u25a0 apemobile.com\nKwant.ai uses proprietary, wearable low-power sensors to automate construction site data collection. The tech combines sensor data with data from mobile devices, and processes the results in real-time on a cloud-based analytics platform.\nThe platform prepares actionable analytics including workforce location, schedule and cost risk issues, and can issue early warnings to predict and prevent safety incidents. Reports include a heat map showing employee location.\nKwant says it has analysed thousands of project schedules using AI as a foundation for the standardisation of all the unstructured site data into meaningful \u201cbuckets.\u201d\nThe software works with Procore construction management software. The company claims it has validated data showing 11% productivity increase and 80% reduction of safety incidents by its current users. Named customers include Walsh Brothers, OHL Judlau, Malbro Construction, Vorea, Hollister Construction Services, Magil Construction, and Paric Corporation. Pricing direct from Kwant. \u25a0 kwant.ai\nIntsite uses AI to optimise and automate the use of heavy equipment for construction, mining, and container ports. The stated goal is to \u201ctransform heavy machinery into smart and autonomous robots quickly and cost effectively.\u201d\nThe technology started as a guidance system and smart dashboard for construction cranes. The system combines computer vision, deep learning, and AI algorithms originally developed for aerospace with off-the-shelf cameras and other sensors. All Intsite machines are linked globally via the cloud, creating a network of machines each learning from the other.\nIntsite has two products: ForeSite combines ADAS safety technology with AI to optimise movement, guiding operators to the most efficient trajectories and providing safety alerts; AutoSite builds on the knowledge gained by ForeSite to extend guidance into autonomous decisions and actions. Named users include Infralab and Shikun & Binui Solel Boneh. Pricing is quoted on an individual basis. \u25a0 intsite.ai\nNucon is building its software by applying machine learning to selected construction industry models. It then applies context-aware image analysis, speech recognition, dynamic translation, and voice input similar to what is available from Apple Siri or Amazon Alexa.\nThe company says Nucon technology will be useful for defect resolution, prediction and discovery of high-risk issues, and identification of workflow lapses that may snowball.\nThe product is still under development; a beta version is available. \u25a0 nucon.io\nAI Construct is an early-stage venture working on the integration of robotics into a 5D BIM planned environment. The company is looking towards a time when on-site deployment of robotic equipment becomes commonplace. Using 5D models (BIM plus time and money), the software is aimed to guide companies with big data to achieve workflow and safety improvements.\nThe company will use AI to create virtual project builds, to identify potential problem areas and assist in finding solutions. Data collected from from robots, sensors, and cameras will aid autonomous activities and be used for real-time reporting. \u25a0 ai-construct.co.uk.\nLearn more about AI in AEC\nThis story is part of a longer article about Artificial Intelligence in the AEC sector.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-lenovo-unveils-2-in-1-mobile-workstation-for-cad-and-sketching\/","title":"NEWS: Lenovo unveils 2-in-1 mobile workstation for CAD and sketching","date":1448841600000,"text":"Innovative design combines Wacom pen technology pressure sensitive tablet with entry-level 3D mobile workstation\nLenovo has launched a unique 2-in-1 laptop for product designers and architects that combines a pressure sensitive pen tablet with an entry-level 3D CAD mobile workstation in a single device.\nThe ThinkPad P40 Yoga will allow designers to start work in a digital sketching conceptual design tool like SketchBook Pro, Made with Mischief or SolidWorks Industrial Designer then move into detailed design in 3D CAD without having to change devices.\nThe tablet part of the multimode mobile workstation is said to offer the \u2018sensitivity and accuracy of a premium sketching experience\u2019.\nIt is powered by Wacom\u2019s Active ES technology \u2013 the very same technology found in Wacom\u2019s Intuos and Cintiq products \u2013 and offers a full, 2,048 levels of pressure sensitivity.\nThe ThinkPad P40 Yoga comes with a ThinkPad Pen Pro that can be docked inside the machine for storage and charging. Lenovo says a 30 second charge will give 8-hours of use thanks to a built-in super-capacitor.\nThe slimline pen comes with a durable POM plastic tip, but there are also two optional felt tips for those who prefer a more natural sketching experience. A chunkier pen is also available, powered by a single AAAA battery.\nThe machine comes with a choice of 14-inch displays: a 2,560 x 1,440 glossy IPS panel or 1,920 x 1,080 matte panel.\nThanks to a versatile 360-degree hinge, the ThinkPad P40 Yoga can be used in standard laptop mode, tablet mode as well as \u2018stand\u2019 and \u2018tent\u2019.\nIn tablet mode Lenovo\u2019s Lift \u2018n Lock keyboard automatically raises the frame around the keys. This not only stops unwanted keystrokes when sketching, but is said to make the machine more comfortable to hold. Windows 10 users will also benefit from the machine automatically activating the operating system\u2019s tablet mode.\nFor 3D CAD work, the Lenovo ThinkPad P40 Yoga has the specifications of an entry-level mobile workstation.\nPowered by an Nvidia Quadro M500M GPU (2GB), it is certified to run a full range of professional 3D CAD applications.\nIt also features a dual core Intel \u2018Skylake\u2019 6th generation Core i7 CPU, up to the Core i7 6600U (2.6GHz to 3.4GHz), up to 16GB RAM and up to 512GB SSD.\nThe SSD is a 2.5-inch form factor SATA drive and not the new generation M.2 NVMe PCIe SSDs, which are a hallmark of other \u2018Skylake\u2019 mobile workstations.\nThe ThinkPad P40 Yoga is 19mm thin and starts at 3.9lb (1.77kg). It includes three USB 3.0 ports (one always-on charging), HDMI, mini-DisplayPort and microSIM. Prices start at $1,399.\nAlong with the ThinkPad P40 Yoga, Lenovo has also announced the ThinkPad P50s Ultrabook mobile workstation and the ThinkStation P310 desktop workstation.\nThinkPad P50s\nThe ThinkPad P50s is essentially an updated version of the ThinkPad W550s which we reviewed earlier this year.\nEnhancements include new Intel \u2018Skylake\u2019 CPUs up to the Core i7 6600U (2.6GHz to 3.4GHz), the new Nvidia Quadro M500M entry-level GPU and up to 32GB RAM.\nThe mobile workstation stands out for it impressive battery life thanks to two batteries. Dubbed, Lenovo Power Bridge technology, one of the batteries is built in while the other is removable and can even be swapped for a fresh one when the machine is powered on.\nOther features include a 15.6-inch display (up to 3K IPS) with a touchscreen option, up to 1TB storage via a single 2.5-inch drive, three USB 3.0 ports (one always-on charging), HDMI, mini-DisplayPort and SDXC.\nThinkStation P310\nThe ThinkStation P310 is Lenovo\u2019s new entry-level CAD workstation, a replacement for the ThinkStation P300. It is available as a mini tower or small form factor (SFF) workstation.\nIn addition to Intel \u2018Skylake\u2019 Xeon E3-1200v5 processors and new Nvidia Quadro GPUs, with 64GB of memory the machine has double the capacity of the ThinkStation P300.\nIt also features high-performance Samsung M.2 NVMe SSDs via a PCIe add-in board. For additional storage the Mini Tower can support up to four 3.5-inch or 2.5-inch drives, while the small form factor version can host two 3.5-inch drives and one 2.5-inch drive.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-archicad-21-centres-on-optimised-stair-design\/","title":"NEWS: ArchiCAD 21 centres on optimised stair design","date":1493683200000,"text":"New Predictive Design Technology uses algorithms to validate thousands of stair design options in the background\nArchiCAD 21, the latest release of Graphisoft\u2019s Windows and OSX BIM tool, includes a new stair and railing tool that automatically validates thousands of design options against relevant global and local design standards. Other new features include an updated rendering engine, a more flexible way of classifying elements, IFC model referencing and collision detection.\nThe Stair feature can be seen at work here\nAnd here, modelling the Bauhaus Stairs of Budapest \u201cSzalay Street\u201d\nArchitects simply place stairs with simple polyline input then chooses from various design options that fit both the user\u2019s graphical input and the chosen standards. The stair\u2019s shape can then be tweaked using \u2018intuitive\u2019 graphical methods then structural and finish components customised. Railing can be added with \u2018one-click\u2019, creating an instant associative railing along stairs or other building elements, whose posts and panels can be assigned as patterns, or customised individually.\nPrevious ArchiCAD versions provided a fixed set of Element Classifications, but ArchiCAD 21 introduces a more flexible way to classify elements and spaces for better support of national or company standards and improved interoperability between project participants.\nClassifications for construction elements or zones can be handled in a centralized way within the scope of the project using the new Classification Manager function. Classifications can also be transferred between projects via XML file format.\nThe way in which external IFC models created by consultants \u2013 such as Structural or MEP engineers \u2013 has also been improved. IFC files can now be placed as hotlinks into ArchiCAD projects as protected reference content. Model Filtering can also narrows the inserted IFC reference content by categories, such as Structural or MEP, or by element selection. The inserted IFC model content can then be updated easily from the linked source file. If the link is broken, elements of the inserted IFC modules can be edited as regular ARCHICAD elements.\nCollision Detection often the preserve of dedicated BIM co-ordination software tools, is now part of the standard feature set in ArchiCAD 21. Detection allows two groups of elements to be compared through user-defined criteria sets, including Element Types, Classification values, Property values and Attributes, such as Layer names or Building Materials. Using ArchiCAD\u2019s Mark-Up palette the detected collisions between various elements can be highlighted, identified and edited as needed.\nFinally, for photorealistic rendering for architects, the latest version of the ArchiCAD integrated CineRender by MAXON introduces Light Mapping and Secondary GI methods for more realistic, yet fast rendering.\nOther features include Touch Bar support for the Apple MacBook Pro, which allows architects to easily access the most relevant ARCHICAD controls and commands, and Uniform Rich Text Format, which has been introduced in all text-related tools: texts, labels, dimensions, interactive schedules.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/visualisation\/intel-to-take-on-nvidia-and-amd-with-oneapi-rendering-toolkit\/","title":"Intel to take on Nvidia and AMD with oneAPI Rendering Toolkit","date":1598227200000,"text":"Intel expanding open-source rendering and ray-tracing libraries to encourage software developers to come on board\nIntel is expanding the capabilities of its oneAPI Rendering Toolkit as it looks to encourage CAD, M&E and gaming \/ XR software developers to use its open-source rendering and ray-tracing libraries in their products.\nNew additions due to launch soon include Intel OSPRay Studio, a scene graph application designed to make it easier to develop \u2018high fidelity ray traced, interactive, real time rendering tools\u2019, and Intel OSPRay for Hydra, which uses the Universal Scene Description (USD) format to deliver a viewport focused renderer. USD was originally developed by Pixar for use in visual effects and animation but is starting to gain traction in other industries including architecture and forms the backbone to Nvidia\u2019s new Omniverse collaboration platform.\nIntel has been ramping up the development of its rendering libraries for some years now and there are currently five in the oneAPI Rendering Toolkit. The ones most relevant to design visualisation are Intel Embree, which helps developers get more performance out of their ray tracing applications and Intel OSPRay, a higher level ray tracing API, akin to OpenGL, for developing interactive visualization applications. There\u2019s also Intel Open Image Denoise, which improves image quality with machine learning algorithms that selectively filter visual noise.\nThe main driving force behind this expansion is, of course, for Intel to sell more processors. But this isn\u2019t just Intel Core and Intel Xeon CPUs. Later this year Intel will release the first products in its new Intel Xe Graphics family which will eventually cover mobile, desktop, workstation and datacentre. In the past, Intel has focused mainly on the entry-level, with integrated graphics built into its CPUs. However, many of the new Intel Xe Graphics will be discrete GPUs, some of which will support hardware ray tracing to go head to head with AMD and Nvidia.\nOf course, to sell hardware, there has to be software to run on it, and while Intel has done exceedingly well in CPU rendering over the years it is now facing stiff competition from GPU renderers powered by Nvidia and AMD technology.\nNvidia is arguably Intel\u2019s biggest threat \u2013 not least because it has very powerful RTX GPUs with built in hardware ray tracing and AI de-noising, but also because applications that use its OptiX ray tracing API or Nvidia Iray rendering engine require an Nvidia CUDA-capable GPU, which mean Intel gets firmly shut out.\nAMD, on other hand, has gone down the open source route and its Radeon ProRender rendering engine uses OpenCL so will run on any hardware.\nNvidia also has the advantage of there being many applications that use its proprietary software technology. In product design, engineering and architecture, for example, developers include Altair, Autodesk, Chaos Group, DS Solidworks, ESI, Foundry, Luxion (KeyShot), Unity and others.\nIntel has its own list of third party applications that use its libraries, including V-Ray and Corona Renderer (Chaos Group), Cinema4D (Maxon), AutoCAD, 3ds Max, Revit (Autodesk) and Stellar \/ Deltagen (Dassault Syst\u00e8mes). However, the libraries used in these applications currently relate to CPUs, and it\u2019s not clear if support in all of these applications will extend to Intel\u2019s full XPU platform, where XPU is short for CPUs, GPUs, FPGAs and other processors.\nIntel\u2019s oneAPI is designed to be flexible and scalable, as Jim Jeffers. Sr. Principal Engineer, Sr. Director, Advanced Rendering and Visualisation at Intel explains \u201cThe idea is smart software can take advantage of the platform it\u2019s presented with, whether it\u2019s a laptop or a supercomputer. Our goal is to support any workload at any size at any scale, especially with our rendering capability.\u201d\nJeffers points out that by using oneAPI ISVs can develop for multiple platforms without having to have multiple code bases. This single API approach might work for some software developers, but for others it would mean dropping Nvidia Optix, which seems unlikely considering the momentum behind it.\nSoftware developers are also not averse to supporting multiple rendering engines. Chaos Group V-Ray and Luxion KeyShot can both use CPUs and GPUs, and Solidworks Visualize actually offers a choice of two GPU rendering engines \u2013 Nvidia Iray and AMD Radeon ProRender.\nAMD or Nvidia could in theory choose to add support for Intel\u2019s oneAPI. \u201cIt\u2019s an open specification, and we are willing to work with and encourage hardware vendors to apply that, and we also believe that the market will ask them to do that as well, but it is for them to optimise,\u201d says Jeffers.\nAt the moment it seems highly improbable that Nvidia would do this, considering the penetration of its ray tracing software. It seems more likely that AMD would go down this route, but AMD will be well aware that open source from Intel isn\u2019t the same as an open standard from a third party like the Khronos Group, who develops OpenGL, OpenCL, WebGL and Vulkan.\nOf course, all of this will depend on how successful Intel is with getting software developers on board and also how well it does with Intel Xe Graphics. This is not the first time that Intel has tried to take on AMD and Nvidia in the discrete graphics market, with its ill-fated Larrabee project many years ago. We should find out more when Intel Xe Graphics launches later this year.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/trimble-to-provide-construction-equipments-for-u-s-marine-corps\/","title":"Trimble to provide construction equipments for U.S. Marine Corps","date":1181001600000,"text":"California, USA, 24 May 2007: Trimble announced that it has received order from the U.S. Marine Corps Systems Command to provide survey, grade control and laser leveling systems for its construction activities. The purchase order is valued at approximately USD 8.8 million.\nUnder the contract Trimble will provide machine control and survey simulation training software, grade control systems, reference station base kits for graders, robotic total stations, automatic self-leveling laser packages, automatic levels, and associated accessories, warranties, and training for Trimble systems used on Caterpillar 130G road graders which will be used by the U.S. Marine Corps Units.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/information-not-data\/","title":"Information, not data","date":1133395200000,"text":"James Cutler, CEO, eMapSite, looks at how Web services have enabled a new breed of digital geographic data supplier to play the multi-part role of data broker, data assimilation service and presentation toolkit.\nRecent years have in part been characterised by the growing ubiquity of tools and technologies that were meant to make us more efficient and productive but have frequently come to be persistent, invasive and disruptive. As business comes to term with these impacts, we are beginning to see some consensus around the need to align these tools with, and integrate them into, business processes rather than see them as a panacea in their own right. Equally, business is coming to terms with the difference between data, the raw material of many activities, and information, or value added and contextualised data as some would have it, in the pathway to knowledge, wisdom and commercial advantage.\nIn this article we look at this dichotomy and the ways in which the challenges they represent are being served in the world of mapping and related information.\nWhere are we now?\nWell actually many of us are stranded, awash with data while in an information void. In many spheres experts and specialists are being tasked with processing the raw material, often in very standard ways, in order to get to a point where they can begin to exploit it. Where the resources to undertake this are underpinned by costly infrastructure, software licensing, recruitment and retention in the face of repetitive and unchallenging responsibilities, the opportunity (and real) cost of such processing (and of the skills not utilised) is a necessary evil in the absence of any alternative.\nAt the other end of the spectrum are reports (available from third parties) that use the same raw material and through a combination of commercial imperative and market research apply a series of standardised methods and algorithms to produce a standardised quasi-information product. To an extent these reports have reduced the need for serious analysis, site investigation and modelling as part of the knowledge and wisdom gathering process. However, in the process there is a feeling that the outputs can be too rigid and compromise the capacity for professional advice whilst also creating an unhealthy dependency. In some industries these have been incredibly successful and meet a real need while in others they have often been welcomed initially, become de facto over time and only later challenged or usurped as users become more questioning and sophisticated. To an extent professionals that use digital mapping and related geographic information fall into this latter space.\nThe timeframe: \u201cthe quicker the better\u201d and \u201cthe cheaper the better\u201d\nThe options: local knowledge, site visit, web search etc\nThe risk: poor site selection, spiralling costs, lost opportunity, but \u201cit\u2019s what we\u2019ve always done\u201d\nThe alternative: unambiguous map-centric web site with fixed visible costs offering multiple search mechanisms, multiple visible frames of reference (mapping) with the opportunity to drill down to other information around the area of interest and to customise output for individual sites and customers.\nSound too good to be true? Well, it\u2019s not; these services are here today and are continually adding to the layers of information that can be interrogated, all in one place. Such services are not fixed reports and there is no substitution of skills at the user\u2019s end; rather there is a recognition that businesses gain value and advantage from understanding the implications of what these services overcome and what they provide.\nThese services are able to exist and expand thanks to the evolution of web services and the opportunity they offer to move away from resource intensive business towards light-weight client-responsive solutions accessible from anywhere. The underlying technologies provide, almost by definition, a reusable framework of components that enable rapid customisation to ensure inclusion or exclusion of specific functions and alignment with business processes on an as-required basis. This is a truly liberating development, freeing up businesses and professionals to apply their skills to interpreting, advising and consulting.\nOf course, the volume of information based products still needs to remain accessible and it is essential that such services embed associated authentication, management, distribution and licensing tools, something we shall examine further in the new year.\nRecently, omni-present broadband and the advent of web services have enabled a new breed of supplier to step into the evident gap between raw material and pre-packaged report, playing the multi-part role of data broker, data assimilation service and presentation toolkit. Their mandate \u2013 to eliminate lengthy data search, to integrate relevant selected data sources and to offer up a range of outputs (or deliverables) by which the user can access and use what they have selected.\nInformation\/data?\nStill readily used inter-changeably by many, the proliferation of data (or more specifically access to it via the web) has accelerated acknowledgement of the difference between it and the information we actually crave. In doing so it has inspired both those who capture the data and those who disseminate it to seek new ways of making data truly accessible. For example, intermediaries are reducing or eliminating visibility of the underlying raw material either through assimilation or integration with other context sensitive data or, as in reports, via interpretative and analytical tools and approaches.\nIn Great Britain, despite the protestations of some to the contrary, we live in a data rich (and sometimes seemingly costly) environment with mapping and other data available at a level of detail far greater than elsewhere. Owing to regulatory factors and other, primarily government backed drivers data really is accessible, though the precise method of distribution, cost, copyright, royalty, format, metadata, acquisition, delivery etc. may not be to everyone\u2019s liking.\nThere are a relatively small number of sources for digital mapping and related information (aerial photography, terrain data, building heights, environmental factors, addresses, routing and so on) in any country. As far as anyone can tell, in part from the lack of a highly competitive map production market place, users can be confident that the sources of raw material are with us for the long term. Thus, to a large extent, everyone is using the same raw material and competitive advantage stems from how that is deployed.\nUsers can and should (on occasion) challenge the adequacy (i.e. quality, integrity, age\/currency, frequency, accuracy, coverage, completeness, reliability, sustainability, consistency, timeliness, scale, resolution, collection, sampling methods etc.) of such sources to be sure that they understand the data and how it can be or is being used. Indeed the feedback process is essential in the improvement of existing raw material and in the refinement of various approaches to how the data is processed, analysed and presented.\nFor some, many even, the raw material remains the key ingredient for their activities and on which their businesses depend. However, for many others, the raw material remains precisely that, inaccessible bits and bytes requiring expertise, resources and time to turn into their key ingredient and it is on this vast array of professionals-under-pressure that this new generation of information intermediary services is focussed.\nThis article was written by James Cutler, CEO at eMapSite, a platinum partner of Ordnance Survey and online mapping service to professional users.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/chaos-group-releases-v-ray-next-for-sketchup\/","title":"Chaos Group releases V-Ray Next for SketchUp","date":1550707200000,"text":"Photorealistic renderer said to accelerate SketchUp workflow, through massive speed and intelligence gains\nSketchUp is the latest application to benefit from the advanced features of V-Ray Next, the next generation photorealistic renderer from Chaos Group. V-Ray Next for SketchUp is tightly integrated into SketchUp and runs on CPU or GPU. It promises high-quality ray-traced visuals in a few clicks.\nThe headline feature is V-Ray Scene Intelligence, which automatically analyses a 3D scene at the start of a render, then optimises some of the most common decisions an artist will make. For lighting, the new Adaptive Dome Light (ADL) is said to offer more accurate, image-based environment lighting that\u2019s up to 7x faster. Chaos Group says ADL is exceptionally fast when working with interiors and removes the need to add Portal Lights at windows and openings.\nThe developers also state that finding the perfect camera exposure or white balance is no longer an issue. Once a scene loads, Auto White Balance and Exposure return the right settings, making the entire process \u2018point-and-shoot simple\u2019. A new Nvidia AI Denoiser has also been embedded, so users get automatic noise removal and clean updates as they work.\nIn terms of performance, V-Ray Next is said to offer a 30-50 percent speed increase across the board, plus the additional increases one can expect from the ADL. The GPU renderer is over 200 percent faster alone, says Chaos Group, accelerating nearly every V-Ray feature including fog and atmospheric effects.\nA full list of features can be found here.\nV-Ray Next for SketchUp is available now and is compatible with SketchUp 2016-2019. A full Workstation license is priced at $790, with upgrades available for $395. Annual licenses are available as well for $350 per year. For a free 30-day trial, click here\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-autodesk-unveils-revit-2019\/","title":"NEWS: Autodesk unveils Revit 2019","date":1525996800000,"text":"New release focuses on delivering updates and features that are most requested by users\nIt\u2019s that time of year again when Autodesk unleashes a tsunami of updates to its portfolio of products and industry compilations \u2013 the most important component of which is Revit 2019. This version brings all the new features which were rolled out to subscribers in the previous year 2018.1 and 2018.2 together with an array of new ones.\nWith attempts to further involve the installed base in deriving new features, it seems Autodesk is actively trying to deliver updates and features that are most requested. Autodesk has broken down its key development areas to design, optimise and connect, which could also be viewed as a better modelling, new analysis tools and integration with its collaboration platform.\nFor structural engineers and detailers, there are new tools for structural framing and column elements. Steel components like plates, bolts, anchors, shear studs and wells can now be placed in a 3D model to connect structural members. There have also been improvements to the interoperability with Advance steel.\nOne of the most obvious benefits of Revit 2019 is the improvement to the view management system, which now supports multiple monitors and greater control over view filters. The new release also includes expanded modelling capabilities for detailed steel design and fabrication functionality for precast concrete.\nThere are improvements to the modelling of primary and secondary piping networks. It\u2019s also possible to analyse pumps in parallel in piping networks, computing flow, duty and standby pump quantities.\nOther improvements include dimensions for curved objects in section view, forthcoming structural precast extension, enhanced communication tools for working with fabricators, an API for precast automation, improvements to IFC4, and optimisations for Autodesk\u2019s BIM 360 collaboration.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/autodesk-university\/","title":"EVENT REPORT: Autodesk University 2014","date":1422230400000,"text":"With over 10,000 attendees, Autodesk University in Las Vegas is the biggest CAD industry event of the year. With plenty of razzamatazz, dozens of speakers, training sessions, social events and a massive hands-on exhibition, there is nothing quite like it.\nThe annual pilgrimage to Las Vegas for Autodesk University is something I both look forward to and dread. On the good side, there are always great speakers, technology developments galore, gossip, new contacts and old friends to see. On the downside there is the never-ending twilight, the constant jingling of slot machines, the regular electric shocks from static discharge, lack of sleep and accidental and repetitive collateral liver damage.\nAutodesk is in the midst of company-wide change, moving from boxed product to Suites, upgrade revenue to subscription-only and desktop applications to cloud and mobile. All this while maintaining its extensive dealer network, support distribution and growing direct sales force.\nIn the past, customers could decide when it was right for them to upgrade and budget accordingly but the new subscription-only model means Autodesk has to work to ensure its customers see the value in its ongoing enhancements and services. The company is moving away from its annual release cycle to providing ongoing updates throughout the subscription period.\nWhile AutoCAD is forever popular with customers, it has long since diminished from being a central plank for the company\u2019s future vision. Applications like Revit, Fusion, Inventor and InfraWorks get most of the limelight, as design transitions to adopt an intelligent 3D paradigm. Historically CAD was all about the documentation of design, while the latest generation of tools are for amplifying creativity, virtually prototyping, testing and improving in addition to generating documentation.\nKeynotes\nJeff Kowalski, Autodesk CTO, took to the stage to give a rousing speech on what that change means for Autodesk, and how he believes that the iterative evolution found in nature is the future of design.\n\u201cAll the buildings and bridges, machines, cars, products and devices all have one thing in common. They are all dead!\u201d he said. \u201cAt Autodesk we are starting to look at technology that designs itself through the lens of nature.\n\u201cGenerative design starts with your goals and runs through all the possible permutations, through successive generations until the best design is found. We think about the design process as a living process.\u201d\nNature refines a solution to a problem through iteration, while mankind tended to start from scratch. Mr Kowalski explained his theory that we need to take into consideration all the solutions that already exist as a start point for the next design.\nAutodesk is developing machine-learning algorithms to identify key concepts in millions of 3D models, classifying assemblies and components and the relationships between them. Mr Kowalski demonstrated the algorithm identifying items in an assembly, such as gears and screws. Once the context has been identified the next stage is to have a CAD system understand what these parts do; and that is exactly what Autodesk is working on.\nMr Kowalski envisages that in the future a CAD system will provide a variety of configurations to assist the design to achieve the best solution. \u201cWe have to stop telling the CAD system what to do and start telling it what we want to achieve.\u201d\nMr Kowalski explained how, using generative design, it is possible to derive the best design in the same time it would have taken to come up with the first proposal.\nIn a given example, Kowalski said that for a defined criteria the computer could come up with a range of solutions with optimal geometry. The design could then select the most aesthetic and then the system would take that criteria as a constraint for the next iteration, and so on.\nIn the future it will be possible to design complex, aesthetic buildings without drawings, but instead setting the criteria and expressing the goals. Arup has already used some of this code for the design of tensile structures.\nMr Kowalski also looked at the issue of planned obsolescence in design. Because our designs are dead the moment they are produced, they do not react or adapt to changes in their environment but what if they could dynamically interact? There has been a huge leap in the creation of small cheap sensors and smart materials and the emergence of the \u2018Internet of Things\u2019 means objects could be aware of proximity and could potentially communicate and collaborate \u2014 but we are not there yet.\nMr Kowalski wrapped up asking attendees, \u201cWhat if you were to ignore all these concepts and ideas but your competitor were to introduce a living device to compete against your dead one?\u201d\nAutodesk CEO, Carl Bass, was ebullient about the opportunities that change can bring to Autodesk and customers.\nHis opening line set the scene: \u201cNot since the industrial revolution has there been such a radical rethink of the way that we make things.\u201d\nLow-cost computer power has enabled Autodesk and its customers to work with much richer data, which reflects the real world. Advances in data capture, with laser scanning and photogrammetry brings the 3D world inside the computer. It is possible to take \u2018dumb\u2019 XYZ point clouds into software, create a surface mesh and then perform analysis or edits.\nThis concept of reality capture is going to be big for Autodesk in the near future, with products such as Autodesk Recap for point cloud capture and registration, as well as Autodesk Memento, currently in beta, for surface creation, edit and 3D print.\nA360 for cloud-based project collaboration was announced in 2013 and now has 60,000 active licenses. It is built into all Autodesk 360 products and will be in all forthcoming releases. Everyone who attended AU got a free year\u2019s subscription, \u201cIt\u2019s like Google Docs for designers and engineers\u201d, explained Mr Bass.\nMoving on to creation and fabrication, Bass said, \u201c3D printing is the closest thing we have to the Star Trek replicator but we haven\u2019t delivered on the promise of 3D printing yet. The machines are too slow, the materials are too limiting and expensive and the process is too unreliable.\u201d\nAutodesk\u2019s Spark platform is an open system that makes getting parts out easier. The company is working with an array of partners to do this and has launched a $100 million fund for start-ups and researchers.\nMr Bass also highlighted the Autodesk Amber 3D printer, which is intended as a reference device and is open, with Autodesk sharing the design plans for those that want to build their own.\nFurther to this Autodesk is working with robotic 3D printing technology that prints in metals like stainless steel.\nMr Bass said he was excited with the potential of digital tools on building sites. Highlighting Autodesk BIM360 for project management, he said it is the fastest growing product the company has ever had.\nAutodesk has worked with Topcon to include support for its point layout tools, which enables the BIM model to be queried on-site, with a laser pointing to the respective real-world space.\nMr Bass then announced a forthcoming new Subscription option called \u2018Subscribe to Autodesk\u2019, which will give customers access to all of Autodesk products for a single price. Coming in 2015, Bass promised, \u201cYou will be able to use any software, on any machine, anywhere in the world,\u201d however no price was given.\nIn his parting shot, Bass announced that Autodesk was making \u201call of its software free to any student, any teacher, any institution, anywhere.\u201d\nAEC products\nAutodesk is concentrating a lot of effort on the construction market and has started to get traction within US Departments of Transport, which are a key markets for competitor Bentley Systems.\nTo help combat the Trimble and Bentley alliance, Autodesk has teamed up with Topcon to try and offer feature parity for the construction phase. In many respects Autodesk\u2019s long dedication to developing the Architecture element of AEC, left the market open for Trimble to slowly and quietly grow its presence. The battle for Building Information Modelling (BIM) in AEC is shaping up.\nAutodesk A360, or BIM 360 for AEC users, is the future backbone for design teams. Teamworking has been one of Revit\u2019s weaknesses and Autodesk plans to overcome this by using the cloud. It has already started to develop 360-based AEC tools that hook back to the 360 cloud platform \u2013 Glue, Field and Layout, extending office-based design information for use onsite or gathered and reported back. Over time this will be expanded with the addition of costing, scheduling and estimation tools.\nThis is clearly a transitional phase of development as Autodesk continues to fill in the missing pieces until using a cloud-based design process becomes a no-brainer.\nWhilst talking with several customers at AU 2014, it was clear that there was some resentment at the lack of pace in Revit development. Despite Autodesk\u2019s move to sell Suites over single applications, firms tend to use one of the products in the Suite way more than the others and for architects that is Revit.\nAutodesk will need to get the measure of new functionality right for customers to feel that they are getting value for the fees. However this could just be early teething issues. As the cloud-based approach to using AEC tools fills out, Revit\u2019s current bottlenecks may well fade away as file size, Revit versioning and speed get crushed through server power and omnipresence.\nMemento\nSo-called \u2018Reality Capture\u2019 is a big focus for Autodesk. This is the ability to \u2018bring in\u2019 topography, buildings, infrastructure, objects \u2014 anything \u2014 and convert it to CAD geometry for reuse in a computer.\nSurveyors and mapmakers have traditionally used expensive laser scans to provide measurement. With advances in photogrammetry and the conversion of video to 3D, the cost of doing this has drastically reduced and Autodesk proposes that we can rethink design by using accurate, captured data.\nIts first offering in reality capture was a product called 123D Catch, which could take hundreds of photos of an object and build a 3D mesh. The second was Recap, a survey tool to collate laser scans and quickly generate detailed point cloud models.\nNow comes Autodesk Memento, currently in beta. Memento can import point cloud data photographs or video and convert these to extremely dense meshed surfaces. Through a simple interface it is possible to manipulate these meshes, healing holes, smoothing surfaces or completely editing the shape. The software can output to a 3D printer.\nIt could be used to capture a building for renovation or for rapid energy modelling, rescuing an antique object such as a statue for archive, to capture complex natural forms for use in a design or for monitoring coral growth over time. Memento will have many, many applications.\nCurrently it is being used by the Smithsonian Institute to capture and catalogue many of its artifacts and has been used in Sweden to capture an Egyptian mummy both sarcophagus and in 3D via MRI scans. Using the software, researchers found that within the sealed mummy bindings were amulets and jewellery and without ever opening it up these pieces were isolated and manufactured.\nAutodesk wants to take the various scanning technologies and \u2018democratise\u2019 their application within all areas of design. This will have some challenges with issues like copyright, as it will now be even easier to have product ideas stolen but will offer to many benefits too. Imagine being able to quickly get any real world object in your CAD system and being able to edit it? My mind is truly boggling. See Memento in action here (tinyurl.com\/memento3D).\nExhibition\nA major component of Autodesk University is the exhibition, which occupies a cavernous hall and is filled with firms that work within the Autodesk Ecosystem, from a behemoth such as Hewlett-Packard to a small start-up that just got funded on Kickstarter.\nTraditionally, when Autodesk was more AutoCAD-centric, the exhibition was filled with symbol library providers, drawing management and DWG viewer applications and made for a pretty dull afternoon. I have to say that his year the exhibition area was probably where the best things were to be found. While there were the traditional player and productivity tools, extensive space was set aside for customer displays, product demonstrations, live stage shows and hands-on booths.\nDesign tools and the digitisation of everything has generated an explosion of new companies and technologies that really make you think about what we can do with CAD. This year\u2019s exhibition had many laser scanning and image capture firms showing off their wares. Topcon, fresh from the announcement of deep co-operation with Autodesk had a huge booth. Autodesk\u2019s products now support Topcon scanners and laser pointers, furthering Autodesk\u2019s charge into the construction space.\nThere were lots of UAVs (Unmanned Aerial Vehicle). One company in particular blew my mind, Skycatch (skycatch.com) offers a UAV drone service, which is capable of flying around building sites \/ landscapes \/ infrastructure, automatically capturing video at 1cm \/ pixel resolution, which can be then turned into fairly a accurate 3D point cloud model by products such as Autodesk Memento.\nThis mesh can then be compared back to the BIM model to check for differences between the as-built and the original Revit model. It\u2019s possible to do this on a scheduled basis and could be used to assess volumetric measurements, thermal imaging, ortho mapping and 3D point cloud generation. The UAV carries the payload underneath the quadcopter body and when it needs a new HD or power, automatically flies back to its box and swaps its payload to continue. Absolutely fantastic stuff.\nStaying with scanning, Autodesk Reality capture team had a sizeable space dedicated to demonstrating various methods of bringing the real world into the computer. The largest display piece was a mechatronics set up to automate the capture of artifacts from a museum. With millions of items to capture and scan, this framework would carefully move items along a motorised conveyor, stopping for a moving light hemisphere and multi-camera set up to capture hundreds of images. These can then be loaded to the cloud and a high resolution 3D model created using the forthcoming Memento product.\nAutodesk REAL 2015\nAutodesk\u2019s Reality Capture event takes place next month in San Francisco and we will be there to report from it.\nDotproduct (dotproduct3d.com) is another interesting reality capture start up. Running on a standard small tablet, RGB scan-head (like Microsoft Kinnect\u2019s) with a simple hand held grip, the user simply \u2018paints\u2019 over an object or space as it captures geometry. On screen the device eventually highlights green showing the 3D capture is complete. This technology could be used by architects or remote workers to capture onsite models. At around $5,000 this is one of the lowest cost professional scanners on the market.\nOn display were robot arms, manufacturing machines, a 3D printed car, a foldable kayak and hundreds of other customer samples, created using Autodesk tools.\nIrisVR is a great little start up that was at the exhibition and we have written about them in the past. The software offers a simple drag and drop interface to accelerate the conversion of BIM models from Revit (or SketchUp), into beautiful virtual reality (VR) worlds that can be explored using the Oculus Rift. While at the show, I also heard that Autodesk had acquired the Unity (unity3d.com\/unity) game engine, which is a complete ecosystem for games design and a technology that I am sure will find its way into Revit BIM tools.\nIt is remarkable and an exciting prospect that this kind of technology will soon be used by practices worldwide. The VR headset market is set to explode with many high-resolution, low cost products in development.\nAU 2014 was the first major outing for Autodesk\u2019s Ember SLA 3D printer and Spark software (spark.autodesk.com\/ember). A dedicated stand was set up with a number of these machines cranking out 3D models. As it is still early days, and each one is pretty much handmade, there was plenty of maintenance going on, but the build quality of the output really is quite remarkable. Autodesk took orders on the stand and is looking at ramping up production beyond these early pre-production machines.\nConclusion\nAutodesk is changing in all areas \u2014 from the way it sells and delivers its software, and is even looking to fundamentally change the way we use computers to design. Mr Bass and Mr Kowalski\u2019s keynotes were visionary and inspirational, concerning much of the high-end research the firm was doing with partners and demonstrating it was running with the new revolutionary concepts that will impact the next generation of designers.\nLooking around the audience, I wondered how many of the delegates were still using AutoCAD and LT to just draw and document and their main collaboration tool was an email. The prerequisite of being on the new frontier of design is to start using 3D and BIM and only then can you start to benefit from a lot of what was presented.\nThe fact is that Autodesk has a lot of customers that are still 2D, so this is going to be a long process of education.\nAs Autodesk gets increasingly more cloud-centric, new features will just automatically appear and everyone will get them at once on whatever device, with Autodesk\u2019s commitment to provide their range of tools wherever they need to be, on whatever device is at hand.\nVideos of the presentations are available for free on the AU site, together with many of the classes. Even if you can\u2019t go to Vegas, Vegas can come to you.\n\u25a0 au.autodesk.com\/au-online\/overview\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/lenovo-embraces-ray-tracing-and-ai-with-nvidia-quadro-rtx\/","title":"Lenovo embraces ray tracing and AI with Nvidia Quadro RTX","date":1542067200000,"text":"Nvidia Quadro RTX GPUs, including the Quadro RTX 4000, supported in the ThinkStation P330 Tower up to the ThinkStation P920\nLenovo has introduced Nvidia RTX technology to its portfolio of workstations and has announced support for Nvidia Quadro RTX GPUs in the ThinkStation P330 Tower up to the ThinkStation P920. This includes the Quadro RTX 4000 GPU that Nvidia announced today.\nQuadro RTX GPUs are focused on real-time ray tracing and AI-enhanced workflows and feature new RT and Tensor cores. One of the companies driving development of Nvidia RTX technology is Epic Games, the developer of Unreal Engine, who recently turned its attention to design visualization. Earlier this year it became the first software developer to showcase Nvidia RTX technology in an experimental real time ray tracing demo using Star Wars characters from The Force Awakens and The Last Jedi.\n\u201cAt Epic, we are constantly striving to set new visual fidelity standards with Unreal Engine to empower architects, designers and engineers to create fully interactive experiences that convey the creative intent behind their designs,\u201d said Marc Petit, General Manager, Unreal Engine Enterprise at Epic Games. \u201cLenovo understands the needs of the Unreal Engine community, offering AEC professionals powerful workstation solutions featuring the new Nvidia Quadro RTX cards that enable the development of photorealistic real-time applications.\u201d\nMeanwhile, MX3D, known for creating the first-ever, 3D-printed steel bridge, is also using AI to revolutionize the way bridges are built with its new IoT \u201cnervous system\u201d \u2013 a system of sensors that use AI to monitor the bridge\u2019s health over its lifespan. The company uses a ThinkStation P920 workstation with dual Intel Xeon Gold processors, three Nvidia Quadro GV100 GPUs and 2TB M.2 NVMe SSDs.\n\u201cThe opportunity to examine real-time data from these sensors will provide insight to inform designs for future 3D- printed metallic structures. For something like this to succeed, you need partners who are going to help you reimagine the way things are designed and help you stay ahead,\u201d said Gijs van der Velden, CEO of MX3D. \u201cLenovo workstations have been key in providing the computing power we need to run these complex AI workflows to help our team revolutionize how bridges are built.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-marc-fornes-theverymany\/","title":"Video: NXT BLD 2019 \u2013 Marc Fornes, THEVERYMANY","date":1565049600000,"text":"Exploring forms through Computational Design to Digital Fabrication \u2013 NXT BLD London, June 2019\nMarc Fornes\u2019 Brooklyn Studio, Theverymany, is all about advancing computational protocols and complex assemblies in architecture and beyond. Over the last ten years it has designed and built a number of thin-shell pavilions, installations and building-scale works that push the limits of form, structure and space. Fornes explains the role that computational design and digital construction play in the creation of his beautiful complex curved architecture.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.rication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/watergems-haestad-method-launches-the-first-true-geospatial-water-distribution-modeling-and-management-software\/","title":"WaterGEMS: Haestad Method launches the first true geospatial water distribution modeling and management software","date":1049068800000,"text":"Haestad Methods announced the release of WaterGEMS, the world\u2019s first true Geospatial Water Modeling and Management System for the enterprise. WaterGEMS is a powerful GIS-based solution available for efficiently modeling, managing, and protecting our most valuable resource\u2014water.\nHaestad Methods\u2019 product philosophy is to make intellectual investments in strong technologies and to consistently adapt to customer\u2019s changing needs. WaterGEMS was developed using cutting-edge ArcObjects technology from GIS market leader ESRI and state-of-the-art computing tools from Microsoft\u2019s .NET initiative.\nThe development of the GEMS (Geographic Engineering Modeling System) framework is a technological leap for Haestad Methods and forms the backbone of its future product lines. WaterGEMS\u2019 suite of modeling tools includes:\n\u00b7 Skelebrator for reducing complex GIS assets and CAD data sets into accurate modeling representations while automatically preserving pipe network integrity and hydraulic capacity\n\u00b7 Reflective Data Model for extending the data coverage in real-time by adding custom fields, attributes, and objects (GEMSLinks)\n\u00b7 LoadBuilder for defining, assigning, forecasting, and managing customer demand information as defined by meter records, landuse, parcels, and zoning plans\n\u00b7 TRex terrain extraction utility for automatic mining of elevation data from DEM (digital elevation model) and DTM (digital terrain model) datasets\n\u00b7 WaterSafe technology for simulating potential terrorist attacks and disruptions to the water system, predicting contaminant propagation and concentration, preparing for emergencies, identifying impacted populations, and supporting real-time decision-making\n\u00b7 ModelForge for linking any type of enterprise and GIS data to the GEMS environment and constructing a living model from existing data assets\n\u00b7 WaterObjects for adding custom functionality and developing components and full applications using standard programming languages and scripts\n\u00b7 GeoGrapher for advanced visualization, graphic presentation, and publishing of any data store\n\u00b7 SCADA Connections for automatically incorporating field data into the calibration process and updating the model on the fly with live boundary conditions\n\u00b7 Darwin Optimization tools based on multi-parameter genetic algorithms and MAGIC technology that streamline and automate calibration, system design, and pump operations\n\u00b7 Scenario Control Center for organizing, comparing, and managing historical, current, proposed, and real-time modeling scenarios\n\u00b7 Variable-Speed Pump Modeling with APEX Technology that automatically calculates pump speeds during EPS runs, enables full logical controls, and targets nodes anywhere in the system\n\u00b7 Advanced Hydraulic and Water Quality Capabilities such as automated fireflow analysis; constituent, age, and trace analysis; leakage and sprinkler modeling; energy and capital cost evaluation; and more.\nUsing ArcObjects, the new ArcGIS 8 technology foundation from ESRI, Haestad Methods\u2019 WaterGEMS capitalizes on the intelligence of the new geodatabase format instead of relying solely on shapefile technology and data exchange methods like other available water models. While WaterGEMS still accommodates shapefiles and ArcInfo coverages, it provides users with the tools to migrate to the modern geodatabase format on their own schedule. Plus users get all of the functionality that comes with ArcGIS 8 including advanced symbology options that turn hydraulic models into powerful visual presentation tools.\nWaterGEMS builds on the ease of use of Haestad Methods\u2019 flagship product WaterCAD, the world\u2019s most widely used and recognized water distribution model. WaterGEMS incorporates all the modeling functionality from WaterCAD, and adds advanced geospatial capabilities for taking full advantage of existing GIS data and quickly maximizing the return on investment. Dozens of new features have been added to answer the demands of Haestad Methods\u2019 existing worldwide customer base, which is quickly becoming reliant on GIS technology to perform its day-to-day jobs.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-epic-games-optimises-workflow-from-cad-to-unreal-engine\/","title":"NEWS: Epic Games optimises workflow from CAD to Unreal Engine","date":1501545600000,"text":"Datasmith workflow toolkit provides high-fidelity translation of numerous common scene assets\nEpic Games is looking to simplify and optimise the process of importing CAD and DCC data into Unreal Engine for architectural and design visualisation.\nAt Siggraph today, the company gave a technology preview of a new workflow toolkit for the popular game engine, which is used for real time design viz and VR.\nDatasmith provides a high-fidelity translation of numerous common scene assets including geometry, textures, materials, lights and cameras. One of the key aims is to significantly reduce the amount of rework content creators need to carry out in the engine, particularly when working with ever-changing design assets, resulting in significant time savings.\nFor the preview, Epic\u2019s Chris Murray used data from Italian architects Lissoni and designers at Harley-Davidson Motorcycles to show how Datasmith\u2019s 3ds Max plug-in and CAD importers enable users to import files that retain visual fidelity to the source.\nAccording to Epic, this saves hours in data transfer and preparation time, taking the user most of the way to creating a fully interactive, photoreal, real-time visualisation experience.\nDatasmith is focused on the four main stages of data workflow: aggregation, preparation, optimisation and automation.\nAggregation assembles data from diverse sources into a single \u201csandbox\u201d where you can work with all the data in one place.\nPreparation provides tools to fix holes, weld seams, flip normals, eliminate overlaps, or all the tasks involved in cleaning up a data set.\nOptimisation is the stage where you can group thousands of like objects, simplify heavy meshes or re-tessellate complex surfaces to better fit the requirements of your workflow.\nAutomation provides a mechanism to redo all of the above tasks in order to have a non-destructive workflow. It\u2019s a general principle that upstream design or creative data will always be changing and downstream tools and workflows need to anticipate such changes allow refresh of source data without all the manual reworking.\nDatasmith will be available as a private beta in August.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/when-gis-meets-bim\/","title":"When GIS meets BIM","date":1531267200000,"text":"In November 2017 at Autodesk University, the CEOs of Autodesk and Esri announced the two companies would better integrate links between their products. In May 2018 at Esri\u2019s annual UK conference, Martyn Day saw the first products of this strategic partnership\nAutodesk produces some of the most popular design tools for BIM and civil engineering. Esri is the dominant force in Geographic Information Systems (GIS). Historically the areas have been poorly integrated and remained in very defined data silos. The reality is that all projects have a location and a co-ordinate and context is becoming increasingly important. Something had to be done to ease integration.\nRevit was designed to define the geometry of 3D intelligent models and produce co-ordinated documents. It really was not designed for site development. In the Autodesk world topology was left to AutoCAD Map 3D (which is now a toolset included with Autodesk subscription) handled in authoring products like AutoCAD Civil 3D, or conceptualised in Autodesk\u2019s relatively new Infraworks, which came from an acquisition. This means lots of layers of related data in different, uncoordinated systems. It\u2019s here that GIS can really help to make sense of this important data.\nGIS tools are designed to handle huge 2D, 3D and layered spatial data sets. They represent a single source to gather, manage and analyse project data and assets. Esri is a privately held Californian company that has dominated this space since 1969, with products such as ArcGIS. It has an estimated 43% of the GIS market; the second largest supplier has just 11%.\nAs we enter the era of \u2018Big Data\u2019 and projects grow in complexity and scale, Autodesk and Esri came to the conclusion that neither was likely to achieve this transformation alone, and could only accelerate community success by working with others. This meant better integration was needed with other leading applications.\nThe joint effort between the two companies aims to enable \u2018frictionless, live data integration [to] help create a deeper understanding of infrastructure in the larger context of built and natural environments.\u2019\nWhile this may seem like a mutual agreement to access each other\u2019s APIs (Application Programming Interface) to enhance connectivity, we have been led to believe that Autodesk and Esri are spending a lot of time looking at data flow within the process and key areas to transform the project life cycle, enabling the modelling of sites in context. The companies are calling this \u2018Geodesign\u2019, designing in the natural environment from the start, with regular scans updating the site as the project progresses, capturing every stage of development.\nIoT (Internet of Things)\nWith a lot of excitement around IoT, the companies are interested to explore remote sensors to monitor every tremor and temperature change. In the future it\u2019s envisaged that there will be billions of sensors embedded within infrastructure, enabling models to become \u2018Digital Twin\u2019 assets which can be interrogated for real time information.\nEsri and Autodesk will continue to develop and release themed updates to deliver on this integration and lifecycle story. 2018 sees the first and second phases of development arrive for customers.\nConnections\nThe first development in this partnership, which is already available, connects InfraWorks and ArcGIS Online. Designers and engineers can now bring GIS content from Esri ArcGIS Online into InfraWorks. It\u2019s now also possible to directly import roads, pipelines, and electrical transformers into ArcGIS.\nPhase two, which is currently in development, is the integration between Esri ArcGIS Pro, Autodesk Revit and Autodesk BIM 360. ArcGIS Pro will be able to read native Revit files. This means that GIS folks will be able to add Revit data to ArcGIS Pro projects and save as \/ turn it into GIS content. Similarly, Revit will be able to bring in ArcGIS Pro datasets for reference and context.\nUnexpectedly, at Esri\u2019s London User meeting, Robin Appleby, a consultant at Esri gave a demonstration of the unreleased development work with Autodesk that has been done to date. In the next release, ArcGIS pro will feature the ability to read Revit files directly, understand the data structure and bring the geometry and data, natively into ArcGIS Pro.\nAppleby also gave a live demonstration of an integration of an ArcGIS web scene and Autodesk BIM 360, bringing through a model of a building into a city model and then carrying out shadow analysis. Esri is integrating not just at the desktop level but also with Autodesk\u2019s cloud-backbone.\nWhile the demonstration was very quick, it gave a flavour as to the level of work and integration that users can expect to see in 2018. Autodesk\u2019s next opportunity to show integrations from its side will probably come in the Autumn \/ Winter with subscription updates and Autodesk University in Las Vegas.\nThe Esri User Group video on BIM can be seen here. The Autodesk related demonstration starts at 18:30mins.\nConclusion\nWhen Esri CEO Jack Dangermond joined Autodesk CEO Andrew Anagnost on stage at AU Vegas, it was hard to scope the true depth of the partnership. How much more than just an API deal was really on the table? And was this Autodesk forming an alliance to attack firms like Bentley that have been more successful in the GIS and transport markets?\nFrom what we have seen demonstrated and the stated intentions specifically from Esri\u2019s side of the fence, this is absolutely a big deal. Relying on IFC import was not going to cut a big slice of the BIM market for Esri in the future; it needed native support. For Autodesk, Esri will help it with its bigger clients, help win city-scale projects, empowering Revit workflows and get a serious analysis and simulation developer to boot. However, bigger things are being planned between the companies and as cities get smarter and infrastructure renewed, expect BIM models of completed buildings, bridges, roads and transport systems to be hosted in Esri cloud products, delivering live data and analysis.\n\u25a0 autodesk.com \u25a0 esri.com\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bluesky-uses-smart-phone-footage-to-create-3d-models-of-infrastructure\/","title":"NEWS: Bluesky uses smart phone footage to create 3D models","date":1518393600000,"text":"Algorithms used to extract accurate measurements of essential infrastructure, such as overhead power lines\nBluesky has completed a research project backed by the UK government\u2019s innovation agency, Innovate UK, to develop the use of smart phones for capturing accurate 3D spatial information.\nThe nine-month investigation focussed on using video footage captured by a smart phone to create 3D data so accurate measurements of essential infrastructure, such as overhead power lines and other utility facilities, could then be extracted using specially developed algorithms and workflows.\nDesigned to provide an accurate record of the feature\u2019s location and its environment, the Bluesky project is expected to appeal to electricity Distribution Network Operators (DNO) and other organisations with a dispersed asset base, as a low-cost measurement and auditing tool.\nDuring the project, Bluesky tested a number of hardware, software and deployment options. These included the use of aerial photography to add control points to the video footage. As the project progressed, it was established that in remote areas there were insufficient features, for example road markings, lamp posts or buildings, to establish the required control. Bluesky therefore developed alternative methodologies including the use of a calibration object or the measurement of a feature within the imagery.\nWorking alongside project partner ADAS, Bluesky also undertook rigorous testing of the solution establishing and documenting the field data capture process, identifying minimum hardware requirements, such as camera pixel capacity, and additional developments to the data delivery mechanism. Following minor enhancements and additional trials Bluesky hopes to launch the mobile phone mapping tool, complete with data processing and hosting services, in Q2 2018.\n\u201cWe know from previous work with ADAS, electricity companies and other utility service operators that by providing a more cost-effective data capture, analysis, auditing, and dissemination solution we will significantly decrease maintenance costs releasing essential funds for network upgrades and service improvements,\u201d said Rachel Tidmarsh, Managing Director of Bluesky. \u201cWe will also explore other applications of the solution in sectors such as forensics, insurance and emergency response.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/www.wired.com\/story\/openai-router-relaunch-gpt-5-sam-altman\/","title":"OpenAI Rolls Back ChatGPT\u2019s Model Router System for Most Users","date":1765843200000,"text":"OpenAI has quietly reversed a major change to how hundreds of millions of people use ChatGPT.\nOn a low-profile blog that tracks product changes, the company said that it rolled back ChatGPT\u2019s model router\u2014an automated system that sends complicated user questions to more advanced \u201creasoning\u201d models\u2014for users on its Free and $5-a-month Go tiers. Instead, those users will now default to GPT-5.2 Instant, the fastest and cheapest-to-serve version of OpenAI\u2019s new model series. Free and Go users will still be able to access reasoning models, but they will have to select them manually.\nThe model router launched just four months ago as part of OpenAI\u2019s push to unify the user experience with the debut of GPT-5. The feature analyzes user questions before choosing whether ChatGPT answers them with a fast-responding, cheap-to-serve AI model or a slower, more expensive reasoning AI model. Ideally, the router is supposed to direct users to OpenAI\u2019s smartest AI models exactly when they need them. Previously, users accessed advanced systems through a confusing \u201cmodel picker\u201d menu; a feature that CEO Sam Altman said the company hates \u201cas much as you do.\u201d\nIn practice, the router seemed to send many more free users to OpenAI\u2019s advanced reasoning models, which are more expensive for OpenAI to serve. Shortly after its launch, Altman said the router increased usage of reasoning models among free users from less than 1 percent to 7 percent. It was a costly bet aimed at improving ChatGPT\u2019s answers, but the model router was not as widely embraced as OpenAI expected.\n| Got a Tip? |\n|---|\n| Are you a current or former OpenAI employee who wants to talk about what's happening? We'd like to hear from you. Using a nonwork phone or computer, contact the reporter securely on Signal at mzeff.88. |\nOne source familiar with the matter tells WIRED that the router negatively affected the company\u2019s daily active users metric. While reasoning models are widely seen as the frontier of AI performance, they can spend minutes working through complex questions at significantly higher computational cost. Most consumers don\u2019t want to wait, even if it means getting a better answer.\nFast-responding AI models continue to dominate in general consumer chatbots, according to Chris Clark, the chief operating officer of AI inference provider OpenRouter. On these platforms, he says, the speed and tone of responses tend to be paramount.\n\u201cIf somebody types something, and then you have to show thinking dots for 20 seconds, it\u2019s just not very engaging,\u201d says Clark. \u201cFor general AI chatbots, you\u2019re competing with Google [Search]. Google has always focused on making Search as fast as possible; they were never like, \u2018Gosh, we should get a better answer, but do it slower.\u2019\u201d\nAn OpenAI spokesperson tells WIRED that the company determined, based on user feedback, that Free and Go users preferred staying in the default chat experience, with the option to manually select a reasoning experience when needed. OpenAI declined to specify which user signals informed that decision. The company also said its Instant models can now take more time to answer questions, much like its reasoning models, narrowing the gap for most users.\nThe spokesperson said ChatGPT\u2019s paid users, however, continue to value the model router, and the company expects the technology underlying it to keep evolving. OpenAI will likely relaunch the model router for free and Go users when it\u2019s improved, according to sources familiar with the situation.\nHeated Rivalry\nThe change comes as OpenAI scrambles to shore up ChatGPT amid intensifying competition, particularly from Google. Last month, Altman announced a company-wide \u201ccode red\u201d to marshal resources around improving its core consumer product. While ChatGPT is a juggernaut in the AI space, with more than 800 million weekly active users, OpenAI CFO Sarah Friar reportedly told investors that the amount of time users spend on the platform has dipped slightly following recent content restrictions. Since August, OpenAI has introduced several safeguards into ChatGPT, such as having the chatbot suggest breaks to users during excessively long conversations, and increasing the scope of its classifiers to block more content that\u2019s deemed unsafe.\nThird-party data suggests competitive pressure is mounting. According to audience-tracking firm SimilarWeb, Google Gemini has grown significantly in recent months, while ChatGPT\u2019s growth has flattened. SimilarWeb\u2019s vice president of data and DaaS product, Omri Shtayer, tells WIRED that average visit duration on ChatGPT has fallen below Gemini since September.\nThe model router has been controversial since it was launched in August. OpenAI\u2019s executives were caught off-guard by a strong backlash from users who preferred chatting with specific models. Within a week, OpenAI reinstated the model picker and some of its older AI models, while keeping the router as the default in ChatGPT via a newly branded mode: \u201cAuto.\u201d\nThe episode shows how leading tech companies are still figuring out the best way to integrate powerful AI models into mass-market consumer products.\nRerouting\nAs OpenAI pushes for higher usage in ChatGPT, it has to do so without backtracking on its safety commitments. In a recent report, the company said hundreds of thousands of ChatGPT users exhibited possible signs of mental health emergencies related to psychosis or mania every week, and it was taking steps to address how its AI models respond to them. OpenAI\u2019s model router was one of those steps, routing sensitive queries to reasoning models, which were previously deemed better equipped to handle users in distress.\nAn OpenAI spokesperson tells WIRED that it will no longer route sensitive conversations to reasoning models due to GPT-5.2 Instant\u2019s increased performance on safety benchmarks..\nThe model router still exists for ChatGPT\u2019s paid subscribers, including its $20-a-month Plus or $200-a-month Pro tiers, signaling that the company is still committed to the idea. Robert Nishihara, cofounder of the AI training and inference platform Anyscale, says he expects model routers to stick around in the long term, even if the versions available today aren\u2019t perfect.\n\u201cFundamentally, using different models and amounts of computational power is appropriate for different problems,\u201d says Nishihara. \u201cNo matter what happens in the short term, I expect routing to continue to be right.\u201d","source":"wired.com"}
{"url":"https:\/\/geospatialworld.net\/news\/tripod-data-systems-appoints-kerwin-as-new-regional-sales-manager\/","title":"Tripod Data Systems appoints Kerwin as new regional sales manager","date":1031097600000,"text":"Sept 3, 2002-Tripod Data Systems has announced that it has appointed Raymond G. Kerwin as regional sales manager for the Northeast region. Kerwin\u2019s appointment continues TDS\u2019 expansion of sales and service efforts as well as support for its dealer distribution network.\nKerwin comes to TDS after 12 years at Topcon, where he was the product manager of the robotic, optical and data collection product lines.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-2016-releases\/","title":"Autodesk 2016 releases","date":1438041600000,"text":"Autodesk\u2019s product portfolio of desktop and cloud services continues to grow. Martyn Day visited the company\u2019s forthcoming new headquarters to find out what we can expect.\nAutodesk recently held an AEC summit in the AEC division\u2019s home town of Boston. The purpose was to launch the new 2016 range of products, go through some of the developments taking place within the division, to take a look at the industry as a whole and identify trends.\nAutodesk has long since removed itself from the big yearly release cycle by delivering in-stream improvements. However, a yearly release still provides an opportunity to go through the new features delivered and announce to its customers product news; and there is always lots of new products.\nRevit 2016\nAt the core of the Autodesk AEC offering sits Revit, which forms the basis of the company\u2019s Architectural, Structural and Mechanical, Electrical, Plumbing (MEP) Building Information Modelling (BIM) offerings. As Revit has moved to Suites and subscription, its core development appears to have slowed as Autodesk adjusts to the \u2018portion\u2019 sizing of subscription releases.\nThe team has managed to find a lot more capacity to improve the core performance of the graphics, as well as opening files, viewing, printing and rendering.\nBy implementing changes to the graphics pipeline, Revit 2016 can now make some use of multi-core processing and, for the first time, can better harness the power of the GPU to provide some dramatic speed increases.\nWe understand another element of the performance benefit has come from stopping Revit from constantly checking if there have been model updates when only view manipulation has been done. This has lessened the load on the CPU and makes old models that may have previously pushed the limit of your hardware appear to be like the proverbial hot knife through butter.\nThere is a new physical-realistic rendering engine, which is really quick and replaces the old mental ray app for static images. Revit continues to use the Nvidia mental ray engine for functions such as walkthrough export, FBX export, and previews. IFCs can now be lined, and now use a reference geometry for snapping and reference. Structural and Steel detailing has been improved and MEP gets better links to fabrication.\nIt is possible to use LOD 400 content from Autodesk Fabrication products (CADmep, ESTmep, and CAMduct) in Revit to create a more co-ordinated models.\nInfraworks\nAutodesk\u2019s \u2018go to\u2019 product for infrastructure design continues to steal all the limelight from Civil 3D, which rarely gets a mention these days.\nInfraworks is undoubtedly a very cool application that seems to devour large data sets and allow interactions at game-like frame rates. We saw a demonstration on how in under ten minutes, the application could import and geo-reference huge and disparate data sets to combine and provide city-scale, spatial databases, which could be used for a variety of purposes, in addition to transport, traffic and drainage. This year\u2019s release sees tighter integration with Autodesk\u2019s other products, plus better DGN and IFC support.\nFormIt and Dynamo\nAutodesk has been wrestling with the conceptual design part of the equation for a long time. SketchUp is still out there and, despite its basic nature, is one of the industry\u2019s most popular tools. Autodesk\u2019s answer, FormIt, is a slick application for desktop and mobile but was, until now, still embryonic in development.\nAutodesk has now linked FormIt with Dynamo, the company\u2019s computational design application to rival McNeel Grasshopper and Bentley System\u2019s GC (formerly called Generative Components).\nThe company did not stop there, by hooking these up with the cloud, you get FormIt 360 Pro, which can use Revit components and adds in solar analysis and model collaboration. This not only links conceptual with BIM, but driven by generative design and guided by analysis, we can actually have the computer \u2018aid\u2019 the design process as opposed to just document it.\nBIM 360\nAutodesk is laser focused on bringing cloud services to the AEC design environment. Every year there are more \u2018360\u2019 solutions, which means they are, in some part or wholly, online services. So they are available everywhere \u2014 from the drawing office to onsite, on desktops and via mobile devices. This year\u2019s additions include BIM 360 Plan for scheduling, BIM 360 Docs and BIM 360 Enterprise Insight.\nBIM 360 Plan is based on lean construction principles and provides an easy to use front end to collate construction plans across multiple teams and disciplines, displaying commitments, deadlines and hand offs. The software tracks performance metrics by phase, trade or location.\nBIM 360 Docs will be available soon and enables project information to be available anywhere in the field.\nBIM 360 Enterprise insight is pretty much a company-wide view of projects with dashboards for analysis. There are many serious players in this part of the construction market already and it will be interesting to see how Autodesk fairs.\nWe also saw a new application called Autodesk Building Ops, which oddly does not have a 360 in the title. This may be because it is a mobile application that feeds data back to operations?\nBuilding Ops is Autodesk\u2019s first foray into the sleepy world of Facilities Management (FM). It is a mobile-first building maintenance app aimed at contractors for snag list management and for building managers operations to manage day to day asset, maintenance and operations tasks. It has a fairly simple interface, and offers a basic job ticketing system, with reminders for upcoming, overdue, awaiting parts and completed.\nProject Akaba\nAutodesk gave a brief demo of another up and coming cloud-based technology called \u2018Akaba\u2019. Here, instead of using the computer to draw the geometry, the aim is to tell the computer what the end goal should be and let it come up with a number of solutions.\nBy inputting a range of rules and constraints, like the number of bedrooms, bathrooms, energy usage, amount of light per room, minimum square footage of each room, site boundaries etc, which all compete, Akaba will generate the geometry and arrangements that solve the brief. The designer is able to select the one which is most appealing and then iterate again or detail or massage the design.\nProject Akaba has a very clean interface with an entry point for Goals, and Project criteria, using sliders. The main display shows an array of solutions organised by score of meeting the initial design goal. This really is exciting stuff. Autodesk has similar plans for Urban-level city planning tools.\nConclusion\nLooking at the real innovation coming out of Autodesk\u2019s AEC team at the moment, indicates that a lot of the work being done in trying to solve the conceptual conundrum. While CAD has been great for documentation, conceptual products have typically not offered that much improvement over pen and paper and yet it is here at the early stages that most of the bad decisions are made. With Dynamo and FormIt Autodesk is tackling the geometry of conceptual design and while it is taking a while to get there, the addition of analysis means the company will soon provide a suite of powerful tools to give architect\u2019s real insight into the performance of their buildings. Add Project Akaba into the mix and computers will finally amplify a designer\u2019s talent.\nDevelopments would also indicate that Autodesk is keen to drive further into the construction and post operation side of the business, although here Autodesk will be operating in markets that already have many large players offering mature point solutions. It still baffles me how Autodesk does not really have an industrial strength document management system to pull all of this together.\nRevit has seen some good software architecture improvements this revision, which is bound to be a big crowd pleaser, although core architectural functionality has only seen minor updates. Steel, Concrete and Fabrication see the best enhancements.\nLooking at Autodesk\u2019s Manufacturing division with new products like Fusion, which reside in the cloud and are clearly meant as a replacement for Inventor, I can not clearly see what the AEC team is going to do when it comes to getting Revit on the cloud, or coming up with a new code stream that leverages contemporary computer programming and computer architectures. The AEC sector is notoriously slow moving when it comes to technology change so perhaps the company does not feel the pressure to refresh, and for the majority of the market this is probably fine.\nI just can not help thinking what second generation BIM tools will look like and I regularly meet advanced users who are pushing today\u2019s software beyond to their maximum capabilities.\nHarbour views\nNew offices Autodesk\u2019s AEC team is currently centered in Waltham which is on the outskirts of Boston, in a fairly typical office building. With the changes at Autodesk HQ in San Francisco, which had an entire Pier of fabrication and workshops added for employees and local customers, the AEC division has decided to relocate to downtown Boston and have its own workshops.\nLocated in the North End area Autodesk\u2019s new building looks out onto the inner harbour, and is due to open next year. The AEC summit was held in one of the as-yet untouched spaces, which, while being just a shell, will be a really impressive space once completed. The main problem for Autodesk here is, as it is the AEC division moving in, there are many ex-architects turned employees that will want to have their say on the refit and layout! Janet Echelman Autodesk took us to see an art installation at Boston\u2019s Rose Kennedy Greenway. The work by Janet Echelman, called \u2018As if it were Already Here\u2019 is handmade from over 100 miles of tensioned, coloured rope, tied together with over half a million knots, suspended between three downtown skyscrapers. The net forms three voids, which represent the three hills that were raised to create Boston Harbour\nEchelman first came in contct with Autodesk while giving a lecture on her work. Autodesk CEO Carl Bass and CTO Jeff Kowalski were in the audience and, when she mentioned how hard it was to model the elastic motion of her work, they offered to help develop a tool to model her designs. Using Maya, Autodesk wrote a plug-in which enables her to more easily explore net densities, shape, and scale, and simulate the effects of gravity and wind.\n.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/bentley-merges-design-with-reality\/","title":"Bentley merges design with reality","date":1449446400000,"text":"From reality capture with drones to augmented reality, Bentley Systems is embracing the real world to excite the next generation of engineers and designers. By Greg Corke and Martyn Day\nBentley Systems chief executive officer Greg Bentley believes that, as an industry, we really need to start appealing to the next generation of designers, engineers and construction planners. For decades we have lost the best talent to areas like business studies and banking, which may have, until recently, appeared to be better career options.\nBut through exciting new interactive visualisation tools and slick new reality capture technologies, design and engineering has suddenly become highly visual, and enlivened.\nBentley is certainly doing its bit to capture the imagination of the \u2018digital natives\u2019. It is investing heavily in a whole number of new generation tools, from augmented reality to mobile technologies.\nLumenRT, for example, which was recently acquired from e-on software, puts immersive games-like visualisation capabilities in the hands of engineers and architects.\nMeanwhile, Bentley\u2019s ContextCapture software can generate mm accurate 3D reality models from digital photographs.\nThe AEC industry used to be about line drawings, engineering calculations and theodolites, now you can immerse yourself in an interactive virtual reality of a proposed design or pilot a drone to capture as-built site conditions.\nBentley calls this Reality Modelling and at its annual Year in Infrastructure event in London last month showed the exciting role its technology played in planning the Pope\u2019s visit to Philadelphia. Using photographs taken from a helicopter it helped ESM Productions create a reality model of 60 acres of downtown Philadelphia in just one day (read this article for more information).\nMuch of the London event was flavoured with big chunks of new capabilities in the concept side of the business. Bentley talked of \u2018conceptioneering\u2019 and how with MicroStation Connect, ContextCapture and LumenRT, it was possible to start the conceptual design and move this seamlessly throughout the Bentley software suite for continuous project delivery.\nAll too often, conceptual models have to be ditched, as they are incompatible with the documentation tools used for detailed design. Now it is possible to stay within a single common environment.\nThe first noticeable delivery of this is a product called OpenRoads ConceptStation which offers a highly interactive front end and allows designers to quickly evaluate and see design vs financial, environmental, and engineering performance. Bentley also showed that by combining AECOsim Energy Simulator and Generative Components inside AECOsim Building Designer, designers can get early stage energy performance feedback on a whole range of different design options.\nThe big, little product push was for customers to try the new Bentley Navigator Connect Edition (Windows or iPad), which has just been released. As an added incentive, each month there is a prize draw for a Microsoft Surface. Navigator Connect Edition offers collaborative model review for co-ordination, queries, drawings \/ docs and models and connects to ProjectWise.\nTalking of ProjectWise, the Connect Edition is now in General Access, with AssetWise Connect Edition coming out next year, alongside many of the modelling applications. In passing Bentley stated that two thirds of its subscribing customers use the project collaboration software.\nData mobility has been identified by Bentley a number of times as a critical component in a BIM process and the company\u2019s \u2018i-model\u2019 format is a flexible and extensible package for collating and sending a wide variety of intelligent data between project participants.\nWith so many standards in flux and so many countries chasing their own standards, Bentley explained how the i-model format combined with ProjectWise was an enabler for visual \u2018widespread work packaging\u2019 for controlled, tracked information exchange. We expect to hear more on this next year.\nDriving BIM advancement\nGreg Bentley\u2019s keynote was filled with compliments for the UK and its commitment for not only sustaining its infrastructure but also pushing BIM advancement by requiring the industry to adopt cutting edge design, construction, tools and process. With BIM level 2 targets looking like they will be achieved, Bentley said the UK ad a national consensus on return on investment for infrastructure.\nFrom many of the customer projects highlighted both this year and previously it is clear just how engaged Bentley Systems is with the major projects or infrastructure in the country.\nLondon Underground, a long-time customer, the master plan for the Olympics, Crossrail, and Thames Water, are just some of the names. Now both Network Rail and Highways England are engaged in deploying Bentley\u2019s asset management and tracking technology \u2014 and from what we hear HS2 is also being planned using Bentley\u2019s tools.\nWhile MicroStation has always been popular with London architects, Bentley\u2019s popularity in government, or government-sponsored infrastructure projects, has been a major success for the company in both document and asset management.\nGreg Bentley usually gives an update on the company\u2019s financial growth, together with some insight into usage and trends but as the company is intending to go for an initial public offering (IPO), this was not on the cards.\nHe did say that the financial markets are not ideal at the moment for tech flotations and so the company has decided to wait for the market. This suggests an IPO is not imminent but could be some time next year.\nMr Bentley wanted to reassure the audience that nothing would change and Bentley Systems would still carry on delivering and spending substantial amounts on R&D, moving the MicroStation product stack onwards and upwards.\nNaturally, MicroStation Connect Edition and the ecosystem around it was a key focus of his presentation.\nThe whole concept of \u2018Connect\u2019 is to give customers Comprehensive Project Delivery, spanning design modelling, analytical modelling, and construction modelling, through a connected project infrastructure \u2014 and now extending into management and construction execution.\nWith MicroStation as the foundation, Mr Bentley said that the common modelling environment uniquely spanned across multiple disciplines with one file format and a familiar interface.\nMany of the vertical Connect products were in the process of being released for testing. Bentley has three status\u2019 for its products: Limited access, Early Access and General Access, relating to which development phase they are currently in \u2014 the equivalents of alpha, beta and \u2018released\u2019.\nA lot of the solutions we saw were in the early access phase, graduating to general access over the next six months. For those that want to deploy the Connect Edition now, it was good to hear that it plays friendly with the previous incarnation, V8i, even happy to sit on the same PC.\nStructural\nThe big structural news coming out of London was the long awaited technology preview of Scenario Services Connect Edition \u2014 for what Bentley calls \u2018high-performance optioneering\u2019. The cloud service is all about using engineering analysis to compare and manage large numbers of different design options (scenarios).\nDesigns can be measured against a variety of \u2018performance metrics\u2019, such as vertical deflections, material utilisation or number of connections.\nThe engineer then creates a large population of alternative designs or scenarios. This can be done manually in desktop products such as STAAD Pro or automatically using Excel or Generative Components. Scenario Services then uses the power of the Microsoft Azure cloud to initiate the simultaneous analysis of all of these solutions.\nThe alternatives are then compared and contrasted through a web visualisation dashboard, with the engineer picking out one or two solutions for further investigation. Scenario Services is powerful way to explore new ideas rather than always taking the safe, well trodden path and using simulation for verification.\nBentley is also working to extend the reach of Scenario Services with a new add-on service called Structural Insights. The idea is that engineers will be able to improve designs further by comparing them against industry benchmarks or historic projects within the organisation (read this article for more information).\nElsewhere, Bentley launched its Structural Connect Editions for RAM (Bentley\u2019s special purpose building analysis and design product) and STAAD (Bentley\u2019s general purpose analyses and design product). Both of these products benefit from all the core project-sharing and collaboration capabilities of the Connect platform and also give direct access to Scenario Services.\nThe new applications include a direct link to Bentley Cloud services making it easy to associate a model with a Connect project. There is also a direct link to the new Structural Synchronizer Connect Edition, which allows i-models, complete with analytical results, to be published and shared.\nBentley is also using Connect to roll out standards with settings synchronised across all project members. If anything changes during a project, rather than sharing a new file with all team members, the project manager simply updates the project settings in Connect and the new configurations are updated. This is currently in limited access for STAAD and Autopipe.\nBentley continues to invest heavily in mobile apps, particularly for the iPad. It senses a huge opportunity to put up-to-date project information into the hands of mobile site workers and also provide the mechanisms to feed back data to the office.\nStructural Navigator, which is available for iPad and iPhone, takes the new capacities of Navigator Connect Edition and applies them to structural engineering workflows (see Making project data mobile below).\nCivils and transport\nThe big news is that Bentley has finally combined its three highway design applications: InRoads, Geopak and MX into a single MicroStation-based application called OpenRoads Connect Edition. It is scheduled for release next year and sounds like a hybrid of the three products.\nOver the past few years Bentley has been slowly bringing the three products together under the OpenRoads technology umbrella. This consolidation will come as no surprise to Bentley civils customers but it will still be interesting to see how the fiercely loyal users adapt.\nOpenRoads Connect Edition is said to be able to open pretty much any data source, offer 3D \/ plan \/ profile and cross section workflows and include storm water modelling and analysis. It benefits from the new MicroStation Connect user interface, 64-bit power, and support for functional components. There are also links to a new conceptual product called ConceptStation, OpenBridge Modeler, and LumenRT.\nTo give it its full name, OpenRoads ConceptStation Connect Edition introduces the idea of \u2018conceptioneering\u2019. According to Bentley, conceptioneering spans context capture through compelling communication of a design proposal. It brings together simplified analytical modelling and design modelling at the early conceptual stages.\nAs big and bad design decisions are made early on and cost more downstream to fix, ConceptStation is intended to help users explore preliminary design options, to assess which ones fulfil the design, performance and cost criteria.\nThen, throughout the project, users explore design alternatives through what Bentley calls optioneering, applying detailed engineering analyses to improve decision making.\nThe solution will play well with the new ContextCapture product to bring 3D meshed models of existing sites ready for re-engineering. Bentley explained that engineers will be able to evaluate designs and associated costs faster and more easily, present projects through immersive visualisation.\nIn action, the software looks very impressive and offers a visual fidelity similar to Autodesk\u2019s InfraWorks. This all points towards a very interesting competition brewing.\nIn MicroStation, Bentley has the one common environment already. However, Autodesk InfraWorks is a different code stream to AutoCAD Civil3D.\nFrom talking with Autodesk the intention appears to be to flesh out InfraWorks to replace Civil3D, but this seems to be a long way off. Bentley appears to have a more integrated plan.\nSITEOPS also has a new Connect Edition. The software is a cloud service concept and preliminary design tool for site design and is exceptionally powerful \u2014 so much so, that it appears to be powered by magic.\nIt can take site evaluations from days to hours. Simply upload a digital terrain model and enter design requirements, such as building footprints, parking requirements, setbacks, roadway parameters and other constraints.\nSITEOPS then weighs up hundreds of thousands of options, and provides the most cost-effective grading and storm water drainage plan that meets the specified design constraints.\nResearch\nOne of the regular \u2018must see\u2019 events at YII is the Bentley Fellows, who give an update on their research over lunch. For the last two years there has been a definite slant towards Augmented Reality research and we are glad to say it was the same this year, perhaps even more so.\nSt\u00e9phane C\u00f4t\u00e9, Faraz Ravi, Zheng Wu and Mark Smith gave excellent updates on what is cooking in Bentley\u2019s Applied R&D department.\nMark Smith demonstrated an Augmented Reality case where dimensions were \u2018projected\u2019 into the view of the engineer looking at a real world building fa\u00e7ade. By simply using a positioned QR code on the wall, it was possible for dimensions from the model view to be visible to the engineer. The QR code had to be in the field of view for the computer to reference the geometry, so there was some Artificial Intelligence (AI), but it needed to be precisely placed on the wall to get accurate results.\nZheng Wu showed a video on remote sensing using motion magnification software algorithms. It may have looked like a normal video but when magnified could be used to determine the load and vibration on a motorway bridge. He compared the camera results against accelerometers attached to the bridge and found the modal frequencies measures were very similar.\nSt\u00e9phane C\u00f4t\u00e9 is fascinated with the problem of getting accurate positioning in the real world for Augmented Reality (AR). When inside a structure GPS is no good. Last year he presented research into various ways to try and overcome this problem, using devices such as Apple iBeacons to triangulate internal spaces. This year he identified one solution as using a laser-enabled total station at a known location to reference. The key problem here, however, was line of site.\nMr C\u00f4t\u00e9 is also experimenting with drones. Having destroyed his first one in a fight with a tree, he had more success flying the second and produced some excellent models. He created an Acute3D model, then superimposed it over a MicroStation BIM model to compare planned building work to 4D data from the CAD system.\nFaraz Ravi joined Bentley when his Pointools company was acquired. The technology is the point cloud engine beneath MicroStation. Having a deep understanding of laser scanning, he compared photogrammetry results against laser scanned images and found the photogrammetry results from image-based meshing to be very similar to that of a laser scanner.\nObviously there are caveats where terrestrial or LIDAR scanning have advantages but it would seem that photogrammetry with a high resolution camera, in daylight, gave more than good enough results, to mm accuracy.\nConclusion\nThis year\u2019s YII was much more of a visual feast than we were expecting. With a concentration on conceptual design and Bentley putting two of this year\u2019s acquisitions into \u2018production\u2019 so quickly, the company really managed to put out a compelling argument for its single platform strategy from concept to asset management.\nWith a new website and rationalisation of products Bentley is also becoming more transparent and it is much easier to see which product does what, and what makes up the various industry portfolios. This has needed to happen for a very long time and we wonder if the planned IPO gave it good reason to spring clean and de-clutter its shop front?\nAcute3D and e-on software\u2019s LumenRT are exceptionally impressive products and great additions to Bentley\u2019s technology stack. They will transform the richness with which we can grab reality and communicate our ideas.\nFor a long time we have wondered why navigating huge, high quality rendered textured polygon models was possible in a \u00a340 game but not in engineering software, which costs thousands.\nThose days, it seems, are over because products like LumenRT really blow the doors off. Visualisation is not just about immersive real time but making it so easy that anyone can do it.\nIt is an exciting time to be involved in design and engineering software and, with applications like these, our industry looks set to become much more appealing to the next generation.\nMaking project data mobile\nBentley Navigator Connect Edition was first previewed in 2014 but it has only been commercially available since September 2015. Running on Windows, iOS, and Android (on mobile and desktop devices) it allows construction professionals on site and in the office to access, interact with and append all manner of project data.\nThe interface is optimised for touch gestures on tablet and smart phones as well as large touch screens, like the Microsoft Surface Hub, for team collaboration. As you would expect, Navigator has extensive 3D navigation capabilities but users can also search and filter model information, create visual reports, and check for clashes.\nVirtually any type of data can be placed in the spatial context of the i-model. This could be Excel data, scheduling data, materials data or estimate data, for example.\nThe software is said to be particularly hot on issue resolution where everyone can collaborate on a list of issues associated with the project. Site workers can add notes, append photographs or screen grabs and this information is synchronised and shared with the rest of the team. There does not have to be a live network connection, edits can be made offline and synced when a connection is available.\nBentley has now developed vertical versions of Bentley Navigator, putting domain-specific capabilities into the software. To date there are two versions: one for civils and one for structural. For civil design, the Bentley OpenRoads Navigator App will enable users to view, analyse, and enhance a wide variety of project information, including terrain, 3D models and 2D topographies.\nFor structural, Bentley\u2019s Structural Navigator for iPad or iPhone allows users to view or interrogate structural models sent as i-models from any Integrated Structural Modelling (ISM) enabled application, including RAM, STAAD and ProStructures. The app replaces Structural Synchonizer View.\nThere are a whole host of structural-specific features, including visualisation enhancements, where models can be viewed in plan or by storeys, and a transparency mode where reinforcements and reactions of the analytical results can be exposed. Analytical results can also be attached directly to the model.\nRaoul Karp, VP Structural and BrIM at Bentley Systems, told AEC Magazine that in the future users may also have access to some simple design tools directly inside the mobile application.\n\u201cIf you are in the field and you want to make a change or the load changes, you will be able to do a quick little design in Structural Navigator,\u201d he said.\nImmersive real time visualisation made easy\nIn September 2015, Bentley acquired e-on software, a developer of applications for \u2018simulation, and integration of natural 3D environments\u2019, which more famously have been used in the making of blockbuster films such as Avatar, Terminator, The Hunger Games and The Avengers.\nBeyond Hollywood, the company\u2019s sweet spot is displayed in its real time rendering tool, LumenRT, which can be used to easily create amazingly detailed environments that feature trees, people, water and even wind.\nLumenRT provides high-quality \u2018movie production\u2019 to architects and engineers, enabling the capture of existing conditions and providing context for proposed infrastructure designs. There are tight integrations for MicroStation, Autodesk Revit, ArchiCAD, SketchUp and ESRI CityEngine.\nThe \u2018digital nature\u2019 you can add is just insane. Depending on the time of year the fractal trees shed their leaves, as long as they are the right deciduous variety, of course.\nThe software really provides a great dynamic context for models and it is all rendered live with easy controls to adjust all the key variables, or place nature-elements, people or other agents \/ objects.\nThe really exiting thing about LumenRT is that you do not need to be a CG specialist to use it. It was designed for engineers, designers and architects and has a very simple user interface \u2014 there are only ten commands! Objects such as plants, shrubs and trees, are literally painted in, automatically sized to add variety and realism to your scene. It is incredibly easy to sketch out paths for vehicles and animated characters.\nFor ultra realistic real time vehicle simulation, it is even possible to integrate traffic data. The software has a direct interface to MicroStation Traffic and VISSIM.\nIt is also easy to share the immersive 3D experience with others. Simply package up the project as a \u2018LiveCube\u2019, and distribute the self-extracting executable to clients and colleagues.\nThe developers are working a new \u2018streaming\u2019 service called LumenRT Live that will allow stakeholders to view LiveCubes instantly on any device (tablets, smartphones, laptops) through a web browser. It uses Frame\u2019s cloud platform.\nLumenRT looks to be a hugely impressive addition to Bentley\u2019s portfolio. The screen shots seen here really do not do it justice so we would recommend you check out some of the videos.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-nbs-enhances-bim-content-revit-plug-in\/","title":"NEWS: NBS enhances BIM content Revit plug-in","date":1424304000000,"text":"Free tool allows users to search and browse NBS BIM content before dragging-and-dropping objects into Revit.\nNBS has updated its free-to-download Revit plug-in, which allows users to search and browse NBS BIM content before dragging-and-dropping objects into their designs.\nStephen Hamil, director of design and innovation and head of BIM at RIBA Enterprises, has documented the workflow on his Construction Code blog.\nThe NBS National BIM Library is a comprehensive and growing collection of BIM objects spanning all building fabric systems. It includes an extensive collection of both generic and manufacturer BIM objects authored by NBS experts to the NBS standard.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nEpic Games unveils Unreal Studio and lifts lid on SketchUp support\nAEC Magazine reporting live from Autodesk University 2015\nHexagon acquires digital twin company LocLab\nRhino 8 boosts architectural modelling\nNEWS: SimScale webinar explores natural ventilation\nNEWS: Bluebeam launches Revu Mac\nVideo: NXT BLD London conference - Arthur Mamou-Mani, Mamou-Mani\nSamsung introduces compact M.2 and mSATA SSDs\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/digital-fabrication\/embracing-digital-fabrication\/","title":"Embracing digital fabrication","date":1558310400000,"text":"It has taken almost 20 years for the building industry to widely adopt 3D modelling and espouse BIM processes in the design phase. By comparison, the move to the fabrication of buildings in factories is happening in a blink of an eye, writes Martyn Day\nThe Architecture Engineering and Construction (AEC) industry is undergoing a revolution; we live in the era in which we will likely see its complete digital transformation, from concept to fabrication. While many may think the adoption of BIM is an end in itself, moving from 2D to 3D was really only an initial phase in a longer digitisation process. It will eventually lead to computationally assisted design, automated manufacture and assembly. While this all sounds far-fetched, we have seen this happen before, in the world of manufacturing. AEC\u2019s destination is a case of history repeating itself.\nThere are firms now racing to get the benefits of digital pre-fabrication, modular in mass production of buildings in mainstream markets \u2013 houses, offices, hotels \u2013 through the convergence of many technologies and processes appropriated from the rather more mature market of industrial-scale manufacturing, such as aerospace and automotive. To add fuel to the fire, there has been a huge level of interest and investment from Venture Capital firms hoping to back the \u2018Tesla\u2019 of buildings. This has led to a massive increase in the creation and development of \u2018building factories\u2019, with new start-ups and mature players all spending billions to be the first to dominate the market.\nIn preparation for this article, I found all sorts of new wannabe building design and digital fabricators: FactoryOS, Katerra, D*Haus, Boklok, Go Modular, connect-home, Popup House, Cube Haus, Fabcab, Kiss house, Plant Prefab, Blokable, Module, Kasita, Fullstack Modular \u2013 to name but a few! Then there are the players who are already deeply invested in housing or functional buildings, like Marriott, or in the case of the UK, Berkley Homes, Legal and General and Ilke Homes. There will be at least six more factories for producing prefab buildings in the UK within the next three years. Additionally, many architecture firms have experimented with producing their own in-house modular teams\/ brands and tried some form of off-site construction with varying degrees of success and failure; many working with prefab makers in China.\nThere have always been innovators in the AEC space; few know that Thomas Edison (he of the Lightbulb and phonograph) was absolutely fascinated with concrete and realised it could be used to make cheap housing. He set up a firm around a 1908 patent for cast-in-place concrete houses made from single-poured, facades in reusable formwork. He didn\u2019t stop there; concrete roofs, partitions, bath tubs, floors, picture frames and even a piano. He lost millions in the process, but it\u2019s not a million miles away from fans wanting to see 3D printing buildings onsite. But for the scope of this article, even I deem that 3D printed buildings will be an atypical way of mass constructing complete buildings for quite some time.\nI will admit that there are cases where 3D printing has been used to great effect, such as by Laing O\u2019Rourke on the concrete panels at the new Crossrail stations. Laing used a 6-axis gantry robot with a 3D printing attachment to make 1,400 moulds for the 36,000 concrete panels.\nDigital Fabrication\nSignature architects, those pushing the boundaries of form and materials, have frequently had to look outside of the AEC industry to fabricate components, with amazing results. I\u2019m thinking of Herzog Du Meuron \/ Arup Sports\u2019 Bird\u2019s Nest Stadium for the Chinese Olympics, or many designs by Foster + Partners, Zaha Hadid Architects and others who turned to shipbuilders and their CAD systems and processes to fabricate large-scale components for buildings. Frank Gehry, who famously said he couldn\u2019t find the power switch on a computer, let alone use one, has a team of experts who take his paper models and use Dassault Syst\u00e8mes\u2019 Catia to define the structure and digitise the metal cutting so his practice can make a warped wall for the same price as a straight one. What\u2019s important here is that Gehry chose to be outside of typical providers of architectural design tools, opting for a system more commonly used in aerospace, automotive and shipbuilding because the key benefit was in the digital fabrication element.\nPrior to using this system to precisely model the structures, Gehry\u2019s buildings were seen as too high risk and cost to make, due to the complexity which generated wildly inflated quotes from fabricators. When 2D drawings were dropped in favour of explicit Catia models, Gehry\u2019s fabrication quotes all came in within 1% of each other.\nIn the mass market, there has obviously been a longstanding prefab building market and many, many attempts at modular modernist designs, but these have always been niche. Very few of these have benefitted from true digital fabrication, as they are more often assembled within a factory by builders and then shipped onsite. This offers the benefit of rapid assembly onsite but keeps the inefficiencies of manual labour.\nHowever, connecting building design to digital fabrication has the potential to change the game, with automated cutting and configuration systems to enable customers\u2019 design choices to be fed directly into the production system. The big drivers for this methodology have been in countries where timber-frame is popular, such as USA, Australia, Scandinavia, Switzerland and mainland Europe, ranging from \u2018garden offices\u2019 up to the famous German Huf-Haus and the \u2018off-the-shelf\u2019 Katerra 8 floor Office Blocks (the current tallest Cross-laminated timber (CLT) building is 18 storeys, 85.4 metre Mj\u00f8st\u00e5rnet in Norway, by Voll Arkitekter).\nThis concept is now catching fire even in countries without a strong history of prefabrication. In the UK, Legal and General and Berkeley Homes are building huge factories for the assembly of mass housing, requiring significant investment. These are just two of at least six UK house builders changing the way they will, in future, deliver residential new builds.\nThis has gone beyond the early experiment stage and is now a race for house builders to improve their efficiency and adopt more automation. It will be interesting to see if timber frame takes off in the UK, and if timber frame will be acceptable to a population that has tended to shun wooden houses. Also, will they be mock Tudor, brick clad or will people want something more modern? Developers such as Urban Splash have built a number of developments using modern pre-fab aesthetics.\nThe $4 billion \u2018startup\u2019\nWhen examining this movement to digital fabrication, it\u2019s perhaps best to look at one of the more extreme examples of a firm which is attempting to significantly change everything about the process, from the role of the developer, through the manufacture of the building, to its sale, rental and lifespan.\nKaterra is a Silicon Valley design build firm, owned by Michael Marks, Jim Davidson and Fritz H Wolff. Marks was former CEO and chairman of Flextronics (a huge electronics design, fabrication, assembly, and test company) and a former interim CEO to Tesla. The company was only started in 2015 but has since trailblazed following significant investment. It is now worth an estimated $4 billion. In just four years it has grown to over 5,000 employees and is working on $3.7 billion worth of projects in the US alone \u2013 but rumour is that it may have ten times that in its global bookings pipeline.\nThe aim of the company is to remove the inefficiencies in the construction industry by defining its own processes to handle everything from the architectural design of a building to off-site construction and installation. To do this it\u2019s relying heavily on new technology and automation in its factories, as well as for site development, schematic design, fabrication of parts and onsite construction.\nThe scale of the company in terms of mass timber is really incredible, having its own cross laminated timber factory and aiming to be one of the world\u2019s leading suppliers. In fact, owning resources has become a key part of the company\u2019s mix, having expanded from timber frame assembly, to CLT production, to steel frame and now having acquired a concrete firm too, with eyes on markets in the Middle East and Beyond.\nIn many respects, Katerra is aiming to be a \u2018Boeing\u2019 of buildings. It will offer a range of scalable and configurable residential and office designs, which can be bought and ordered \u2018off the peg\u2019, or a range of pre-configured modular parts, like floor systems which have multiple applications. The company is refining its own end-to-end production system, which utilises manufacturing-level CNC precision, together with parts management, using IoT technology to track and deliver all the flat pack assemblies and components necessary onsite, requiring considerably less labour than before and enabling completion measured in days and weeks.\nAEC Magazine attended the launch event of its new building platforms in Las Vegas in February. We saw a number of residential and office designs which had been created with manufacturing and assembly from the outset. The firm has its own integrated team of architects and engineers to define wall and floor systems, casework, bathroom, kitchen kits, including multiple configurable elements, such as finishes. Each design complies with 48 State building and energy codes, comes with a complete bill of materials and provides developers with swift feasibility, permitting and cost estimates.\nBut the company isn\u2019t just stopping at the building framework, it has completely developed its own energy efficient windows, reinvented the heating and cooling system for each apartment and developed its own AI-enabled power cabinet for the whole building. If the engineering team finds an element of a building that can be improved, they seem inclined to do it. The packaging for the bathroom when shipped to site, even forms part of the installation.\nOne of the interesting drawbacks to factory- assembled buildings is quite surprising. Katerra\u2019s dry walls contain everything when shipped \u2013 electrical, plumbing, heating, controls \u2013 for quick assembly. Each State has different rules on building inspection, but it\u2019s usual that if a wall is shipped out of State it has to be opened up during construction so it can be inspected and checked if it conforms to code. This is like buying a Ford motor car in London and driving to Scotland, only to be stopped to check its wiring conforms to UK specification. It\u2019s clear that the building regulations and inspection needs to catch up with prefabricated components.\nThe rapid rise of Katerra from 2015 to now, zero to working on $3.7 billion in projects, is just astounding. The reality is that it\u2019s still early days for Katerra, which has really only just started to properly define its processes and it\u2019s clear that a lot of what\u2019s available, should that be building components or off the shelf software, will not be up to scratch, so they will develop it themselves.\nKaterra and CAD\nWe all know the AEC market has suffered from tremendous inefficiencies. Building Information Modelling was seen as an advance on old 2D systems because the 3D model could generate all the drawings legally required with co-ordinated updates when edited. The additional benefits of renderings, potential analysis and being a central repository for all relevant data has further excited the industry.\nHowever, and there really are no two ways of saying this, all the BIM systems currently on the market were never designed to drive a digital fabrication process, to actually generate the GCODE that runs CNC machines. They have predominantly been written to address the current workflows based around documentation. The phrase Digital Twin is now being used in the context of BIM but the reality is that BIM models do not contain enough information to be a real \u2018twin\u2019, they are just geometrical representations, lacking fabrication level detail.\nTo drive fabrication machines, models would have to be created with a much higher level of 1:1 detail and accuracy, a level which would quickly kill their performance and become unusable. Today\u2019s BIM tools will not provide an end-to-end solution for any digital fabricator, who tend to use manufacturing-based tools like Solidworks or Tekla Structures.\nFrom talking to the Katerra team, the company is re-evaluating its product development technology stack and in fact has hired a host of ex Autodesk employees to join its software division. In the same vein of developing or redeveloping building equipment which it thought it could do better, Katerra launched its intention to deliver what it calls, Apollo, a cloud-based SaaS, which will be an operating platform with its own applications delivering \u2018persistent data so teams can better execute timely decisions as well as increasingly automate tasks\u2019.\nThis software provides a source of persistent data with zero loss from program inception, across design, construction, and the duration of the building\u2019s life. From what I can tell, the intention was to first develop an application, similar to Procore construction management software, for its own internal use, which will be the first offering to the development and building community, having proven it on its own process.\nVery soon afterwards, Apollo will add several other components. Apollo Insight for rapid site evaluation, automated site planning, early cost and schedule forecast, and 3D design analysis tools. Apollo Connect for material selection, design configurations and design review. Apollo Construct for construction management, budget and schedule tracking, centralised documentation.\nApollo also features an open API for further 3rd party integrations. By all accounts the launch and feature set did not go down too well within Autodesk HQ.\nCurrently the backbone of Katerra\u2019s development work has been Autodesk Revit, with models having to be exported and then fed\/remodelled through a range of other applications to get the Gcode required to drive the cutting machines. As the process has become more automated, the building types larger, and the company relying more on digital fabrication, the gap between traditional BIM capabilities and those required for manufacturing made for an interesting conversation with the design team.\nThe topic turned to CAD systems such as Dassault Syst\u00e8mes\u2019 Catia, the benefits of which had not been lost on them. In a conversation with the architectural team, the idea that you could model 1:1 with all fabrication details, not have the system slow down to a crawl and be natively able to produce GCODE to drive manufacturing, together with other downstream processes such as analysis, ERP, and PLM was an attractive thought, even if that meant having less architectural flavoured tools at the design end, or possibly using one to drive the other.\nRevit\u2019s family of parts methodology isn\u2019t that far away from the way modular works. It would be possible to have a definition of an architectural model which linked to a pre-defined assembly in Catia. In fact, London-based Facit Homes has already developed a system from which GCODE can be derived from the Revit model, for cutting on site in a shipping container although not at this scale. Bruce Bell\u2019s view of factories to make buildings offers a very different perspective. He spoke last year at NXT BLD.\nOver the past year, our contact with Dassault has increased and there is a growing interest and excitement inside the company about what Digital Fabrication means for the potential of its solutions in the AEC market. Dassault told us it is working with six or more large modular fabricators to optimise their design to fabrication processes, learning from automotive and aerospace.\nThe one key takeaway for me was that, in today\u2019s AEC world, all the concern is on digital document or model management; it\u2019s still all about PDFs and collaboration. In a digital fabrication \/ modular world, it\u2019s all about components, assemblies and process and this is throughout the fabrication and lifecycle of the building. Product Lifecycle Management (PLM), championed in the world of manufacturing, most certainly becomes a new key repository for factory-made buildings, and connections to other fabrication systems like ERP for fabrication are essential, more essential than BIM.\nDesign for Manufacture\nAnother impactful role, in the drive for automation, resides with the initial designers. If the architects have no concept of what is possible in the production process, then they can create inefficiencies or devise assemblies which increase the cost or reduce the quality of the finished building. In the engineering world, it is still possible to find design engineers who create product assemblies which the production engineers have to amend, to enable the manufacture of components.\nThere will be very few architects who fully understand the digital fabrication limitations of building fabrication systems. Design for Manufacture and Assembly (DFMa) is a separate discipline within itself and requires a holistic view of what is possible, what\u2019s available and the cost implications of early design decisions such as processes, material use and serviceability.\nWith growing use of computational tools and algorithms, it is becoming increasingly possible to produce complex geometry, which may benefit from new digital manufacture processes, such as 3D print or Knitcrete. The software could also be deployed to check designs for manufacturability, based on rules, or take a form and approximate it with the tools a digital manufacturer has.\nIn the manufacturing world this is already possible; Autodesk has developed a system which can look at a part and optimise the geometry for the intended production process. Sand casting (cheap) would look dramatically different to metal 3D printed (expensive) but both would meet the functional criteria. I envisage a future where you could simply state the material for a building structure and the geometry would change based on whether it were timber or steel.\nFlexible workflows\nThe reality is there won\u2019t be one correct way to implement digital fabrication in the building industry, because everyone currently involved is having to work it out for themselves. We will see single platform, end-to-end players like Katerra, who will develop and refine everything it needs. Then there will be firms that choose not to be software developers, relying on existing systems, a more componentised approach to design and assembly, and a network of suppliers for trusted components.\nWe\u2019ve already seen design teams create modular designs and then go off to find a fabricator to work with, knowing their skill is in the ideation and not in the fabrication. We will see factories of robots producing buildings, factories with humans producing buildings, and mixtures thereof, but the trend has to be towards full automation at some point, as it is in nearly every manufacturing industry.\nNot all factories are going to survive. As with all trends, we are seeing a glut of investment, as people are betting big on backing the big winners in a multi-trillion dollar industry. The benefit of today\u2019s federated approach to designing construction is its flexibility in being adaptive to the ebbs and flows of building cycles, booms and busts.\nWhen you have an asset like a factory, it really needs constant throughput to make economic sense. One only has to visit the old shipyards around Britain to see what happens when the orders dry up. #\nTherefore, the factories for buildings need to be as flexible as possible to shift from low-cost residential, to offices, to hospitals, to schools, to universities to McMansions. Today\u2019s digital tools, programmable robots and flexible design systems, should enable well-planned factories to be able to dynamically change to meet different economic winds.\nCAD competition\nLooking at the current focus within the CAD world, there are two standout companies that see the potential of the digital fabrication market \u2013 Autodesk and Dassault Syst\u00e8mes. Trimble does have strong capabilities in digital fabrication through the highly competent Tekla Structures but,with SketchUp, doesn\u2019t have the same depth of functionality for front-end design.\nOther traditional CAD \/ BIM software developers are not vocal within the space. Bentley Systems MicroStation and Nemetschek Vectorworks both use the Parasolid solid modelling engine, which is inside Siemens PLM NX, a CAD tool used by many automotive and Aero companies, so the potential is there. Nemetschek is also progressing with McNeel Rhino for computational design. BricsCAD has the ACIS solid modeller and has a manufacturing feature set in the same tool, so also has some potential.\nThere is also always the chance that Siemens PLM could enter the fray as DS\u2019 main competitor in manufacturing and PLM. It already owns 9% of Bentley Systems and has some co-developed AEC tools, based around factories. PTC, another big player in the manufacturing space, has had aspirations in AEC before, but is showing no signs of re-kindling its interest (the company acquired Reflex from Dr Jonathan Ingram, which it then spun off with the developers who created Revit, which, in turn was bought by Autodesk).\nFor now, we see Autodesk and DS jostling for position. Autodesk has Revit for traditional AEC workflows, Dynamo and Maya for computational generation, Inventor and Fusion for manufacturing and has stated its vision to develop for digital fabrication in the building space.\nDassault Syst\u00e8mes owns Solidworks, which is already popular in architectural component manufacturing, and Catia, which is already used by Zaha Hadid Architects and Gehry and is certainly developing something with Vinci Construction, which remains top secret. It also has a range of limited AEC tools.\nThe interesting thing here is that the strengths of these two main competitors are diametrically opposed. Autodesk is strong in AEC development to document, weak on large-scale digital fabrication. DS is strong in large-scale fabrication, engineering management, but weak in architectural design tools. Will the market opt for the manufacturing bias or the architectural design biased tools? Neither technology stack is quite perfect. This could be an epic battle between Titans.\nAutodesk makes its play\nWe talked with Robert Bray, Sr. Director Preconstruction at Autodesk, who is responsible for BIM360 Design and all offerings around pre-construction, including Assemble Systems that it acquired last year.\nIn January, Bray took ownership of a project that had been running for 18 months inside Autodesk, around the convergence of construction and manufacturing. This project, (for which they will not divulge the name), is not a commercial product yet but Bray said, \u201cWe are certainly headed that way and there is very much a need for an end-to-end solution because people today were solving it with bits and pieces. But the problem is, the data doesn\u2019t flow between them very well at all, so there are a lot of manual reflows and remodelling.\n\u201cI\u2019ve been to see customers who are prefabricating at a very large scale, they are modelling in Revit, which they are very good at, but then they are handcrafting shop drawings and bills of materials because they just can\u2019t get that level of detail out of Revit.\n\u201cThe problem I see, from an industry perspective, and the reason these firms are mainly taking a vertically integrated team approach, is because, if you\u2019re going to build something in a modular way or in a componentised way, you pretty much have to design with that intent in mind, at the start. One of the things that we\u2019ve been struggling with, and I\u2019m just going to be brutally honest with you, is we\u2019re struggling with how you reverse engineer a standard architect\u2019s model, now how do you reverse engineer modularity? You literally can\u2019t. You have a very \u2018Dirty BIM\u2019 and trying to reverse engineer modularity on that, it\u2019s an incredibly difficult problem.\nBray explained that his team are building a platform on top of Jim Awe\u2019s Project Plasma (see last edition of AEC Magazine). Autodesk has a proof of concept that\u2019s about 18 months into development, with considerable research and prototyping. The concept is that there is a design model, this could be BIM, or even a bunch of PDFs, the customer imports these into the platform and the software automatically creates what Autodesk calls \u2018system structures\u2019. These are lay out frames, a simple abstraction of all the geometry, decomposing the BIM into its elemental systems. From there, the system ingests the design criteria for the elements to be fabricated \u2013 this data will never come from the BIM model, it will be the specification, the manufacturing notes.\n\u201cThe second part of what we are doing\u201d, Bray added, \u201cis taking the parts content and using Inventor at the moment. We are using that to prototype with but we realise in the future, we need to be open. We recognise the amount of Solidworks out there. Then there\u2019s some logic as to how those parts fit together using a rules-base engine, based on logic and the captured design criteria. We could then automate the generation of a curtain wall system which knows the constraints, or in fact that could be any panelised system. From all this we generate a fabrication solution, which will produce the BOM and the shop drawings.\u201d\nI pointed out that this doesn\u2019t stop any architect designing something that can\u2019t be made or perhaps doesn\u2019t exist. Bray replied, \u201cThat\u2019s why firms that are doing this are vertically integrated, because at the end of the day, the only way this really works is if the designers are actually designing based on these parts catalogues that are robust, rich and have the fabrication of logic. So, my thinking is our initial target with all of this will likely be those already vertically integrated. That\u2019s an easy target for us. Start with early proof of concept. We can work backwards towards broader industry after.\u201d\nI raised the point of PLM being needed in the digital fabrication realm of AEC to manage the complex process. Bray said, \u201cWe need the part catalogue content, and it really needs to be managed inside of a PLM system, whether that\u2019s Fusion Lifecycle, or some future other product, I don\u2019t know yet. But there also has to be a way for this content to be shared and consumed with Revit. This is where the Plasma workflows will really come in and help us make representations of this and use it in a different context.\u201d\nBray concluded, \u201cWe are working with a bunch of firms who are looking how to optimise their process end-to-end. And we are building technology for them, not in a one-off way, but we\u2019re trying to get to a point where we feel like we have a value proposition we can offer them that solves some subset of their problems and then start working outward.\u201d\nConclusion\nIt only feels like yesterday that it seemed BIM would finally revolutionise the building industry and deliver the efficiencies which it so sorely missed. I think the reality of BIM is that it is a stepping stone to the automation and industrialisation of mass-produced buildings. In a digitally fabricated world, the 1:1 fabrication model holds more value, than a rough approximation.\nThe current generation of popular tools were all designed to enhance the 19th-century workflow of documents in federated processes. While this isn\u2019t going to go away any time soon, there are certainly rumblings in the heartland. In a digitally fabricated new world, today\u2019s BIM tools will need to be rewritten or replaced.\nDigital fabrication and modularisation bring lower cost, higher-quality, more speedily delivered solutions. We could see a dramatic phase change in the industry within the next ten years. That said, one wonders where all the talent and knowledge will come from, possible we will see manufacturing engineers enter the industry.\nWhile traditional design build is nothing new, and is frequently looked down on, I think it\u2019s a matter of when, not if, this new segment sequesters a permanent percentage of the building market.\nIn the manufacturing space, aerospace led the way, driving its supply chains and defining process. Similarly, I think leading advocates of digital fabrication will come up with best practice. In manufacturing, the software vendors learnt from aerospace and codified these processes, for all to benefit. There will be design build firms who invest deeply and define their own processes to make them Industry leaders. I suspect again the traditional CAD vendors will learn, codify and popularise best practice. Until then everything is going to be a little bit \u2018Heath Robinson\u2019, getting tools to drive systems they were never intended to work with.\nZaha Hadid Architects and Catia\nZaha Hadid Architects (ZHA) is renowned for using a plethora of design tools: Revit, Maya, Rhino, Grasshopper etc., and whatever digital skillsets are in each team determines the toolsets used.\nIt\u2019s perhaps best known for being one of the early adopters of \u2018Digital Project\u2019, an architectural flavour of Catia V5 which Gehry Technologies created.\nDigital Project was used to design the incredibly complex Guggenheim Museum Bilbao because the software was able to handle huge, complex geometrical forms and send data to be directly manufactured from. In 2014 Trimble bought Gehry Technologies.\nCatia persists in ZHA today although in the form of what Dassault Syst\u00e8mes (DS) calls the latest version, the 3D Experience Platform (3DX). At a \u2018BeyondBIM\u2019 event in London earlier this year, held by DS reseller Desktop Engineering, Cristiano Ceccato, associate director at ZHA gave an example of how modelling in precise detail enabled the architects to convince the construction firm for the Danjiang Bridge in Taiwan to trust their innovative design and enabled distributed collaboration based on a common platform in the construction process.\nThe topic of collaboration was key for Ceccato and it\u2019s not just about sharing models, but tracking changes, tracking issues and having all the information \u2018unequivocally\u2019 in the cloud.\nAlso at the event, Michael Sims, lead designer at ZHA, gave a demonstration of how teams use cloud-based 3DX in R&D to create a BIM Level 2 + workflow. Given the complex geometry and detail of all ZHA buildings, the speed and power of Catia was self-evident. Sims admitted the learning curve was \u2018a bit steep\u2019 leaving the file-based world. He also explained how ZHA works very early on with engineers and fabricators to establish the limits of the design shape and how Catia is that common language.\nFrom talking with the ZHA speakers, there seems to be a feeling that the practice does want to standardise on a single platform with one version of the truth, which easily links to fabricators and engineers with collaborative components. The idea is very compelling.\nDigital fabrication at NXT BLD on 11 June\nIf you\u2019re interested in digital fabrication and the topics raised by this article learn more at NXT BLD in London on 11 June. You\u2019ll hear from some of the early adopters, including Katerra.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-marianna-kopsida-trimble\/","title":"Video: NXT BLD 2018 London conference - Marianna Kopsida, Trimble","date":1530144000000,"text":"Beyond crypto: Digital transformation in construction through blockchain technologies \u2013 NXT BLD London, June 2018\nTrimble researcher, Dr Marianna Kopsida, talked about Mixed Reality workflows in progress monitoring and inspection in construction. Building sites can be captured in 3D by laser scanner or drones. IoT provides material variables, GPS gives the position of machinery. These can be pulled together to enable AR or VR experience to help decision making. Dr Kopsida examined solutions to collaborate within Augmented Reality, as well as the benefits of merging BIM data live onto as built for quality checking and schedule planning.\nMicrosoft\u2019s Hololens was not designed with construction in mind so Trimble has worked with Microsoft to produce a hardhat variant enabling it to be used on construction sites. However, the MR headset does not work well in bright sunlight so Trimble has devised a handheld AR solution, called SiteVision, which has super-accurate GPS and allows the site design model to be overlaid on\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-andrew-watts-newtecnic\/","title":"Video: NXT BLD 2018 London conference - Andrew Watts, Newtecnic","date":1530144000000,"text":"Future Technologies for Architecture, Engineering and Construction (AEC) \u2013 NXT BLD London, June 2018\nAndrew Watts explained how Newtecnic uses tools to lightweight concrete designs, optimise for manufacture and then, through a series of optimised dashboards, keep control of incredibly complex construction processes, while always knowing the cost and optimising for assembly time.\nWatts doesn\u2019t see buildings being made in factories; he believes they will be a mixture of site-based and factory as not all parts benefit from being pre-fabricated. Looking ahead, he also is working on a projects which will be monitored and maintained by drones!\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nDipa Joshi, Director of Assael Architecture\nSmart cities & emerging technologies: Cutting through the noise.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/lenovo-workstations\/","title":"Lenovo workstations","date":1196640000000,"text":"This month Lenovo unveiled its first ThinkStation line of workstations. The company partnered with long term customer, the AT&T Williams Formula One team, for the launch event. John Marchant reports.\nOn November 6, 2007, Lenovo launched their new ThinkStation line of workstation PCs in conjunction with the AT&T Williams Formula One team at the RBS Williams F1 Conference Centre. This is the first time in ten years that a major PC vendor has entered a market that the likes of Dell, HP and Sun have largely had to themselves.\nAlthough the workstation market is small compared with the laptop and business desktop market, growth in Europe is around 10% pa compared with 6% or so for servers. Lenovo is setting its sights on winning a substantial share of the sector and is already collaborating with 26 leading independent software vendors to certify seamless hardware and software interaction. Alex Burns, chief operating officer, Williams F1 is already impressed, \u00fdThe new workstations will help change the way our engineers work with faster interaction and design simulation and we are evaluating them for integration into our operations.\u00af\nLatest Intel technology\nTargeted at professionals in the computer-aided design\/engineering, digital content creation, oil and gas exploration and electronic design automation segments, the Lenovo ThinkStation S10 and D10 workstations are built on the very latest technology, the first Intel processors based on 45 nanometer technology, nearly doubling the density of transistors on the chips compared with previous Intel processors. The dual processor ThinkStation D10 workstations use the fastest Quad Core Intel Xeon processor 5400 series, while the single processor ThinkStation S10 workstations use the latest Intel Core 2 processors including the Intel Core 2 Extreme processor QX9650. The new workstations can take two of the latest high-end GPUs from Nvidia and come equipped with dual Gigabit Ethernet and multiple slots, bays and USB ports.\nLenovo has designed the workstations to comply with the Environmental Protection Agency\u00dds Energy Star 4.0 requirements for energy usage with 80% efficient power supplies. Additionally, they contain more than 50% recycled plastics content.\nThe workstations also incorporate a removable top handle for moving the workstation as well as a side cover latch for easy access to the system. The workstations are claimed to have the same acoustic noise levels as the standard Lenovo desktop PC.\nThe workstations also come equipped with a range of ThinkVantage technologies including Rescue and Recovery, Client Security System and Image Ultra Builder and Lenovo will offer a variety of storage, graphics adapters, memory and rail kits for the workstations.\nThinkStation workstations should be available starting in January through Lenovo business partners and www.lenovo.com. Pricing for the ThinkStation S10 workstation will start at around \u00fa600 and the ThinkStation D10 workstation will start at around \u00fa900.\nWilliams F1 and Lenovo\nFor those of you who have been on a desert island for the last 29 years, AT&T Williams is one of the world\u00dds leading Formula One teams, with 16 FIA Formula One World Championship titles and 113 Grand Prix victories to its credit. Williams F1 is based at a 40 hectare technology campus in the heart of the UK\u00dds motorsport valley in rural Oxfordshire. Williams F1, which was formed in 1978 and is privately owned by Sir Frank Williams and his long-term business partner, Patrick Head, designs, manufactures and races Formula One cars and employs some 520 people, 80 percent of whom are involved in design, manufacturing and race operations. Also, for those who don\u00ddt know already, Lenovo, formed by Lenovo Group\u00dds acquisition of the former IBM Personal Computing Division, develops, manufactures and markets technology products and services worldwide with major research centers in Yamato, Japan; Beijing, Shanghai and Shenzhen, China; and Raleigh, North Carolina.\nBrand awareness and technology partnership\nAt the beginning of the 2007 race season, Lenovo announced its support as an Official Partner of AT&T Williams. F1 is a computing-intensive endeavour and is highly dependent on information technology, with the team using Lenovo PC technology in every facet of its business. The company uses around 400 Lenovo PCs \u2013 approximately 130 notebook PCs and 270 desktop PCs. Although Lenovo\u00dds sponsorship of the team is about brand awareness, it is also about sending the Lenovo message directly to its customers. Williams, though sees it as more of a technology partnership, with workstation technology regarded by both Lenovo and Williams as similar to Formula One in the way it pushes the boundaries of technology with an eventual trickle-down into general use.\nLooking for improvement\nAccording to Chris Taylor, IT Manager at Williams F1, \u00fdIn Formula One, the car changes completely every year. In fact, we design some components specifically for use for only one race at one circuit and workstation performance can be a limiting factor for us when we are redesigning parts to such tight deadlines. When we moved to dual core workstations, we got a real improvement in productivity, allowing us to run certain CAD and CAE applications at the same time, really compressing our design times. We are looking to the new ThinkStations to provide a further significant improvement.\u00af\nSupercomputing power for aerodynamic simulation\nLenovo and Williams have also been working together on the development of a supercomputer to run CFD simulations of airflow around a virtual model of a three-dimensional racing car to help predict how changes in component shape and placement will affect drag and downforce, with resulting impacts on speed and handling. \u00fdAerodynamics plays a critical role in determining how competitive we are for each of the race circuits we visit,\u00af according to Alex Burns. The team uses the supercomputer to examine numerous aerodynamic variables, such as surface geometry, wheel turbulence and track surface. With a peak performance of eight teraflops, the supercomputer, based on a cluster of 166 Lenovo R520 SuperServers, is four times more powerful than the team\u00dds previous solution. This enables the team to run aerodynamic simulations approximately 75 percent faster than before.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nvidia-launches-quadro-rtx-for-mobile-workstations\/","title":"Nvidia launches Quadro RTX for mobile workstations","date":1559001600000,"text":"All of the new GPUs are VR Ready and are heavily focussed on ray trace rendering\nNvidia is bringing its RTX ray trace rendering technology to the mobile workstation platform with a new range of Turing-based GPUs, the Quadro RTX 5000 (16GB), RTX 4000 (8GB) and RTX 3000 (6GB). The new lineup will be available in mobile workstations from global OEMs like Dell, HP and Lenovo.\nAll of the new models are VR Ready, look well suited to real time visualisation and are heavily focussed on ray trace rendering. They feature dedicated RT (ray tracing) cores and Tensor cores, which are used for AI denoising. So instead of having to wait for the GPU to compute thousands of passes, it can conduct a few passes and then use deep learning to remove the noise. It essentially gives a best guess as to what a fully resolved image would look like.\nTo take full advantage of the new mobile GPUs, applications have to be RTX-enabled and we expect this to come in the next releases of V-Ray, Unreal Engine, Enscape, Autodesk VRED, Solidworks Visualise and many other pro viz applications.\nNvidia has also released four additional new mobile Quadro GPUs, the Quadro T2000 (4GB), T1000 (4GB), P520 (4GB) and P620 (2GB) GPUs. None of these models feature RT or Tensor cores so will not support RTX technology, but they should be well suited to 3D CAD \/ BIM and entry-level real time viz workflows. Nviida has also introduced the RTX Studio laptop, a new brand of high-performance laptop purpose-built for GPU-accelerated content creation. RTX Studio laptops will be available from seven manufacturers and feature specialised Nvidia Studio Drivers for creative apps, including those from Adobe, Autodesk (3ds max and Maya), Avid, Blackmagic Design (DaVinci Resolve), Maxon, Unity and Epic.\nRTX Studio laptops are powered by a range of GPUs, including the Quadro RTX 5000 and GeForce RTX 2080, 2070 and 2060. Many of the new laptops will also feature Nvidia Max-Q technology, to enable \u2018incredible performance in sleek, thin and light designs\u2019.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/kohn-pedersen-fox-explores-the-omniverse\/","title":"Kohn Pedersen Fox explores Nvidia Omniverse","date":1605139200000,"text":"In the run up to Autodesk University 2020, Greg Corke caught up with Cobus Bothma, director of applied research at KPF, to find out why he\u2019s so excited by Nvidia\u2019s new virtual collaboration platform, Nvidia Omniverse.\nThe future of architectural design will rely on the collective accessibility of all design data and geometries, in one accurate visualisation and simulation application. Those are the words of Cobus Bothma, director of applied research at global architectural firm Kohn Pedersen Fox (KPF) who has been exploring Nvidia Omniverse, the new virtual collaboration platform which places a big emphasis on real-time physically accurate visualisation.\nBothma is excited by how the platform could help bring together KPF\u2019s global offices to work simultaneously on projects. \u201cIt shows great potential to allow multiple contributors, from across the entire design team using an array of applications, to collaborate effectively \u2013 wherever they\u2019re currently working.\u201d\nNvidia Omniverse is built around Pixar\u2019s Universal Scene Description (USD), an open source file format with origins in visual effects and animation. With the ability to include models, animations, materials, lights, and cameras, it can be used to share a variety of viz-focused data seamlessly between 3D applications.\nIn the AEC space, Nvidia Omniverse works with several 3D design tools, including SketchUp, Revit, and Rhino, and viz tools 3ds Max, Unity, Maya, Unreal Engine, Houdini and (soon) Blender. Importantly, it replaces traditional file-based import \/ export workflows, with data flowing freely from each 3D application via a plug-in \u2018connector\u2019, which creates a live link to the Omniverse \u2018nucleus\u2019.\nOnce the link is established, and the initial model is synced, the connectors only transmit what has changed in the scene, allowing everything to be \u2018real time and dynamic\u2019. Move a wall in Revit, for example, and it will update live in Omniverse, along with any other connected application. It means teams can use whichever tool best suits the design or modelling task at hand and switch seamlessly between them.\n\u201cWith this sort of technology, and the way we are connecting the tools together, it changes our view of the workflows,\u201d says Nicolas Fonta, Sr. Product Manager for AR\/VR\/MR at Autodesk, who sees Omniverse as a way of breaking down the barriers between Autodesk products and others. \u201cIt\u2019s no longer a waterfall file conversion from one tool to another down the line.\n\u201cAll of those tools are sharing this common representation of the project. Omniverse is one view of it, which is beautiful, but as you\u2019re tweaking things in Revit, it\u2019s one common project that\u2019s sitting on this USD foundation that is exchanged between the different tools.\n\u201cIt\u2019s no longer about \u2018I need to take my Revit file, send it to Omniverse, send it to [3ds] max\u2019, they\u2019re all just the common representation.\u201d\nAggregated models can be viewed in Omniverse View, an Omniverse App with a toolkit designed specifically for visualising architectural and engineering projects using physically-based real-time rendering with global illumination, reflections and refractions.\nView includes libraries of materials, skies, trees and furniture, and painting tools to scatter large amounts of assets like trees and grass. Dynamic clouds and animated sun studies are also included, along with section tools. More functionality will be added as the platform evolves and it\u2019s also possible to create custom features using C++ or Python extensions.\nImportantly, to help speed decision making at any phase of the design process, Omniverse View allows all project participants, and not just users of 3D authoring tools, to navigate the model, as well as modify and render content.\n\u201cThings happen much quicker and rapidly in the boardrooms, in the meeting rooms, at design phases, especially now,\u201d says Bothma. \u201cWe get to a point where things have to happen within a click. And this is kind of one of those things that does allow that to happen.\u201d\nKohn Pedersen Fox\nKPF is one of Nvidia\u2019s \u2018lighthouse\u2019 accounts and has been exploring the potential of Omniverse for several months. However, as the technology is currently in beta and, as Bothma explains, undergoing a \u2018rapid rate of development\u2019, it\u2019s not currently being used on live projects.\nTo demonstrate how it could help optimise KPF\u2019s collaborative workflows, Bothma showed AEC Magazine an example from its 52 Lime Street project, also known as \u2018The Scalpel\u2019, a striking new office tower in the heart of the City of London.\nRhino (with Grasshopper) was used for the design of the canopy and immediate landscaping; 3ds Max to provide some context using a section of KPF\u2019s London model\u2019 and Revit for the construction document model, including the fa\u00e7ade and full internals of the 42 storey building. \u201cThe Revit file could be sitting in a London office and the Rhino model could be in the New York office, says Bothma.\n\u201cWhen you publish, it starts collating automatically on the server side and it does it quite cleverly where it actually names the file names and the application, and then the .USD, so you always know where the source was coming from, and you can keep this up to date.\u201d\nNvidia Omniverse \u2013 further reading\nVisual intelligence\nNvidia Omniverse isn\u2019t just about optimising the flow of design data to improve interoperability and team collaboration. Bothma believes there\u2019s a huge value in giving teams much earlier access to models rendered using an accurate physical representation of light, \u201cIt\u2019s getting to see exactly what the project is going to look like much quicker, from probably applications that didn\u2019t always play nicely together.\u201d\nScenes can be viewed in Omniverse \u2018View\u2019, which includes a real-time ray tracing mode as well as an RTX path traced renderer, which can be accelerated by multiple Nvidia RTX GPUs and coupled with \u2018physically accurate\u2019 MDL materials and lighting.\nThe path traced stills that Bothma produced for his presentation are quite simple but were rendered in seconds using a workstation with a single Nvidia Quadro RTX 6000 GPU. \u201cWe didn\u2019t do much on materials, we didn\u2019t do much on lighting, I literally opened up the sun, dragged the slider a little bit, and then screen grabbed it, and that\u2019s what you kind of get straight out of the box as a designer\u2019s working model.\u201d\nBut Bothma says there is potential to take things much higher, describing the render quality from Omniverse as \u2018top of the line\u2019, also noting that it works with Substance Designer for advanced materials. \u201cWe can now actually use Substance Designer with Revit and Max and Rhino at the same time, in Omniverse environments.\u201d\nBothma envisions Omniverse scenes could become dynamic assets used throughout the entire design process. \u201cThis would be where I\u2019d do my final renders, this would be where I do design discussions, possibly streaming in the future, testing variations, looking at shadow studies, whatever the case may be \u2013 even running my animations and so forth, that\u2019s kind of where you want to go.\u201d\n\u201cWe don\u2019t like hopping around too much, so the idea is really like what some of the CG and VFX companies are doing; they\u2019re running into one pipeline where all the data and the geometry is aggregated and that\u2019s the pipeline that then will produce the images \u2013 at whatever level you want in whatever format you kind of want.\u201d\nStreaming to any device with Nvidia Omniverse\nOmniverse View can run on an Nvidia RTX powered desktop or mobile workstation, allowing users to get an interactive viewport into the shared scene. However, the real power of the platform comes into play when using Nvidia RTX Server, a reference design available from a range of OEMs with multiple Nvidia RTX GPUs.\nRTX Server can perform multiple roles. It not only provides buckets of processing power for path tracing, but with Nvidia Quadro Virtual Data Center Workstation (Quadro vDWS) software users can access the Omniverse platform using GPU-accelerated virtual machines. It means all collaborators can view projects in full interactive ray traced quality on low powered hardware \u2013 be it a laptop, tablet or phone.\nFor Bothma this is one of the most exciting developments, and he\u2019s looking forward to being able to stream Ominverse projects live to any lightweight device. \u201cWe can\u2019t put a big [GPU-accelerated] machine in everybody\u2019s face, the whole time,\u201d he says.\nOne might presume that Bothma sees this an opportunity to improve communication with clients, but this isn\u2019t the priority. \u201cWhen I look at technology and the use of it at KPF I always look at team collaboration first, team communication, and the second thing is clients,\u201d he says.\n\u201cWe don\u2019t have a top down design necessarily, we enable everybody to give their feedback and design input, and so forth,\u201d adding that Omniverse streaming will allow principals, directors, or project managers that are not au fait with 3D applications, to look themselves. \u201cIt\u2019ll give them the ability to go and say \u2018well if a designer says the sun is going to shine on this, for example, and the glass is not going to be too blue\u2019, they\u2019re going to be able to say \u2018well let me have a look quickly and open my iPad and go in, adjust the sun go look at it.\u2019\u201d\n\u201cDemocratisation of technology in the industry is one of the big things we\u2019re driving right now, how to get more people to use the technology without having to be coders and scripters or visualisation experts. And we\u2019re seeing it happen,\u201d he says.\nBothma has several options on where to host Omniverse. To date, most of the testing has been done using on-premise hardware, but AWS [Amazon Web Services] is also an option. \u201cI know some other companies \u2013 Woods Bagot, for example \u2013 have been doing some testing on the [Omniverse] View side on AWS with quite good results,\u201d he says.\nWhichever route KPF ends up taking, the Omniverse nucleus will certainly be centralised and maybe imaged in one or two locations. \u201cThat\u2019s the real benefit to us, for someone in Shanghai to be able view this model that\u2019s been published by three of our other offices and be able to see it the same as we see it,\u201d says Bothma. \u201cAnd they don\u2019t have to wait for large files and [wonder] \u2018do we have the right version of whatever software was used in the past.\u2019\u201d\nNvidia Omniverse \u2013 beyond viz\nAlways on the front foot, Bothma is already looking beyond core AEC workflows and thinking about where Omniverse could take KPF in the future.\n\u201cIt has the capabilities to do a lot more than just architectural collation and so forth \u2013 robotics, and a whole bunch of other things,\u201d he says. \u201cAnd those things are going to be very appealing to me because we\u2019re going to start looking at not just how we look at a building from its visual aspect, we might want to start looking at how we construct a building, and that might be even with robotics, and the use of machine learning and so forth. And, and, and\u2026\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/researching-the-future-of-aec\/","title":"Researching the future of AEC","date":1252281600000,"text":"Salford University is one of the leading international research institutes for Built Environment technology. The press was recently invited to visit its impressive facilities and gain an insight into the types of projects and research its academics and students undertake. Martyn Day reports.\nIt seems like only yesterday the industry was making the slow and painful move from drawing boards to 2D CAD systems. A migration to 3D was just a pipedream and, while the industry knew that the mode of working was highly inefficient, nothing was being done about it. Yet in a very short space of time, the breadth and scope of 3D technologies available to architects and those working in construction has mushroomed, with a corresponding increase in appetite to understand and deploy this new thinking.\nOne of the driving factors for this U-turn is the extreme challenges the industry is facing, which have never been more serious or complex. The economic downturn has put tremendous pressure on firms to cut costs and find new work, often for reduced fees. It may have been more than 10 years since Sir John Egan published his Rethinking Construction report, which called for radical improvement within the construction industry, but it would appear that painful economic conditions have been more persuasive in getting the industry to evaluate better ways of doing its business.\nMany of the new 3D technologies are still embryonic, with best practice and processes to be fully worked out. Enter Salford University and its futuristic virtual environment centre for research into interaction and collaboration, called THINKlab. The institution has been engaged by commercial and academic customers to look at virtual urban planning, regeneration and digital design. Based in the north west of England, it has strong contacts with local businesses, together with the wider academic research community. The recent press open day was an outreach to tell the story of Salford University\u2019s Built Environment research and explain a little about some of the projects the team has been working on.\nAs with nearly all British Universities, Salford campus is rather unremarkable with a mixture of 1960s and 1970s structures. There was little to prepare one for what had been done to the seventh floor of the Maxwell building. Walking out of the lift was like entering something from Star Trek with walls of lights, curved surfaces and glassed-off work areas. The THINKlab is a dedicated space where the Built Environment faculty has created a laboratory for research into technology-enabled collaboration and virtual technologies. Presentations were given in the THINKpod, an even more futuristic meeting room within the lab that bristles with the very latest in high resolution projection, tracking and immersive stereoscopic hardware.\nWe were welcomed by Professor Peter Brandon, director of the THINKlab. According to Brandon, the THINKlab is a place to challenge the existing and prevailing orthodoxy, to find new ideas and the best technology to suit our needs, which could lead to a new era of enlightenment. Brandon highlighted advances in instant communication, knowledge-based systems, better integration of systems and the removal of geographical boundaries as being driving forces to provide new machine-based support for the creation of knowledge.\nSalford University\u2019s vital statistics are impressive. Originating from the merger of multiple departments, the School of Built Environment was started in 2006 and now has 1,350 undergraduates and 309 postgraduates from 57 countries (175 of whom are doing PhDs). In total it has 69 staff, including 22 professors and 37 external visiting professors. Since 1993, Salford has been the base for the Foundation for the Research Institute of the Built and Human Environment and is the education partner for the government-sponsored Center of Education in the Built Environment (CEBE), as well as being the Centre for Construction Innovation, which links to 15,000 local firms. With a special focus on research, over the last five years Salford has brought in over \u00a320 million in construction-related research funding.\nThe key areas of thought leadership that THINKlab is engaged in are wide and varied, and cover the threat of global warming, sustainable development, improved performance, changing social patterns and the change in the way built assets are managed. On pure technology development, expertise is being developed in Building Information Management, Virtual Environments, Future Workspaces, Off-site fabrication, distributed intelligence and procurement.\nFollowing the introduction we were given a series of presentations by faculty members on a selection of the diverse range of projects that they are currently involved in.\nOptimal learning spaces\nCarl Abbott explained how Manchester City Council had asked for the latest ideas on the optimal design of primary schools. A team of experts from the built environment faculty at the University worked with the council on all the key areas that influence learning, covering everything from air quality, use of light, colour, layout, plants and acoustics. The net result will be new designs for places that assist the learning process taking into account physical and social criteria.\nE-Readiness and IT maturity\nProfessor Mustafa Alshawi examined the current process and implementation of Building Information Modelling (BIM) systems that use a master model concept, together with the sophistication of other business systems such as Enterprise Resource Planning (ERP). According to studies as much as 80%-90% of IT investments do not meet their performance objectives. Prof Alshawi has designed a program to measure a company\u2019s \u2018e-Readiness\u2019 prior to adopting process centric applications, checking the people, the organisation and the process to hopefully ensure a higher degree of success.\nWith an estimated market of \u00a32 billion, rising to \u00a320 billion in the next five years, a prefabrication research engagement was also highlighted by Prof Alshawi. ManuBuild, which makes pre-fabricated buildings, is looking to bring the building industry into the industrialised age by manufacturing pre-fabricated buildings and delivering them onsite. The development of training project simulators and dashboards using PDAs for project managers was explored, concentrating on the kinds of dynamic decisions that are frequently made on site.\nFuture collaborative workspaces\nUsing all the power of the THINKpod, Professor Terrence Fernando took us into a Virtual Workspace to see the research work on co-located, distributed and mobile collaboration.\nIn one live urban planning scenario, a user could physically interact with a 3D displayed city model. While at first we could see him as a video feed within the environment, Fernando then enabled the extraction of 3D data from this video, actually bringing in a 3D representation of the user. While quite jagged, the next phase involved mapping the user\u2019s face to a human head mesh. This is the closest you will get, for the time being at least, to living in the \u2018Matrix\u2019.\nInteractive learning\nOver the past twelve months Salford has been working closely with the Specialist Schools and Academies Trust to produce interactive resources for 14-19 diplomas.\nBuilt on games technology the researchers have come up with engaging and intuitive scenarios that back up traditional classroom work, together with matching employer needs. In the example shown, a student can navigate around a building site, visit the offices, read health and safety information, see schedules and examine the machinery. There is even a tutorial on how to lay bricks and make different patterns.\nBIM and construction\nThroughout many of the presentations it was clear that Salford has access to all the latest CAD tools, together with some of the very high-end solutions such as Gehry Technologies\u2019 Digital Project, which is based on Dassault Systemes\u2019 Catia engine.\nDr Tuba Kocaturk, programme director MSc Digital Architectural Design, gave a great insight into the wide scope of work undertaken, examining best practices in BIM and being at the cutting edge of free form generative modelling.\nSalford University recently held a symposium on generative design, which attracted major architects from around the world. While the University only offers postgraduate courses at the moment, there are firm plans to offer degrees within a few years. Looking at the exemplary focus on digital design, Salford could become a real centre of excellence here.\nResilient homes\nProfessor Erik Bichard worked with the Environment Agency to study the needs for improved flood protection and energy conservation. One initiative is to offer households rewards for installing insulation and lowering power consumption by offering free local transport, fruit and vegetables, landscape gardening and tickets to sporting events.\nConclusion\nWhile these presentations were only a small selection of the ongoing projects that Salford University is undertaking, the sheer breadth of topics was impressive. The two stand-out sessions for me were the immersive collaborative workspaces and the digital architecture overview. While Virtual Reality has become a bit of an overused term and a technology that has never really made it, the THINKlab demo proved that there are real applications now that design data is becoming increasingly 3D.\nTo be actively teaching and engaging with the Generative design family (Bentley Generative Components, Rhino Grasshopper, Gehry Digital Project), pushes Salford to only a handful of institutions in the world that are really at the cutting edge of digital design. Students with exposure to this technology will be a in a very small band of in-demand architects that get experience with the most expressive design systems under development. I really look forward to hearing more about the degree course Salford is planning.\nSalford University obviously has a huge focus on research for the Built Environment. The dedication, enthusiasm and resources available obviously attract a lot of investment. The facilities not only look out of this world but are also the best I have seen at any university or company for that matter. Firms can use the THINKlab and THINKpod for collaborative design sessions or meetings and can engage with the Built Environment team to run their own research projects.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-topcon-announces-seminar-tour-for-ln-100-layout-navigator\/","title":"NEWS: Topcon announces seminar tour for LN-100 Layout Navigator","date":1430870400000,"text":"Attendees to get hands on with new dedicated 3D layout tool for buildings, foundations, structures, HVAC, and general site layout.\nTopcon Positioning Group is embarking on a tour of the US and Canada to give construction professionals the chance to get hands on with its new LN-100 Layout Navigator. The one-operator system, which blends Topcon\u2019s self-levelling laser and robotic total station technologies, is designed to offer a new way to perform construction and BIM layout functions.\n\u201cWhether it\u2019s traditional construction, complex landscape layout, or quickly checking existing stakeout locations on a project \u2014 the LN-100 allows an operator to use the simplicity of a self-leveling laser and combine it with a reliable 3D layout technology to increase productivity on a job site,\u201d said Brice Walker, vice president of survey sales. \u201cThese seminars break down the basics of using the LN-100 and demonstrate the simplicity it offers to every day users on job sites.\u201d\nThe program is divided into two free half-day sessions that include lunch, with continuing education credits available. The second day focuses on advanced workflow techniques, with incorporation of Autodesk Point Layout in Revit, AutoCAD and Navisworks for model-based job site layout, as well as quality assurance and quality control using the latest technology.\nNine cities remain on the tour with stops in Atlanta, Georgia on May 20 and 21 followed by Washington, D.C.; Philadelphia, Pennsylvania; Pittsburgh, Pennsylvania; Columbus, Ohio; Chicago, Illinois; Phoenix, Arizona; Vancouver, British Columbia; and Portland, Oregon.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/bricsys-digital-summit\/","title":"Bricsys Digital Summit","date":1606089600000,"text":"The DWG-based BricsCAD BIM has deservedly earned the reputation of the most rapidly developed design authoring tool in BIM. Martyn Day reports from the Bricsys Digital Summit on the latest features in the V21 update, plus other news\nWhile 2020 has been the oddest of years, the Internet has managed to save us from sitting at home wondering what to do with ourselves. All the major software developers have moved their conferences online, and complex new products have continued to be released, on time, even though we can\u2019t find toilet paper, flour and dried pasta in our local stores.\nBricsys Digital Summit provides five hours of design technology entertainment and they even have a house-band of employees that occasionally pop in to entertain.\nBefore I jump into what was shown, a little about Bricsys. Based in Ghent, Belgium, the company has been developing since 2002 and the founder, Erik de Keyser sold the company to the process plant division of Hexagon in 2018.\nHexagon had previously ported its AutoCAD-based plant application onto BricsCAD and were so impressed they bought the company. Bricsys continues to be a brand within the Hexagon family and actively markets and sells BricsCAD and its vertical products.\nThe best way to think about BricsCAD is that it\u2019s a 2D\/3D DWG-based CAD platform that has a Swiss Army knife of vertical applications built on top. For people who need a draughting tool, it\u2019s a powerful CAD system moulded to replace AutoCAD, and even supports the development APIs ARx, Lisp etc.).\nFor architects and engineers, it\u2019s a design-centric BIM modeller with automated associative 2D take offs, with support for generative design with McNeel Grasshopper and real time viz with Enscape. For civil engineers, as of last year, it now has terrain\/grading tools, road and pipe design. For mechanical engineers, it\u2019s a full 3D ACIS-based solid modelling application with amazing sheet folding capabilities. It also handles point clouds, VR, runs on Mac, Windows and Linux and comes with a perpetual licence or on subscription and in single or network licence configurations. There is also a cloud collaboration document manager with automated workflows.\nAs AEC Magazine is primarily about BIM, it\u2019s worth explaining why BricsCAD BIM is a little different. In products like Revit and ArchiCAD, users select components to model their designs. Each of these components is predefined and comes with many options that need to be configured. This is the \u2018Lego\u2019 approach to modelling.\nBricsCAD BIM presents the designer with a blank sheet and starts by modelling the envelope of the building using solids. This can be expressive and freeform and the designer does not have to worry about anything except sculping the form. For openings, simple Boolean commands punch holes. Invoking the \u2018BIMify\u2019 command then sets the AI loose to automatically recognise walls, windows, slabs, columns etc. and automatically assigns IFC tags to each. From there on the refinement can continue.\nThe software also offers a whole host of other tools to rapidly get from a solid model to a BIM model. The one thing that Lego CAD gets wrong is sweating the details too early in the process, where what needs to happen is the act of experimentation and design.\nThe digital summit\nThis is the third year in a row that we have followed the developments from Bricsys. Each year has brought leaps in expanded functionality. 2020 is no different.\nThe foundation for all Bricsys tools is BricsCAD. While it started out as an AutoCAD clone, it has surpassed that functionality and has been on its own trajectory for a number of years. While it\u2019s based on DWG, the DWG engine is designed for modern processors and is multithreaded, so it\u2019s fast.\nThis year BricsCAD has had improvements to image handling, meshes, direct modelling, conversion from mesh to solids, auto mapping of layers in a drawing to those set out in a reference drawing and DGN export added (it already did DGN import).\nThere have been some truly amazing parametric features added \u2013 it can make 2D or 3D parametric blocks simply by selecting geometry and auto-constrains the geometry, giving the user control over which dimensions can be parametrically controlled. At the virtual event, this was shown on a 3D i-beam structural model. One beam was selected, auto constrained, and the software then found all the other elements in the structure that were the same and turned them into parametric 3D blocks.\nSometimes you import a drawing from hell that has an exploded polyline that is made up of many smaller vertices \u2013 they are pigs to edit and they make files huge. A demonstration was given of a new \u2018Simplify\u2019 command. It took a polyline from a map that was made of 350,000 vertices and reduced it down to 450.\nInfrastructure\nUp until last year, BricsCAD was not a civil design application. Within 12 months so much work has been done, it\u2019s now tackling road layout across 3D topology. Last year TIN (triangular irregular networks) surfaces were added. In V21 it now has powerful tools to generate topology maps and TIN surfaces, overlay satellite imagery, create 3D alignments and extract 2D profiles from the terrain. Corridor templates can be applied and roads modelled and drawing sets created. Impressive stuff.\nBricsCAD BIM\nQuickBuilding is a new command that appears to be BIMify on steroids. While using BricsCAD BIM to perform massing studies \u2014 which can be done with direct modelling, generatively with Grasshopper or imported geometry \u2014 the new Quick building command will automatically produce a shelled solid, with automatic placement of walls and slabs, storey segmentation, as well and identified spaces. Using a quantities dialogue, users can configure how the Quick Building command implements these intelligent conversions. In seconds, it is possible to go from a shaped mass solid, to the shell of a multi-floor intelligent BIM model.\nLast year in Sweden, Bricsys announced an initiative to further refine its capabilities, intelligently turning dumb point clouds into IFC BIM models, partnering with global design, architecture, engineering and planning firm HOK and Leica.\nIn the demonstration, the start point was a 98 GB laser scan of a building, both inside and exterior. Smart algorithms were used to identify BIM elements within the point cloud, identifying multiple planar surfaces, which then define a space.\nIn addition to surfaces, it\u2019s also possible to identify solids, such as walls, slabs or columns in a semi-automatic process. This is the start of a process that will eventually deliver the elusive scan-toBIM automation. For now, this is a great workflow tool for those needing to turn point clouds to IFC, much better than tracing as a reference.\nGreg Schleusner, director of design technology, HOK explained how they were working with Bricsys to experiment, improving drawing automation. In this instance, the laborious process of creating fire safety drawings was automated.\nFire-specific components can be identified and specified; properties mapped for style definition. Theses are reflected in the drawings and are completely customisable, and reusable in other projects. In fact, BricsCAD BIM lets users create hundreds of sheet-sets automatically and create exactly what they look like. Sheet sets are a background process, leaving the system free to carry on working.\nThe most important feature this year is BricsCAD BIM\u2019s ability to import filtered RVT models as underlays or as solid lump of geometry. In this release, only the geometry from Revit is imported but because the software has BIMify, it can be used to turn the Revit geometry model into identified IFC components \u2013 walls, doors, windows, spaces etc. This RVT is once more an intelligent BIM model.\nOne wonders if this might even be useful to Revit users who can\u2019t access their old models.\nMulti-user collaboration is now possible through a GIT-based version control (open source management tool) which records changes to files over time which works online and offline. As this develops, expect to see some gamechanging, collaborative, granular workflows coming to market.\nDesign visualisation\nLast year Enscape joined the \u2018Bricsys Collective\u2019. These are developers with whom Bricsys has a high level of integration. This year Epic Games also joined with Twinmotion, which creates ArchViz environments for rendering and animations and also feeds into Unreal Engine for real-time VR experiences.\nConclusion\nIf I had to say BricsCAD had a superpower, it\u2019s in its the ability to recognise patterns and shapes and then perform automation. Blockify, Simplify, BIMify, QuickBuilding, Propagate, DetectFloor are all examples of how the software can search through its database, across all entity types, in 2D or 3D, recognise shapes, patterns, or building components, and then do something quite magical \u2013 make dumb things smart.\nWith every release there is something added to the feature set to assist in recognising or identifying geometry and then applying intelligence, should that be creating a block, constraining geometry, converting 300,000 polylines into 400, identifying a door in a solid model or now getting temptingly close to converting point clouds to BIM elements. It\u2019s an authoring tool with intelligence.\nBricsys is maintaining its development velocity, it\u2019s fleshing out its offerings at speed in multiple verticals and adding intelligence in a way that nobody else in this industry is doing. I always assumed that having a solid modelling engine underneath a 2D CAD tool, or BIM modeller was a liability \u2013 solids are heavy and overkill. Time and again Bricsys developers use it to their advantage, from importing Revit models, doing scan-to-BIM or going from mass model to BIM building model in one click.\nCheck out the demonstrations given in the Summit.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/esris-gis-selected-for-analysis-of-nationwide-educational-facilities-in-kenya\/","title":"ESRI's GIS selected for analysis of Nationwide Educational Facilities in Kenya","date":1177027200000,"text":"California, 19 April 2007: ESRI announced that the Ministry of Education (MOE) in Kenya has contracted Oakar Services Ltd. (OSL), ESRI\u2019s distributor in eastern Africa, to develop a GIS database of the educational facilities in Kenya.\nUnder the current project OSL is implementing a GIS database which includes ESRI\u2019s ArcGIS Server, ArcView, and ArcPad and Trimble\u2019s GPS GeoCollector. The project involves recording the names of the schools, their locations, and their primary physical features. This information will be integrated with attribute data including school enrollment, number of teachers, and the school\u2019s educational resources, which will be collected in the field.\nThe project, started in December 2006, is valued at US$1 million, and is being funded with a grant from the United States Agency for International Development (USAID). During the project the Kenyan government was concerned about a number of issues including overcrowded classrooms and the availability of educational resources. Other concerns included the lack of a clear view of the location and distribution of educational facilities in the country, as well as their condition, and the number, availability, and distribution of classroom instructors.\nThomas Ndegwa, technical director at OSL, says, \u201cIn a developing country such as ours where there is a large demand for donor aid, we are very fortunate that our government has taken the bold step to approve this project. It is a tribute to the forward thinking of the Kenyan government and its recognition of the critical importance of the education of our youth as well as its clear understanding of the value of spatial analysis in the decision-making process.\u201d\nOakar Services Ltd. (OSL) is an authorized distributor for ESRI and Leica Geosystems Geospatial Imaging in eastern Africa as well as a reseller for a variety of other products such as Trimble GPS receivers, ArcCadastre. For more information visit OSL at www.esriea.co.ke.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/design-analysis-optimisation\/","title":"Design analysis & optimisation","date":1459382400000,"text":"Formerly the preserve of specialists and engineers, software makers are bringing analysis tools to the conceptual phase of design as well as moving traditionally compute-intensive analysis to the cloud for faster results. By Martyn Day\nDesign analysis and optimisa tion of buildings is far reaching and there are all manner of analytical solutions available. These range from finding out how a building will respond under typical loads, react to earthquakes or interact with the wind, to how it will absorb solar energy, how much it will cost to heat \/ cool or how long it will take to evacuate occupants in an emergency.\nIn recent years, tougher restrictions on building codes have been introduced by many governments. With zero carbon targets and regulations on glazing ratios, buildings now have competing constraints that need to be solved early on in the design process.\nAnalysis used to be a specialist skill but a new wave of software is giving architects access to design feedback tools early on in the design process. At the same time, with many heavy specialist systems moving to the cloud, results can now be found in minutes.\nSoftware has also started to assist in the design process. Computers are now able to run through thousands of design alternatives, recommend optimal configurations of structures, layouts and orientation, without the need for the designers to create the geometry.\nAt this point \u2018Computer Aided Design\u2019 becomes a reality in the true sense as CAD moves from merely assisting in documenting design to actively suggesting a range of viable solutions. Many are calling this capability \u2018optioneering\u2019.\nConcept analysis\nDesign errors made early on cost more to solve downstream. This is especially true of environmental considerations. Performance is increasingly becoming part of building codes and architects need to have some assistance when optimising their designs.\nThe problem with this phase of the process is that current tools are mainly designed for experts and the way in which results are presented usually mean little to most designers and architects. While multi-disciplinary firms may well have in-house specialists, this is a challenge to small practices.\nAs design is an iterative process regular analysis phases can help optimise design solutions before they get to the detail level, bypassing the need for rework.\nIt is also here that collaboration tools can provide invaluable feedback on the progress of a design by enabling the sharing of models and results within project teams, perhaps providing expert input on key performance indicators.\nAutodesk FormIt 360 Pro, which specialises in linking conceptual tools with analysis programs, aims to give enough feedback in an easily digestible format and is simple and quick to set up.\nCloud-based optioneering\nOptimisation through analysis is set to take on a whole different meaning for computational design. Architects can set whatever criteria they want the computer to assess, from automatically optimising structures to be the strongest and lightest to the easiest and cheapest to fabricate.\nBentley Systems has a considerable armoury of industry-proven analysis tools such as STAAD and RAM for structures. The company has rewritten many of its analysis engines so they can be used in the Microsoft Azure Cloud, making use of scalable computational power to crunch big datasets in double-quick time.\nBentley\u2019s Simulation Services will, in time, expand to cover civils, structural, energy and more, as well as third party analysis tools. It can be applied to SketchUp, Rhino and Revit, as well as Bentley\u2019s MicroStation software.\nWhile speedy results are welcome, the aim is to enable the analysis of multiple design variants at the same time. Instead of a linear, \u2018design, test, change, repeat\u2019, process, Bentley Scenario Services can harness the power of GC (Generative Components) technology to produce design variables that can be automatically manipulated, analysed and then rerun to give feedback on a number of configurations \u2014 or optimised to get a specific result.\nResults are presented in pareto charts and users can give weight to different criteria in terms of their importance to the project. Bentley\u2019s approach also enables simultaneous analysis in multiple disciplines.\nAutodesk has recently demonstrated project Dreamcatcher, which took force \/ load data collected from a car driving around a race track, then set its optimisation algorithm loose to design the lightest chassis to meet real-world loading characteristics. The end result was an amazing lattice-based chassis that could only be 3D printed and would perfectly match the circuit with the best power to weight ratio.\nAutodesk also used Dreamcatcher with Airbus on the design of internal partitions on its aircraft. The metal structure produced by the software would never have been devised by a human designer, yet it met the leading criteria. The frame was 3D printed in titanium and saved loads of weight, which also reduced fuel costs.\nLifecycle analysis\nBuilding analysis firm IES is researching the application of analysis for daily building usage. Predicted energy performance characteristics from a building model can be compared to data from real-time dynamic sensors.\nIES is now evaluating predictive maintenance and problem-solving from these live sensor feeds \u2014 going so far as to alert building managers to probable faults when sensors indicate non-optimal performance.\nConclusion\nIn the not too distant future optioneering technology will be deployed in the AEC space to optimise structural steel, facades, and space planning. It will be used to lighten buildings, saving material costs.\nTools like Autodesk\u2019s Dreamcatcher and Bentley Scenario Services can give many possible solutions to problems, quickly, but it is still up to the designer to choose and drive the process.\nIf the same amount of time is spent designing, but at each stage the performance conforms with optimised material and energy usage, the end product will be higher quality.\nFor now the biggest drawback to adoption is educating users to understand the basic results, the physics and materials science. Fortunately the software industry understands this and is attempting to parcel up the results in easily digestible pieces.\nThis article is part of an AEC Magazine Special Report into the Future of Building Design, which takes a holistic view of the technologies and processes, which are set to change and enhance the AEC industry in the coming years \u2014 from concept design all the way to construction.\nClick to read the other articles that make up the report.\n1) Introduction New technologies are empowering architectural firms to improve quality, capabilities and process.\n2) Conceptual design There are a whole host of digital tools for early stage design experimentation.\n3) Rapid site design The rapid capture of site topology is being aided by new technologies.\n4) Benefits of 3D design Evolution, not revolution when making the move to 3D CAD.\n5) Moving to model-based design How to get from 2D to 3D, how to roll out training and how to overcome common issues encountered along the way.\n6) Design viz Advanced new rendering technologies are opening the door to design realism in architectural workflow.\n7) Design, analysis and optimisation Once you have a 3D CAD model, optimse your design for daylighting, energy performance and much more.\n8) Collaboration and model checking How to share models with clients, contractors and construction firms and test the quality of your model.\n9) Workstations What to look out for when choosing a workstation for 3D CAD.\n10) Virtual Reality New technologies are now available to support powerful new design workflows.\n11) 3D printing Architects are 3D printing architectural models with impressive results.\n12) Fabrication As building time gets compressed what will revolutionise fabrication and construction time?\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/tenderfield-adds-mark-up-to-cloud-based-construction-management-platform\/","title":"Mark-up added to Tenderfield construction management platform","date":1541721600000,"text":"New tools allow team members to collaborate with the latest construction plans on any device\nAustralian company, Tenderfield Construction Software, has released new mark-up tools that allow contractors to collaborate on their construction plans and add marked-up drawings to construction notices. The company has also implemented BIM & CAD file viewing capabilities that can be used on any smart device.\nTenderfield director and co-founder, Jason Kamha, said: \u201cOur mission is to continue to provide innovative solutions to help our clients optimise their project management practices to achieve more efficiency and to reduce project risks.\u201d\n\u201cThese latest tools are available for clients who manage projects on the Tenderfield platform, as well as for other project stakeholders who collaborate with the company\u2019s clients.\u201d\nThe cloud-based construction management platform is designed to help contractors improve their construction project management practices without incurring a large upfront capital outlay. The platform provides tools to manage the tender\/bidding process, project document control, project management and gives contractors access to a pre-qualified network of sub-contractors.\n\u201cThe design of the platform focuses on ease of use, the automation of construction management processes and enabling all project stakeholders to work together simultaneously and collaboratively\u201d, added Kamha.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/workstations\/nvidia-rtx-a6000-review-vray-keyshot-vred-threadripper\/","title":"Nvidia RTX A6000 review","date":1613952000000,"text":"The Nvidia RTX A6000, one of the most eagerly anticipated workstation GPUs in recent years, is finally here. And it\u2019s certainly been worth the wait, writes Greg Corke.\nNvidia first announced its new Nvidia RTX A6000 workstation GPU back in October 2020. But with delays, we\u2019ve been eagerly awaiting its arrival.\nAnd now it\u2019s finally here \u2013 well, nearly. UK firm Scan is taking pre-orders for the PNY Nvidia RTX A6000 from 3 March and sent us a sample to review inside its new 3XS GWP-ME N1-32T AMD Ryzen Threadripper workstation, which we review in full here.\nTuned for visualisation workflows, the Nvidia RTX A6000 (A for Ampere) is Nvidia\u2019s second generation RTX GPU. With an emphasis on hardware-based ray tracing, it promises to deliver more than double the GPU rendering performance of its predecessor, the Turing-based Nvidia Quadro RTX 6000. And, of course, it also offers a significant boost for 3D graphics, VR, and AI workflows. And all for \u00a33,730 + VAT.\nThe Nvidia RTX A6000 is the first workstation GPU to be built on the Nvidia Ampere architecture, and also the first to support PCIe Gen 4. With double the bandwidth of PCIe Gen 3, it should mean data can move in and out of the GPU quicker, but it won\u2019t benefit all workflows. PCIe Gen 4 is currently only available in workstations with the latest AMD CPUs.\nWhat no Quadro?\nThe observant among you will have noticed that the Nvidia RTX A6000 is missing something. Yes, after 17 years, it appears that Nvidia is retiring its Quadro brand.\nFor someone who has written about Nvidia professional GPUs since the early 2000s, it\u2019s hard to get used to this change. And, it would seem, Nvidia is struggling too. For the October launch, product photos were sent out with \u2018Nvidia Quadro RTX A6000\u2019 in the file name, which suggests a last minute re-branding.\nBut the Nvidia RTX A6000 is certainly a Quadro card in everything but name. It has certified drivers for pro applications, renowned reliability, ECC memory and plenty of niche features for pro visualisation, such as stereo and Frame Lock for viz clusters. There\u2019s also support for Nvidia virtual GPU (vGPU) software, which allows a workstation to be repurposed into multiple high-performance virtual workstation instances.\nWith 48 GB of VRAM, the Nvidia RTX A6000 has the most memory of any professional GPU \u2013 on par with the Quadro RTX 8000, and double that of the Quadro RTX 6000.\nThis is particularly relevant for those working with high poly count models and very high resolution textures in GPU renderers or real time visualisation \/ VR. But more on this later.\nWhat\u2019s more, two Nvidia RTX A6000s can be bridged together with an NVLink adapter to create a memory pool of 96 GB. However, sharing geometry between two GPUs can come with a significant performance hit.\nBeauty and the beast\nNvidia\u2019s product design team has really gone to town on the Nvidia RTX A6000 and the card is a thing of beauty, with a minimal angular design and black mirror finish. NVLink, Stereo and Sync connectors are all hidden away behind discrete panels.\nThe board is rated at 300W, so you\u2019ll need a fairly hefty PSU in your workstation. It gets its power from a special 8-pin connector, that is connected to two standard 8-pin PSU connectors via an adapter. It is cooled by two fans, top and bottom. There are four DisplayPort 1.4a connectors nestled below a heat sink.\nTesting the Nvidia RTX A6000\nAEC Magazine put the PNY Nvidia RTX A6000 through a series of real-world application benchmarks, both GPU rendering and real time visualisation.\nThe GPU is simply overkill for current generation CAD and BIM software, so we didn\u2019t do any testing in that regard. However, it\u2019s important to note that it will still be certified for the likes of Revit and ArchiCAD, which is useful if you plan to use those kinds of applications alongside more viz focused tools like Enscape, V-Ray and Lumion.\nThe full spec of the Scan 3XS GWP-ME N1-32T test machine can be seen below.\nYou can read a full review here.\nScan 3XS GWP-ME N1-32T\n- Processor: AMD Ryzen Threadripper 3970X (32C\/64T) (3.7GHz \u2013 4.5GHz)\n- Graphics card: Nvidia RTX A6000 (48 GB) (461.09 driver)\n- Memory: 128 GB Corsair Vengeance LPX 3600MHz DDR4\n- System drive: 2TB WD Black SN850 PCIe 4.0 NVMe M.2 SSD\n- Storage drive: 4TB Samsung 860 EVO SSD\n- Motherboard: ASUS ROG STRIX TRX40-E\n- Case: Fractal Design Define 7\n- CPU cooler: Cooler Master MasterLiquid ML360\n- Power supply: Corsair RM1000i \u2013 80PLUS Gold\n- Operating system: Microsoft Windows 10 Pro 64-bit\n- Price \u00a37,500 + VAT\nFor comparison, we used historic data from the Nvidia Quadro RTX 6000 GPU (457.09 driver), tested in a Lenovo ThinkStation P620 workstation with AMD Ryzen Threadripper Pro 3995WX CPU and 128GB RAM, as well as a Quadro RTX 4000.\nRendering with the Nvidia RTX A6000\nGPU rendering has now reached a stage where it is becoming ubiquitous. While renderers built into CAD and BIM applications still tend to rely on the CPU, most of the major design viz focused renderers now offer a GPU rendering capability.\nThis includes traditional ray trace renderers such as Chaos Group V-Ray, Luxion KeyShot and Solidworks Visualize, all of which can take full advantage of Nvidia RTX technology, including the dedicated Ray Tracing cores and Tensor cores for AI denoising.\nThen there\u2019s real time ray tracing tools like Unreal Engine and the new Chaos Vantage (formerly Project Lavina), which works with V-Ray scenes. (N.B. you can currently pick up a free one year licence if you register before June 2021).\nAs a side note, it\u2019s important to check that your chosen GPU rendering tool supports the new Nvidia RTX A6000. Users of Solidworks Visualize 2021, for example, will have to wait until Service Pack 3 in April 2021 before they can take advantage of any \u2018Ampere\u2019 GPU.\nChaos Group V-Ray\nV-Ray is arguably the No. 1 physically-based rendering tool for architectural visualisation. We put the Nvidia RTX A6000 through its paces using the new, freely downloadable, V-Ray 5 benchmark, which has dedicated tests for Nvidia CUDA GPUs, Nvidia RTX GPUs, as well as CPUs.\nThe results were extremely compelling, with the Nvidia RTX A6000 showing itself to be 2.32 faster than the Nvidia Quadro RTX 6000 in the CUDA test and 1.85 times faster in the RTX test. Considering this is just a generation on generation comparison, it\u2019s a phenomenal leap in performance.\nLuxion KeyShot\nKeyShot, a CPU rendering stalwart, is a relative newcomer to the world of GPU rendering. But it\u2019s one of the slickest implementations we\u2019ve seen, allowing users to switch between CPU and GPU rendering at the click of a button.\nIn the Keyshot 10 benchmark, part of the free KeyShot Viewer, we saw similar results to V-Ray. The Nvidia RTX A6000 outperformed the Quadro RTX 6000 by a factor of 1.97.\nGPU memory \u2013 why 48 GB?\nOne aspect of technology that has long held back GPU rendering is the limited amount of GPU memory available on each card. When a scene, including geometry, materials and lighting, doesn\u2019t fit entirely into GPU memory, the render can fail, slow down (if it runs \u2018out of core\u2019 using system memory), or simply fall back to the CPU.\nThere are many established workarounds to help users ensure a dataset fits entirely within GPU memory \u2013 such as resizing or optimising textures, simplifying or stripping out geometry, or rendering in separate passes \u2013 but all of this takes time to prep. What the user really wants is not to have to worry about GPU memory at all.\nWith 48 GB, the Nvidia RTX A6000 is arguably the GPU that will enable this. And while it\u2019s not the first to offer that much memory (the Quadro RTX 8000 also came with 48 GB) it\u2019s the first to do so at a sub \u00a34,000 price point.\nOf course, for some workflows 48 GB will be overkill. However, for those pushing the boundaries of realism, using very hi-fidelity textures (such as those captured from real-life scans) or colossal engineering accurate datasets, it should open up a wealth of opportunities.\nLuxion, for example, has in the past reported ray tracing a KeyShot scene with 1.37 billion unique triangles using two Quadro RTX 5000 cards and NVLink for a combined 32 GB of memory.\nIndeed, despite its designer-friendly workflows, KeyShot is doing a lot to break new ground. A new feature in KeyShot 10 called RealCloth 2.0 allows users to generate visually accurate woven materials using thread geometry. However, this can be incredibly memory hungry when using the option to represent each individual thread as a geometric entity, even with relatively small scenes.\nPreviously, we\u2019ve explored this feature in our sister publication DEVELOP3D. Using a 16 GB Quadro RTX 5000, however, we quickly ran out of memory, with the scene falling back to the CPU to render. However, things were entirely different with the Nvidia RTX A6000.\nWith 48 GB to play with we were able to much better match the scale of the weave in KeyShot to that of the physical fabric. As you will see from the images below, the difference in accuracy, in terms of how the material is represented, is quite incredible.\nOf course, RealCloth 2.0 is quite a niche example. However, there will certainly be some architectural or automotive visualisers out there that would benefit from 48 GB (or even 96 GB with two RTX A6000s over NVlink) now or in the future. You\u2019ll know who you are.\nReal time 3D with the Nvidia RTX A6000\nWhile GPU rendering is a major play for the Nvidia RTX A6000, real time 3D using OpenGL and DirectX continues to be a very important part of architectural visualisation, with applications including TwinMotion, Lumion, Enscape, Unreal Engine and others. And, of course, the boundaries between real time 3D and ray tracing continue to blur.\nTo test frame rates we used FRAPS in combination with a 3DConnexion SpaceMouse to ensure the models moved in a consistent way every time. We only tested at 4K (3,840 x 2,160) resolution. At FHD (1,920 x 1,080) resolution this level of GPU simply isn\u2019t stressed enough.\nEnscape\nEnscape is a real-time viz and VR tool for architects that uses OpenGL and delivers very high-quality graphics in the viewport. Enscape has used elements of ray tracing in its software for some time. Newer versions of the software are RTX-enabled, so full ray tracing can be toggled on and off.\nFor our tests, Enscape provided a large architectural scene of a museum and its surrounding area. At 7.5GB, the GPU memory requirements of this model are relatively high, but Enscape models can take up much more and this is a drop in the ocean for the Quadro RTX A6000.\nIn terms of performance, the Nvidia RTX A6000 delivered a phenomenal 53 frames per second (FPS) for an incredibly smooth experience. This is around 1.39 times faster than the Nvidia Quadro RTX 6000 and 2.79 times faster than the Nvidia Quadro RTX 4000.\nAutodesk VRED Professional\nAutodesk VRED Professional is an automotive-focused 3D visualisation, virtual prototyping and VR tool. It uses OpenGL and delivers very high-quality visuals in the viewport. It offers several levels of real time anti-aliasing (AA), which is important for automotive styling, as it smooths the edges of body panels. However, AA calculations use a lot of GPU resources, both in terms of processing and memory. We tested our automotive model with AA set to \u2018off\u2019, \u2018medium\u2019 and \u2018ultra-high\u2019.\nThe Nvidia RTX A6000 was significantly faster than the Quadro RTX 6000 delivering between 1.46 and 1.50 more frames per second in all of our tests. Most notably, the card delivered a very smooth 25.15 FPS when AA set to \u2018ultra high\u2019, which is unheard for a model of this complexity. This is 2.97 times faster than a Quadro RTX 4000.\nConclusion\nNvidia has put a huge effort into developing its hardware-based ray tracing technology and it\u2019s now really starting to see the fruits.\nIt\u2019s quite incredible to think it has managed to double the GPU rendering performance, generation on generation (Turing to Ampere). And, with a 1.4 to 1.5 boost, it\u2019s hardly dragging its heels in real time 3D.\nRendering has become the big battleground for hardware manufacturers and, with the 64-core AMD Ryzen Threadripper CPU, competition between GPU and CPU has never been so fierce.\nCPUs have always had the lead when it comes to addressable memory and there\u2019s an ongoing debate about rendering performance vs accuracy. But with 48 GB per GPU, and the ability to scale up cards as and when required (the new Supermicro SuperWorkstation 5014A-TT can support up to four Nvidia RTX A6000s), the argument for GPU rendering has never been more compelling.\nIn many ways the biggest competition for the Nvidia RTX A6000 is from Nvidia itself. The consumer-focused Nvidia GeForce RTX 3090 should offer roughly the same performance for one third of the price. It does have half the memory (24GB) but with support for NVLink, two can be linked together to create a virtual 48 GB render resource. The downside is \u2013 it\u2019s currently nigh on impossible to get hold of a board and prices are inflated.\nOf course, for some firms it will always need to be Quadro all the way. Or should we say Quadro in features, warranty, reliability and certification, but not in name. It\u2019s going to take some time to get used to the rebranding.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/bentley-systems-technology-update\/","title":"Bentley Systems technology update: the birth of CONNECT","date":1415923200000,"text":"Bentley Systems launched its new CONNECT technology at its annual Year In Infrastructure conference in London, laying the ground for engineering data and applications to be much more dynamic and pervasive.\nThe recent Bentley 2014 Year In Infrastructure (YII) event in London provided the platform for the company to launch its next generation MicroStation, CONNECT Edition, together with updates to its comprehensive range of engineering products and services. The last generation, MicroStation V8, was launched in 2001 with subsequent editions V8.5 and V8i.\nEven though Bentley hasn\u2019t changed its file format since 2001, the DGN format stays the same with this new generation, which is an impressive feat. That said, in the company\u2019s pitch for the new release you would be hard pressed to hear much about all the work that has gone into the new geometry engine. Bentley\u2019s portfolio of solutions is now more about project management, data mobility, connecting teams and information modelling.\nAfter 30 years in operation, Bentley Systems has pretty much nailed CAD drawing and modelling so is now aiming to tackle both higher and deeper problems, such as information longevity from concept to a facilities operation as well as to provide compute-intensive \u2018optioneering\u2019 analysis to find the best performing or more cost-efficient design solution.\nWith this change in focus, Bentley\u2019s vision and messaging shifts to centre on ProjectWise. This is the company\u2019s project integration environment, either hosted in the cloud or behind a firewall. ProjectWise manages and distributes information on demand to all project participants, in a controlled manner.\nIn large, multi-billion pound engineering projects, such as London\u2019s Crossrail, ProjectWise has been used as a data backbone throughout all phases, and was attributed to saving more than 5,000 man-hours in just 12 months on the tunnel project alone. Bentley CONNECT Edition is project centric, rather than file centric, so every model or piece of content that is created has to be part of a project.\nMicroStation CONNECT features a brand new user interface. There are specific ribbons for different workflows, such as drawings, modelling, and visualisation and context-sensitive menus where a related stack of tools pop up at the cursor.\nWhen the new MicroStation CONNECT is launched the user is taken to a welcome screen where they are asked to sign into the CONNECT server. Each profile stores information about the kinds of projects or discipline the user is involved in and this will allow Bentley to make recommendations of which products to use, what learning content to watch or what features the user may be missing out on based on what their peers are using.\nMicroStation CONNECT is project centric, rather than file centric, so every piece of content or model or piece of content that is created has to be part of a project. This will be the case for all of the new Bentley CONNECT products, MicroStation or otherwise.\nThe system will warn the user if he or she attempts to use content that is not officially part of that project \u2014 the idea being that it should aid conformance to project standards.\nData can still be stored locally; it is only the information about the project that is held on a central server. For example, i-model representations of models can be shared so participants can view models or shared content. It is also possible to see who else is working on the same project, helping connect often dispersed teams.\nMicroStation CONNECT features a range of powerful new constraint tools that allow users to capture design intent and create functional components.\nWhereas in the past you may have needed dozens of different variations of a particular cell type, each with different heights, widths, etc, these can now be represented with one parametrically variable functional component that can adjust to changing geometrical conditions.\nFunctional components can also be managed and made available to project participants in the new ProjectWise Catalog Service, a cloud service.\nAccording to David Huie, Product Marketing Manager at Bentley Systems, in theory this will allow components to live and persist through the lifecycle of a project. For example, as-built information or installation details could be added to components as the project moves from construction and into operations.\nOf course, catalogue content does not have to parametric. It can also be from standards bodies, manufacturers or indeed from Bentley.\nCONNECT Editions\nWith this new generation of products Bentley moves a step closer to providing a more seamless, integrated end-to-end environment, where data is created in projects, managed and distributed to wherever it needs to be viewed, edited, interacted with and marked-up. Similarly the data can be searched and filtered from any point of access with a choice of architectures, behind the firewall, private cloud or public cloud.\nWhile much of the finesse appears to be in the new modern interfaces, a lot of the development work is in the background, connecting up the applications, hiding product silos and making a hybrid cloud\/server project backbone.\nBentley calls the products \u2018software at your service\u2019. The company\u2019s flexible licensing provides access to its entire product suite through configurable application \u2018playlists\u2019, flexible licensing and hourly usage billing.\nFor mobile, Bentley has built up a pretty extensive portfolio of apps over the last few years with a view to putting data into the hands of the site worker. Greg Bentley\u2019s \u2018data mobility\u2019 mantra, the theme of YII 2011, has become a reality.\nTwo of the mobile applications demonstrated at the launch were of special interest. ProjectWise WorkSite CONNECT Edition provides comprehensive remote access to data stored in ProjectWise and supports file viewing, mark-up, forms, and complex searches. Users can access latest project documentation with review and redlining capabilities. And they don\u2019t need to worry about where data is stored as every user is given a single task-based view to all of their project data. Bentley Navigator, for viewing, analysing and augmenting, has been completely rewritten for CONNECT Edition. It has a very slick, minimal touch-enabled interface, which is virtually the same on all platforms \u2014 iOS, Android and Windows. This lends itself to handheld tablets as well as large touchscreens for collaborative design \/ review\nBoth applications use Microsoft\u2019s Azure cloud platform and are the first instalments of a brand new underlying architecture for the company\u2019s product development teams; but more on that later.\nThe new generation of connected applications will be made available in the coming 18 months, starting at the end of the year. The first to reach customers will be MicroStation CONNECT Edition (the successor to MicroStation V8i), ProjectWise CONNECT Edition, the completely revamped Bentley Navigator CONNECT Edition and ProjectWise WorkSite CONNECT Edition.\nTrimble and Siemens\nBentley\u2019s partnerships, even with direct competitors, is continuing to blossom, especially so with Trimble and Siemens\nWith Trimble the alliance focuses on construction with i-model and laser-enabled site layout. With Siemens it\u2019s all about bringing Pointools point cloud technology into factory and process design.\nThe Bentley\/Trimble announcement implies a much deeper coalition between the two firms to work together to extend the benefits of the design BIM model into construction. Areas include temporary works, intelligent positioning, detail for fabrication, workface planning, construction work packaging and providing support for distributed construction.\nBoth firms feel that, to date, BIM tools do not deliver for construction firms, who usually have to rebuild the BIM model, and often just for construction visualisation. This results in the loss of valuable engineering information that is actually useful to the construction firm and for operation post completion.\nBentley and Trimble have announced that they will work together to develop a \u2018construction modelling\u2019 schema to preserve and reference architecture and engineering data with a construction overlay together with as-built changes.\n\u25a0 Bentley CONNECT The successor to Bentley\u2019s V8i software generation will feature a common project environment and offer cloud-based capabilities to deliver a new \u2018connected user experience\u2019\n\u25a0 Bentley and Trimble Two firms now working even closer together to extend the benefits of the BIM model from design into construction\n\u25a0 ProjectWise Essentials Cloud-based, industrial-strength collaboration application for smaller firms (up to 40 seats) that is leased by the quarter\n\u25a0 MicroStation CONNECT A brand new user interface, a project-centric workflow, 64-bit support for larger models, plus a range of constraint tools for creating functional components\n\u25a0 Codename Graphite A brand new platform technology which Bentley Systems is using to create new products for multiple operating systems\n\u25a0 Scenario Services Using the power of the cloud to test out hundreds of different designs in the quest for a truly optimal solution\nThe two firms have said they will pool resources for product development and sharing schemas across design and construction applications. When necessary they will use common modelling software for virtual and physical alignment and construction deliverables will utilise Bentley\u2019s i-model portable document format.\nBentley has included support for Trimble Total Stations at platform level and, by combining Bentley Navigator and ProjectWise with Trimble Field Solutions via Trimble Field Link, it is possible to position points from the construction model in the design office and positioning devices in the field.\nThis year is the 30th anniversary of the founding of Bentley Systems by brothers Keith and Barry Bentley in Pennsylvania.\nIn 1985, they introduced the commercial version of PseudoStation, which allowed users of VAX mainframe systems to use low-cost graphics terminals to view and modify the designs on their Intergraph IGDS (Interactive Graphics Design System). In 1986, with the advent of the IBM PC, Bentley developed MicroStation Version 1, which allowed users to view and plot the files generated on the mainframe and introduced the DGN file format. This eventually became a full 2D\/3D CAD package, which to this day is the foundation platform for the comprehensive design ecosystem that Bentley Systems has created.\nToday, Bentley Systems is headed up by CEO and brother Greg Bentley and has an annual revenue of $593 million, and over 1 million commercial users in 165 countries. Typical Bentley customers tend to deliver large infrastructure projects, including Plant, Civil, Architecture, Roads, Rail, Army and Power.\nAn early proponent of the subscription ownership, Bentley has a 97% subscriber retention rate, which provides around 70% of total revenue.\nSpending 25% of income on R&D, the firm continues to focus on solving technically complex engineering problems such as Building Information Modelling (BIM) and has been at the forefront of connecting project teams with industrial strength collaboration tools under the ProjectWise brand.\nGreg Bentley explained that this partnership would enable engineers to work \u201cfrom BIM to bulldozer\u201d.\nBentley has been working with Siemens to provide a solution for factory design and facility management for quite some time. Siemens is now embedding Bentley\u2019s Pointools point cloud technology within its popular process simulation tool Tecnomatix, which will allow the import and display of factory laser-scans in its \u2018as-operated\u2019 configuration. This can be used to model product process lifecycle or test new production designs in a \u2018reality modelling\u2019 context.\nSiemens also has the potential to use the Pointools Vortex point cloud engine in other Siemens products such as NX for product development or Teamcenter for product lifecycle management (PLM). This would be a big coup for Bentley if this happens as Siemens has millions of users.\nRoad, rail and underground\nIn June Bentley released Subsurface Utility Engineering (SUE), specifically designed to build and manage intelligent 3D feature-based models of underground utilities.\nBuilt on OpenRoads technology, the foundation for Bentley\u2019s road, rail, drainage and bridge modelling tools, the software is designed to create models that are spatially accurate so hard and soft clashes can be ironed out underground, helping saving construction costs downstream.\nWhile it is still early days for this technology senior product manager Ian Rosam highlighted how Michigan Department of Transportation has implemented SUE to help standardise the way utilities are captured, presented and analysed. The Geospatial Utility Infrastructure Data Exchange (GUIDE) will be used to manage the assets throughout their lifecycle as well as reduce risk and construction delays.\nBentley has big plans for SUE and is already extending the reach of the product. The new Subsurface Utility Design and Analysis (SUDA) software gives the SUE model more intelligence by including hydraulic design and analysis capabilities for drainage networks.\nCivil Cells, one of the key features of OpenRoads, is a set of graphical macros that capture design intent through the use of constraints and relationships. The result is a functional component that can be used again and again, automatically adapting to changing environments.\nCivil Cells are typically stored in DGN libraries that can be shared between projects. This lends them perfectly to the new CONNECT Edition products and its shared project environment where users can get easy access for current and future projects.\nRon Gant, industry marketing director, road, told us that the Bentley Civil CONNECT releases would be rolled out over the next 18 months.\nMr Gant also touched on the growing relationship with Trimble and how ProjectWise is helping to manage the flow of project information from engineering (OpenRoads) to construction (Trimble Business Centre) through the use of i-models.\nHere the major focus is on producing constructible models, which harness valuable engineering design and analysis data for use downstream in construction. The end goal is to produce a complete model that can then be handed over to the operations team.\nData is also flowing from the conceptual to detailed design phases thanks to a new link between the Trimble Quantm Alignment Planning System and OpenRoads. Mr Gant explained how Quantm, which specialises in corridor planning and routing analysis, can now publish preliminary design information in an i-model that can be used in OpenRoads.\nBentley ProjectWise Scenario Services is a cloud service for early stage design optimisation \u2014 or as Bentley likes to call it, \u2018optioneering\u2019. The idea is designers can use the power of the cloud to test out hundreds of designs, instead of the usual two or three, in the quest for an optimal solution.\nThe cost benefits can be huge, as Santanu Das, senior vice-president, design and modelling, explained. He told AEC Magazine that one customer who has been trialling the technology managed to save $450,000-$500,000 in material costs on a 14-storey residential building project. Using RAM Concept and Scenario Services it took just four hours to get feedback on 1,200 different designs that featured various combinations of post tension and rebar. The optimisation also allowed the designer to extend the building to 15 storeys, which was of huge benefit to the client.\nSavings can also be made in labour costs, said Mr Das, citing an example of an offshore structure where a design with smaller welds makes it much easier (and therefore cheaper) for workers to prepare in situ.\nTo date, Scenario Services has only been rolled out to a limited number of Bentley customers. This soft launch is partly because Bentley has been negotiating pricing with Microsoft for use of its Azure cloud platform, which has proved tricky due to unpredictable demands of analysis.\nThe big launch will come in the first quarter 2015 when the service will be accessible from most of Bentley\u2019s simulation products, including SACS, Moses, STAAD, AutoPIPE, RAM and RM Bridge.\nThere is still work to do on automation. With the exception of STAAD, which has a pretty unique input mechanism in that it allows users to set up different scenarios, each and every design option from the other analysis tools will need to be created individually. This will change over time though and long-term it is likely that Generative Components technology will be made available in the cloud so hundreds of design options can be automatically produced based on input ranges.\nAt present, the only way to automatically generate such a variety of design options is on the desktop through Generative Components or AECOsim Building Designer, prior to uploading to the cloud. However, as these applications are primarily focused on design, users will need to add additional attribute information to fully prepare the model for structural analysis.\nIn the first quarter 2015 Bentley will also launch beta releases of Scenario Services for Autodesk Revit and Rhino, where data from these third-party products will be able to be fed into Bentley\u2019s cloud-based analysis engines. Mr Das said support for SketchUp was proving more problematic due to its data structure and the way it stores geometry, which does not really have a hierarchy.\nThe holy grail for ProjectWise Scenario Services is to optimise using multiple engines. For example, to move a building by a few degrees and see what impact it has on site, energy and structural. This capability should be available in the first half 2015, but Bentley needs to fine-tune the way the different options are presented. In 2013 the focus was on pareto charts, but according to Mr Das, Bentley has now realised these are not necessarily the clearest way to convey this information to non-experts.\nAs the service matures other engines will be added, including heating and cooling and airflow. Bentley has also developed an API and is working with partners on acoustics, fire and smoke propagation and others.\nThe other obvious technology to be integrated into Scenario Services is SITEOPS, the software that Bentley acquired in its purchase of Blueridge Analytics in late September 2014. The software works on the same \u2018optioneering\u2019 principle of Scenario Services insofar as it uses the power of the cloud to enable designers to explore different engineering alternatives and their costs.\nWith SITEOPS you can trace out a building footprint onto a proposed site (or import an AECOsim Building Designer i-model), orient it and it will give real time design and cost feedback on parking, grading and drainage. Saved site designs can be \u2018drag and dropped\u2019 to another site for comparison. SITEOPS is used by retail and restaurant chains such as Target, Walmart and McDonalds.\nWhile the long-term plan is to bring the SITEOPS engine into Scenario Services, this would be done slowly. The software would run on both the SITEOPS and the ProjectWise Scenario Services cloud frameworks for some time, Mr Das said.\nThe SITEOPS technology will also be integrated into OpenRoads, the umbrella technology that forms the backbone to Bentley\u2019s civil design tools \u2014 InRoads, Geopak and MX \u2014 which sounds like a very exciting proposition.\nTo learn more about Scenario Services, check out our in-depth article from earlier in the year\nProjectWise Essentials\nLaunched just before YII 14, Bentley ProjectWise Essentials is a pretty exciting product for those that have wanted to have the industrial-strength collaboration application but were put off by the scope or price.\nProjectWise Essentials is a cloud-based design integration service that is leased by the quarter for smaller firms (up to 40 seats) with lower budgets. As it is based in the cloud, access is immediate and users get to choose from a number of best practice templates, following standards, to streamline workflows and connect team members.\nProjectWise Essentials is CAD and BIM agnostic and works just as well with Revit files, as it does with AutoCAD, Civil3D and MicroStation. The system provides version control, change management, document relationship creation, search and fast distribution.\nIt comes with free Bentley CONNECTIONS Passport access to the Bentley range of mobile solutions including Bentley Map Mobile, Field Supervisor, InspectTech Collector Mobile, Navigator Mobile, ProjectWise Explorer Mobile, and the Bentley LEARN App. Mobile access is secure and data can be made available to those onsite, either on or offline.\nArmed with the virtually unlimited power of the cloud, Mr Karp envisages two big growth areas for analysis moving forward. The first is to use analysis much earlier on in the design process, where it could give real time structural performance feedback when modelling in a product like AECOsim.\nThe second is in operations and maintenance. Here, sensors placed on the as-built asset can be used to feed back data into the analytical model for health monitoring and to assess on-going performance.\nThis is already being done in bridge maintenance, where vibration data can be collected periodically to find out where strength might be degrading. However, in industries such as plant it could also be used to monitor structural strength and fluid flow, which may have been impacted by corrosion, all in real time.\nFor a technology that has traditionally been used in design validation workflows on a handful of proposals it is a compelling vision for analysis in the future.\nWith a huge number of acquisitions over the past decade or so Bentley has established itself as a powerhouse in structural design and analysis. But with close to twenty products in its portfolio it continues to have a major challenge of how to best handle data interoperability and future software development.\nIts Structural Synchronizer software has done a great job of matching Bentley model data with third party applications, but the longer-term goal is to get more of its products working together.\nConsolidation is an essential part of this process but that does not mean Bentley is about to merge its products into a single design or analysis tool. Instead, its aim is to harness its technology better and to make a consistent experience for its users.\nAt the heart of this transition is the \u2018structural modelling component\u2019, which creates functional modelling tools that can be shared between Bentley\u2019s structural products. This not only gives users a consistent way to model structural components, but a consistent data schema between tools that is a massive benefit for interoperability.\nRaoul Karp VP, structural and BIM products, explains that by going down to the data level you will be able to ensure that a beam created in OpenPlant, AECOsim or Bulk Material Handling is the exact same beam and, users will not lose data as they move back and forth. This could have big benefits for workflow: a detailer, for example, could open an AECOsim model directly inside ProStructures and start work immediately.\nBentley has also done some technology sharing between its concrete tools and has migrated some 2D labelling and symbol tools from Bentley Rebar, which is big in bridges, into the advanced 3D modelling features of ProConcrete. By next year Mr Karp hopes he can talk about how ProConcrete is not just a general purpose detailer but a special purpose tool for bridges as well.\nConsistency of results is also a big priority. Any design code that has been introduced in the last two or three years has been shared, explained Mr Karp. For example, Bentley has just brought out the Canadian codes and the results will be the same across all of its products. Moving forward, this will extend to a common analysis engine so analytical results will also be consistent in all products.\nThe new engine will be able to adapt to different workflows including the higher non-linear demands of an offshore product like SACS and the different loading requirements of Bentley\u2019s bridge design and analysis tools.\nOf course, in line with its overall CONNECT strategy, the cloud is playing an increasingly important role in Bentley\u2019s structural plans. Analysis will continue to be carried out locally, but the cloud will allow solve times for very large models to be significantly reduced. More importantly, it will allow engineers to test out many different options in the quest for a more optimal design.\nBentley calls this \u2018optioneering\u2019 and the enabling technology is a cloud service called Bentley ProjectWise Scenario Services, which can be used in conjunction with Generative Components to automatically generate hundreds of different options. Genetic algorithms are then applied to pick the best ones.\nArmed with the virtually unlimited power of the cloud, Mr Karp envisages two big growth areas for analysis moving forward. The first is to use analysis much earlier on in the design process, where it could give real time structural performance feedback when modelling in a product like AECOsim.\nThe second is in operations and maintenance. Here, sensors placed on the as-built asset can be used to feed back data into the analytical model for health monitoring and to assess on-going performance.\nThis is already being done in bridge maintenance, where vibration data can be collected periodically to find out where strength might be degrading. However, in industries such as plant it could also be used to monitor structural strength and fluid flow, which may have been impacted by corrosion, all in real time.\nFor a technology that has traditionally been used in design validation workflows on a handful of proposals it is a compelling vision for analysis in the future.\nMicroStation under the bonnet\nWith any significant upgrades comes the opportunity to change the user interface \u2014 which will probably be the biggest challenge for experienced users, as there has been quite a radical reworking of how MicroStation looks, feels and responds to element selection.\nThe first thing you will see is that all sessions are started by selecting a project, either existing or new. Model management and keeping related documents together is now built in from the get go.\nOnce loaded, the new ribbon toolbar is clean with multiple tabs for different tool palette selection. The menu is context sensitive and so will change depending on what items are selected.\nUnder the hood, MicroStation has become fully 64-bit throughout which will allow users to work with much bigger models and should offer better performance.\nParametric content is also big in this release.Bentley has included the capability to download cloud-provisioned content with catalogues of \u2018functional components\u2019 which include managed specifications, for reference throughout comprehensive project delivery and handover. These go way beyond 2D cells\/blocks and have the potential to rapidly improve design productivity with users producing highly intelligent downloadable content that can be widely distributed through a managed system.\nCodename Graphite\nNot content with showing off the next generation of MicroStation, Bentley presented products that were developed on a new platform, codenamed \u2018Graphite\u2019.\nKeith Bentley, CTO, told AEC magazine that Graphite is an environment with which Bentley Systems can create new products and easily compile code for multiple operating systems.\nOnce upon a time, Bentley supported 14 operating systems, but quickly dropped to just Microsoft Windows, and the company has been deeply tied into Microsoft\u2019s .Net and programming tools ever since. However, the industry is changing and Keith Bentley stated that Apple and Google had won the mobile space, meaning the company had to consider that, once more, it needed to develop for different target platforms.\nIn its first incarnation, Graphite was used to create the new Bentley Navigator CONNECT application. The platform has a new SQLite-based file format that enables file streaming and is more suited to distributed server-based access. The SQLite format makes design information more granular and provides relational links, enabling multiple data schemas (e.g. COBie) to be included. It also provides powerful and fast search\/filter capabilities.\nFor now it is being used to create a file viewing and annotation application, not unlike the very first version of MicroStation written in 1985. This will probably expand to replace DGN in the future as the transaction conduit of project information.\nWe asked if we would see a version of MicroStation for the Apple OSX and Keith Bentley replied \u201cNo, it wouldn\u2019t look or feel like MicroStation if we did a port to OSX but we won\u2019t rule out other future products coming out for the Mac.\u201d\nReading between the lines, Bentley has a new development platform, a new \u2018format\u2019 and has done a lot of work to prepare the way for a multi-operating system future for both desktop and mobile. For cloud, Bentley is completely committed and optimised for Microsoft\u2019s Azure infrastructure. Keith Bentley is confident that distributed computing \u2014 powerful servers with local workstations \u2014 is still the way to go.\nThis compares with competitors, such as Autodesk, who are looking to deliver \u2018virtualised\u2019 access to their software via remote hosted applications through pixel streaming technologies provided by the likes of OTOY, Citrix and Mainframe2.\nBentley is currently experimenting with a technology from Numescent, which allows fast download\/distribution of Windows-based applications that run in an application \u2018player\u2019, but the actual application is streamed and still uses the CPU and graphics of the local machine.\nBentley Research\nThe Bentley Research presentations at YII are becoming a highlight for those that love seeing the cutting edge of development. This year we had presentations on augmented reality, on-site positioning, laser scanning, converting photos to 3D models, real-world object recognition, rapid bridge structural analysis and thoughts on construction efficiency, as to how it was different from typical manufacturing.\nThe key problem is in accuracy of locating and tracking the viewer both outside and inside buildings. Bentley has been experimenting with a wide variety of technologies to improve this, from Bluetooth tracking triangulation via iBeacons and signal strength, to registering external views with Google Streetview.\nExperiments with projecting Augmented Reality images on construction sites quickly established that blocking the visibility of workers could have an impact on safety for those near by, particularly if driving an excavator. Bentley is working on a way to project transparent \u2018painted\u2019 3D model data instead, ensuring clarity of view.\nIn another experiment, laser-scanned point model data collected on a motorway was combined with Google Streetview images to identify objects in the scene. Optical Character Recognition (OCR) was also used to add on street signs for inclusion in the models.\nConclusion\nBentley used YII to unleash Azure applications being developed for the next release, showing that its development strategy is defining products many years in advance.\nFor this release it is all about the \u2018commonness\u2019 of its converging technology stack and extending mobile by creating and integrating apps seamlessly into its established server-based solution.\nBentley has been teasing us with its cloud-based analysis plans for a few years now and next year this will become reality. Optioneering is being built into the company\u2019s products and Bentley has harnessed the power of the cloud to offer subscribers easy access to its \u2018solvers\u2019 with near unlimited power.\nThe promise that next year we will see complete building analysis with multiple simulation and analysis solvers running simultaneously via the cloud will mean every design should be proven to work and the best solution possible. This is incredibly exciting.\nWith ProjectWise Essentials, cloud delivery, term licensing and the possibility to download design applications via Numescent technology, the barriers to entry will become much lower for new customers with smaller projects.\nWith the CONNECT Edition Bentley has laid the ground for engineering data and applications to be much more dynamic and pervasive.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-arithmetica-launches-pointfuse-v3\/","title":"NEWS: Arithmetica launches Pointfuse V3","date":1505865600000,"text":"Point cloud processing tool gets new \u2018selectable surfaces\u2019 feature and monthly licensing options\nTo coincide with the V3 release of point cloud conversion software Pointfuse, Arithmetica has introduced a new monthly licensing option suited to project-specific usage.\nPointfuse is a modelling engine that converts the vast point cloud datasets generated by laser scanners or photogrammetry into \u2018high fidelity\u2019 vector models. The resulting models are said to be 1:4 the size of the original point cloud.\nThe big new feature for Pointfuse V3 is \u2018selectable surfaces\u2019 which allows surfaces within Pointfuse 3D models to be identified, grouped and classified. According to Arithmetica, these characteristics bring workflow efficiencies to the project that are not possible when working with point clouds or traditional mesh models.\nOther features include changes to the user interface and data processing algorithms, making the software faster \u2013 with point clouds converted to high fidelity models in a \u2018matter of minutes\u2019, and simpler \u2013 with one parameter, one button processing.\nPointfuse is designed for anyone capturing or using point cloud data and uses advanced statistical techniques to create vector models which can exported to IFC and FBX for manipulation in any industry-standard CAD system.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/computational-design-symposium\/","title":"Computational Design Symposium","date":1268956800000,"text":"In the second part of his analysis of the Computational Design Symposium at Autodesk University, Martyn Day asks what the impact of computational design tools will be and when Autodesk\u2019s will be available.\nRobert Aish has been at Autodesk for around 18 months now. Formerly working at Bentley Systems, where he designed the Generative Components (GC) layer for MicroStation, Dr Aish opted to move to Autodesk and leave that work behind. This meant he had to start from scratch to develop a new application for computational design on a new platform. To date, the annual Computational Design Symposium, held the day before Autodesk University in Las Vegas, is the only real chance to see what advances Dr Aish\u2019s team has made.\nThe technology shown at the Computational Design Symposium did not fail to impress, with Dr Aish providing a compressed demonstration of his scripting language, codenamed D Sharp, to produce the complex structures and surfaces of the Metz Pompidou Centre, a competition prize winning design by Shigeru Ban, Jean de Gastines and et Philip Gumuchdjian.\nChoosing to model something difficult and real is in keeping with the theme of the event, which aims to showcase existing, completed, work with an array of existing computational solutions. At the inaugural event, Neil Katz of SOM, gave a memorable talk on his work for the World Trade Centre, which involved him creating special programs to explore different options in configurations.\nAlgorithmic Design\nDr Aish believes that with computing power and 3D, we are at a point of design innovation. Pre-CAD, when designing, an architect would grab a felt tip or pencil and draw. When computers came along the software mainly emulated those old drafting techniques of creating representational 2D geometry. CAD meant we could edit, copy and change the drawings, increasing productivity. This is very similar to what word processing did to writing, but it but did not change the way we write. Drafting tools do not change the way we draw and it is still a surprisingly manual process.\nComputational design is designed to take some of these manual tasks away and reduce the risk of handling complexity. A conglomeration of rules defines buildings, for instance, to populate a facade with a specific type of glazing panel, or use specific mullions, the computer can complete all this.\nThen there is the \u2018what ifs\u2019: should you chose a different glazing solution, or wish to study the impact of a shading solution, testing every option is repetitive and tedious, requiring the redrawing the facade many times, due to the number of different possible options. With technologies like Computational Design, sliders can be created to make geometrical changes to the model, with the computer\u2019s processor taking the strain.\nThis does, however, alter the design process, as the architects or the person creating the design rules for the computational design system needs to analyse and define what make up the building\u2019s important rules before they are scripted into a D Sharp program. This bespoke scripting language will give teams of users the ability to make the design rules themselves.\nVancouver Convention Centre\nOne of the talks given this year highlighted the benefit of this approach to common problems, specifically curtain walling. The stunning Vancouver Convention Centre design has 150,000 square feet of inclined structural glass curtain wall, which needed to be analysed using advanced Finite Element Analysis (FEA) and then optimised to minimise deflection from its own weight and wind. This end result was a unique curtain wall that is supported by wind trusses hung from the roof structure above.\nThe $833 million project was conducted as a Building Information Modelling design, using integrated models and advanced CAD techniques. While BIM brought its challenges, there is little doubt that this steel job would have been more difficult without it. With more than 19,000 unique steel pieces fabricated for the structure, some of an enormous size, it was imperative that the owners, engineers, and fabricators could see what was being built and where conflicts may occur. In the end, communication lines were open and BIM was instrumental in understanding the geometry, minimising the amount of steel required, and solving problems before they happened.\nConclusion\nDr Aish is in the Autodesk\u2019s Platform Group, not the Architecture\/ Engineering\/ Constructions division, which would indicate that the technology is seen as a core capability that all the verticals could use. It does after all manipulate standard AutoCAD geometry. While much of the emphasis is around architecture and fabrication for that industry, it is clear that Autodesk sees this as potentially beneficial to all its customers.\nWith the new releases of Autodesk product almost upon us, it would be wild speculation to think that Autodesk is ready to release its computational design tools to the market. Looking at the company\u2019s modus operandi, there should be a lengthy trial on the Autodesk Labs website before any commercial version would be available.\nAlso, starting from scratch, 18 months is not a lot of time for Dr Aish to flesh out all that would be required. However, the complexity of the Metz model, which was created, albeit in a canned demo, was exceedingly more advanced than any geometry demonstrated in the previous Symposium. I would hazard a guess at seeing something more concrete on Labs in the next 18 months.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/workstation-specialists-ws-m1510\/","title":"Workstation Specialists WS-M1510","date":1323388800000,"text":"A powerful mobile workstation, with many of the hallmarks of a desktop machine.\nRegular readers of AEC Magazine will no doubt be familiar with Workstation Specialists\u2019 desktop machines, but may not be aware that the company also does a line in mobile workstations.\nThe WS-M1510 is the firm\u2019s latest mobile offering and, with a high-end CPU and GPU, looks to deliver \u2018desktop\u2019 workstation class performance in a portable package. The compact 3.1kg chassis features a 15.6\u201d W-UXGA (1,920 x 1,080) LED backlit display and is certainly light enough to carry around for the day, but with power-hungry components, battery life is short and you can never venture too far from a mains socket.\nThe biggest culprit when it comes to power consumption is Nvidia\u2019s ultra high-end Quadro 5010M Graphics Processing Unit (GPU), which draws 100W at peak. But what this GPU lacks in power efficiency it more than makes up for in performance, delivering excellent results in our 3D graphics tests. But there is more to the Quadro 5010M than just 3D frame rates. With 4GB GDDR5 memory and 384 CUDA GPU cores the Quadro 5010M can be used to accelerate render times in iRay-enabled applications, such as Autodesk 3ds Max. Users of simulation software, including DS Simulia Abaqus and Ansys can also make use of this powerful GPU compute chip.\nAt the heart of the WS-M1510 is Intel\u2019s new Core i7 2960XM Extreme Edition, currently the fastest mobile CPU out there. This quad core chip runs at 2.70GHz as standard, but automatically clocks up to 3.70GHz whenever it can. Support for Intel HyperThreading means a total of eight cores (four physical and four virtual) can accelerate render times in applications such as 3ds Max. However, with all CPU cores running flat out, the Turbo Boost speed up will be limited. This helps explain why the WS-M1500 was a little off the pace in our multi-threaded 3ds Max Design test. However, it compared much more favourably to a desktop quad core Core i7 workstation under our standard CAD test, which does not push all four CPU cores to the limit.\nElsewhere, the WS-M1510 has other hallmarks of an entry-level desktop workstation. It boasts a sizeable 16GB of PC3-1600MHz high speed DDR-3 memory and a 250GB high performance Intel 510 Series SATA3 Solid State Drive (SSD). There is no support for a second drive, but larger capacity drives are available.\nLooking beyond the key components, the WS-M1510 is a sleek laptop. The matt grey chassis is well built, with grills at the back to expel excess heat. It features good connectivity options including 2x USB 3.0, 2x USB 2.0, mini FireWire 400, eSATA (USB Combo) Intel 5300 Wireless LAN and Bluetooth. It also has a Mini PCI-E card slot for extras such as mobile broadband. In use, the full sized keyboard is solid and responsive and the numerical keypad a welcome inclusion for precise input.\nOverall, the WS-M1510 is a capable machine for designers and engineers that require \u2018desktop\u2019 workstation performance wherever they go \u2014 providing there is a power supply, of course.\nThe one downside is price. At \u00a33,595, the WS-M1510 is certainly not cheap, but much of this cost is made up of the premium CPU and GPU. Stepping the CPU down a 0.2GHz to a Core i7 2920XM Extreme Edition will shave a significant amount off the price tag. For those that do not require such a powerful GPU, there are some more cost effective options, specifically targeted at CAD users, coming soon.Greg Corke","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-revit-opened-up-with-teigha-bim\/","title":"NEWS: Revit opened up with Teigha BIM","date":1484611200000,"text":"Open Design Alliance (ODA) makes Revit file SDK available for general licensing\nTeigha BIM, a stand-alone Software Development Kit (SDK) from the ODA for working with Autodesk Revit files, is now available for general licensing. The aim of the BIM Kernel is to allow CAD software developers to build in Revit RVT and RFA read and write capabilities into their applications.\n\u201cWe are very pleased to offer Teigha BIM licensing to the general public,\u201d commented Neil Peterson, ODA President. \u201cThrough the end of last year, Teigha BIM was only available to our top-tier subscribers. But import and visualization are now ready for production use\u2014so it\u2019s the right time to offer this technology to a wider audience.\u201d\nSergey Vishnevetsky, Development Director at ODA, added, \u201cFeedback from early adopters has been quite positive. A number of our customers are already shipping importers and visualization solutions based on Teigha BIM.\n\u201cOur focus this year has shifted to parametric entity creation, a feature that is in high demand from users looking to streamline their automation processes. We expect rapid progress in this area in 2017, and interested parties can register on the Open Design website to receive real-time progress updates.\u201d\nLast year Bentley Systems told AEC Magazine that it is testing Revit (RVT) file read capabilities inside Bentley AECOsim Building Designer.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bim-show-live-expands-keynote-speakers\/","title":"NEWS: BIM SHOW LIVE expands keynote speakers","date":1484611200000,"text":"Conference looks beyond BIM, exploring creative thinking and progressive technologies\nBIM Show Live 2017, which will be hosted in Newcastle upon Tyne on 1 and 2 February, has added Mark Shayler and Nell Watson to the line up of Keynote Speakers, with a view to offering a fresh perspective on creative thinking and progressive technologies. The full conference programme can be seen here.\nWith an enormously successful background in environmental innovation, throughout his career to date, Mark Shayler has saved his clients over \u00a3120 million by doing what her calls doing things better. Mark will give his Keynote presentation at BIM Show Live 2017 on February 1 at 10:20.\nNell Watson is a leading global expert on artificial intelligence and a future-orientated thinker with a passion for combining psychology and technology. Nell will give her Keynote presentation at BIM Show Live 2017 on February 2 at 13:45.\nBIM Show Live 2017 delves into the world of data driven technology to achieve the UK Governments\u2019 vision of a Digital Build Britain.\nTickets cost \u00a3300 + VAT and include full access to the two day conference & exhibition, lunch, refreshments and entry to the BIM Show Party.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-trimble-updates-tekla-structural-designer-for-engineers\/","title":"NEWS: Trimble updates Tekla Structural designer for engineers","date":1444262400000,"text":"New features designed to further automate building analysis and design as well as expand seismic analysis capabilities\nThe latest release of Tekla Structural Designer, the structural engineering design and analysis tool for steel and concrete buildings, includes a number of new features designed to automate building analysis. This includes the automation of tedious and complex tasks such as wind loading calculations, floor vibration checks and floor loading.\nOther new features include expanded seismic analysis and design features that automate accidental torsional effects and the required seismic design combinations. According to Tekla, this allows engineers to use one product all the way through to code-compliant design.\nIn addition, Tekla Structural Designer now offers expanded beam design options and integration with cellular beam provider, ASD Westok, and its proprietary Cellbeam software that simplifies design with Westok elements and allows for quick comparisons of different floor beam systems.\n\u201cThis new release is a significant update after the initial launch of Tekla Structural Designer earlier this year, further automating structural analysis and design to save time, allowing engineers to gain complete confidence in their design decisions,\u201d said Barry Chapman, director of engineering for Trimble\u2019s Structures Division.\nTekla Structural Designer is available now and is an integral product in the Trimble Buildings portfolio, which also includes Tekla Structures BIM software, Tekla Tedds software for structural calculation automation and SketchUp Pro.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/build-london-2019\/","title":"Build:London 2019","date":1570492800000,"text":"Epic Games\u2019 Build:London event has become an essential diary date for the AEC fraternity. There\u2019s a great balance between showing real-world applications and giving insight into what\u2019s coming next in Unreal Engine\nThere was a time when Virtual Reality was all hype; now it\u2019s a reality in most of the major AEC firms. While VR is still used mainly as a customer-facing technology, there are a number of companies that are deploying it as a design tool, from concept through to validation.\nUnreal Engine by Epic Games has been one of the driving forces behind the rise of VR in the AEC sector. This is mainly because the 3D engine can be used to create visually impressive, custom VR experiences, but it is also down to AEC-focused applications that are built on the 3D engine, such as Twinmotion, which Epic Games acquired earlier this year.\nTo support its growing community, Epic Games now hosts an annual London event focused specifically on the AEC market. Build:London showcases the breadth and depth of real-world applications from users of Unreal Engine as well as giving the community an opportunity to see new developments in the software.\nThe conference\nThe first presentation was an onstage interview with CG artist Alex Roman, creator of \u201cThe Third and Seventh\u201d. Roman manages to create amazing, gobsmacking realism and cinematic beauty in his architectural CG visualisations and films, using V-Ray and Unreal. This was an impossibly high bar to match for rendering quality. Gabriele Sorrento, the CEO of\nMindesk then gave a presentation of Mindesk Bridge, a VR design tool for McNeel Rhino and Solidworks. While the software can be used to model geometry in VR, at the moment the most common way to use it is as a design review tool. Assemblies are imported and the application allows control over transparency of objects, \u2018explosion\u2019 of assemblies and even kinematic movement, all in VR.\nSorrento was joined by Helmut Kinzler and Aleksandra Mnich from Zaha Hadid Architects to show how they used Mindesk with Rhino to explore a project they had recently worked on, the L-Acoustics \u2018Loop Sound lounge\u2019 chair. Mnich gave a live demonstration of geometry creation, creating surfaces to complete an enclosure. As with many live demos, unfortunately it didn\u2019t work for long, so they defaulted to a canned video but kudos for trying!\nAaron Perry of Allford Hall Monaghan Morris (AHMM) gave a fascinating talk about the impact and evolution of using the real-time engine in the whole architectural lifecycle. One of the more impressive uses of Unreal showed how the company uses a giant 3D model of London to place their various project buildings and explore all aspects of their design in context. Within the model, which is many square kilometres in size, it is still possible to go from flying above the blocks and streets, and then into the lobby of a project building to explore the design \u2013 all in real time. An impressive display of both scale and detail.\nJohn Murphy, Creative Director \/ Motion at the BBC talked on its design and deployment of the \u201cMatch of the Day\u201d virtual set, which uses a green screen studio with an Unreal virtual overlay. Gary Lineker has never looked more weird in a sea of green and one can only imagine how hard that would be to do. It\u2019s been such a success for the BBC they are thinking they might just green screen the pundits at major sporting events and save on not building sets around the world.\nDavid Weir-McCall of design practice CallisonRTKL took us in a different direction and demonstrated how the firm uses Unreal Engine and immersive techniques to gather data and better understand how people react to their designs for sustainability, value engineering and fa\u00e7ade development design.\nIn each individual VR experience, user position, target position, viewer angle, and direction of travel are all collected to see how people react in the spaces, where they go, where they don\u2019t go, what they avoid and what they look at. This creates heat maps in the digital space and can inform the designers how a space works. It can be used with a Digital Twin of an as-built, as well as proposed changes or new builds. In terms of modelling at a country scale, Ludvig Loven of AF Consult blew the barn doors off. The impressive demonstration detailed how the firm built and modelled an area around Stockholm, comprising thousands of square kilometres, in Unreal Engine for a proposed train line (160km of track, 200 bridges with 30 tunnels). Within a few seconds we went from soaring above in orbit to resting on the grass, next to the train tracks, alongside a bunny rabbit. We also saw multiple designers interactively create a road in VR, in real-time, across dozens of kilometres of Sweden. In terms of detail on a grand scale, a team from Tencent shared how they\u2019ve built an impressive \u2018Digital Twin\u2019 of Shanghai using Datasmith and Unreal Engine.\nHenry Richardson, Professor of Architecture at Cornell School of Architecture, spoke on the impact of Unreal Engine in teaching urban design.\nTwinmotion\nTwinmotion was acquired by Epic Games earlier this year and it always demonstrates well. With the company\u2019s new-found partnership with Graphisoft, Twinmotion was given an ArchiCAD model to populate. With the rich component library and environments applied with simple sliders, the proposed project area was brought to life. With street furniture, trees, cars, flocks of birds and people we were taken through all the seasons. For architects, it\u2019s like the best SimCity ever made.\nComing next\nIn a fleeting couple of images, we saw some interior and exterior laser scans comprising billions of points imported and rendered in Unreal. This is not a small undertaking, as laser scan files are extremely heavy on data. This will be coming soon, as Unreal continues to focus on being able to integrate common AEC formats.\nWe were also treated to a sneak-peek at early development work on a core geometry modelling engine that\u2019s being created inside Unreal Engine. It looked to be powerful and intuitive (push\/pull driven) and capable of making complex forms. This should be an interesting direction for the company.\nThe next release will also have a real physical sky and while Unreal is all about real-time, the release will be able to render super high resolution 20k in a non-real-time mode.\nConclusion\nThe running themes and messages from the talks were that Unreal Engine can handle huge models of detailed landscape. It eats complex geometry without any impact on the frame rate and speed of experience. Frankly it seems like CG alchemy!\nIt\u2019s clear that there is a trend to deploy VR in conceptual design, model checking, data analysis, training and experimentation. By moving VR from a backend to a front-end tool, early adopters are finding useful insight into their designs. Virtual Reality is in itself a design application not just an experience. However, we have yet to be convinced that accurate surface modelling in VR with current input and manipulation wands can compete with a mouse or pen-based desktop system, but it\u2019s not stopping people from trying.\nWhat seems to work the best is if the models are blocks, like Minecraft, so perhaps massing will become increasingly popular.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/navvis-indoorviewer-extends-reach-of-point-clouds-beyond-bim\/","title":"NavVis IndoorViewer extends reach of point clouds beyond BIM","date":1539129600000,"text":"Web-based application converts static laser scans into immersive 360\u00b0 walkthroughs\nLaser scanning professionals can now automatically convert E57 point cloud files into interactive, realistic 360\u00b0 walkthroughs, using the latest release of NavVis IndoorViewer.\nNavVis IndoorViewer is a web-based application that displays \u2018realistic\u2019 digital twins using 360\u00b0 panoramic images, point clouds and maps generated by 3D scanning devices. Users can move around digital twins of scanned spaces as if they are on site and use the interactive functionality to add, search for and route to geo-tagged information and take accurate measurements.\nNavVis IndoorViewer is designed for laser scanning professionals who want to extend the use of point clouds beyond BIM models and building plans to a wider range of building stakeholders who would also benefit from 3D scan data. According to NavVis, this is particularly relevant for stakeholders working on complex projects or properties, such as manufacturing facilities and construction sites, where IndoorViewer enables remote access to the site and is used as a platform for collaboration and exchanging information.\n\u201cIndoorViewer was originally developed to display the data captured by our indoor mobile mapping system in a way that is accessible to every user. In recent years, we have seen that making scan data available to every building stakeholder is fulfilling an unmet need. Many of our partners using a NavVis indoor mobile mapping system for conventional scanning projects are offering IndoorViewer as an additional deliverable to increase the number of stakeholders who can make use of this data,\u201d said Felix Reinshagen, NavVis CEO.\n\u201cTo meet the growing demand for extending the use of valuable 3D scan data, we developed a feature that automatically renders 360\u00b0 immersive images from structured E57 point cloud files. The latest software release brings the full functionality of IndoorViewer to E57 point cloud files and therefore marks an important step towards our goal of making scan data meaningful for every building stakeholder.\u201d\nNavVis IndoorViewer currently supports third party point cloud files in most standard formats. However, a key component of the immersive experience that NavVis IndoorViewer provides is the 360\u00b0 panoramic images. The new IndoorViewer feature bridges this gap for structured E57 files by automatically rendering 360\u00b0 immersive imagery from E57 point cloud files. This means data collected by terrestrial laser scanners can now also be used to create realistic, immersive 360\u00b0 walkthroughs that can be published and shared online without the need to download or install software.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/archicad-22-and-the-future\/","title":"ArchiCAD 22 and the future","date":1531440000000,"text":"BIM software developer Graphisoft has been busy working on its latest release of ArchiCAD which had its world premiere at its Key Customer Conference in its home town of Budapest. Martyn Day reports\nGraphisoft has come a long way since it launched ArchiCAD back in 1984 on an Apple Lisa computer. Thirty four years later, as BIM tools go, ArchiCAD\u2019s development velocity is never in doubt. Consistently, year on year, Graphisoft adds impressive capability to its multi-platform flagship product. In the last five years, the company has embarked on rewriting core sections of code to make use of modern multi-core processors and add intelligence into the creation of complex and custom objects, such as stairs. Release 22 continues in that same vein and, amongst other additions, sees a rewrite and super-charging of the Fa\u00e7ade\/ Curtain Wall tool.\nAt the Key Customer Conference, \u00c1kos Pfemeter, vice president, marketing explained that ArchiCAD 22 is fundamentally about design workflows, not so much this year about BIM deliverables and 2D outputs. Looking at the style of buildings built today, Pfemeter explained that, despite the capabilities of today\u2019s design tools, there are still many new rectilinear buildings, probably due to cost constraints and to maximise value, so architects look at new ways to make buildings interesting and that seems to be reflected in the diverse and imaginative approaches to fa\u00e7ade design, especially with regards to patterns and shading.\nArchiCAD 22 helps address this by enhancing the fa\u00e7ade tool and supporting complex patterns through easy to use tools. The new workflow means that this can start at the concept stage and be supported all the way to final BIM model and automatic detailing.\nFa\u00e7ade design\nWith traditional curtain wall tools, the out of the box solutions don\u2019t intelligently cater for intersections and design variance. Typically, a curtain wall would be created and then the user would have to work out panelisation line-up and column width. There was a lot of trial and error and, if something changed in the design, it had to be done all over again.\nIn ArchiCAD 22, there is a fully-fledged pattern editor where components and frame types can be selected, edited and deleted on the fly. Diagonal frame patterns can be added, which are associative and will dynamically change with edits to the building.\nEditing panels and introducing custom panels within a fa\u00e7ade are now much more straightforward from the pattern edit interface and these remain part of the scheme and are associative to design changes. Taking this to the extreme, it\u2019s possible to create complex frame-only patterns, or frameless designs.\nOutside of the pattern design tool, Graphisoft has also devised a way to capture sketches to convert to frames through the scheme editor. It\u2019s possible to import scanned images or hand sketches for reference. Simply create an empty fa\u00e7ade and start drawing. As framing is added, ArchiCAD will ghost in the pattern in X and Y, showing how the sketch repeats, enabling the architect to find perfect repeat framing. After a few seconds, the fa\u00e7ade is generated.\nThese sketches can be scaled and manipulated in the scheme editor and ArchiCAD automatically updates and ensures the patterns wrap neatly around the corners of buildings. This can be used for metal, stone cladding or anything you want to wrap a building in. ArchiCAD also looks after documentation from schematics, corner section details with smart corner components with selectable detail levels, down to construction-level.\nTo cater for custom components within a fa\u00e7ade design the complex profile capability has been extended to work within curtain walls. Through the profile manager, profile frames can be drawn, as with any beam or column, and can be used as curtain wall frames. Panels and frames can be grouped to create unitised panels (glazing plus half the frame). It\u2019s also possible to custom draw shapes or text which can be stuck onto panels.\nA new polygonal window component can handle any window that is not rectangular. This can be inserted into curtain walls and is defined by the frames. Edit bounding frames and the window automatically updates. Like any other window it can be opened in the model This is really powerful and simple to do.\nGrasshopper\nArchiCAD is unique among architectural BIM tools in its ability to be driven by McNeel Rhino\/Grasshopper generative scripts. The new curtain wall object has been enabled to be driven from Grasshopper, which should lead to some incredibly complex fa\u00e7ade designs which would not have been possible in previous releases.\nGraphisoft\u2019s connection with Grasshopper is getting deeper with each release. In its first incarnation, the idea was that the concept design could drive the basic definition of the BIM model, which would then be fleshed out and documented. Now, Graphisoft is looking to offer workflows which run in the opposite direction as well, which is leading to grasshopper digging deeper in to the BIM data logic. They have called this workflow \u2018deconstruct\u2019 and it is available in the 2.0 version of the Grasshopper link.\nGraphisoft has now linked grasshopper to BIM elements, geometry, meta data and attributes, not just points and lines, and can be used to drive the Grasshopper design logic. The example given was a conceptual zone model. Using Grasshopper, the zones can be used by the script to generate columns and slabs directly into ArchiCAD in seconds. If the zone areas are updated or the building geometry altered, the columns and slabs are updated automatically, also depending on the zone type, office\/residential, retail etc.\nThrough the use of Grasshopper, it\u2019s possible to generate complex rules in seconds, while the building is edited. These are also reflected in the documentation with schedules and visualisations updating automatically. It\u2019s then easy to go to the next level and add complex elements like the new curtain wall and see updates.\nOut of the box, ArchiCAD is great at handling repetitive fa\u00e7ade patterns. While complex fa\u00e7ades can be created manually, editing these through Grasshopper makes it possible to generate complex designs without the edits, using sliders or more complex programmatic drivers to control all the fa\u00e7ade elements at once. These can be seen in Rhino in real time and then synchronised with ArchiCAD to build the complex fa\u00e7ade as high-detail BIM objects.\nIf you are working on a more complex form, which has double curvature and, therefore, very demanding curtain wall needs, the link between Rhino\/Grasshopper and ArchiCAD enables the creation of pretty much any shape as a shell, be that fa\u00e7ade or roof, so even a one-man band architectural practice can be Foster + Partners. At the launch, in one impressive demonstration, ArchiCAD and Rhino were used to automatically create a fa\u00e7ade made from a pixelated image of Steve Jobs.\nPerformance\nAs users create ever more complex designs, BIM tools play a never-ending game of trying to provide more performance. More objects and detail take more CPU and GPU power. Pfemeter explained that ArchiCAD\u2019s customers year on year increase the size of their BIM models. In fact, over the last five years, they have seen a 73% increase in the number of elements.\nIn ArchiCAD 22, responsiveness of model navigation has seen the appliance of science. Scrolling\/zooming and panning have learnt some tricks form the world of GIS, which tiles data and distributes the load to CPU cores. ArchiCAD now features \u2018mesh learning\u2019 and applies it to BIM models.\nArchiCAD is now always analysing each window tile and how each user interacts with them, as well as assessing unique elements of the model data. The software is always looking ahead to optimise the user experience and responsiveness of ArchiCAD by intelligently processing and caching data in advance.\nTo prove the point of how fast ArchiCAD has become, they loaded a DWG map of London; displaying millions of polylines, zooming, panning and scrolling were very smooth at all levels of detail.\nLevel of Detail\nGraphisoft has made a significant commitment to supporting its Japanese customers, who have very defined workflows and are currently in a nationwide drive to establish BIM processes and standards. In adding functionality to cater for their needs, the benefits are also made available to all ArchiCAD users.\nFukashi is a design term used in Japan which relates to the way buildings are designed with tiling in mind from the outset, to ensure designs of columns perfectly fit the final finish. ArchiCAD now has a specific property layer for the additional concrete layer to allow for the lining up of tiles.\nBIM without limits\nIn addition to ArchiCAD, Graphisoft produces a BIM-sharing application called BIMx which has evolved greatly throughout the years. In its base form, architects could create a BIMx version of their model, which was a self-extracting executable they could share with clients. It would be sent\/downloaded and would expand to open a 3D environment with the BIM model, which could be navigated without the need to have ArchiCAD present and without the need to know how a BIM tool worked.\nSince then BIMx has expanded to support mobile devices and it\u2019s one of the treasured applications for ArchiCAD users. However, it\u2019s been expanded to now not only support mobile and desktop but also browsers, so model experiences can be shared via an email with a web link, or even embedded into a web page.\nThere are also new tools for automatic transitions through saved \u2018favourites\u2019, which can be stopped for dynamic interaction at any point. This is a fantastic addition, not only for client interaction. It means competition entries and past BIM designs can be embedded in company website.\nOne of the key drawbacks of mobile based BIMx is the limited amount of memory that applications can access, which limits the size of BIM models that can be shared on mobile devices.\nGraphisoft demonstrated BIMx Lab, a technology under development which will provide a way to share and display BIM models of unlimited size, as it uses a technology Graphisoft calls \u2018memory streaming\u2019, which live streams data to BIMx instances and applies occlusion culling, which means the software only displays and needs the data which is visible, cutting down the amount of data required at any one time. This is most commonly used in gaming technology. It will be available for public trial this month before the final product is shipped.\nBIMcloud\nGraphisoft\u2019s BIMcloud enables real-time, secure teamwork between offices, through the Internet and it replaces the old BIM Server technology. BIM Server has been replaced with a free version of BIMcloud called BIMcloud Basic for real-time team collaboration.\nThe full version adds support for multiple ArchiCAD versions, BIMX Pro integration with real time messaging, project folders, auto scaling of resources, caching, role-based management, diagnostics and back-ups. A new element reservation system is being implemented to allow users to reserve components they need in a model while others work around them.\nProject Everest\nOne of the more intriguing technologies mentioned in passing was called \u2018Project Everest\u2019. This looks to be a cloud-based open BIM ecosystem which seeks to link the Nemetschek Group solutions into a tightly aligned design workflow. This was described as a \u2018Disruptive Design Workflow\u2019 and \u2018Open Standards based Detailing Solutions\u2019 where collaboration can be facilitated down to parameter level from a fully integrated cloud-based spatial, structural and analytic model.\nNemetschek Group owns three BIM authoring tools \u2013 ArchiCAD, VectorWorks and Allplan and offers a range of associated BIM \/ construction tools such as Bluebeam, DROFUS, Maxon, SCIA and Solibri.\nNot a lot more was said about Everest nor its capabilities, but it did sound similar to perhaps what Autodesk is trying to do with Project Quantum.\nThe code name has me mulling over what would be the biggest challenge in the BIM world? The highest mountain to climb? I\u2019d say making sense out of the relative Tower of Babel the BIM vendors have managed to create would surely be number 1.\nNemetschek has a history of supporting open BIM standards and interoperability, and while IFC plays an important role, it\u2019s very file based and not as common a data format as one would have hoped for. AEC Magazine will endeavour to find out more and report back.\nCustomer presentations\nGraphisoft laid on an array of customer talks from around the globe. There were eleven presentations covering architecture and construction firms, from small practices to industry giants.\nSpecifically of note were presentations from Kajima x Global BIM\u2019s Keiichiro Yoshida from Japan. His presentation gave some really interesting examples of their newly developed SmartCON Planner software for ArchiCAD, which came from Kajima\u2019s supply chain experience in collaborative BIM working. It also covered the use of Internet of Things (IoT) technology from BIM workflows to construction site and IoT sensors from the environment and intent to eventually develop an automated construction system.\nThe presentation by CEO and architect of Aidea, Philippines, Abelardo M. Tolentino Jr was an amazing growth story about his company and \u2018The Power of Technology Integration\u2019. Tolentino explained the 3-year development of an automation plug-in they have created for ArchiCAD, which started off as an automated design summary but led into a full automatic construction documentation plug-in from a detailed model. It takes about a day of processing but produces the whole documentation package, including automated scheduling.\nLink Arkitectur (Norway) is the largest user of ArchiCAD in Scandinavia and has been using ArchiCAD since 2000. The firm mainly works in healthcare, education and residential. Bjorn Erik Lie (founder) and Steen Sunesen (project manager) described their experience of working using IPD [integrated product delivery] and developed a much-improved procurement strategy geared toward collaborative BIM workflows.\nConclusion\nArchiCAD development continues and sets the industry standard in delivering new functionality for the maintenance fee. ArchiCAD 22 will enable some exceptionally interesting fa\u00e7ade designs from its customers, especially if used in conjunction with McNeel Grasshopper. The links with Grasshopper are going ever deeper and the demonstration of ArchiCAD with Grasshopper to conceptually explore a live design and make edits with real-time feedback should pique the interest of any existing customers as to how they could adapt their current workflows to reap the rewards.\nFrom the customer talks it was very interesting to see that there were many aspects where customers were actively taking part in the development of their own applications, to really automate and connect their businesses to get the benefit of BIM all the way to construction. Once firms have seen the benefit of having a BIM process, many are also looking to automate as much of the documentation internally as they can, giving them more time to create. While I think the adoption of all cloud-based BIM servers has been slow from all vendors, the value of what they offer is increasing every year. Some firms will use these systems to trap customers,\nwhile I think Graphisoft with Everest may be trying to liberate BIM data into an open common data environment. We will find out more in the coming months.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/capital-injection-for-ai-based-construction-verification\/","title":"Capital injection for AI-based construction verification","date":1552521600000,"text":"Airsquire\u2019s Airsync survey platform automates the comparison of 3D scans and 3D models\nAirsquire, a Dutch firm that uses Artificial Intelligence to automate construction verification, has received a six-figure capital injection from venture capitalist henQ.\nThe company\u2019s intelligent Airsync as-built survey platform automatically compares 3D models and 3D scans of the physical asset to identify inconsistencies.\nAccording to Airsquire, its technology allows errors in 3D construction projects to be detected faster with 100% coverage. This is in contrast to \u2018error-prone\u2019 manual verification which the company says makes it impossible to verify every inch of the construction space, often resulting in undetected discrepancies.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/mapinfo-introduces-newest-version-of-site-selection-system-mapinfo-anysite\/","title":"MapInfo introduces newest version of site selection system-MapInfo AnySite","date":1053648000000,"text":"MapInfo Corporation announced that MapInfo(R) AnySite(R), its premier location-based software tool, has been significantly enhanced with new capabilities and advanced data to provide analysts and marketers in the retail, restaurant, real estate and financial industries with a more powerful decision support tool. Following the company\u2019s recent acquisition of Thompson Associates, MapInfo AnySite v8.5 combines MapInfo data, including its 2003 demographic estimates, with Thompson\u2019s best-of-breed solutions to deliver critical demographic information with trade area analysis. This enables companies to make better, more informed decisions regarding site location, target marketing and product merchandising in both the U.S. and Canada. Demonstrations of MapInfo AnySite are available at booth #147. MapInfo AnySite is an essential marketing tool that enables retail, restaurant, real estate and financial services to transform their store build-out decisions. The easy-to-use mapping and reporting capabilities allow companies to connect to, retrieve and report both MapInfo data and proprietary, customer databases in order to analyze trade area data more easily and accurately. New enhancements to MapInfo AnySite include predictive analytic modules, which empowers companies to determine the best markets and the optimal number of sites within those markets to maximize their networks. Additionally, this new version of MapInfo AnySite contains demographic updates including Census 2000.\nHome Geospatial Applications LBS MapInfo introduces newest version of site selection system-MapInfo AnySite","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/opinion\/civil-engineering-and-the-internet-of-things\/","title":"Civil engineering and the IoT","date":1502064000000,"text":"Ignasi Vilajosana, CEO of operational intelligence specialist Worldsensing, explains how civil engineering can tap into the power of connected devices to turn data into actionable insights on challenging projects\nBy 2020 there will be 20 billion Internet-connected devices in the world, according to a report by market research company Gartner. As the Internet of Things (IoT) era develops, and devices become smaller, more accurate and more relevant to a wider range of sectors, there is an opportunity for civil engineering to drive greater operational efficiencies as a result of increased access to real-time data.\nIn fact, we\u2019re already witnessing a new age of geotechnical and construction monitoring that allows engineers to analyse projects and existing structures remotely and gather information from a range of assets digitally and in real time.\nIn this article we will consider how, through the use of field-proven sensor technologies and wireless networks, civil engineering organisations are able to benefit from automated monitoring, from the construction phase to the ongoing maintenance and management of critical assets. We will then explore how the data generated can be used to deliver operational insights that improve productivity, quality management and project performance.\nA shot in the arm\nData collection plays a vital and extensive role in construction. From monitoring water pressure changes during soil consolidation, and settlement during soil reclamation works, to assessing terrain deformation during tunnel construction, being able to accurately record measurements over vast working areas is essential to project success.\nData collection, up until now, has primarily been performed manually, with engineers taking key measurements while out in the field \u2013 a method that often impedes productivity. Manual data collection can be inaccurate, complex and timeintensive. Engineers not only have to contend with harsh environments and difficult- to-access sites, but there is also the risk that, due to a lack of processing speed, site conditions can change between readings, rendering data unreliable.\nHowever, the emergence of the Industrial Internet of Things (IIoT) represents a significant shot in the arm for the civil engineering sector. The deployment of low-energy sensors to monitor an infrastructure\u2019s condition and environment allows for a vast range of data to be gathered remotely, before being aggregated and visualised via a secure online dashboard. By effectively streamlining operations and enabling real-time access to information, the key advantages are clear to see: speed, accuracy and efficiency.\nEppenburg Tunnel\nThe construction of the Eppenberg Tunnel in Switzerland is a great example of the way IoT technologies are currently being used to enhance geotechnical monitoring processes. Designed to reduce congestion on one of the most transited railway routes in the country, the tunnel will have a length of 3.1km on completion.\nBuilding this kind of infrastructure requires frequent and reliable geotechnical monitoring to detect any occurrences which may affect construction work. In order to ensure maximum efficiency and accuracy, the project team has deployed a wireless monitoring system, comprising 55 load anchors and 5 extensometers, controlled from outside the tunnel to gather readings from the geotechnical instruments that are being used during construction.\nThe low-consumption wireless sensor system delivers a highly efficient monitoring solution and sensors can be fully integrated with the various types of geotechnical instruments present. Crucially, it is the automated supervision of construction which means that the need for manual readings and control is virtually eliminated, greatly reducing the time and costs which might have previously been dedicated to supervisory tasks.\nThis ability to gather data quickly is also equally important when it comes to monitoring existing structures. Last year, a report by the American Road & Transportation Builders Association (ARTBA) found that almost one in ten bridges in the US are structurally deficient. This equates to 58,495 bridges out of the 609,539 bridges in the United States.\nMeasuring multiple factors such as vibration, inclination and tilt, and recording this information digitally can offer engineers significant assistance when it comes to improving the safety and maintenance of critical structures. Sensors can be deployed on a permanent basis, or moved on and off site each time a fresh set of data is required. New technology now offers ultra-long range solutions to facilitate remote readings in structural and geotechnical monitoring, some up to 15km (9 miles). This is particularly effective for civil engineering projects working on large geographical areas, such as pipelines, railways and tunnels, among others.\nOperational intelligence\nWhile automated processes that allow information to be gathered digitally can greatly reduce manual tasks and associated costs, conversely, the deployment of sensor technology across multiple aspects of a civil engineering project can sometimes result in \u2018data overload\u2019 from an abundance of unconnected data sets.\nAs a result, it is not the collation of data alone that delivers significant increases in productivity. Instead, it comes down to how that data is analysed and used.\nImplementing an operational intelligence solution centralises the data; but instead of limiting analysis to merely the visualisation of this aggregated information through dashboards, operational intelligence looks to take this one a step further. In other words, organisations that successfully exploit operational intelligence capabilities feed real-time insight into the decision- making process, not only for immediate effect, but also to help develop predictive and preventative maintenance approaches that will ultimately enhance productivity.\nGoing back to our earlier example of tunnel construction, the pairing of wireless sensing technology with operational intelligence within such an environment demonstrates how data can be turned into actionable insight. The ability to leverage real-time data means engineers are better placed to establish socalled trigger levels for movement throughout the construction process.\nIndeed, operational intelligence networks are even capable of providing warnings automatically through analysis of previous and existing data sets and machine learning capabilities. In the example of tunnel construction, if sufficient movement is detected, project teams can be alerted in ample time to make any necessary changes to the tunnelling procedure in order to counter such movement and ensure that construction continues as planned.\nSimilarly, operational intelligence can play a key role in helping civil infrastructure operators to form strategies for public safety and cultural heritage preservation. Authorities in the Italian city of Florence, for example, are using a network of geotechnical sensors connected to a software suite in order to monitor the famous Ponte Vecchio. Sensors located along the 32 metre length of the medieval structure provide real-time information on the stability of the bridge and nearby land. Should any unusual movement be detected, officials would be alerted and could act immediately to prevent any threat to public safety in a city that attracts some 16 million visitors each year.\nManual readings eliminated\nFor civil engineering organisations, the digitisation of assets undoubtedly promotes operational efficiency with automated supervision, all but eliminating the need for manual data readings. However, the true return on investment lies in having the platforms and processes in place to then manage this data.\nThe implementation of a relevant operational intelligence strategy to analyse and subsequently influence decisions has the potential to improve project performance and productivity. By using data to better control workforces, supplies and structural analysis, civil engineering organisations not only reduce the time and costs associated with projects, they also place themselves in a better position to evaluate risk and pre-plan responses.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/augmented-thinking-how-ai-is-coming-to-aec\/","title":"Augmented thinking - how AI is coming to AEC","date":1574035200000,"text":"The media is full of stories about Artificial Intelligence (AI) \u2013 both its huge promise and its possible negative effects on all aspects of our society. But what impact will it have on the AEC professions? Martyn Day explores\nIf we are to believe films like Blade Runner, 30 years from now Artificial Intelligence (AI) will have progressed so far that we can replicate ourselves, so precisely that we find it almost impossible to tell \u2018them\u2019 apart from \u2018us\u2019. As we jointly populate a polluted planet, where farmers harvest weevils for protein under acrid orange skies, surrounded by the decay of our concrete cities, we can look forward to a struggle for survival as \u2018they\u2019 seek to escape their weak and stupid creators. And running at 2hrs and 44 minutes, we can all turn against the director who clearly forgot how to use video editing software.\nDystopian futures have become the de facto standard when contemplating the potential fruits of our technical prowess and AI is possibly the most exciting, daunting and feared technology under development today. It is seen as the sword of Damocles, as while it has the potential to help us across a huge range of tasks, it could also make many of us redundant, irrespective of education level or type of job. The deployment of AI and automation is expected to impact doctors, lawyers, accountants, financiers, as well as possibly architects and engineers.\nWith today\u2019s cloud infrastructure, all there needs to be is one algorithm per job function developed, having learnt from thousands, millions of previous legal cases, patients, buildings, construction sequencing and, with the power of the cloud, it can be flicked on and be everywhere \u2013 an overnight global job grab. Plus, the more it\u2019s used, the more it learns. In this vision of the future, AI is more 2001: a space odyssey \u2018HAL\u2019 than Tyrell Corporation replicant, so you might want to put off going on that spacewalk; it might not let you back in!\nBringing matters closer to home, a recent Instagram video post by New York-based Chilean designer Sebastian Errazuriz, went for the jugular.\nOver a warm beverage, Errazuriz delivered his warning, \u201cI think it\u2019s important that architects are warned as soon as possible, that 90% of their jobs are at risk. If you haven\u2019t really realized that, you should be taking measures right now, as soon as possible.\nThe reasons are quite simple. When you happen to be in the type of project, the type of field in which experience requires two to three years, it\u2019s almost impossible for you to compete with any kind of machine that can immediately have 10 times, 50 times, 100 times, a thousand extra million times your experience. \u201cThe reality is that we already have enormous quantities of data, enormous quantities of blueprints and models that hundreds of thousands, millions of houses that have already been developed. Why do I need a new one? They tend to be the same and can be packaged in very similar systems. Why shouldn\u2019t I just go into an app, and choose what kind of household I want?\n\u201cArchitecture as an artistic practice, is the only one that will survive, and it will be developed by a tiny elite. We\u2019re talking five percent, one percent of architects, max. The rest. They are done. They\u2019re doomed. They\u2019re gone. Finito. This is the end. Muerto!\u201d Errazuriz concluded.\nThere are a series of five video posts as Errazuriz reacts to comments from mainly outraged architects and well worth a watch.\nErrazuriz\u2019s post has certainly caused a commotion and is undoubtedly a simplistic view based on some core AI vectors. Removing the emotion for a second, within it there is some logic to potential future capabilities, but AI as the master builder is a long, long way from what is available today.\nErrazuriz does talk about the exponential nature of AI learning and how with enough data, an expert system can easily assimilate all our knowledge, and this is undoubtedly happening. But architecture is essentially creative, shape, form, light, materials, systems and very personal. Conceptual origins of things that are very hard for computers to come up with.\nWhile there are AI algorithms in research which have learnt techniques from Rembrandt, Canaletto and other great masters, and can do impressive party pieces, you have to question if an AI-based system could come up with Cubism, Pointillism, Brutalism, Gothic or Art Deco. Could we ever respect a branded AI that starts a new artistic wave, simply by being a bit random and remixing its data?\nThere will always be signature architects. However, as we look at meeting the needs of a global population of nine billion in housing and infrastructure, maybe we are going to need a bit of help to get this done efficiently.\nFrom everything I have heard and from talking with developers, the AI systems we will see in our industry will augment our design abilities, not move humans away from the process of design. In the main, they are chiefly striving to automate the mundane tasks and provide oversight to complex processes and interactions.\nGetting back to design\nRacel Williams, AI development manager at Autodesk has spent the last year travelling the world, talking to Autodesk customers about what their biggest pain points are and what they are really struggling with. Unsurprisingly, these were mainly centred around design changes. So the team is researching Machine Learning (ML) and AI when applied to CAD, PDF and BIM, how the industry documents data and how these objects and components relate to one another.\nLooking away from AI and ML in specific Autodesk products, Williams muses on the bigger picture, \u201cThere\u2019s obviously lots of things that as an industry (not just Autodesk, but the whole CAD industry) we can do to work better together to figure out how we can be more strategic in what data we\u2019re collecting and what we\u2019re trying to do with that data later on. When we were doing Google Analytics in the early days, like any company out there we were just trawling through this lake of information; there was no strategy and companies were just collecting all this data. Now we realise that we need to be more strategic about what we\u2019re doing with the data to potentially solve specific issues.\n\u201cFrom our side, the industry needs collaborate together to try to figure out what those standards are so we can actually solve bigger problems. As a former architect, I had to do a lot of manual tasks, and that wasn\u2019t what I signed up to do! Generative Design, AI, Machine Learning can help bring people back to where they can, and do what they signed up to do in the first place, which is design awesome things.\u201d\nAI accuracy\nKeith Bentley, EVP, CTO of Bentley Systems, has a frank assessment, \u201cYou will not be a competitive software company unless you\u2019re a good AI company. Five years from now our entire conference [Year In Infrastructure] will be about different types of machine learning. Algorithms are helping with the process of either designing, operating or building.\u201d\nBentley is one of our go-to guys to understand the impact of new design technology. In 2018, the company acquired AIworx, a machine learning and Internet of Things (IoT) developer, to specifically bring in house a team of AI programmers to boost the company\u2019s Digital Twin advancements. Bentley highlighted areas where the AIworx team has been adding its AI know how, recognising information in context, via image recognition (finding cracks in highways and bridges or rust on towers). The second area for AI attention is recognising patterns in BIM models.\nBentley explains that, unfortunately, a lot of what people create today and what they believe is an accurate representation of things is, in fact, incomplete. When data is collated and mapped within an iModel, it\u2019s often the case that there is inaccurate data, incomplete data, inconsistent or redundant data. In the future, Bentley\u2019s software aims to do a much better job of sorting out consistency, and checking for standards, with algorithms. Bentley sees solutions in offering smart project predictors and many other tasks.\nAutodesk\u2019s Racel Williams agrees that there is an issue with model quality checks and rules-based solvers, \u201cWe don\u2019t want people to do that for the rest of their lives!\u201d she notes, \u201cWe are trying to understand those relationships and work out a way those people don\u2019t have to write rules again and again. We are finding the quality of drawings and the quality of modelling isn\u2019t consistent across the board, but first we need some understanding of what \u2018good\u2019 is like, what makes for a \u2018good\u2019 model?\u201d\nAI in BIM\nOne of the more recent and interesting examples of the application of artificial intelligence comes from Belgian developer Bricsys. Having developed a competent AutoCAD compatible product, the company has turned its eye to creating a new BIM tool. Instead of modelling with an array of pre-configured components \u2013 Lego CAD, as it were \u2013 BricsCAD BIM allows the designer to work with solid geometry, crafting the shape and punching holes for windows and doors. Once happy with the form, the designer simply types BIMify into the command line, then AI goes through the model automatically identifying building components such as walls, doors, columns, floors and assigning IFC tags. This is the antithesis of current working methodologies and once more allows architects to experiment with form at the conceptual phase, knowing intelligence can be added afterwards.\nBricsCAD BIM now supports Rhino and Grasshopper, so it\u2019s possible to bring in mesh geometry from pretty much any design or visualisation system. It means one could BIMify dumb geometry from products such as SketchUp, Blender, Unreal, Unity or AutoCAD etc.\nThe next challenge for Bricsys is to apply this AI to the scanning world. Working with Leica and HOK, the plan is to scan buildings internally and externally and let the AI interpolate between the external and internal meshes, to interpolate the voids where the scanners can\u2019t penetrate. It\u2019s possible to \u2018infer\u2019 the walls, windows, floors, doors and columns. Scan-to-BIM may actually become a reality.\nAs Graphisoft has been rewriting the core features of its flagship BIM tool over the last few years. ArchiCAD has seen some subtle uses of AI in features such as its stair design tool, for instance, which now uses predictive technology to automate the generation of complex forms, whilst taking into account hundreds of design codes. Also, Graphisoft has deployed AI behind the scenes, to optimise software performance with a self-learning multithread- balancing algorithm to optimise navigation predicting what to cache and where a user is likely to move in a view.\nAkos Pfemeter, vice president, marketing told AEC Magazine, \u201cWe believe it will be mission critical for the future of our industry but, in our opinion, we have still yet to see a breakthrough practical application that would move the needle for our industry by and large. In development terms, the company\u2019s next focus for applying AI will be automation of mundane tasks such as annotation and layout.\u201d\nLimiting risk\nFrom the conversations we\u2019ve had in researching this article, it\u2019s clear that only a small number of AI-enabled features and applications have found their way into our industry\u2019s core products. As the biggest player in the market, it is perhaps to be expected that Autodesk has the widest AI footprint but even here, it\u2019s still a niche technology that is appearing in a number of its acquired web service products, namely Construction IQ, which helps construction project teams manage risk and improve performance, and Building Connected, which is used for online bid management.\nConstruction IQ is Autodesk\u2019s most mature AI solution. Manu Venugopal, senior product manager, building information, explained the aims of using AI in the product, \u201cWe are trying to make our tools more assistive when they are dealing with a lot of information and data. We use AI and Machine Learning to learn from the data, both past and present in design and construction, and assist in that decision-making process. We have more than 1,500 active projects now and are working with many companies, including BAM, AECOM, and US-based firms PARIC, Swinerton and Danis.\u201d Venugopal added that Construction IQ has a database of over 30,000 construction projects, which had 150 million issues and related inspections from which to learn.\n\u201cWith recent developments we have been taking this intelligence slightly upstream into the design and construction space, \u2018the design risk management aspect of construction\u2019. We found that many of the issues that Construction IQ is finding, there is some link back to the design and preconstruction phase. Customers want to apply the technology even earlier.\n\u201cIt turns out that over 70% of RFIs in construction have a root cause in design and documentation errors [from Autodesk data science team\u2019s research] Our research also showed that 38% of all the litigation problems, came from the design and documentation phase [from Engineers Daily (2011)].\u201d\nIn short, AI will be checking the meta data in submittals, together with components which have been marked-up in BIM 360 Docs. This new work was launched at Autodesk University in Las Vegas.\nFrom Uber to dating\nThere\u2019s nothing wrong with taking inspiration from how some of the most impactful web service providers have deployed AI. Autodesk\u2019s Building Connected web services (available in North America only at the moment) use machine learning in two of its core services: Bid Board Pro, which helps subcontractors track projects they have be invited to bid on, and, in BC Pro, which helps general contractors find and invite subcontractors to work on projects.\nChelsea Hodge, product lead for Autodesk\u2019s Building Connected, explains, \u201cTypically sub-contractors are getting dozens of invitations to bid from different general contractors every week. Some are even getting hundreds of invitations each week, and this manual process can hugely painful.\u201d\nBid Board is a centralised service that tracks and logs these bid invites and ensures they don\u2019t get missed. Bid invite emails are forwarded to Building Connected, which parses each invite with natural language processing, irrespective of layout variation, and formats them consistently for system access. Any due dates or deadlines get logged and warning of impending deadlines appear. This is similar to the way Expensify (expensify.com) or Tripit (tripIt.com) work.\nWhen it comes to match making contractors and sub-contractors, it\u2019s a recommendation issue, Hodge explained, \u201cWe use statistical modelling to predict the likelihood of a given sub performing a given trade in any given location. And our inputs into the model include things like the sub\u2019s previous actions on Building Connected, their trades and service areas in their profiles and other factors.\u201d Hodge admitted they took inspiration from Uber passenger scores, combined with a bit of online date matching.\nConclusion\nThe topic of AI is so mired in societal fear of its implications that it clearly needs to be knocked off its pedestal. Most of the coverage in the media looks at the concerns of governments\u2019 utilisation of face recognition, tracking and its application to big data from insurance companies and social media networks. While of course, those concerns may well be valid in the very long term, it somewhat obfuscates the positives that AI can bring to other areas. AI will save lives and it will enable better buildings and free up more time to do more design.\nFrom talking to industry leaders, it seems the next five years is going to be a crucial time for the development of AI in the industry. AI will be in most applications; it will be on demand through cloud services. AI will be watching project management; construction sites and it will be deployed from concept through the whole product lifecycle. What we are seeing today is really only the start, as the major software developers are only just building out their AI teams and so, even before we see tools, software companies admit that they have to sell AI internally to their development teams, as well as learning from the customers what are the low hanging fruit that they should be tackling with this new capability. It\u2019s a clear sign how far we have to travel before we have to worry about replicants, terminators and murderous mainframes, not least because Autodesk believes we have still yet to define what \u2018a good\u2019 drawing or model is.\nMy concern is not about job losses. The problem, as always, is human. As a child, I studied mathematics with my father and he used to get irate at me for being lazy and using a calculator. I thought I was so lucky that I lived in the time when we were allowed to use calculators in class and in exams. However, my father realised when checking my work, that while I would get the overall process usually correct, I would get the answer wrong, as I was literally accepting the result that the calculator was spitting out, without any contemplation as to whether the number was vaguely in the right ballpark. I learnt to at least mentally pre-calculate the target area and where the decimal point should be!\nIn an increasingly automated world, our AI work assistants can only go on the data that is input and usually that data will come from us. If we make incorrect assumptions, or put bad data in, there is a danger that we just accept the outcome and move the process on, underpinned with bad data. It is perhaps not unsurprising that many of the early AI developments are there to check quality and coherence of data. Mental atrophy from relying on computers could be a problem.\nAlways remember: garbage in = garbage out.\nNow we have seen what AI is currently capable of and who are the main developers. In subsequent AEC articles, we will look deeper into AI, to what hopes developers and designers have for this technology.\nArtificial Intelligence (AI) explained\nAccording to Autodesk\u2019s Kyle Bernhardt, AI logic can come mainly in two forms: Machine Learning (ML) and human-defined. \u201cMachine learning is a particular technological approach that takes large scale datasets to train a set of algorithms to produce an output that would otherwise be very challenging to write with manual code, such as determining if there\u2019s a dog in a picture. This is essentially brute force computing and especially useful in pattern recognition. It is a subset of AI. The more data it has, the better it should be. Bernhardt expands on the theme, \u201cAI is really all about codifying advanced logic, the expertise of the human condition, in a technological way. We talk a lot about that idea of delivering more, in a better way, with less of an impact; it\u2019s something we take really seriously. AI is all about enhancing the intense creativity of the creative professionals; giving them a superpower. Machine learning is one of our best vectors to deliver on that promise.\u201d\nEleven AI start-ups to watch in AEC\nRead our article in which we explore some of the emerging AEC focused software tools that are harnessing the power of Artificial Intelligence.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/what-does-blockchain-really-mean-for-aec\/","title":"What does blockchain really mean for construction?","date":1542931200000,"text":"Randall Newton cuts through the hype and explores what impact the distributed database technology could have on BIM, contracts and even people\nWhen the popular business advisers and father-andson team Don and Alex Tapscott turned their attention to blockchain technology in 2016, it set off a firestorm in the business world. \u201cThe blockchain is an incorruptible digital ledger of economic transactions that can be programmed to record not just financial transactions but virtually everything of value,\u201d they wrote in \u201cBlockchain Revolution.\u201d The praise was up and down the corporate ranks. No longer was the hype just about Bitcoin, but something more useful. The reaction from Microsoft CEO Satya Nadella was typical: \u201cThis book has had an enormous impact on the evolution of blockchain in the world.\u201d\nFast forward to late 2018. Where are all the blockchain solutions for business? More specifically, where are the blockchains for AEC? To answer, let\u2019s back up a bit. Because hype seems to travel faster than reality, let\u2019s start with definitions.\n\u201cBlockchain is a distributed database where many copies of the data are replicated and synchronised,\u201d says Casey Mullen, vice president of technology strategy at Bentley Systems, one of many companies in BIM software examining the use of blockchain tech. \u201cIt solves the problem of trust \u2014 or lack thereof \u2014 among two or more parties.\u201d Data is replicated and synchronised, but not copied, a key distinction. Blockchain offers a new type of Internet, where digital information exists in a state of consensus \u2014 not a state of \u201ccopied\u201d \u2014 among two or more parties.\nIn financial technology (fintech) circles, blockchain is often called \u201cdistributed ledger technology\u201d because the ledger of transactions is distributed among all nodes of a vast peer-to-peer network, generally thousands of computers. The network uses cryptographic techniques to verify transactions, reward nodes on the network with a tradable token (where cryptocurrency comes from) and prevent corruption of the ledger. The blockchain ledger establishes its trust because every copy of the ledger must always be in agreement with all others. Once written to the ledger, a transaction cannot be unwritten. Terms such as \u201ctrustless,\u201d \u201cimmutable,\u201d and \u201cdecentralised\u201d also describe blockchain technology.\nThe first blockchain came with Bitcoin, the digital currency invented by the anonymous programmer(s) known as Satoshi Nakamoto. In Bitcoin the blockchain is a transaction record created by the constant solving of cryptographic puzzles that generate coin. Notes, records, and agreements can be appended to coin transactions. For Bitcoin and other cryptocurrencies, those transactions are generally buying and selling of the currency. But it doesn\u2019t have to be. Newer blockchains including Ethereum, Hyperledger, IOTA, and EOS offer various forms of \u201csmart contracts\u201d which enable parties to record and monitor transactions. There are ways to minimise or eliminate the use of cryptocurrency tokens for such transactions, depending on the blockchain used.\n\u201cThe basis for blockchain security is encryption,\u201d says Scott Sheppard, a long-time researcher and tech evangelist with Autodesk Labs who now works in the office of Autodesk\u2019s chief technology officer. The encryption uses key pairs, one public and one private. The public key is a randomly- generated string of numbers representing a user\u2019s address on the blockchain. The private key is like a user\u2019s password that gives the user access to their assets in the blockchain. As part of ensuring that assets on the blockchain are incorruptible, users must safeguard their private keys much in the same way they protect their passwords. All instances of cryptocurrency theft happened not because somebody hacked the blockchain, but because they stole private keys or hacked apps that work with blockchain data.\nFragility of trust in AEC\n\u201cTrust in the design and construction industry is a very fragile thing,\u201d notes Malachy Mathews, a senior lecturer in architecture at the Dublin Institute of Technology. The industry is \u201cnot set up to deal with trusting relationships, even though we need them.\u201d While doing doctoral research, Mathews took a close look at how new technologies could improve collaborative processes between contractors and their supply chain partners. \u201cThe process of procurement seems to be set against trust,\u201d Mathews says.\nTwo years ago Mathews started looking at other disciplines for ideas and technologies. \u201cA convergence of language\u201d led Mathews to examine blockchain. \u201cI see big database companies like Oracle and Google taking an interest in design and construction,\u201d Mathews says. \u201cIt is a trillion- dollar industry ripe for disruption.\u201d Mathews believes blockchain technology can be the catalyst, despite the distraction of Bitcoin as a very public financial phenomenon. \u201cIt colours the public view of what blockchain is; we need to divorce blockchain from currency. Blockchain will have more impact in the world than Bitcoin will ever have.\u201d\nBIM and blockchain\n\u201cBIM is very data-centric, and the best way to establish trust is to trust the data itself, which is not the case today,\u201d says Arnaud Gueguen, co-founder of Bimchain.io. The French startup is working on a web-based application to track communications and commitments between project participants in the complete design and build ecosystem. The app will connect directly to Autodesk Revit. Gueguen is seeking beta testers willing to use Bimchain on a real project.\nTo Bimchain, trust means \u201ceverything on the signed paper, not the 3D model,\u201d says Gueguen. \u201cAll the legal signed stuff. That was our entry point into the solution, trying to connect the contractual paper base with the operational digital data.\u201d Bimchain\u2019s goal is to make the model the unique source of truth. \u201cToday BIM data is not contractual; there are many sources of truth: BIM, PDM, DWG, paper,\u201d says Gueguen. \u201cWe believe using Revit as the single source of truth will make the model far better.\u201d\nBimchain\u2019s first step is a Proof of Contribution module, where contributions, agreements, and validations are certified on the blockchain and made part of the Revit BIM model. \u201cIt is a \u2018proof of handshake\u2019 to prove the BIM Manager, the architect, [and others] agreed on the model,\u201d says Gueguen. \u201cWe believe this will be able to replace the scattered papers-and-signatures process with an indisputable system.\u201d Bimchain is using the Ethereum blockchain, the second-largest cryptocurrency by market capitalisation and the first to introduce specific smart contract tools to the blockchain ecosystem.\nOne step at a time\n\u201cThe typical AEC project involves tens if not hundreds of parties,\u201d notes Autodesk\u2019s Sheppard. \u201cUnfortunately, the interaction of so many parties can lead to confusion. Sometimes this confusion is significant enough to lead to litigation. Blockchain is a technology that can help reduce confusion and resulting litigation.\u201d Sheppard is quick to point out that, while Autodesk employees are taking a close look at blockchain, the company is not commenting on any specific development initiatives that may or may not exist within the company. Such a cautious attitude is typical of established players in any industry. When the major players in AEC, like Autodesk, Bentley, or Trimble move into a new technology space, they generally do it by acquisition.\nMoving slowly into such potentially disruptive technology is about more than software developers wanting to get it right. The current methods \u2014 as inefficient as they are \u2014 are deeply entrenched. \u201cEveryone trusts the old system with its inherent faults, and may even be deeply vested in mitigating those faults,\u201d says Mathews. \u201cThe same or greater level of trust must be demonstrated and maintained in any new system in order to be adopted and lead to commercial success.\u201d The use of blockchain in AEC \u201cis quite a fundamental thing,\u201d says Abel Maciel, a director at Design Computation, Ltd., a London consultancy. Maciel is also a founding director at the Construction Blockchain Consortium (CBC) and a faculty member associate at the University College London in both architecture and blockchain technologies. \u201cThere is so much improvisation in construction now. Things arrive on the job site wrong, broken, or not as ordered.\u201d Maciel gives the example of cast pipes and folded ducts. They have very different performance purposes, are used for different things, but they look identical and sometimes come from the same factory and can be mislabelled. If the duct is used in a high-pressure application, it could explode.\n\u201cHow do we avoid such a problem?\u201d asks Maciel. \u201cBlockchain could de-risk the process.\u201d A tightly controlled manufacturing environment could be certified and posted to the blockchain. The result could be \u201cde-risking the construction process and creating a better designed asset. The tendering process would be accurate, with real costs.\u201d By using Ethereum smart contract technology, \u201cwe can move away from the idea of \u2018every two weeks of signing is good enough.\u2019 We can build a smart contract to trigger payments based on validation.\u201d\nMaciel and Robert Aish, known to many as the father of generative design technology, are working through the CBC to launch three open source projects related to BIM and blockchain. One for distributed financing, one for smart procurement, and one \u201cmost ambitious\u201d project related to the use of blockchain, machine learning, and BIM for design processes.\nWhat\u2019s next?\nBlockchain for AEC is neither pie-in-the sky or pie-on-my-plate at this moment. But the day is close when AEC firms can try out applications and see how they might improve workflows. Based on how long it took BIM to hit the 50% tipping point in design firms, blockchain adoption has a long road ahead.\n\u201cIt is all a bit theoretical and hypothetical at the moment,\u201d notes Mathews, \u201cbut I think there is merit in it.\u201d Mathews believes, that like most AEC technologies, it will be clients that drive adoption not practitioners. \u201cHow did the handshake develop? It was simple. I have a chicken, you have the beans. We hold hands to make the exchange.\u201d Blockchain as the new transactional handshake will require AEC firms holding the deliveries and clients holding the money to be willing to \u201cshake\u201d on the trustless, immutable, decentralised record found on their blockchain. \u201cThere can still be independent operators from project to project but they will have trust among the group. They won\u2019t be tied to legacy necessities. Interesting things could happen.\u201d\nBut is blockchain the answer? \u201cWe must always ask, \u2018is blockchain better than the current solution?\u2019\u201d says Mathews. The three crucial elements of blockchain \u2014 a record of value transactions, the formation of consensus, and the coin or token \u2014 are all needed. In blockchain the coin or token is the incentive, but the incentive does not have to be money. Mathews thinks reputation can be tokenised in AEC. \u201cReputation is a currency. Look at Uber, Airbnb and others offering comments on services,\u201d says Mathews. \u201cWhy wouldn\u2019t we have a facility to comment on people we have worked with? This changes the dynamic of the working relationship.\u201d Mathews foresees a system whereby coin is awarded to suppliers by clients \u201clike merit badges not currency.\u201d Contractors could win new work based on certified reputations written onto a blockchain.\n\u201cConstruction has been notorious for not only inefficiencies and ineffectiveness but also for traditionally importing innovations from other sectors and adopting them in an ad-hoc manner,\u201d notes Eleni Papadonikolaki, a researcher at UCL Bartlett School. She foresees blockchain being just one of several new technologies gaining adoption. \u2018The Internet of Things, Artificial Intelligence, and BIM all promise to provide simplification, transparency, and an accountability chain not only to the transactions across the construction supply chain but also for the interactions between physical and digital systems.\u201d\nThe construction industry in US and UK are \u201cnotorious for being largely adversarial.\u201d The hierarchical nature of contracting relationships is also an issue, Papadonikolaki notes, especially at a time when network relations are proving to be more efficient. Blockchain could help eliminate the doubt and mistrust. \u201cThe supply chain map is a mess. It is a \u2018blame the customer, don\u2019t trust, don\u2019t share\u2019 environment.\u201d\nRandall S. Newton is USA Contributing Editor for AEC Magazine, and managing director of Consilia Vektor, a business advisory service working at the nexus of blockchain and design-based industries.\nNot waiting for the academics\nNot everyone who thinks blockchain for AEC is a great idea are working through the theory or creating consortiums; entrepreneurs are hard at work. The following list should not be considered as an endorsement; due diligence is always prudent when considering new technology. Most of these companies are actively seeking investment funding as well as working on their products.\nBuildcash (USA) says its new cryptocurrency product \u201cis freely tradable digital money, developed to revolutionise the construction industry through real-time, peer-to-peer payments of goods and services.\u201d\nBopti (France) is a cryptocurrency from the makers of Batopti construction management platform. The French company claims \u201ca leading European insurance company\u201d is supporting the initiative.\nCustomCoin (Switzerland) is a project to streamline the formation of capital for construction. (ccnowpro.com)\nBuilderium (Switzerland) is building a reverse engine bidding system to connect clients to contractors. (builderium.io)\nBitRent (UK) is building a cryptocurrency platform to help users \u201cland reliable investment deals\u201d for construction. (bitrent.uk)\nEtch (UK) is focusing on construction as one of the primary users of its blockchain-based payroll platform. It is working with the global accounting firm KPMG and the Construction Blockchain Consortium mentioned previously. (etch.work)\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/imodel-unleashed\/","title":"iModel unleashed","date":1542931200000,"text":"For over 10 years, iModel has been Bentley\u2019s format for exchanging project information between design, construction and operations. It has expanded into a fundamental platform in its own right. But, what we didn\u2019t expect, writes Martyn Day, was for Bentley to open source its development\nOut of all the firms we cover in the industry, Bentley Systems is perhaps the most cautious and long-term developer we have seen. While others all around are changing their file formats with every other release, or buying applications and never integrating the technologies, I can only remember two file changes from Bentley in my lifetime and everything eventually works on the MicroStation backbone. With cofounder and CTO Keith Bentley still writing code and actively involved in the system architecture, the company just doesn\u2019t do short-term software architecture.\niModel has become a growing differentiator for Bentley, with wide adoption in its base, as well as other AEC developers like Topcon, building an ecosystem of firms which actively want to share BIM data, despite being in competition.\nAt last month\u2019s Year In Infrastructure (YII) event in London, iModel became even more important. It became the connection between all project participants and the hosted Digital Twin, though the new Bentley iTwin services offering, which connects Bentley\u2019s iModel- Hub, with a Connected Data Environment (CDE). Attendees witnessed Microsoft, Siemens and Topcon all buying into the Azure hosted Digital Twin strategy.\nBentley Systems then announced iModel. js, an open source library to enable developers to incorporate web-based hooks that can bring in design and project data, which is stored in iModels on Bentley\u2019s iModelHub. The idea being that any developer can now opt to use the iModel wrapper to integrate 2D, 3D, 4D and documents into custom cloud applications or services.\niModelHub is secure, so permission is always required to access the model data, but developers or customers will be free to \u2018iModel enable\u2019 any application they want. Customers of custom web applications that wish to use iModel will have to take up an iTwin subscription with Bentley to host the data.\nBentley picked Java as it is the most flexible standard for modern cloud and web development. It is written in TypeScript and leverages open technologies including SQLite, Node.js, NPM, WebGL, Electron, Docker, Kubernetes, and of course HTML5 and CSS. The same codebase can produce cloud services and web, mobile, and desktop applications. The source code is hosted on GitHub, a platform for hosting software code \u2014 especially open-source projects \u2014 and is distributed under the MIT licence. GitHub was recently bought by Microsoft for $7.5 billion.\nBy opting for an open source deployment, developers are also free to alter the code and Bentley will have to do the job of policing and maintaining its open development platform, as well as collating wish lists and changes which it will incorporate.\nKeith Bentley explained that \u201cby opensourcing the libraries we use to create our \u2018iTwin\u2019 cloud services, we expect to foster a substantial and vibrant ecosystem of innovation.\u201d Clearly this is a play for Bentley to drive the standard into the burgeoning cloud-development fraternity, who have very few tools to integrate complex BIM data into their solutions.\niModel is not providing developers with a dumb viewer of 3D files, but a powerful suite of tools that can interrogate a staggering array of project information and metrics, including project history and changes. In short, adopting iModel.js gives access to the iModelHub and opens a window into the live Digital Twin data in a completely web friendly way, and at no cost to the developer.\nKeith Bentley told AEC Magazine that the development landscape has changed and, with the Googles and GitHubs of this world, development tools are predominantly free. He posed the question, how would you get a developer interested in using your tools if there was an upfront cost to become an accredited developer?\nBentley also explained that as a programmer it\u2019s more exciting to get tools which you can look into and edit as you wish. The net result will be something interesting, he said, adding that by giving iModel.js away as open source, he hopes to proliferate the format but also enable innovation in our space.\nOne example of how iModel.js could be used was given by Shenzhen Expressway Consulting Group, which already uses iModelHub and sees a use of iModel.js for crafting a custom web-based tool for digital handover, after the completion of a project. It would enable visual operations and maintenance, feeding from data stored in its own iModelHub. Any changes made and documented by the company post-handover would also be immediately updated online.\nTracking origins\nAs with all Bentley innovations, it\u2019s worth recapping on what the company delivered last year, to make iModel even more powerful. Bentley has always had a passion for trying to capture history and provenance of project changes. It has delivered a number of capabilities over time to capture the delta in model revisions.\nLast year\u2019s innovation brought this passion to iModel with the introduction of the iModelHub. When deployed, the Hub will capture all the changes that happen in a project\u2019s lifecycle in an unalterable timeline. Just as accounting systems track money in and out, iModelHub provides digital security to identify what changed when and by whom. It\u2019s a journal of record and could be invaluable to identify in projects when problems occurred. With a machine learning hat on, that could be valuable data to use in predicting problems on other projects in the future, but we will come back to that later.\niModels can now hold this data too. The cloud is where the changes are stored but everyone using iModels could theoretically get access to the project timeline. It\u2019s safe, as the master is held in the cloud and the local instances are just synchronised. This also works with non-Bentley products, for which it has written \u2018Bridges\u2019, so the data is collated and added to the iModelHub timeline. This capability is unique in our industry and is yet another powerful iModel benefit. It\u2019s similar to blockchain to a certain degree, although the authentication chain is online, and stored in one place, not in every iModel file.\nDigital Twin\nDigital Twin has to be the phrase of the moment. People have almost stopped using the word BIM and now seamlessly drop \u2018Digital Twin\u2019 in as a replacement. In many respects we welcome the substitution, as BIM is utterly meaningless outside of the gleaming digital spires of our enlightened object-oriented castles.\nDigital Twin is short, simple and gives a glimmer of hope that the uninitiated would have some clue as to what we are on about. In the space of one month, I\u2019ve heard Digital Twin on the mainstage at Bentley, Autodesk and Trimble events, as well as from Microsoft, Topcon and others.\nAccording to Keith Bentley, it was completely serendipitous that both Microsoft and Bentley were pursuing the same track. Microsoft has been focusing on the construction space for a couple of years, everything from BIM to Digital Cities and IoT. It has been working on layers of machine learning which reside in the Azure cloud and have been used to learn from big data caches on the city scale. Bentley explained to us, \u201cIt sounds like we have been working on this for a while together but it was only when we talked about it that we discovered they had a Digital Twin strategy.\u201d\nAs Bentley is almost a 100% Microsoft-based company, its choice of Azure for cloud storage will also benefit, in time, from Microsoft\u2019s focus on machine leaning in the space. If the data is hosted on Azure, special built environment-centric analysis tools are in the works from both Bentley and Microsoft. Autodesk won\u2019t benefit from this partnership as its customers\u2019 data is hosted on Amazon Web Services.\nConclusion\niModel.js is a big technology story from Bentley\u2019s stand point. For fear of calling the company a control freak, it would have been a huge surprise for it to open source anything, let alone something so fundamental to project collaboration.\nBentley is looking to gain traction and proliferate its formats. It realises that there is a lot of web-based development yet to happen in this space and by lowering the barrier to entry and being open about who uses it, it might find it has an advantage over our traditional proprietary industry mindsets.\nThe other benefit is that Bentley doesn\u2019t have to create all the tools that customers want. It\u2019s now possible for customers to create their own solutions and leverage their investment in Bentley back-end technology.\nFrom this year\u2019s YII, there was an overpowering feeling of momentum behind Bentley\u2019s vision of Digital Twins and iModel. For years, Bentley Systems has been Microsoft\u2019s biggest, best and most loyal AEC developer, but had benefitted little from this.\nIn the past, Microsoft people who covered the AEC space didn\u2019t seem to be fully switched on, as to trends in the industries or the technologies that were in play. The Microsoft people we met at YII were a completely different class to those we have met before, showing a deep knowledge of IoT, BIM, Digital Twins, as well as properly understanding what Bentley has developed. Topcon too has really closely partnered with Bentley and is certainly speaking from the same page.\nThe elephant in the room was Siemens \u2014 Bentley\u2019s giant development buddy, who is increasingly looking to focus on the AEC space. Bentley is integrating into Siemens\u2019 uber document management system, Teamcenter, and even licensing technology to Siemens.\nKeith Bentley told us that he likes the partnership as it is, the companies talk the same language and have little overlap, and it\u2019s certainly helping their combined efforts in Power and Governmental customers. One wonders how long the engagement goes on before there is a marriage.\nWhat is an iModel?\nAn iModel is a Bentley authored model and project data exchange format specifically designed to work in the lifecycle of infrastructure assets. It\u2019s not just a CAD file. Essentially, it\u2019s a wrapper for a multitude of data types: business properties, geometry, graphics, and relationships. This means a whole project can be contained within one file, irrespective of how many sources the original data came from.\nUnlike proprietary formats, iModel is open and can be used by many competitive infrastructure developers.\nThe format is very efficient, crushing giant files, and designed for today\u2019s cloud-based and mobile workflows. They are secure and play well with serverlevel security.\nAdditional information can be added to the iModel without impacting the data originally contained within it, great for lifecycle data capture tasks.\nAs of last year, iModels can also contain all the history of changes that have happened in a project\u2019s lifetime when used in conjunction with Bentley iModelHub.\niModels can be created with Bentley products, as well as those from other sources, including PDMS, Revit, AutoCAD, Rhino, ArchiCAD, IFC, JT (plug-in) to name but a few. iModels can be filtered prior to crea \u2013 tion, limiting what data is exposed.\niModels can be used for many tasks: viewing 2D, 3D data, review (desktop or on mobile), access ODBC data, spreadsheets, reports, clash detection and now as a container for access to Digital Twin models.\niModel.js will mean that Digital Twin assets can be utilised in custom web applications. As iModel.js is open source, the capabilities are also expected to expand with input from the wider community.\nInvesting in machine learning and IoT\nOne of the big surprises at YII (it also surprised some Bentley employees) was the under the radar announcement that Bentley had acquired machine learning and IoT development company AIworx.\nKeith Bentley gave us some background to the acquisition and the importance of machine learning going forward, \u201cIt came together very quickly, they are a great talented group of smart guys,\u201d he said. \u201cI\u2019d guess in today\u2019s terms it would be described as an \u2018acqui-hire\u2019 \u2014 we bought the company for the talent, as they don\u2019t actually have any products.\n\u201cWe had an \u2018AI\u2019 summit and invited some CIOs to shoot around ideas and the most common answer is, \u2018we just want an easy button for my project\u2019.\n\u201cIn reality, firms need help to not repeat mistakes and to fine tune their performance, which is reasonable if you have enough data.\n\u201cLast year we asked firms if they would like to contribute their project data (iModelHub), but we didn\u2019t get a whole lot of traction but this year we asked again and there seems to be a more positive reaction to pooling data, so we can machine learn. The problem now is going to be narrowing down the development opportunities,\u201d he said.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/cad\/gensler-develops-proprietary-data-driven-design-tool\/","title":"Gensler develops proprietary data-driven design tool","date":1592784000000,"text":"Blox uses real time computation to balance form, function and business insights on design projects\nGlobal architecture firm Gensler has released details of \u2018blox\u2019, its new internal data driven design platform that combines information metrics with geometry form finding in a real time environment.\nThe software is designed to balance form, function and business insights using metrics defined by architect and client at the onset of design projects. It provides a study into design trade-offs driven by capital assets, business intel and municipal requirements and can be used at building, city block or masterplan levels.\nFeatures include sophisticated algorithms with real-time compute of complex geometry and metrics; advanced form-sculpting and data assignment schemas; and a Heads-Up Display with dynamic and real-time metrics.\nThe software uses dashboards to give real time feedback on different design options, taking into account program design metrics such as Gross floor area (GFA), Floor area ratio (FAR), Zoning, rentable \/ usable metrics, as well as client defined parking metrics above and below ground.\nIt also gives insight into construction costs, defined \/ computed at building, floor or program levels, as well as proforma projections, including client defined income, rentable ratios, and yield percentages.\nblox is the first in a series of product releases in Gensler\u2019s inFORM, an ecosystem of internally developed proprietary products to boost Gensler\u2019s overall design capabilities.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/civil-automation-with-topcon\/","title":"Civil automation with Topcon","date":1444003200000,"text":"BIM is revolutionising site work and construction. Topcon recently hosted a series of global events to highlight new technologies and the latest techniques. By Martyn Day\nMoving architectural design to embrace 3D has been a long and drawn out process but the market is mature and with governments across the globe looking to make the process standard, it looks like things are moving quickly.\nSoftware and hardware developers are now concentrating on the construction phase of the process and there has been a convergence in competition and development, from the likes of Autodesk, Bentley, Trimble and Hexagon (which owns Leica, Intergraph and others).\nWith the division lines drawn, expect to see many advances in construction technology through accelerated development and integration of systems.\nTrimble and Bentley have already joined forces and develop solutions for the construction industry based on a joint construction schema and common format Bentley\u2019s \u2018imodel\u2019, and Bentley has embedded native support for Trimble\u2019s scanners into MicroStation.\nTrimble has acquired an impressive array of construction technology firms over the last few years and Autodesk reacted by working with survey and laser-scanning firm Faro.\nAutodesk is now teaming up with Trimble\u2019s rival, Topcon Positioning Systems, developers of positioning technology for surveyors, civil engineers, and construction contractors. Autodesk has embedded native support for Topcon devices into AutoCAD and Revit and is developing Civil3D, Recap and Infraworks with feedback from both Topcon and Faro.\nTechnology Day\nTopcon recently held a series of technology days in the UK and US, to demonstrate the latest range of automation, surveying, monitoring and control systems. Autodesk was on hand to demonstrate the links between its software and Topcon\u2019s construction capabilities.\nIn the UK, Topcon took over a field at Stoneleigh Park, Coventry, and took attendees through the complete process of creating a roadway. Starting with a green field, the event took a day to demonstrate every step from data capture using Totalstations, laser scanners, mobile mapping and UAS (Unmanned Aerial System), through to digging, grading and monitoring using remote machine control. Finally paving was laid along the prepared surface, ensuring a level surface.\nThe most notable feature of this demonstration was the omission of any setting-out, there were no pins, flags, stakes or fencing. Software determines a machine\u2019s current position and then compares that with the desired design surface, with everything was driven from a 3D model with highly accurate GPS location controlling the machines paths and cuts.\nThe humans driving the vehicles were mainly going along for the ride. It was made very clear that the technology is available today to link the civil 3D model to drive the real world, this is CNC (Computer Numerical Control) machining on a grand scale.\nSiteLink3D Another Topcon technology on show was the SiteLink3D remote service which offers view, access and control from anywhere on the planet, from a mobile or desktop machine, to all the operators on a job, showing location, activity and which file is driving the task. It is also possible to text operators and transfer new files to machinery onsite.\nThe software shows a worksite with machine locations and paths, together with feedback and analysis of the work being done. If the grading along a path is shown as being green then it has been worked to being within tolerance. Any red marked areas have yet to be fully graded. So, it is very clear at a remote glance how much work has been done and how much is left to do.\nShould there be a design change, it is possible to edit the site model and send new files to the machines on site.\nSkanska and Walters Group\nInbetween the field demonstrations there were a number of product and customer presentations. The stand out talk was given by Topcon customers Skanska and Walters Group, which are currently jointly working on a motorway intersection in the UK and heavily utilised machine control.\nWe were told that this was the first time that Skanska had opted to try out machine control to this level, omitting setting-out and using remote monitoring.\nThe net result was no setting-out errors, a higher-quality end deliverable with significant savings in both time and money. Engineering decisions that were made in the field were fed back to the design office, the Digital Terrain Map (DTM) was edited and new files were generated and sent to the machines on-site within minutes. There were additional benefits with reduction in labour and with that better on-site health and safety.\nDrones\nIn addition to traditional surveying equipment Topcon offers two unmanned aerial vehicles for surveying and monitoring. The Sirus Pro is a lightweight, hand launched fix-wined unmanned aircraft that has an autopilot to fly pre-planned paths, offering a built-in 16MP Fujifilm X-M1 camera with highly accuracy GNSS RTK terrain mapping. The UAV comes with post processing software for creation of DTMs and orthomosaics. These models can be measured, overlaid with 3D models for cut and fill analysis, and used for as-built design comparisons.\nNewly launched, the 2.3kg, carbon fiber Falcon 8 octocopter was demonstrated. Manufactured by Ascending Technologies, the German-made UAS was incredibly impressive in its lightweight design and dogged stability in the windy conditions.\nThe Falcon can be launched by hand or from the floor and at the end of the flight was plucked out of the air by the operator. While it was hovering off the floor, the operator pushed and moved the drone with force, only for the device to recover its height and position.\nThe Falcon 8 can be flown manually or fully automated and can fly both 2D and 3D waypoint driven routes via the provided Navigator software. It can fly 360-degree panoramics or circle areas of interest. It can also perform as range of imaging services with a variety of payloads, from high definition (HD) imaging, thermal and RGB stills, as well as real-time video. Topcon suggests the Falcon is best used for surveying areas up to 35 hectares.\nFor an octocopter it is also extremely quiet despite having eight 100-watt motors and has the ability to fly on even with a number of motor failures. The Falcon has three autopilots, redundant electronics, a redundant propulsion system, and redundant radio links.\nGLS Laser Scanners\nTopcon also recently announced three new model laser scanners in its GLS line, the GLS-2000S, GLS-2000M and the GLS-2000L. The scanners are designed to capture data based on the measurement range needs of specific applications. The S model is optimised for short-range applications (130 metres), M medium (350 metre range) and L, long (up to 500 metres).\nThe new Topcon Precise Scan Technology II, enables the GLS-2000 models to emit pulses three times faster than the previous generation and feature built-in dual 5MP cameras. One camera has a 170-degree wide-angle lens for high-speed imaging, and the other has an 8.9-degree telephoto lens that is coaxial with the measuring axis.\nThe GLS range is \u2018eye safe\u2019 and can be operated with one touch. Topcon is positioning them for the \u2018Scan-to-BIM\u2019 and \u2018As-Built\u2019 markets. The machines feature a 360-degree dome scan capability, which enables the scanning of difficult structures, interiors, towers and bridges.\nConclusion\nIt is clear that there is a rapid convergence happening between 3D modelling in the building and civil design space with the integration of computer controlled machine tools in the construction industry. Disparate technologies are now combining to let the digital world lay out and shape the construction of the real world.\nFor companies like Skanska, linking the office with the worksite has obviously brought dividends and the company looks set to adapt its construction processes to rely on these \u2018invisible\u2019 control systems, which offer flexibility, monitoring and safety within an automated remote environment.\nThe UAV market is set to explode, not only in the professional world but also with consumers. The Falcon 8 was very impressive to watch go through its paces and comes highly recommended.\nRecent concerns of aerospace management and the potential for accidental harm are rapidly accelerating. This Christmas, over a million drones are expected to be sold. This could impact professional use and we suspect that there will be stronger regulation to manage the drone market as it \u2018lifts off\u2019.\nThere are other concerns with regard to remote surveying and terrorism, recent unexplained flights over France\u2019s Nuclear power stations have caused alarm, for example. The digital virtual and real words are starting to blur.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-carlos-cristerna-neoscape\/","title":"Video: NXT BLD 2019 \u2013 Carlos Cristerna, Neoscape","date":1565049600000,"text":"Harnessing the power of real-time ray tracing \u2013 NXT BLD London, June 2019\nFor over 23 years Neoscape has been crafting experiences of built environments for clients around the world. Join us to see how we are using real time raytracing from the lens of Neoscape\u2019s production experience and the impact on the internal design and decision making process, as well as our clients. With the aid of NVIDIA Quadro RTX technology and Lenovo workstations we will deep dive into the process and challenges of Exterior HDRI lighting, large and small office interiors with numerous polygons, and complex shaders created in 3DSmax-Vray imported through UE4 Datasmith. All of this in one of Manhattans newest skyscrapers.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-centerprise-signs-uk-agreement-with-boxx-technologies\/","title":"NEWS: Centerprise signs UK agreement with BOXX Technologies","date":1439769600000,"text":"British IT provider to manufacture BOXX workstations in Wales and distribute in the UK\nCenterprise has signed an agreement with high performance workstation manufacturer BOXX Technologies to become its exclusive manufacturing and distribution partner in the UK.\nCenterprise will use its purpose built production and logistics centre in Caerphilly, South Wales to build the complete portfolio of BOXX workstations and rendering devices. These include desktop, mobile and remote\/virtual workstations, as well as dedicated render farms \/ clusters for rendering and simulation.\nBOXX and Centerprise have launched a dedicated UK website.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nNEWS: ArchiCAD 20 tuned into data\nNXT BLD 2019 preview - robotics meets digital fabrication\nScia Engineer 24 launches with new solver\nNEWS: GEOSLAM enhances SLAM registration software\nNEWS: Autodesk launches Revit Collaboration Suite\nNEWS: HP launches Intel Skylake-based HP Z240 workstation\nNEWS: Graitec delivers new tools for BIM and steel fabrication\nNEWS: Chaos Group introduces GPU + CPU hybrid rendering\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/bentley-the-future\/","title":"Bentley: the future","date":1480982400000,"text":"Bentley may still be preparing for an IPO, but there has been some important news on its relationship with Siemens that could have big ramifications in the future. By Martyn Day\nNovember 2016 was the second Year in Infrastructure event at which Bentley has been under its IPO rules, which means it can\u2019t reveal its financial earnings \u2013 something which has been a bit of a tradition, despite being a private company and, as such, not required to do. The company is biding its time as to when the market conditions best suit a Bentley Systems IPO. It\u2019s still one of the largest privately held software firms in the USA.\nWhile the election of Trump may be a shock to many of us, one of the things he has said he is committed to is a deal to allow US firms to repatriate income earned outside of the USA without punitive taxation (USA corporate and state taxes are over 40%).\nMany American Corporations keep money outside of the US \u2014 so many that it\u2019s estimated that there is over $2 Trillion of offshore profits. One wonders how this will impact US tech firms and the whole investment and shares market should this money make it to America. Combine that with a campaign strategy to rebuild America\u2019s Infrastructure, with Bentley customers including the majority of US State Department of Transports, this could very well change market conditions in its favour\nSiemens Invests\nFor a few years now it\u2019s been rumoured that Bentley and Siemens could be more than strategic business partners. The companies work closely together in the Factory Design market combining manufacturing, architectural and structural technology with factory components and layout tools. Both firms are \u2018corporate\u2019 in style and have superb engineering pedigrees.\nIn the week following YII 2016, an announcement was made indicating that Siemens and Bentley Systems are in fact getting closer together with a formalised, strategic alliance agreement to advance infrastructure project delivery and asset performance in complementary business areas. Siemens and Bentley will initially invest at least \u20ac50 million in developing joint solutions to enlarge their respective offerings for infrastructure and industry to the benefit of the end-customers.\nThis work will uniquely leverage new cloud services for a connected data environment to converge respective digital engineering models from both companies. In addition to those elements of the agreement, approximately \u20ac70 million of secondary shares of Bentley\u2019s common stock has been acquired by Siemens, under a company program that will continue until such time as Bentley Systems\u2019 stock is publicly traded.\nThis ongoing acquisition of Bentley stock is extremely significant. The last time a software \/ engineering firm owned so much Bentley stock was back in the days when Intergraph owned a significant number of shares. With Siemens having also just acquired Mentor Graphics (electronics CAD software firm), it\u2019s clear that Siemens has an appetite for AEC, and is astutely watchful of what its main rival Dassault Syst\u00e8mes is doing in the space.\nSiemens and Bentley Systems have a track record of complementing their respective portfolios through the licensing of each other\u2019s technology. For example, Bentley\u2019s reality modelling software has been integrated into Siemens Process Simulate to leverage laser-scanned point clouds in modelling the existing context of brownfield industrial environments.\nThe automotive industry manufacturer Turnkey Manufacturing Systems (TMS) successfully employed the innovative point cloud capabilities to create a \u201cdigital twin\u201d of its production line to significantly enhance its planning and validation processes, while saving time and costs.\nThe new investment initiatives will involve virtually all Siemens divisions. The major benefit will be accumulating intelligence from Siemens solutions throughout Bentley\u2019s complementary applications for design modelling, analytical modelling, construction modelling and asset performance modelling. As a result, the integrated and accessible digital engineering models, such as the \u201cdigital twin\u201d viewed through an immersive 3D interface, will, according to Bentley, enable unprecedented operational performance, visibility and reliability. This work will converge digital engineering models: physical engineering models in their 3D physical reality context by way of Bentley\u2019s software solutions and the corresponding functional engineering 2D models within Siemens\u2019 solutions.\nSiemens and Bentley Systems have identified opportunities to work together in Energy Management, Power Generation, Building Technology and Mobility where each company can leverage their respective technology and industry expertise to bring new business value to the market. For example, Bentley\u2019s applications for the 3D modelling and structural analysis of industrial and infrastructure assets, complement Siemens\u2019 solutions and domain expertise in electrification and automation. Siemens and Bentley Systems will each provide software from the other to deliver complete solutions from either company to the benefit of their respective customers in order to improve their project and asset performance through simulation and virtual commissioning. Development work will benefit from and extend Siemens\u2019 and Bentley Systems\u2019 established commitments to openness and interoperability.\nBentley Systems CEO Greg Bentley explained, \u201cOnly with Siemens could we so purposefully advance beyond merely linking the \u2018Industrial Internet of Things\u2019, to ultimately leverage digital engineering models for visual operations and connected infrastructure asset performance. Given our long history of sharing complementary technologies, we are very excited to now contribute so broadly to Siemens\u2019 industrial digitalisation leadership.\u201d\nConclusion\nLooking at route to market, style of customer, depth of product and a total focus on team and document management Bentley and Siemens are very well matched in this alliance. Siemens for the first time is actively looking outside of factory design and can see that Bentley\u2019s technology and user base would also benefit from existing Siemens technologies and wide array of automation products. It will be interesting to see the fruits of the co-development and to see what the net result of these initial moves will be.\nThis article is part of a Bentley Systems Year In Infrastructure special report. Read the other articles below\nThe age of the reality mesh: Bentley Systems is forging ahead with Reality Capture by embedding reality meshes into its foundation platform.\nOpenRoads Designer: This new breed of civil design tool combines mesh data with detail design in one dynamic environment.\nCloud analytics: Knowledge is power Bentley is helping engineers \u2018design out\u2019 delays before they happen, by giving them access to real-time supply info.\nThe future of AR: The HoloLens and the use of reality meshes have got round tracking issues, paving the way for new AR applications.\nLumenRT embraces VR: LumenRT, an easy-to-use real time viz tool with a powerful \u2018game engine\u2019 environment, now offers VR support\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bim-show-live-announces-2015-programme\/","title":"NEWS: BIM Show Live announces 2015 programme","date":1422489600000,"text":"Two-day conference, which takes place at Manchester Central, 8-9 April 2015, caters for both BIM beginners and advanced practitioners\nBIM Show Live, the UK conference dedicated to building information modelling, has announced its programme with over 32 hours of seminars, live demonstrations and master classes presented by over 50 industry experts.\nThe programme features two days of content spanning the four stages of BIM, enabling delegates to pick relevant sessions from Define & Validate, through to Operate & Maintain.\nThe scope of BIM continues to grow, as many are still just starting to implement it, others are pushing the boundaries far beyond the Government\u2019s level 2 requirements for 2016. The organisers of BIM Show Live say it is the perfect place for beginners to dip their toe in the world of BIM and learn the real benefits of implementing it in their projects, whilst advanced users can hear about then next steps in widening their implementation to include HSE, facilities managers and strengthen the connection with suppliers and clients.\nWith 700 BIM users from all disciplines from architects and engineers to quantity surveyors, clients and suppliers, BIM Show Live also offers networking opportunities with the UK community. The BIM Show Party on the first night allows for discussion of the day\u2019s topics, and reconnecting with colleagues old and new over a drink or two.\nDelegates can save \u00a350 with an Early Bird rate, which ends 27 February. Group discounts and student rates are also available, along with discounted accommodation options for all delegates.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nNEWS: Three new sponsors back construction skills scheme\nGraphisoft ArchiCAD 14\nREVIEW: Graphisoft ArchiCAD 20\nTekla 2024 structural tools launch\n14th Gen Intel Core processors launch\nVectorworks simplifies GIS data integration\nTestFit Generative Design targets building optimisation\nStereoscopy in architecture\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/ai\/testfit-integrates-with-tomasettis-asterisk-to-accelerate-aec-projects\/","title":"TestFit integrates with Tomasetti Asterisk to drive AEC projects","date":1614124800000,"text":"Combining their advanced AI-driven platforms, the partnership aims to cut design tasks from weeks to seconds\nTestfit.io, the generative building configuration developer, has announced a technology partnership with the structural engineering firm Thornton Tomasetti to accelerate data sharing between their applications.\nThe firms have developed an API connecting TestFit\u2019s design and visualisation platform with Thornton Tomasetti\u2019s Asterisk structural optioneering tool. Tomasetti\u2019s Asterisk is developed by the firm\u2019s CORE studio, Tomasetti Asterisk uses computational geometry and artificial intelligence based on Tomasetti\u2019s extensive structural engineering experience.\nThe new integration enables users to solve a variety of design challenges. For example, an architect or developer contemplating the design of a parking garage might want to optimise the sizing of columns to maximise the amount of parking stalls in that garage.\nLaying out the design in TestFit, the user can get an instant answer via the knowledge embedded in Tomasetti Asterisk\u2019s Structural AI. The user can then see how the columns\u2019 width and shape would change as variables are modified \u2013 a critical task that previously would have required a coordination meeting, adding costs and delays to the project.\nThe column-width AI for parking structures is available now, with many more collaborations on the horizon. \u201cIn 2020, we developed the first IBC compliant core and shell generator for office buildings,\u201d said TestFit CEO Clifton Harness. \u201cWorking with the Thornton Tomasetti team in 2021, we hope to bring structural optimisation to our office offering, as well as to coordinate structural systems in our industry-leading multifamily building configurators.\u201d\nThornton Tomasetti\u2019s Rob Otani, senior principal and chief technology officer (CTO) in charge of the firm\u2019s CORE studio, said. \u201cUsing the API to link the knowledge represented by TestFit and Tomasetti Asterisk results in processes that are a hundred times faster than today\u2019s standard. This is just a hint of what we believe is to come with this partnership, resulting in far more efficient, reliable and proven solutions to architectural challenges \u2013 all while driving down costs by saving time and effort.\u201d\nTestFit has expanded its multifamily housing customer footprint in architectural, real estate developer and general contractor markets with its solution for rapid design in feasibility studies without the need for generative design programming knowledge.\nThornton Tomasetti, founded in 1949, has been associated with architectural projects of note worldwide, including skyscrapers such as The Jeddah Tower and Comcast Tower; stadiums such as AT&T Park, U.S. Bank Stadium and Yankee Stadium; and renewal projects including the Chrysler Building and United States Capitol dome.\n\u201cWe\u2019re thrilled to have this new partnership with Thornton Tomasetti not only because of how it will help us quickly address structural engineering issues, but because it reinforces our conviction that our approach to generative design is the wave of the future,\u201d said Harness.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/collaboration\/projectwise-365\/","title":"ProjectWise 365","date":1590969600000,"text":"AEC Magazine interviewed Dustin Parkman, Bentley Systems Vice President of Project Delivery, on the release of the company\u2019s most affordable, cloud-based document management service\nAEC Magazine: Who is ProjectWise 365 aimed at?\nDustin Parkman: At Year In Infrastructure (YII) [Singapore, 2019] we had already announced plans to come up with a new ProjectWise offering, called ProjectWise 365. We launched an invitation only beta to allow early users to do some evaluations, give us feedback, and allow us to be a bit more agile in our response time before we launched it to the general public.\nThis spring we had a global launch and the target audience has been small to mid-sized firms, for people who are not using ProjectWise today \u2014 whether that\u2019s a small firm or small workgroup within a larger organisation. It was really meant at that lower end of the scalability, so being able to quickly adapt something at a small workgroup level, and then scale based on to your needs from there.\nBecause it\u2019s a cloud service, it is instant on, there is no prerequisite, there is no infrastructure so to speak that\u2019s required by the organisation that actually speeds their time from decision to implementation, to actual production use. Whereas tried and true ProjectWise, which has been around for a number of years, is more of an enterprise level system.\nSo when you\u2019re thinking about traditional ProjectWise design integration, you\u2019re usually thinking about large groups, complex work in progress, complex spatial management across a lot of different disciplines, whether that be mechanical, drainage, civil site works, etc. where you\u2019re having to manage all that in one environment and share space, through collision checking, interference, typical space management type of stuff. Because of that, that tends to be used more so with larger organisations, larger capital projects.\nWhereas ProjectWise 365, while it can also be used on complex projects, we didn\u2019t want to confuse the market with how we positioned it. We positioned it at that midtier firm or workgroup, so even a large, multinational engineering consultant organisation could use ProjectWise 365.\nAEC: How tightly integrated is ProjectWise 365 with Office 365?\nDP: ProjectWise 365 allows you to take the engineering collaboration capabilities that are there and embed that within your day to day Microsoft Teams \/ Office environment. You already have a generic collaboration platform that you\u2019re using; ProjectWise 365 can fit into that rather than be an alternative. What we are really trying to focus on, is not so much the generic collaboration that Microsoft or Zoom or someone like that is going to be focused on, we try to focus on what differentiates us between them, which is really the detail for engineering and construction correspondence \u2014 so the ability to share documents and BIM data, the capability to spatially analyse and review information, to capture issues, to be able to do markups and review sessions \u2014 whether those be 2D PDF and DWG\/DGN types of workflows, or digital twin-based workflows with the new iTwin services Design Review that that comes as a part of ProjectWise 365.\nYou kind of get the best of both worlds: you\u2019re able to do traditional engineering design, review, QA, QCD types of processes, but you can also do more of the cutting-edge digital twin immersive types of reviews as well, so when you\u2019re in that digital twin modelling environment, your issues, your conversations, your points of interest and observations can all be spatially anchored inside the model. It\u2019s a multiuser environment where everybody can collaborate in real time.\nAEC: Compared to ProjectWise, what\u2019s missing from ProjectWise 365?\nDP: The thing that ProjectWise 365 doesn\u2019t do yet is a lot of the kind of \u2018heavyweight\u2019 features, particularly around things like title block integrations, a lot of the complexity of metadata and attribution on documents that ProjectWise does a really good job with. Also, the batch services for being able to take engineering models and periodically, on a schedule basis, being able to extract that data, create all those downstream contract deliverables through automation.\nIt doesn\u2019t also have all the engineering application plugins, so ProjectWise has integration built into MicroStation, OpenRoads, OpenBuildings, Revit, AutoCAD \u2014 all the Autodesk flavours of products \u2014 Tekla and so on. We haven\u2019t yet built those pieces out, so it is today a bit more transactional. It\u2019s not immersive in those actual design environments, which is also why it\u2019s positioned as a smaller work group\/ midsize firms solution.\nWe\u2019re not aiming for these very complex multi-discipline projects with hundreds of users all trying to collaborate and have automation running, producing the deliverables downstream. We will get there, but this is our first launch into this space and we are really trying to address the less complex side of the market, or the side of the market where you\u2019re not actually a designer, maybe you\u2019re a stakeholder, you\u2019re a viewer, you\u2019re a QA\/QC person, you\u2019re a contractor who needs to be involved with RFIs and construction correspondence \u2014 everything but the actual hardcore physical design in a complex multi-discipline environment.\nThe experience that we will have will be very much like PowerPoint or Excel has with [Microsoft] OneDrive. But today, you have to go through our web environment to upload your content, and then start your collaboration sessions from there.\nAEC: You haven\u2019t got integration links into MicroStation or Revit etc. Is that a technical barrier, or is that a feature set barrier and you\u2019re trying to make this a more attractive price point?\nDP: I don\u2019t think it\u2019s really either. Design integration was really more about trying to address capabilities that we felt we didn\u2019t already have good coverage on. [ProjectWise] has the best design integration of any collaboration software for infrastructure design right now, so we didn\u2019t really see that as a critical first step for us to address. What we really wanted to address was the people who were not doing the heavyweight design, but the broader ecosystem that need to participate in that information [workflow], to be able to collaborate without any toolset. Being 100% web, whether 2D or 3D, and just making that information accessible through a browser, through your phone, through your tablet. That was kind of like the critical first step that we wanted to attack first.\nAEC: Does ProjectWise 365 have any particular features for civil, architectural or structural?\nDP: It does have some specific civil features for doing design reviews, particularly the ability to deal with alignments and profiles and offsets for civil-based 3D digital twin models \u2014 so a lot of the station offset measurements, a lot of things that you would expect to have in a linear type of model.\nFor example, if it\u2019s a railway system or a rail track system or if it\u2019s a road \u2014 anything that\u2019s linear, pipelines, any of that type of stuff \u2014 it has capabilities for that, to capture that into the review system to actually capture measurements and conversations within the context of the project, for QA\/QC types of processes.\nAEC: Can subcontractors and supply chain firms use ProjectWise 365 to be integrated into my full ProjectWise documentation system?\nDP: Absolutely.\nAEC: What\u2019s the price?\nDP: It\u2019s $60 per quarter, but right now we are waiving all subscription fees until September 30th to help out during the Covid-19 pandemic. So $60 per quarter per person.\nAEC: Any constraints on web space?\nDP: Right now, we don\u2019t have a constraint on it, so you have unlimited at the moment.\nAEC: And how do people get on board with ProjectWise 365? DP: We have made it available to all of our partners to resell and we do our own direct sales. We\u2019re working with Microsoft now to have ProjectWise 365 available on the Microsoft Store because it is an MS Azure based product.\nBentley Systems \u2013 trends during Covid-19 lockdown\nAEC: What trends have you seen happening with Covid-19 and people working from home needing to access work data?\nDustin Parkman: What we\u2019ve seen is the organisations that already had ProjectWise and had good global distribution usage of it, we really haven\u2019t seen much of a change.\nFor the most part, all of those organisations already had a good data policy, they already had good cloud strategies in place and were already working as a global unit. People were on the road a lot so they already had good work from home or good mobile work processes in place, and they already had good data management with ProjectWise, so we didn\u2019t really see their productivity decline too much.\nWhere we saw probably the largest aspects of decline [software usage], where we saw organisations and geographies get caught a bit flat footed, was particularly in the government space. A lot of geographies around the world did not have a good work from home policy, they did not have a good data sovereignty policy for people working from home. They didn\u2019t have the hardware required in some cases, particularly for graphically intense BIM applications. That was pretty consistent across all government sectors globally \u2014 obviously, there were some that were better prepared than others\nThere were some particular countries that did not have a good work from home policy, even at a labour law perspective. Countries like India, China and a few others, they kind of struggled as well because they had to very quickly create a culture of working from home and because there was already a culture of not working at home they also had the same issues with hardware and bandwidth and things like that.\nNow we\u2019re starting to see them kind of emerge, getting their data policy and hardware in place. They\u2019re establishing not only federal and provincial policy, but each organisation is creating their own policies. The encouraging part is, because of the situation, everybody\u2019s figured it out pretty quickly. They had to, in order to survive.\nAEC: With Covid-19, have you seen more people up for a more open cloud?\nDP: I think most organisations in our space had a hybrid approach. Some of the larger ENR top 50 organisations they were ahead of the game and had already moved mostly of everything to the cloud already. But once you get past that, most firms had a hybrid model and you still get a lot of countries where there are very specific guidelines around data sovereignty. It can\u2019t be a Microsoft Cloud, it can\u2019t be an AWS cloud, it has to be some regional cloud provider. I think this is, without a doubt, going to be a catalyst to get 100% to the cloud. Most people realise they were not prepared for this from a cloud infrastructure standpoint and it introduced all kinds of challenges in their data offices.\nAEC: Have you had any customers that are looking into cloud workstations, so the data is in the cloud, the workstations are in the cloud, and they\u2019re just streaming pixels?\nDP: Yeah, we have, we\u2019ve seen a fair amount of that, particularly in areas where it was going to be an issue to get all the hardware. India is a case in point where you know that entire workforce, for the most part, wasn\u2019t ready to work from home \u2014 bandwidth issues, most of them did not have the intense graphical hardware they needed with GPUs to do graphical web and BIM.\nSo the quickest way for us to actually accommodate those users \u2013 I\u2019m speaking for Bentley \u2013 was to actually use the cloud-based virtual machines to do all that and just stream it through that. So we quickly reacted to a few different primary providers of that \u2013 Citrix XenApp being one platform, Microsoft Azure virtualisation platform, and then Frame being the other one.\nAEC: Presumably, as a company, you\u2019ve had to be more flexible with licencing?\nDP: Yes, it\u2019s been mostly on a one by one basis. What we had in the early days was users trying to use their home computers to do work. And so immediately, a lot of organisations said, \u2018please stop that. You know, we\u2019re afraid we\u2019re going to get invoiced and overcharged for new licences and things like that.\u2019\nAnd so we reached out and said \u2018hey, during this time of need, don\u2019t worry about that. Later on you\u2019ll need to get it sorted out, but as long as the total usage kind of feels within the same ballpark [it\u2019s OK].\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/www.constructiondive.com\/news\/foxconn-anthro-energy-swire-coca-cola-bad-boy-mowers-us-investments\/807996\/","title":"Foxconn and Anthro Energy among latest companies to invest in US facilities","date":1765843200000,"text":"A host of U.S. manufacturing expansions are underway, supported by federal and state incentives to bolster domestic production and create local jobs.\nJust last week, officials in Alabama, Kentucky and Colorado touted investments from manufacturers in the battery materials, farm equipment, beverage and electronics industries.\nHere are more details about those projects.\nCoca-Cola bottling partner to invest $475M in Colorado site\nUtah-based Swire Coca-Cola, USA plans to build a 620,000-square-foot manufacturing plant in Colorado Springs, Colorado, a local chamber of commerce announced Dec. 11.\nThe facility will produce more than 230 beverage options across more than 60 brands, including soft drinks, waters, teas, juices and sports beverages, according to a news release. It is expected to double Swire Coca-Cola\u2019s local workforce with the creation of 170 jobs, \u201cprovide its employees with a modern working environment, meet rising customer demand and advance its sustainability goals.\u201d\n\u201cColorado Springs has been a great partner for our existing distribution facility where we employ 170 people,\u201d Bryan Sink, SVP of supply chain at Swire Coca-Cola, said in a statement. \u201cThe city offers a highly skilled workforce and a strong sense of community \u2014 all of which make it an ideal location for this strategic investment.\u201d\nOfficials said the project will support about 1,190 construction and installation jobs, generating an estimated $103 million for workers and families in El Paso County. Swire Coca-Cola is pursuing LEED Gold certification for its facility.\nThe company plans to break ground on the project next year and consolidate its existing operations into the new facility. It will replace a 90-year-old production plant in Denver, according to the release. Currently, Swire Coca-Cola employs 1,300 people in Colorado.\nFoxconn to build first US manufacturing facility in Kentucky\nTaiwan-based Foxconn Technology Co. will invest $173 million to build its first U.S. manufacturing facility in Louisville, Kentucky, creating 180 jobs, Mayor Craig Greenberg announced Dec. 9.\nThe 350,000-square-foot \u201cfactory of the future\u201d will be outfitted with artificial intelligence and robotics in all phases of consumer electronics production, from design and assembly to logistics, according to a news release. It is part of Foxconn\u2019s \u201cMade in America\u201d initiative, aimed at strengthening the country\u2019s supply chains.\n\u201cFoxconn helped pioneer the world\u2019s most advanced production systems, from smartphones to computing devices,\u201d Foxconn CEO Ben Liaw said in a statement. \u201cNow we\u2019re bringing that same precision and innovation to the United States.\u201d\nThe project received preliminary approval for an investment incentive valued at up to $3.4 million by the Kentucky Economic Development Finance Authority. The agency also approved up to $600,000 in tax incentives.\nOperations are expected to begin in the third quarter of 2026.\nAnthro Energy to bring $142M battery materials factory to Kentucky\nAlameda, California-based Anthro Energy plans to establish a battery materials manufacturing facility in Louisville, Kentucky, Mayor Craig Greenberg announced Dec. 12.\nThe 25-GWh factory will be capable of making 12,000 metric tons per year of Anthro\u2019s proprietary injectable phase change electrolyte, a material used for lithium-ion batteries in electric vehicles, defense and consumer electronics, according to a news release. The investment is expected to create 110 factory jobs and support 390 construction jobs.\n\u201cBy onshoring critical battery-supply-chain infrastructure, we\u2019re strengthening both our national security and our industrial base,\u201d Anthro Chief Technology Officer Joe Papp said in a statement. \u201cWe look forward to contributing to Kentucky\u2019s leadership in the advanced battery economy.\u201d\nAnthro received a $24.9 million grant from the U.S. Department of Energy earlier this year to establish the \u201cfirst large-scale, U.S.-owned and operated advanced electrolyte manufacturing facility.\u201d It also received $18.4 million in investment tax credits under the Inflation Reduction Act.\nAdditionally, Anthro said it\u2019s eligible for more than $3 million in grants and incentives from the Kentucky Cabinet for Economic Development for its project, including more than $2 million in job creation and payroll tax incentives.\nBad Boy Mowers selects Alabama for tractor plant\nBatesville, Arkansas-based Bad Boy Mowers will spend $10.5 million to establish a tractor assembly plant in Monroeville, Alabama, Gov. Kay Ivey announced Dec. 10.\nThe plant will be located at a former Vanity Fair lingerie distribution center, where work is underway to transform the site, according to a news release. It is expected to have a capacity of about 9,000 tractors per year, once completed, and bring 50 jobs to the area.\n\u201cMonroeville is truly excited about Bad Boy\u2019s decision to locate here and begin production in the very near future,\u201d Mike Colquett, executive director of the Monroeville\/Monroe County Economic Development Authority, said in a statement.\nEstablished in 2002, Bad Boy became popular among land and home owners for its zero-turn mowers. The company has since expanded into tractors, handheld tools and utility task vehicles.\nThe project is eligible for state jobs and investment credits valued at $3.4 million, according to the Alabama Department of Commerce.","source":"constructiondive.com"}
{"url":"https:\/\/geospatialworld.net\/news\/prof-f\/","title":"Prof. F","date":1109635200000,"text":"The 2005 Photogrammetric Award (Fairchild) is awarded to Prof. Dr.-Ing Wolfgang F\u00f6rstner in honor of his major contributions to the science of photogrammetry, by helping to establish the increasingly important ties between photogrammetry, digital image processing, and computer vision. The award will be presented during the upcoming ASPRS 2005 Annual Conference in Baltimore, Maryland, March 7-11.\nThe Photogrammetric Award (Fairchild) was established in 1943 to stimulate the development of the art of aerial photogrammetry in the United States. This award was originally sponsored by the Loral Fairchild Corporation and is now supported by Lockheed Martin. It includes an engraved plaque.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-bim-holoview-brings-mixed-reality-to-revit\/","title":"NEWS: BIM HoloView brings mixed reality to Revit","date":1511481600000,"text":"Software uses Microsoft HoloLens to overlay full-scale BIM models into real buildings\nBIM Holoview is a new mixed reality visualisation tool for Revit and Navisworks that uses the Microsoft HoloLens to overlay full-scale BIM models into real buildings.\nThe software can help verify constructability and aid communication between designers, managers, and contractors. Post-construction, facility managers can \u2018look through walls\u2019 to locate hidden MEP services for maintenance and upgrades. The software is designed to handle large models without a reduction in quality (i.e. polycount reduction). According to the developers, this was done for three reasons.\nOne, so users could avoid having to manipulate or downscale the models before viewing them in BIM Holoview. Two, so detailed objects (e.g. fittings, special equipment) would look exactly the same as they are modelled to give users a clear visual understanding of how a model is to be implemented in a real building \u2013 to the level of identifying correct components (e.g. sprinklers) and fitting items in their correct position (e.g. bolt positioning). Three, so users were able to move room to room in full scale, with full quality, without loading new sections of the model.\nDespite the focus on model quality, the developers says model load times are quick. The software\u2019s pre-processing algorithm means the biggest file (120MB) can be loaded into the HoloLens in around 12 seconds. This, says BIM Holoview, is particularly important when showing a team member a build element, or when you need to quickly switch between models (e.g. Structure and MEP).\nBy default, models are loaded as transparent which allows users to see through walls and means construction teams essentially have \u2018x-ray\u2019 vision of the construction layout. However, for safety, there\u2019s also \u2018walk mode\u2019 which hides the model so you can have an unobstructed view when walking around a construction site and going up and down stairs.\nFinally, the developers says accuracy is high thanks to a 2 position marker system to align the BIM model to the real environments. However, in the event of any drift caused by the HoloLens spatial mapping, a movement function allows users to shunt the model back into alignment.\nPricing starts at $195 per month. There\u2019s also a free 14 day trial.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/smartgeometry-2010-report\/","title":"SmartGeometry 2010 report","date":1276473600000,"text":"Starting out as a forum to discuss computational and parametric approaches to design, SmartGeometry now touches all aspects of architecture and construction from concept all the way to fabrication. Greg reports from the recent event, held in Barcelona.\nSmartGeometry is one of the most important conferences in the AEC (Architecture, Engineering Construction) calendar. It brings together architects, engineers, programmers and mathematicians from practice, education and research, all of which have a shared interest in developing new computational and parametric approaches to design. There is a core focus on technology that is already in use or in development, but it also offers a chance for brand new ideas to be discussed and debated, some of which are really quite \u2018out there\u2019, so to speak.\nNow in its seventh year, SmartGeometry has become a truly global event. Previously held in San Francisco, Munich, New York and Cambridge (Ontario and UK) the location for this year\u2019s gathering could not have been more inspirational. Barcelona is the birthplace of Gaudi, who through his many innovative approaches to design and structure was one of the modern pioneers of complex geometry.\nOf course, the city is also home to his most famous construction site, the Sagrada Familia and Gaudi\u2019s masterpiece is a regular topic of conversation at the event, with the cathedral\u2019s executive architect and researcher, Mark Burry, updating attendees on the latest progress and innovations in design and fabrication. For those lucky enough to stay on after the event, the organisers also arranged a private behind the scenes tour of the cathedral.\nWhile the original SmartGeometry events concentrated largely on form and structure, over the years the underlying theme has changed to help drive forward other areas of architecture and construction. The last couple of events have focused on the hot topics of sustainability and environmental design, but this year it was the turn of fabrication. The idea was to help attendees engage directly with the fabrication process through design, building and testing of working prototypes.\nThe event comprises two conference days: a shop talk day, which sets up the stage for panel discussions on a variety of technical issues, plus a symposium day which features presentations on the theory and practice of computational and parametric design approaches. Alongside this, there is the workshop, the format of which was freshened up this year by moving beyond the confines of a computer and challenging attendees to produce \u2018working prototypes\u2019 to help solve a variety of design and fabrication problems. This took place in the industrial space of the Institute for Advanced Architecture of Catalunia (IaaC), while the main conference was held at the somewhat more refined environs of the Palau de la M\u00fasica Catalana.\nNew digital manufacturing\nOne of the major themes at SmartGeometry 2010 was that of adapting additive manufacturing for construction. Also known as 3D printing or rapid prototyping, the additive manufacturing process typically builds 3D models layer by layer using a variety of technologies.\nIn the AEC sector, 3D printing is used widely for architectural model making, but with new processes and larger scale machines there are many potential new applications. These include making complex forms that could not be manufactured by conventional means, producing bespoke architectural components, or even printing an entire building.\nThere are currently many different approaches to additive manufacturing in AEC. Materialise, a company better known for making bespoke components for the automotive sector is already making commercial architectural components with its mammoth Stereolithography machines. More for interior design than actual architecture it makes production quality products from limited edition furniture to customised lighting fittings for hotels.\nInterestingly, Materialise said that the only thing that was stopping it making even bigger machines was demand. One would expect that the only thing stopping demand was cost, which is still high for virtually all 3D printing technologies.\nMarta Mal\u00e9-Alemany from the Institute for advanced architecture of Catalonia (IaaC) made the very valid point that adapting digital fabrication processes from other industries may not be the best approach in order for digital fabrication to succeed. Instead, she said that technology needed to be developed specifically for construction, a view shared by many other presenters at the event.\nMs Mal\u00e9-Alemany went on to show some of the fantastic work her students at the Architectural Association Design Research Laboratory (AADRL), London, have undertaken in this field. One involved a process called contour crafting where a paste-like material is deposited in layers in much the same way a chef applies icing to a cake with a pastry bag. She explained how her students hacked into a CNC milling machine, attached a pump with clay-like material and carried out a series of experiments to investigate the types of geometries that could be built with this technique. They then started to develop architecture based on the process.\nDomes were found to be more stable when they had sinusoidal walls and the students realised that by building envelopes within envelopes and spaces within spaces entire buildings could be constructed. Algorithms were then created to help generate actual architectural designs.\nThe research project has even progressed to the stage of investigating how to develop the mechanics of machines to work on site, with huge robotic arms building entire communities. All of this was illustrated with some amazing videos that simulated the whole process.\nAnother team looked into the use of phase changing materials to build structures, specifically the use of wax-like materials that would solidify in water. Delivering wax underwater at pressure through air nozzles the initial results were extremely erratic. However, by experimenting with different nozzle sizes and shapes, pressures and material makeup and viscosity more control was achieved. The future vision for this technology was a machine called robo(a)t, swarms of which float on the surface of the ocean and create new inhabitations and environments to promote underwater ecologies.\n{mospagebreak}\nBringing things back to reality was Richard Buswell, whose Freeform Construction project (www.buildfreeform.com) at Loughborough University is investigating the potential of large-scale additive manufacturing for producing full-scale building components.\nA number of additive processes have been investigated, but the project has focused on using traditional construction materials such as gypsums and cement-based mortars. The technique is called concrete printing, but unlike traditional powder-based 3D printing processes, such as that used by Z Corp in its Z Printer machines (www.zcorp.com), it deposits a \u2018liquid\u2019 concrete \u2013 think squeezing a very big tube of grey toothpaste. The machine uses a gantry and can produce components in a build volume of up to 2m x 2.5m x 5m. The project has already delivered a one tonne reinforced concrete architectural piece.\nDr Buswell explained that the challenge has been to develop components that are not only aesthetically pleasing but can be tangibly used as part of the construction process. This not only means understanding the strengths, weaknesses and physical properties of each process, verified through compression and flexural testing, but the importance of precision in relation to fabrication.\n\u201cIf we are going to produce real components in buildings that are likely to be components that will fit together, things like tolerance are very important,\u201d he said.\nThis focus on precision has also resulted in research into manufacturing walls with voids that can accommodate building service requirements such as pipes and cables.\nWhile the Freeform Construction project focuses on the use of additive manufacturing at more of a component scale, Italian born Enrico Dini of Monolite UK, is hoping to take things to new levels with his D-Shape machine that he described as his \u201ccrazy vision of making entire buildings\u201d.\nStarting out in 2004, Mr Dini has developed the world\u2019s biggest 3D printer, which is capable of building structurally sound sandstone buildings automatically with virtually no human intervention. Unlike Buswell\u2019s concrete printer, Dini\u2019s D-Shape uses a process very similar to that used in Z Corp\u2019s Z printer. However, for powder he uses sand mixed with an inorganic binder.\nHis largest structure to date is the Radiolaria, a 2m tall structure inspired by the architect Andrea Morgante. The structure was modelled in CAD, then stress analysis software was used to validate its integrity without having to add additional reinforcement. The next stage was to export to STL, the industry standard for rapid prototyping, and then import the file into D-Shape\u2019s dedicated software ready for \u2018printing\u2019.\nIt took ten days to print the structure, which included a containing shell comprising the binded model and loose sand, which was carefully removed by hammer. Mr Dini admitted that he was afraid that the structure would collapse during this process, but to his relief it remained intact. It was then hand finished to add to its aesthetic appeal. A full scale, 8.5m high version of the Radiolaria has been commissioned for installation in Pontedera Italy later this year, but this will be built in parts.\nMr Dini alluded to the fact that funding has been a continual struggle, compounded by the recent economic downturn. He has boosted the project by manufacturing bespoke limited edition furniture, and while this is impressive in itself, you get the feeling it is not these projects that fuels his passion for the technology.\nMr Dini, a very humble man, was honoured to be speaking at the event and was particularly excited about presenting in Barcelona, proclaiming his love for Gaudi and all of his work. With this in mind it came as no surprise to learn that many of his dreams centre on the Catalan architect\u2019s work.\nWithin five years he is confident that he will be able to build a portion of Gaudi\u2019s Casa Batlo, with a structure ensuring strength, thermal properties, and mechanical properties. His dream, however, would be to complete another of Gaudi\u2019s unfinished buildings, the Church of Col\u00f2nia G\u00fcell in Barcelona. For a man who has put his heart and soul into printing 3D buildings this would be the pinnacle of what looks certain to be a fascinating career. More details at www.d-shape.com. Look out for a more in depth look at the technology in the September\/October edition of AEC magazine.\nBringing things back to a more familiar scale for 3D printing, was Adrian Bowyer of Bath University, who demonstrated his RepRap, a replicating rapid prototyping machine that can copy about half of its parts. Arguably the most enthusiastic presenter at the two-day event, Adrian captivated the audience, explaining that 3D printing does not need to be expensive. I wondered what Z Corp, a sponsor at the event felt about this.\nBowyer\u2019s open source machine can print parts out of most plastics, which means they are strong enough to make actual architectural components. The size of printed parts is currently limited to 200 x 200, x 140 mm3 but by changing around some components, he said that the machine could quite easily be made to build bigger parts. I am sure his talk inspired many attendees to give 3D printing a go. Check out www.reprap.com to see the technology and www.thingiverse.com, a place to share digital designs.\nUsing prototypes for subjective analysis\nFrom acoustics to light and form there were a number of presentations where prototypes were used to communicate fundamentally less tangible concepts to the design team and clients.\nHugo Mulder of Arup\u2019s Advanced Technology and research group, showed an interesting project where both virtual and physical prototypes were produced. The Ijmuiden Wind Sculpture, a permanent art installation in Holland, is designed to hide the industrial backdrop as Amsterdam residents make their way to the beach.\nComprising a 1,000 strong array of 10m high poles that sway in the wind like reeds, local wind data was fed into a computational program, which simulated how the sculpture would react at different winds speeds. This enabled Arup to verify the structural integrity of the design, but also gave a great insight into the movement of the sculpture from an architectural perspective. The aesthetics of the design was further evaluated by building a physical prototype that used a motor to move the individual parts of the sculpture.\nAnother project where prototypes were used to communicate less tangible aspects of a design was the Louvre in Abu Dhabi, a collaboration between the French and UAE governments to build a sister museum to the Paris art gallery on an artificial island in the UAE capital city.\nAl Fisher of Buro Happold showed how full scale mockups were used to communicate both structure and environmental design to different members of the design team. They produced a 15m x 15m section of the roof on site to validate and test something that was quite subjective \u2013 how the light would come through the roof.\n{mospagebreak}\nTectonic prototypes\nAnother major theme of the event was the use of tectonic prototypes and how their use is helping deliver innovations in assembly. Russell Loveridge gave an interesting insight into the work he does as Lapa lab at EPFL (Swiss Federal Institute of Technology), Lausanne, where he tests architectural components on a 1:1 scale. This is from the perspective of design, fabrication and materiality, where the lab has close links to industry to test new architectural materials and composites to destruction.\nMr Loveridge showed an interesting example of a mini cinema that was built as a temporary structure for a film festival that not only needed to look good but, with the help of scripting to develop the individual pieces into fabricatable code, could be built quickly.\nMartha Tsigkari, associate at Foster and Partners specialist modeling group, gave a very lively presentation on the increasing importance of tectonic prototypes, both digital and physical.\nFosters is heavily into 3D printing and has a huge resource in house to help with anything from form finding, to lighting and daylight analysis. The practice also uses colour prints to communicate different aspects of a building, such as results from solar insulation analysis.\nBut Ms Tsigkari\u2019s real passion is in making the sure the model, whether a physical or virtual CAD, is used to aid the fabrication process. She explained how she likes working with STLs (Stereoligthography) where models can be used to work out connections, how they fit together and how they will be constructed on site.\nTo improve the design to fabrication process, she explained how real detail needs to be added to the model to truly understand how the building works, otherwise it is too easy to get a false sense of security. \u201cYou see the [3D printed] model, it stands on the table, you are happy with it, but when you go to build it you get a problem because you have not really solved the CAD model,\u201d she said.\nAll of this is becoming increasingly important, she explained, as we are entering an era where all buildings will be built from information provided directly in the model. However, she also acknowledged that even with highly detailed 3D CAD models that show how complex forms will fit together on site, sometimes this is not in tune with current fabrication processes.\nContinuing the theme of tectonic prototypes, Achim Menges of the University of Stuttgart, shared his research into the relationship between material and computation. He showed how his design approach is being driven more by the behaviour of materials, giving a specific example of wood, and less by geometric data. By analysing the behaviour of plywood through computational structural analysis and physical testing, he showed how a building that had been designed to respond to changes in relative humidity by opening and closing louvres all by itself.\nWorkshop clusters\nFrom robots and CNC machines, to laser cutters and sowing machines, the new look SmartGeometry workshops offered all the tools required for participants to get their hands dirty as they embraced the challenge of proving out computational CAD models with physical working prototypes.\nHeld at the Institute for Advanced Architecture of Catalunia (IaaC), a world-renowned education and research centre, the industrial space packed in over 180 participants from all disciplines. Together with the huge collection of heavy-duty machinery this created a truly unique workshop atmosphere, complete with a cacophony of industrial bangs, the pleasant aroma of smoking wood, and the not so pleasant aroma of burning plastic and metal.\nThe workshop was organised around ten individual clusters, each tasked with proving and testing a concept and design. Each cluster was given advance training in Bentley\u2019s GenerativeComponents (GC) software, the starting point for many of the clusters, although many of the participants were already well versed in the generative design software as they were in other such tools, including Rhino.\nThe workshop actually started four days in advance of the conference, and by the time the conference commenced and it was time to present the projects, there were some very tired faces. Considering the amazing amount of work that each cluster had produced in such a short timescale, this came as little surprise. Lars Hesselgren of PLP Architecture, and one of the founding members of SmartGeometry, showed a video that compressed the entire four days into three minutes. I, for one, struggled to see a moment where the workspace was empty.\nDespite having burned the candle at both ends, this did not dampen the enthusiasm of the cluster leaders, who presented their projects to the rest of the conference attendees. There were some excellent projects on show and it was particularly interesting to see how architects, engineers and mathematicians exchanged ideas, processes and techniques to come to some interesting conclusions. It was also fascinating to hear more about the lessons learned when moving from virtual to physical prototypes and vice versa.\nThe rapid assembly cluster focused directly on fabrication and challenged participants to turn their back on mass produced fixings such as nail plates, bolts and screws and draw inspiration from manufactured goods such as cars or phones to create \u2018snap fit\u2019 elements that make assembly on site quicker and cheaper.\nMembers of another cluster honed their sowing skills as they swapped computational meshes for needle and thread to turn the theory of tensile fabric structures into working prototypes. This helped them better understand material behaviour as it changed with scale. To develop the complex forms and anchor points of the multi-layered fabric structure the team used a combination of Rhino and Generative Components. Another group armed with sewing machines was that tasked with creating inflatable envelopes. Inspired by structures from the sixties, Grasshoper and GC were used to design the complex cell geometry with the resulting flattened patterns made on a CNC fabric cutter.\nOne cluster that initially rejected computers in favour of the traditional \u2018prototype\u2019 material of paper was the curved folding group. The idea was to investigate how to design complex sheet metal forms with curved fold lines and build them by robot without any tooling. Hundreds of concepts were worked up with paper and knife, one of which was then scanned in 3D by a Z Corp scanner. The 3D model was worked into a 2D surface inside GenerativeComponents and a range of perforation types and mechanisms for connecting parts together were investigated. The end result was an impressive 3.5m high sculpture that was assembled by robot. Some interesting videos that explain the robotic manufacturing process can be found at www.robofold.com.\nOne of the most playful clusters was \u2018design to destruction\u2019, where the brief was to design a 1.2m wooden cantilever with voids cut by a laser cutter through an iterative process of computational analysis using GenerativeComponents and small-scale prototyping. Each design was then to destruction by piling some pretty hefty weights to a palette \u2013 bringing even more noise to the IaaC workshop. The winning design was the one that had the lowest weight, but highest loaded capacity, and much to the amusement of the audience this was designed by an architect, while the losing entry was made by an engineer.\n{mospagebreak}\nGenerativeComponents\nSoftware has always been the driving force of SmartGeometry and Bentley\u2019s GenerativeComponents design tool has been an integral part of the event since the beginning. As a primary sponsor of SmartGeometry, Bentley has a significant presence, but this year the developer had some very important news to announce.\nBentley\u2019s Huw Roberts revealed on stage that GenerativeComponents would now be available as a no-charge, stand-alone technology preview release. Previously only available inside MicroStation the move will certainly open the software up to a much wider audience and attendees that I spoke to responded favourably to the move. It appealed not only to those who did not already use MicroStation, but also to Bentley customers who would now be able to free up MicroStation seats inside their practices.\nMr Roberts explained the rationale behind the move was to encourage a broader community and help them explore the real potential of computational and generative design. He also said that feedback from users would help Bentley continue to integrate it with the rest of its applications and platform technologies.\nLooking at the announcement from a business perspective one could also suggest that it is a defensive move by Bentley in response to feeling increased competition from other CAD software developers. It has been two years since McNeel released Grasshopper for Rhino and from its appearance in a number of presentations at the two-day event, it is clear that it is gaining traction.\nProbably of equal concern for Bentley, is Autodesk\u2019s own generative design software for AutoCAD. Still a work in progress, it is being developed by Robert Aish who was the original driving force behind Bentley\u2019s Generative Components software.\nFor more information on GenerativeComponents and to download a free copy go to www.bentley.com\/getGC\nConclusion\nThis was my first SmartGeometry event so it is hard to compare it to those that came before. However, from talking to regular attendees there was a feeling that it was the best yet. Much of this was down to the hands-on workshops, which through the chaotic environment of the industrial space at the IaaC gave a real feeling of unity and achievement. Developing an intelligent computer model is one thing but realising these concepts in physical form (and making a racket on an industrial scale whilst doing so) really added to the workshop format.\nFrom my perspective I was somewhat surprised at how the event was less about complex geometry modelling for architecture and more about digital fabrication. I guess this is testament to the success of previous years as most major architecture firms are already progressing well with such technologies. Keeping the agenda fresh is important to not only keep it alive, but to help the industry move forward.\nDesign for fabrication has always been an important element at SmartGeometry, but this year the event gave a real insight into the future of construction. Printing entire buildings in 3D may be the stuff of science fiction, both now and well into the future, but by providing a forum to discuss such \u2018crazy visions\u2019 \u2013 on stage, over coffee or over bottle of wine in the evening \u2013 it encourages the industry to look at architecture and construction with fresh eyes, which can only be a good thing. Roll on SmartGeometry 2011.\nMore information, including videos of all of the presentations from SmartGeometry 2010, can be found at www.SmartGeometryConference.com\/2010","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/workstations\/review-scan-3xs-wi4000-viz\/","title":"Review: Scan 3XS WI4000 Viz","date":1551398400000,"text":"A workstation packing an incredible punch for real time viz, VR and GPU rendering but it\u2019s also no slouch when comes to CPU intensive workflows\nThe CAD workstation has had a pretty standard template for some time now \u2014 high frequency quad core or six core CPU, mid-range pro GPU, 32GB of RAM and an SSD.\nOf course, this spec is fluid and can be adjusted as required. For workflows that include real time viz or VR, for example, you\u2019ll need a more powerful GPU; for quicker ray trace rendering, a CPU with more cores. And if you do both, a bigger investment in both areas.\nWith the rise of GPU rendering, however, things are starting to change. Firms can now put the lion\u2019s share of their budget into the GPU to solve most of their computational demands and stick with a relatively cheap CPU.\nThis is all great in theory, providing your ray trace rendering software of choice runs on the GPU. But this still isn\u2019t always the case. KeyShot, for example, one of the most popular amongst product designers, still only runs on the CPU (well, for the time being, at least, as it was recently demonstrated running on Nvidia GPUs).\nScan\u2019s new 3XS WI4000 Viz workstation finds some middle ground, coupling the relatively low-cost eight core Intel Core i9 9900K CPU with the phenomenally powerful Nvidia Quadro RTX 4000 GPU. The result is an incredibly fast desktop workstation that can do pretty much most of what your average product designer will need for real time viz, VR and GPU rendering, and all for under \u00a32,000.\n8 core power\nUntil recently, eight core CPUs came at a big premium, but the new Intel Core i9 9900K delivers this spec for a very palatable \u00a3417, a mere \u00a384 more than the CAD-standard six core Intel Core i7 8700K. Out of the box, the Core i9 9900K has a base frequency of 3.60 GHz and a Max Turbo of 5.0 GHz but Scan has applied its usual overclocking expertise to deliver all 8 cores at a phenomenal 4.90GHz.\nFor single threaded CAD and BIM applications it doesn\u2019t get much faster than this. In SolidWorks, the machine felt lightning fast and it set a new record of 75 secs in our IGES export test. What\u2019s more, it also impressed in our multi-threaded rendering benchmarks, completing our 4K KeyShot scene in 255 secs and the V-Ray benchmark in 60 secs. To provide some context, this is more than twice as fast as the Dell Precision 5820 we reviewed in June 2018, which had a quad core Intel Xeon W-2125 (4.0 GHz to 4.5GHz). This CAD focused workstation clocked 628 secs for KeyShot and 133 secs for V-Ray.\nScan\u2019s overclocked Intel Core i9 9900K is still some way behind the 32-core AMD Threadripper 2990WX (116 secs for KeyShot and 27 secs for V-ray) but because it runs at 4.9GHz, it will absolutely trounce AMD\u2019s monster CPU in single threaded workflows.\nThe star of the show\nWhile the Scan 3XS WI4000 Viz is no slouch when it comes to CPU rendering, the star of the show is arguably the new Nvidia Quadro RTX 4000 GPU. We review this in detail here but the long and short of it is, it\u2019s a phenomenally fast GPU for real time viz and VR. In terms of performance, we found it to be anywhere from 53% to 109% faster than the Quadro P4000 it replaces, which was an impressive GPU in its own right. But the icing on the cake is its performance in GPU renderers. In SolidWorks Visualize, for example, it delivered a very good quality 4K render in 278 secs. Then, when AI denoising was enabled, an equivalent image in a breath taking 49 secs.\nThis is faster than any other GPU we have tested, but the Quadro RTX does appear to be bottlenecked somewhat when using AI denoising in Solidworks Visualize 2019. For more details on this, check out our full review here.\nFew would argue that the performance of this GPU is phenomenal, but it will get even better when RTX-enabled applications start to ship. If you haven\u2019t considered GPU rendering before, now is definitely a good time.\nThe workstation itself is trademark Scan. It\u2019s relatively quiet in operation, thanks to its dual-fan Corsair H100x hydrocooler, which needs to disperse even more heat than a standard non-overclocked 95W Intel Core i9 9900K would need to. Even after rendering in KeyShot for over an hour, it only produced a gentle hum, but the fans did rev up and down a little, which some may find annoying.\nEverything is arranged really neatly inside. You get easy access to the single 2TB Seagate Barracuda Pro HDD, while there\u2019s a 500GB Samsung 970 Evo Plus SSD hidden beneath an M.2 heatsink. A second M.2 SSD can be added, if required.\nOne of the nice features of the Asus Prime Z390-A motherboard is that it can support up to three PCIe x 16 GPUs, so if you really want to go hard on GPU rendering, simply add a couple of additional Quadro RTX 4000 GPUs, which cost \u00a3835 a piece. This configuration, however would need a more powerful PSU as the Corsair TX550M 80-Plus Gold is only rated at 550W.\nThe 3XS WI4000 Viz comes with 32GB (2 x 16GB) of Corsair Vengeance LPX DDR4 memory running at 3,000MHz, but there are two free slots should you wish to upgrade at a later date.\nConclusion\nOverall, Scan has delivered an impressive new desktop workstation. It may have a mainstream, sub \u00a32,000 price tag, but there\u2019s nothing ordinary about the performance. What you can get out of the Quadro RTX 4000 GPU is quite phenomenal, but this machine also has plenty of grunt for CPU-centric workflows, both single and multi-threaded. In summary, it\u2019s a fantastic all-round workstation that can handle many different viz workflows and one we\u2019d highly recommend.\nPrice \u00a31,962 + VAT\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/avatech-authorized-educational-reseller-of-autodesk-to-sell-to-higher-education-institutions-and-students-in-48-us-states\/","title":"Avatech - Authorized Educational Reseller of Autodesk to sell to higher education institutions and students in 48 US States","date":1039564800000,"text":"Avatech Solutions a provider of design automation and quality assurance solutions for the manufacturing, building design, civil engineering and GIS markets, has announced that it has been authorized by Autodesk as an Authorized Educational Reseller (AER) to sell products and services to post-secondary educational institutions and students in 48 US states, previously authorized to sell in only 5 states.\nAvatech offers a unique multi-disciplined team approach to help institutions implement the latest design automation technology in their classrooms-created to keep students and institutions on the leading edge. Avatech will help students and companies come together through the endorsement of programs and services such as internships, co-op programs and identifying employment needs among its commercial clients.\nAvatech will also host an ongoing series of guest presentations by Avatech experts and customers, designed to promote the professional field of engineering and provide more insight into the possibilities available by using the latest digital design software.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-public-beta-for-rhino-grasshopper-archicad-connection\/","title":"NEWS: Public beta for Rhino\u2013Grasshopper\u2013ArchiCAD Connection","date":1442275200000,"text":"Bi-directional \u201creal-time\u201d link encourages architects to explore design variations, and create and fine-tune building details and structures\nGraphisoft has gone live with the public beta of its Rhino\u2013Grasshopper\u2013ArchiCAD connection. The bi-directional \u201creal-time\u201d link allows architects and designers to use the algorithmic design tool (Grasshopper) in combination with professional BIM software (ArchiCAD).\nThe connection is designed to encourage users to explore a large number of design variations, and create and fine-tune building details and structures using algorithms without exchanging files. It complements Graphisoft\u2019s native file format support for Rhino that enables designers to open and save Rhino project files with ArchiCAD.\n\u201cIn addition to a standard BIM design approach that is based on assembling building elements, the Rhino\u2013Grasshopper\u2013ArchiCAD connection will bring the benefits of using BIM at the early design phase, when buildings elements are still undifferentiated,\u201d said Tomohiko Yamanashi, executive officer and deputy head of architectural design department at Nikken Sekkei. \u201dThis connection will also help align the traditional design process with the computational design approach. I am looking forward to seeing BIM become a friendlier design tool for architects.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/visualisation\/3d-repo-using-unreal-engine-for-digital-twin-platform\/","title":"3D Repo using Unreal Engine for digital twin platform","date":1614556800000,"text":"Epic MegaGrant funding will help roll out 3D rendering and data delivery platform\n3D Repo is working on a new digital twin streaming solution that will allow AEC users to share and dynamically view massive 3D engineering models straight from the cloud using Epic Games Unreal Engine.\nThe development of the \u2018infinitely scalable\u2019 3D rendering and data delivery platform is being funded by an Epic MegaGrant from Epic Games.\nVia open APIs, the 3D Repo version control cloud connects with Unreal Engine, where pre-optimised assets are dynamically loaded directly into a \u2018running game\u2019 giving users the ability to log-in via their 3D Repo credentials and \u2018seamlessly retrieve\u2019 any of their assets on the fly.\nAccording to 3D Repo, this is made possible through a persistent storage layer rather than parsing and baking assets into a new game executable each time an asset revision is created.\nFeatures include full support for BIM metadata, native support for RVT, DGN, IFC, and FBX, etc, federation of different file formats, six levels of detail in Revit and access to all 3D Repo APIs including 4D and SafetiBase.\n\u201cOver the past two years, we\u2019ve seen a gradual shift from BIM to Digital Twins, with more and more clients requesting high visual fidelity for both engineering and client presentation purposes, and the ability to explore vast 3D scenes in real-time.\u201d shared Dr Jozef Dobos, CEO, 3D Repo.\n\u201cOur new digital twin streaming solution is a major breakthrough that will enable AEC professionals to collaborate remotely using very large and complex 3D models across the web.\u201d\n\u201cAEC professionals are running into a wall trying to visualise large-scale projects using existing solutions, and this effort shows what\u2019s truly possible when innovative teams collaborate for a mutual goal,\u201d added Ken Pimentel, AEC Industry Management, Epic Games.\nThe 3DRepo4UnrealEngine integration library is currently in beta and freely available for download here.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-nvidia-uses-ai-to-deliver-8x-speed-boost-to-ray-trace-rendering\/","title":"NEWS: Nvidia uses AI to deliver 8x speed boost to ray trace rendering","date":1501545600000,"text":"Takes \u2018noisy\u2019 ray traced image and uses AI to predict what it would look like after thousands of passes\nNvidia is looking to rewrite the ray trace rendering rule book by using Artificial Intelligence (AI) to massively accelerate the time-consuming process, rather than simply using brute force processing.\nThe GPU manufacturer is using AI to significantly lower the number of light ray bounces needed to get a \u2018correct picture\u2019.\nNvidia says speed ups are in the region of 8x faster on the same GPU hardware. Nvidia\u2019s AI-enhanced ray tracing works by rendering a \u2018noisy\u2019 image with minimal light ray bounces and then predicting what the final image would look like if it was rendered with thousands of bounces.\nTo give the neural network its knowledge, Nvidia has trained it with 10,000s of image pairs where one image has done 1 path per pixel and the \u2018reference image\u2019 has used 4,000 paths per pixel. The neural network learns how to map the different types of noise to the correct de-noised pixels.\nNvidia is making its AI de-noising rendering technology available in the OptiX 5.0 SDK, a ray trace development kit that can been used by third parties for developing GPU renderers. OptiX is already used in Nvidia IRAY and mental ray so we expect AI-enhanced ray tracing to appear in these products in the future. IRAY is available for SolidWorks, Siemens NX, Rhino, 3ds Max and other 3D tools. The technology could also have big implications for physically-based rendering in VR.\nNvidia has shared some performance figures for OptiX and AI-optimised ray tracing, including comparisons of its Pascal and forthcoming Volta architectures. See chart above.\nThe OptiX SDK will come with a fully trained neural network. Nvidia says it will work really well in some instances, based on its training set, which includes scenes with products and cars, etc.\nHowever, it acknowledges that it won\u2019t work perfectly for every scene. In the future, it may give developers or users the ability to use their own data and repurpose the trained network through the process of \u2018transfer learning\u2019. Nvidia is also applying AI to other areas of graphics, including AI for anti-aliasing, to smooth the jagged edges or stepped effect on lines.\nThe company has also announced a new personal deep learning supercomputer, the Nvidia DGX Station. The water cooled deskside system is \u2018whisper quiet\u2019 and features 4 x Tesla V100 GPUs (16GB) and 256GB system memory.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/beyond-3d\/","title":"Beyond 3D","date":1193011200000,"text":"Allies and Morrison Architects Stephen Griffin and Paul Eaton have evaluated Gehry Technologies Digital Project 3D BIM software on several recent design projects. The results so far experienced have been better design productivity, enhanced creativity and the promise of other benefits further down the line, writes Nick Lerner.\nAllies and Morrison is an architectural practice that has experienced tremendous success developing solutions for a wide range of high profile clients. For three years the company has been running its projects in 3D. Initially the team reviewed the 3D solutions on offer in the AEC space in relation to their needs as a very busy architectural business where time is always limited.\nTheir investigations revealed that there are tools marketed to increase design productivity, capture creativity and control supply chains. Those three requirements are crucial to Allies and Morrison and needed to be reflected in the evaluation process. It was also considered that a system based on a Building Information Model (BIM) would bring the advantages and benefits of the latest technology to their practice. Following a thorough evaluation they selected Gehry Technologies Digital Project software (DP) supplied in the UK by Desktop Engineering.\n{mospagebreak}\nBenefits are considerable\nThe initial evaluation process applied DP methodology to two buildings in London, 120 Moorgate and a mixed housing and retail development at Elephant and Castle. The architects wanted to consolidate their hard won knowledge on real cases. There is a sometimes-painful learning curve before real benefit derives from using DP, but once the system and its associated methodology is fully grasped the benefits are considerable.\n\" Once the initial digital model is built it is possible to write parametric rules for each element meaning that both minor and major changes can be made quickly with all associated data being amended in real time. \"\nThe evaluation team found that the most satisfying aspects of using DP are the ability to iterate designs very quickly and generate reports automatically \u2013 these being updated as the design develops. Equally, the ability to change the design driven from MS excel spreadsheets is a real benefit. In practice, this allows the design team to alter gross areas and attribute relationships while automatically satisfying the reporting and design needs of developers, planners and other interested parties.\nThese software features are enabled through parametric algorithms that allow the coordinates of attributes to be maintained in relation to each other while designs iterate. This gives the advantage that features do not have to be re-designed to accommodate design changes because attributes effectively reposition themselves automatically.\nThe architects claim that the benefits of building a parametric 3D model are enormous, and the more the model is \u00d9rigged\u00dd with information the more benefits it delivers. Using DP enables simultaneous input from several disciplines and this is where the productivity gain starts because data on, for example, materials, planning, finance, manufacture and other factors can be added to the model before designs are complete.\nAreas, plans, sections, and reports on angles, light and associated rights and compensation amounts, as well as other types of report can be generated to provide all stakeholders with the information that they need in the format that best suits them. This may be visual information in the form of rendered views, 2D output from the model or the most recent mathematic calculations related to the project as a whole.\n{mospagebreak}\nHigher ground\nThe team claims that standard 3D architectural CAD software takes architects to a plateau where it is difficult to become more productive. But using DP and learning new skills it has become possible to introduce more efficient methodologies. Once the initial digital model is built it is possible to write parametric rules for each element meaning that both minor and major changes can be made quickly with all associated data being amended in real time. Further, reporting procedures traditionally slow down the design process; DP\u00dds automatic report generators speeds it up again.\nThe company that supplied and installed DP to Allies and Morrison, Desktop Engineering, also provides training, support and an interface to Gehry Technologies, part of Frank Gehry\u00dds practice. Managing Director Geoff Haines said, \u00fdDP is based on the same core design to manufacture software, Dassault Systemes Catia, used by car and plane manufacturers including Boeing and Toyota. These companies use the most advanced design-to-manufacture software available, which Gehry Technologies continuously develops making it suitable for the AEC industry. This brings the proven benefits of large-scale design and manufacture software to architects, at an affordable price. The Building Information Model, BIM, is a complete set of data that includes 3D design and manufacturing information as well as associated rules, methods and knowledge that govern all aspects of a building or development.\u00af\nThe team at Allies and Morrison found that design intent is captured in DP as a series of rules that the software maintains and applies throughout the design process. This means that conceptual frameworks stay intact throughout the many changes that the design is subjected to, and that all the while, development targets are retained and optimised.\nNew places to go\nThe 3D evaluation team also discovered that the design productivity that DP produces lead to greater flexibility and increased creativity because the software opens options that are not available from other less productive methodologies. It effectively provides new places to go, for example, the architects were able to run 22 fa\u00edade iterations on 120 Moorgate \u2013 in one day. Major changes were made with no loss of design intent and with full reportability throughout. This gave the team tremendous confidence to explore and improvise.\nThe introduction of DP in to Allies and Morrison\u00dds methodology has enabled the architects to respond better to client and stakeholder needs and react to briefs with increased confidence and creativity; an example being that it has become possible to generate viable optimised and fully reported designs that it would be impossible to draw in anything other than DP. In other cases it has become possible to run cost versus aesthetic comparisons quickly and make better and more accurate decisions as a result.\nWhile these benefits are accruing through the use of the parametric 3D digital model, others will be seen further down the line. DP has enhanced the team\u00dds ability to communicate project status in terms that recipients need and can relate to. The same communication enhancement is available using data needed for manufacture with Numerically Controlled Machines or for generating file formats compliant with sub-contractors\u00dd systems and other CAD programmes. Having seen the efficiencies that DP brings to the design and planning process the architects are looking forward to applying the manufacturing capability that DP carries over from the full Dassault Systemes Catia V5 PLM suite on which it is based.\nwww.dte.co.uk\nwww.alliesandmorrison.co.uk","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-revicad-brands-to-rse-technologies\/","title":"NEWS: ReviCAD brands to RSE Technologies","date":1511481600000,"text":"Company takes on new staff to help support expanded range of digital design services\nReviCAD has rebranded to RSE Technologies to better reflect the full scope of the services it offers and its synergy with sister company, RSE Building Services. The company now offers a number of digital design services including point cloud scanning, BIM \/ design Consultancy, 3D Revit coordination and robotic laser setting out.\nAs part of its rebrand, RSE Technologies has also expanded its team, recently adding Stephen Fifield, Operations Director, Jack Dearlove, BIM Strategy Manager and Liam Harris, 3D MEP Co-ordination Manager.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nGamma AR optimises BIM model placement on site\nNXT BLD London 2020 rescheduled to 8 October 2020\nQuadro RTX at heart of new ZBook mobile workstations\nBricsys unleashes V20 BricsCAD design suite\nMintronics to offer 4M\u2019s CAD and BIM tools in the UK\nChaos AI Enhancer to boost visual quality in Enscape\nDalux adds collaboration tools to free BIM Viewer\nOasys Mail Manager\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/bim\/preview-revizto-5-0\/","title":"Preview: Revizto 5.0","date":1599696000000,"text":"Greg Corke takes a sneak peek at the forthcoming release of the easy to use collaboration tool which now puts BIM object data at the heart of the issue tracking process\nIn a crowd of AEC collaboration software, Revizto stands out for its sharp focus on issue tracking. The software combines 2D and 3D models for a \u2018single source of truth\u2019 presented through a simple user interface that can be accessed by anybody on the project team. With a cloud or local server-centric workflow, data is pushed out to a variety of devices, including desktop (Windows and MacOS) and tablet (iOS and Android) so issues can be resolved in the office and on site.\nWhen Revizto started out, most of its customers were on the design side, but in the last few years there\u2019s been a big uptake in construction. Customers now include Atkins, AECOM, Arup, Balfour Beatty, Foster & Partners, Jacobs, Lendlease, Skanska, Vinci, Willmott Dixon and many more. This is reflected in how the product is marketed. It used to be \u2018BIM collaboration\u2019 or \u2018BIM co-ordination\u2019 but it\u2019s now being pitched as an integrated collaboration platform.\nWhile the name might suggest a loose association with Revit, the software actually works with a huge range of CAD\/ BIM tools, focused on buildings and infrastructure. There are plug-ins for Revit, ArchiCAD, Tekla Structures, Navisworks, SketchUp, AutoCAD Civil 3D, Vectorworks and MicroStation (including OpenRoads Designer and OpenBuilding Designer). The software can also read in IFC, OBJ, FBX, PDF, point cloud (RCP and RCS) and BCF files.\nWith its ability to bring in data from a variety of sources, Revizto has become a bit of a hub for BIM model co-ordination. Teams identify issues on site, or within the context of the 3D model or 2D drawing, assign responsibility to individuals, add deadlines and priority, then track each issue until it\u2019s been resolved, with full accountability.\nThis process has worked well, not least because of its simplicity. However, the software has had its limitations because it has placed too much emphasis on the model itself, and not enough on the underlying BIM data. Object data has always been there, but you could only really look at it, and it was very hard to use it to your advantage. Now with Revizto 5.0, currently in beta and due to launch soon, this is set to change.\nThe new engine\nFrom the outside, Revizto 5.0 doesn\u2019t look that different to Revizto 4. The interface still has the same minimal look and feel, albeit with a few tweaks, but it\u2019s under the hood that\u2019s there been a massive amount of development work, which gives users much more control over the data, down to a granular level.\nThis all starts with the intuitive object tree, which allows you to see the models that have been brought into Revizto, break them down by level or category, and then isolate and control them as required.\nThe new search bar now makes the software tick, making it much quicker and easier to find components, and then, more importantly, visualise them and act on them accordingly. In Revizto 4, if you wanted to find a specific component, you had to use generic terms like door or pipe which would return extremely broad results. In Revizto 5 you can go down to a much more granular level, and search on all types of object data. This could be anything from a door of a particular type or material, a tank of a specific volume in a specific system, or a steel section of a certain size or grade.\nOnce found, objects can be highlighted in the model at the click of a button, either by making the rest of the model transparent or switching it off entirely. Alternatively, selected objects can be completely hidden from view. This was just about possible to do in Revizto 4, but the process was far from straightforward. First you had to hide everything, and then start unhiding the objects that you actually wanted.\nA common issue with a lot of BIM-centric software is you click on a component only to find you\u2019ve selected the wrong one. In MEP projects, for example, this could be pipe insulation, instead of the actual pipe.\nThe new search bar can really help here. Simply search on \u2018insulation\u2019 to select every instance of insulation within the project, and then hide all of them from view, making it easy to select the actual pipe you want.\nObjects can also be used to help find similar objects within the model. For example, click on a column, then right click on its section size to automatically find similar objects based on that specific property.\nBy using search sets, it\u2019s now much easier to search for and group components based on certain criteria or system classification. You don\u2019t have to get bogged down with scrolling through vast lists. Instead of having to first choose from a category list, followed by a system type, system classification and so on, you can just type in the classification you want, and Revizto will automatically display which group(s) it comes from. It essentially means you can reclassify the order in which objects are grouped.\nFor example, simply by typing \u2018system\u2019 and clicking the relevant fields, you can now quickly pull out all the components in the model, or those on a specific level, whose system classification equals, for example, either domestic cold water or domestic hot water.\nOf course, in large projects this could still bring up thousands of objects, so if you want to search in a particular room, level or building, you can also narrow things down by creating a 3D section box that focuses on a specific volume inside the model.\nSearch sets can be saved, so common searches don\u2019t have to be redone from scratch. They can also be shared with the team.\nThe results of any search, complete with object data, can also be exported to Microsoft Excel, which could be used to define basic quantities. For example, to quickly tot up the total length of a specific type of steel column. Unfortunately, it\u2019s not yet possible to do this within the software.\nAs you\u2019d expect, when importing BIM models, Revizto automatically brings over all of the object data. But, of course, not all of that information is relevant, so you can now set up a list of favourites. Once an object is selected, all of the most important information appears at the top of the list in the object tree.\nThis focused data can even be seen when hovering the cursor over an object in the model, giving you much quicker access to object data without having to scan the property panel. A construction manager, for example, could set up favourites to show the volumes or grades of concrete, and have different favourites for different workflows.\nCustom data\nIt\u2019s now possible to add custom data to objects within Revizto and then, of course, use that data to create custom searches. It means Revizto can be used to track a lot more information than it previously could, opening up many new use cases.\nOn a construction site, for example, it could be used to track whether or not something has been installed or inspected, and on which date. In theory, this could even extend to phasing information, allowing Revizto to be used as a rudimentary 4D tool.\nIn the design phase, custom properties could help confirm that an engineer has checked and approved a section size change.\nAll of this data can be thematically displayed in the model. It could be as simple as green equals yes, red equals no, or something more nuanced.\nAt the moment, custom data can only be viewed inside Revizto, but longer term, the development team is exploring the possibility of feeding that data back into the original BIM model, which would close the loop.\nPerformance\nRevizto is multi-platform and runs on Windows, MacOS, iOS and Android, on laptop, desktop or tablet. The performance and specs of these machines can vary dramatically and when you start working with multiple 3D models, it\u2019s very easy to run out of memory, and for the system to grind to a halt. This is particularly true for those using Revizto on tablets which tend to have limited memory.\nIn Revizto 4, while it was possible to \u2018hide\u2019 models, which would reduce the load on the CPU and GPU, the model would still be held in memory. With Revizto 5 individual models can now be completely unloaded from memory, so, if you\u2019ve got models that you don\u2019t really need to be turned on all the time \u2014 MEP, architecture or specific buildings, for example \u2014 then simply switch them off.\nIt\u2019s not just through model management that Revizto 5 can free up memory. The new release also uses significantly less memory for tracking issues. According to the developers, version 4 could easily use up 1.5GB for 7,000 issues, which is pretty standard on a project. For the larger projects, where you might have 30,000 issues, this could swell to 6GB.\nIn the new version this memory footprint has been significantly reduced, with 7,000 issues only using 100MB, which of course, will free up space for larger models.\nModel control\nRevizto is all about bringing in models from different sources, and it\u2019s now a lot easier to keep track of what\u2019s inside your federated model. Improvements to the \u2018Scenes and Scheduler\u2019 mean you can now see where the models have come from, who exported them and when they were exported.\nAs data often comes from multiple sources, file names are not necessarily consistent, might not be understood by everyone, and can sometimes be quite bizarre. As a result, it\u2019s now possible to give files an alias to make it easier for the user to recognise data, while still keeping the original file name to maintain the connection to the original source file.\nSpatial awareness\nIn Revizto 4, if a model was brought in and the coordinate system was wrong, you\u2019d have to go back into the original authoring software and correct it. This not only took time but could be particularly challenging if the model came from a third party. With the new version, it\u2019s now possible to change the location of a model within Revizto itself. This also extends to 2D\/3D overlays, so if you bring in the 2D drawings from your BIM software, they will automatically be transformed as well.\nRaising issues\nThe fundamental part of Revizto is issue tracking and the new release refines how issues are raised.\nIssues are now directly attached to an object or objects, whereas previously they were just placed in the general vicinity. This could lead to ambiguity when trying to resolve specific problems.\nIt\u2019s also possible to apply a single issue to multiple components but give each component its own stamp so you can track them individually. For example, you may want to check that a group of columns, selected through search criteria, have been installed correctly. Previously you\u2019d have to create one issue that covered all of the columns. This not only made it harder to track, but it meant the issue could only be closed when all of the columns had been checked.\nPoint clouds\nRevizto is no stranger to point clouds, which have been supported in the software for some time. There\u2019s nothing particularly groundbreaking in Revizto 5.0 \u2014 the team has simply fixed an issue where ghost points might be left behind when creating sections. However, there is some useful new functionality coming soon, which will allow users to measure point clouds. For example, you might want to quickly measure between a pipe and a wall to check there\u2019s enough room for maintenance.\nConclusion\nIt feels like version 5.0 is a pivotal release for Revizto. It might not come with headline grabbing features but don\u2019t underestimate the importance of giving users more control over the data. Rather than having to rely on the model to drive the issue tracking process, users can let the data do the work for them.\nThe new focus on data is also allowing Revizto to spread its wings. It opens up new possibilities for project management, the potential to feed in data from other sources, and it even lays the foundations for built-in clash detection, for which Revizto currently relies on Navisworks or Solibri. However, as Revizto\u2019s Rhys Lewis told us in 2018, adding clash detection to Revizto would be more than just a technical challenge \u2014 there would be business and relationship obstacles as well.\nAll of this points to an exciting future for a product that starts at \u00a3420 per user per year. However, the developers will need to be careful to maintain the good balance between capabilities and ease of use. After all, this is what made the product so appealing to AEC project teams in the first place.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/holobuilder-using-ai-to-analyse-360-images-from-construction-sites\/","title":"HoloBuilder using AI to analyse 360\u00b0 images from construction sites","date":1543536000000,"text":"SiteAI technology designed to automate construction processes through the analysis of materials, objects and structures in 360\u00b0 imagery data.\nHoloBuilder is adding an Artificial Intelligence solution \u201cSiteAI\u201d to its 360\u00b0 construction site reality capture solution. The new technology utilises Computer Vision and Deep Learning to analyze materials, objects and structures captured in 360\u00b0 imagery with a view to automating construction processes, especially progress tracking.\nAccording to the developers, automated progress control calculates progress reports without additional effort from capturing weekly 360\u00b0 progress photos. The progress can be automatically tracked, analysed and compared to the planned schedule to detect discrepancies as fast as possible. HoloBuilder claims that when SiteAI is implemented across projects, it allows processing progress payments faster, knowing the quantity for specific materials, comparing construction progress to schedules and much more.\nSiteAI has been developed in close collaboration with Hensel Phelps, who has captured tens of thousands of 360\u00b0 images to actively pilot HoloBuilder\u2019s Computer Vision algorithm to automatically analyze the work put in place by location.\n\u201cThe partnership that Hensel Phelps has created with Holobuilder is built around their understanding of our jobsite photo documentation process and how their solution brought immediate improvements and efficiencies to our people,\u201d said Will Plato, VDC manager of the Southwest District at Hensel Phelps. \u201cTheir capability to do more with data through the development of SiteAI brings additional value and analytics to our construction scheduling and execution. I am excited about the future as the possibility of bringing data silos to an end is truly going to be obtainable.\u201d\nSiteAI is currently in an early-access program stage for selected customers with the company asking more customers to join the waitlist and will be accessible to HoloBuilder users in early 2019.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/to-the-fourth-dimension-and-beyond\/","title":"To the 4th dimension \u2013 and beyond?","date":1516838400000,"text":"What might the addition of new information types to BIM models help us achieve in terms of construction project efficiency and future building maintenance, asks Rob Charlton, CEO of AEC technology company Space Group\nAs more people in the industry familiarise themselves with 4D simulation and know how to use it, the more popular it becomes for Level 2 BIM projects.\nThe benefit of using 4D is that it adds the time dimension to an information model. That, in turn, enables us to sequence work activities graphically and simulate this in visual formats across the project lifecycle.\nIn the past, the connections and integration between a 3D information model and a construction schedule were minimal. Now, with the introduction of BIM and the integration of information within the BIM model, we can \u2018virtually\u2019 construct, rehearse and test a project before any \u2018real\u2019 construction work begins, with considerable potential to save on costs. And, with the added bonus of 4D, we can add scheduling to the model, further enhancing our view of an asset in all its as-built glory, way ahead of a project\u2019s actual start date.\nHowever, the sequencing of work activities doesn\u2019t have to be restricted to just the sequence of construction. It can also be used for logistics and safety planning, taking into consideration site deliveries, pedestrian and vehicle movements, positioning of equipment and much more. In this way, 4D helps everyone involved in a project to understand not just the actual build, but also all other activities, on and off site. It\u2019s also a big help in comparing schedules against progress, helping to highlight activities that are running late or ahead of schedule.\nWhere 4D and IoT collide\nThe concept of introducing timings and scheduling to a BIM model is a simple one and quite possibly the most logical step in the integration of technology in the design and construct phases of a project. So what comes next?\nThe Internet of Things (IoT), with its use of sensors to connect devices and facilitate the exchange of data, is an excellent example of how 4D might slot into the BIM environment. Just as the IoT ecosystem gives businesses and consumers access to data dashboards and analytics via their devices, 4D can enable a project team to map timings onto activities and view it all in one place.\nBy 2020, there could be more than 24 billion IoT devices on the planet, according to some analyst estimates. That equates to approximately four devices for every human being on Earth and demonstrates the accelerated speed of growth in technology. With that in mind, why should 4D be the end, when 5D could be just around the corner?\nIn fact, 5D is already here. All it really means is the addition of cost information to a model \u2014 another no-brainer. After all, isn\u2019t it the next logical step, once a design is in place along with all the data behind its constituent parts and the timeline as well, that we should start to add in budget data?\nHowever, where I think we should be heading is in the direction of real-time data for predictions, forecasting and monitoring. Typically we work with static, historical data and information. But information tends to change over time. Take, for example, the cost of steel: prices change due to currency fluctuations, supply chain issues, even factors such as natural disasters. With predictive analytics, we can start to predict steel prices based on trends and patterns.\nRob Charlton is CEO of Space Group, the AEC technology company that includes Space Architects, BIM Technologies, bimstore, CAMPUS and Volula. An architect by training, he is passionate about improving the value and performance of buildings using technology.\nLikewise, in Formula 1 racing, predictive analytics based on data gathered by monitoring engine heat or vibration during a race can be used to alert the pit crew if that engine is heading for a failure, prompting them to radio through to the driver to come into the pit for further investigations.\nThe cost of the sensors needed to collect these kinds of information has fallen dramatically over recent years, as has their physical size. It\u2019s possible to embed sensors into a construction\u2019s fabric to monitor the stability of structures such bridges or monitor the curing speed of concrete in a building. Smart ways of thinking \u2014 as seen in the \u2018smart building\u2019 and \u2018smart city\u2019 concepts \u2014 are only the tip of a very large iceberg as far as the AEC industry is concerned. Using real-time data can introduce many new ways of understanding assets and how they change and develop over time.\nA model offers an easily accessible way of understanding the design of the asset and its information. Details that would once have been hidden in non-digital file formats are now easily interrogated graphically. This allow us to proactively anticipate events in the asset\u2019s lifecycle and preempt maintenance and repairs, for lower long-term costs. If we then add in more data on how the asset behaves, we can start to predict patterns and learn lessons that might be applied to new construction projects, based on what did (and didn\u2019t) work well in the past.\nData is an exceptionally powerful tool and understanding how to use it is the way forward \u2014 no matter which dimension we occupy.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-3d-printed-steel-bridge-to-span-amsterdam-canal\/","title":"NEWS: 3D printed steel bridge to span Amsterdam canal","date":1435622400000,"text":"Dutch startup MX3D to use additive metal welding technology and industrial robots to build bridge in situ.\nAdditive manufacturing specialist MX3D is to 3D print a life sized steel bridge over a canal in the centre of Amsterdam. The Dutch startup will use a pair of 6-axis industrial robots kitted out with MX3D\u2019s bespoke 3D printing technology to build the fully functional, intricate metal bridge.\nMX3D\u2019s 3D printing technology is essentially a customised welding machine. By adding small amounts of molten metal at a time, the company is able to \u2018print\u2019 lines in mid air. In house software is used control the complex build process.\nThe project is a collaboration between MX3D, Autodesk, construction company Heijmans and many others. The bridge will be designed by Joris Laarman and is set for completion in 2017.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/generative-components\/","title":"Generative Components","date":1106092800000,"text":"Bentley\u2019s long-term development of parametric modelling tools has finally reached the real world. The Smart Geometry Group continues to evangelise Bentley\u2019s unique approach to form design.\nArchitectural design software comes in two flavours; 2D drawing tools with symbol libraries and intelligent 3D\/2D tools that offer component-level design (walls, doors, windows). In the former you have all the flexibility and freedom of 2D representation, with the advantage of \u2018off the shelf\u2019 2D libraries. The latter creates a 3D model out of components that know how to interact with one another and 2D sections and elevations are derived from this 3D model. The main downside of these modelling tools being that the components are fabricated by the CAD software vendors and limits the capability \u2013 you can\u2019t add in a structural element if there are no structural elements in the library to add.\nThe majority of AEC professionals will find that these two types of product will cater to most design tasks, although with 2D tools, changes and edits could impact hundred of drawings, which have to be done manually. The intelligent 3D solutions overcome this by using parametric and relationship modelling but these concentrate on producing documentation and usually fail to model buildings with complex non-standard geometry.\nThere are new tools emerging in the AEC market to address these complex geometric issues, together with offering design exploration capabilities. Gehry Technologies is offering Digital Project, based on Catia (a high-end, parametric, automotive and aerospace modeller) and Bentley Systems is becoming increasingly vocal about its Generative Components technology that is built on top of MicroStation. At the last few AEC events I\u2019ve attended, Catia and Generative Components are becoming topics for discussion as industry people wonder what their impact will be.\nSmart Geometry\nOne of the key groups leading the promotion of the new technologies is the Smart Geometry Group, founded by four key industry experts; Robert Aish (Bentley Systems), Hugh Whitehead (Foster and Partners), Lars Hesselgren (KPF) and Jay Parish (Arup Sport). The charity has held a number of seminars around the world, targeting the brightest young minds in and out of University\/Practice to share their industry and programming experience and knowledge with the next generation. As the Group is sponsored by Bentley Systems, it is true that the key technology on display is Bentley\u2019s Generative Components, developed by Robert Aish, Bentley\u2019s Director of Research. However, you are just as likely to see other presentations, which include projects designed using Catia, Maya, AutoCAD or Rhino. The key aim of the Group is to educate the next generation of designers by introducing them to design exploration tools which need new approaches and higher degrees of geometric and programmatic skill.\nThe most recent Smart Geometry event was held in January at the Architectural Association School of Architecture, under the heading \u2018Towards a New Paradigm of Architectural Design\u2019. Aish, Hesselgren, Whitehead and Parish, each gave a presentation on the work of students to date, together with an insight in how their practices use computers and advanced software to tackle design problems. Parish talked about wanting to be able to design a Stadium, in detail, in a day, while Whitehead worked through a few Foster and Partners projects explaining how their parametric approach to modelling allows design exploration and experimentation.\nIt looks as if Bentley\u2019s Generative Component technology has matured, while still in development. Hesselgren admitted that KPF was actually using it on an important live project, although he was not at liberty to say which. The work of the students was highlighted and easily demonstrated the incredible capabilities of taking a programmatic approach to modelling.\nGenerative Components is not about walls, doors windows, the intelligence is not at that level. As the name of the Group suggests, it\u2019s about Smart Geometry \u2013 the tool provides an environment in which geometry (lines, arcs, circles, solids, surfaces) can be related, transformed, generated and manipulated within a user-defined framework. While that might sound complicated, the end results are certainly worth seeing, complex, sculpted geometry can be quickly generated and manipulated in real-time, allowing design exploration and variation. In real-terms that could provide a practice with the ability to come up with 20 or 30 designs simply by moving sliders within a Generative Component model.\nConclusion\nThe scale and complexity of the student work is certainly getting more impressive with each event. The news that Generative Components are being used on a live project is also extremely exciting. However, to understand the technology and break down the problem into a program requires a new outlook to problem solving, and this is perhaps why the Smart Geometry Group is concentrating on new minds, as well as those special industry folks who have been struggling to find this kind of tool for years.\nI think the images speak for themselves. These complex forms were all automatically generated using standard geometry, defined by the users, allowing further interaction, manipulation and refinement.\nwww.smartgeometry.org\nwww.bentley.co.uk","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/workstations\/review-lenovo-thinkstation-p320-tiny-2\/","title":"REVIEW: Lenovo ThinkStation P320 Tiny","date":1506297600000,"text":"In a world of shrinking workstations, Lenovo\u2019s ThinkStation P320 Tiny is the smallest yet. But despite its diminutive form, it still delivers the goods for mainstream 3D CAD and BIM workflows, writes Greg Corke.\nLast year, HP redefined the Small Form Factor (SFF) workstation with the introduction of the HP Z2 Mini. With a 216mm x 216mm x 58mm chassis, this machine was dramatically smaller than anything that had come before, but still managed to pack a punch for 3D CAD.\nNow, Lenovo has gone one step further with the ThinkStation P320 Tiny, a new 3D CAD-focused workstation that is even smaller than the HP Z2 Mini. Incredibly, it takes up half the volume and is only 180mm x 183mm x 36mm in size, weighing a mere 1.3kg. And what\u2019s more, despite this significant shrinkage, it still stands shoulder- to-shoulder with the HP Z2 Mini when it comes to performance. In some graphics-centric workflows, it\u2019s actually faster.\nUnlike the HP Z2 Mini, which was built from the ground up to be a workstation, the ThinkStation P320 Tiny has been adapted from Lenovo\u2019s business PC, the ThinkCentre Tiny.\nAs the chassis is so small, the P320 Tiny keeps thermals in check by offering a choice of low-power Intel \u2018Kaby Lake\u2019 CPUs.\nOur test machine came with the top-end model, the Intel Core i7-7700T (4 cores, 8 Threads \/ 2.90 GHz to 3.80 GHz Turbo), but there are other options, including the Core i5-7500T (4 cores, 4 threads, 2.70 GHz to 3.30 GHz Turbo). However, the entry-level Core i5 should only really be considered if budgets are exceedingly tight or you\u2019re not interested in ray trace rendering (it does not support HyperThreading).\nThe Intel Core i7-7700T draws 35W at peak, instead of 70W, which is typical of most high-GHz quad core desktop CPUs. With significantly less power to play with, the stock and Turbo frequencies are lower, but this doesn\u2019t impact performance as much as one might expect (more on this later).\nFor graphics, Lenovo has amazingly managed to pack in an entry-level professional desktop GPU, the Nvidia Quadro P600 (2GB). This low-profile \u2018Pascal\u2019 graphics card is designed for traditional Small Form Factor (SFF) workstations, so to squeeze it into the P320 Tiny\u2019s micro chassis has taken a significant engineering effort.\nFirst, the graphics card is fitted to a riser board, so it sits parallel to the motherboard. Second, Lenovo has removed the card\u2019s standard cooler and has instead attached a custom heatsink that connects via copper piping to a single system fan, which is shared with the CPU.\nThe machine features up to 32GB of DDR4 2400MHz SoDIMM memory, which should be plenty for most 3D CAD workflows. Meanwhile, storage is provided by one or two M.2 NVMe SSDs, up to 1TB in capacity.\nOne sacrifice of having such a small chassis is that there is no room for a 2.5-inch Hard Disk Drive (HDD), so those who want lots of storage for giant CAD datasets will have to pay a premium for high-capacity SSDs. However, Lenovo has placed significant efforts into maintaining serviceability, a hallmark of its larger ThinkStation models.\nThe machine offers tool-free access to two service panels, one on top and one underneath.\nRemove a thumbscrew to slide off the top panel and gain access to the CPU and GPU for maintenance and cleaning, while memory and SSDs can be easily accessed from below.\nThe workstation can be positioned in desktop or tower mode (with the help of a stand).\nIt can also be secured under a desk or behind a VESA display with custom mounting brackets. To keep everything tucked away, there\u2019s even a bracket for the external power supply. This is one of the benefits of using the same chassis as the ThinkCentre Tiny, as there are a number of ready-made accessories.\nIf you want to go the whole hog and essentially turn the P320 Tiny into an all-in-one workstation, then there\u2019s also the ThinkCentre Tiny-In-One 23.8\u201d monitor. And with WiFi built-in, you don\u2019t even need to have a trailing Ethernet cable.\nOn test\nWhen we first started testing the P320 Tiny with a variety of CAD and viz applications, the clock speed of the CPU jumped about quite a lot, cycling from 3.70 GHz all the way down to 0.8 GHz at times.\nDespite this huge variation, we found 3D performance in SolidWorks, Creo and Revit to be reliable and good. However, in Luxion KeyShot, render times were inconsistent, with the same scene sometimes taking twice as long to render than at other times.\nAt the start of September 2017, Lenovo released a firmware update that allowed the machine to be optimised for thermal performance, instead of focusing on acoustics. This simple BIOS setting made a massive difference to our test results \u2013 not only cutting render times dramatically, but also boosting 3D performance in SolidWorks and Creo (as graphics performance in these applications is heavily influenced by the speed of the CPU).\nAs one might expect, fan noise increased significantly, particularly when all four cores were being hammered in KeyShot, but we didn\u2019t find it too distracting on the whole. The most surprising observation was that all four CPU cores appeared to run at 3.50 GHz to 3.60 GHz when rendering, even though we had expected them to slow down to 2.90 GHz, the standard clock speed of the Intel Core i7-7700T).\nThe end result was that our test scene only took marginally longer to render than it did on the HP Z2 Mini, despite the HP Z2 Mini having the more powerful 73W Intel Xeon E3-1245 v6 CPU (3.70 GHz to 4.10 GHz Turbo).\nContinuing the comparisons with HP\u2019s machine, the ThinkStation P320 Tiny actually had the edge in all of our 3D graphics benchmarks. This is thanks to the new Pascal-based Nvidia Quadro P600 GPU, which is more powerful than the Z2 Mini\u2019s Maxwell-based Quadro M620.\nDespite the small 3D performance advantage, the P320 Tiny is still very much a workstation for entry-level to mainstream 3D CAD and BIM. It\u2019s not really geared up for game engine visualisation or GPU rendering and definitely not for Virtual Reality. In addition, it\u2019s important to note that all of our testing was done at FHD resolution (1,920 x 1,080) and, as we found with the Z2 Mini, there will almost certainly be a slowdown at 4K resolution in GPU-hungry applications.\nConclusion\nConsidering the thermal challenges of a small chassis, it\u2019s incredible how much performance Lenovo has managed to pack into the P320 Tiny. With its low-power Intel CPU, we had expected it to significantly lag behind the HP Z2 Mini in multithreaded rendering workflows but, with the latest firmware updates, this was not the case. And, thanks to the Nvidia Quadro P600 GPU, it even has the edge when it comes to 3D performance.\nWe\u2019re big fans of the P320 Tiny. It\u2019s small, deceptively fast, incredibly portable and excellent value for money (our test machine comes in at \u00a3999). But there are downsides \u2013 no 2.5-inch drive, so bigger cost per GB, and it can be quite noisy under heavy loads. However, users do have the choice of prioritising acoustics over performance in the updated BIOS.\nThere are obvious benefits to the space-saving chassis but, as with the HP Z2 Mini, you could get stuck if workflows change. There\u2019s no scope for upgrades to handle VR, GPU rendering or an increase in model complexity, so before parting with your hard-earned cash, you need to be sure that mainstream 3D CAD and BIM will continue to be your bread and butter for years to come.\nGreg Corke\nSpecifications\n\u25a0 Intel Core i7 7700T CPU (2.9GHz, 3.8GHz Turbo) (4 Cores) CPU\n\u25a0 16GB (1 x 16GB) DDR4-2400 memory\n\u25a0 1 x 512GB M.2 NVMe SSD\n\u25a0 Nvidia Quadro P600 GPU (2GB)\n\u25a0 Microsoft Windows 10 Pro 64-Bit\n\u25a0 180mm x 183mm x 36mm (w) x (d) x (h)\n\u25a0 3 Year On-site warranty\n\u25a0 \u00a3999 + VAT\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/boxx-3dboxx-8550-xtreme\/","title":"BOXX 3DBOXX 8550 Xtreme","date":1297987200000,"text":"Probably the fastest dual Xeon workstation on the planet, but it is a serious investment.\nBOXX built its reputation developing high-performance hardware for professional visualisation and fills a somewhat unique role in the workstation marketplace. Its workstations are certified by the likes of Autodesk \u2014 a hallmark of a tier one vendor like HP or Dell \u2014 but it retains a custom-built ethos, typical of smaller boutique system builders.\nHeadquartered in Texas, BOXX used to ship its machines over to the UK. However, a new manufacturing deal signed last year with Boston Ltd means BOXX workstations can now be built in the UK. This should ease the delivery problems experienced by some customers over the past few years.\nThe BOXX 8550 Xtreme is an entirely different beast to the other workstations reviewed in this issue. This dual processor machine is targeted at design visualisation specialists.\nOur test machine featured two Intel Xeon X5680 processors, but instead of the 3.33GHz clock speed they usually run at, both chips have been overclocked to an incredible 4.2GHz. Naturally, this creates a lot of heat and a sophisticated liquid cooled sub-system is employed to keep them running within their thermal limits.\nHaving two Xeon chips run at such speeds is an exceptionally fast workstation. It recorded the fastest ever time our 3ds Max benchmark, using its 12 physical and 12 virtual HyperThreading cores to full effect, rendering our test scene in 96 seconds.\nThe BOXX 8550 Xtreme is also well equipped in the graphics department. The 2GB Quadro 4000, one of Nvidia\u2019s high-end Fermi graphics cards, recorded impressive results in our 3D CAD graphics test.\nStorage is a little underwhelming for a machine of this caliber, comprising one 160GB 2.5\u201d 7,200RPM SATA hard drive for operating system and applications and a 500GB drive for storage. However, optional Solid State Drives (SSDs) are also available in sizes of 60GB \u2014 256GB. All drives are tucked away behind the motherboard, one of the many unique design features that help make the machine particularly compact for a dual processor workstation.\nOverall, the BOXX 8550 Xtreme is an amazing piece of hardware. If you are serious about design visualisation it is an excellent choice to help transform productivity. We have yet to see a faster machine out there.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/enscape-2-6\/","title":"Enscape 2.6","date":1570579200000,"text":"Real-time rendering software developer Enscape has just completed a major overhaul of its flagship product and laid some groundwork for soon-to-be-enabled, ray tracing enhancements with Nvidia RTX\nOut of all the various software technologies that are deployed in the AEC market, visualisation undoubtedly moves at the quickest pace. It is driven by advances in the games industry, graphics hardware, and a plethora of VR and AR developments. Today with increased competition, the net result is regular big improvements to provide amazing on-demand high-fidelity images at incredible speeds, together with VR on demand for both making design decisions and selling projects to customers.\nWhen it first launched Enscape redefined ease of use with a simple \u2018one click\u2019 approach to generating photorealistic real time environments. It is one of the most popular viz-focused add-ons for Revit and SketchUp. More recently, it has added support for Rhinoceros and ArchiCAD.\nTypically, architects work on designs with their BIM tool of choice and, with one click, Enscape delivers high-quality real-time environments to explore for instant design feedback. The software can also create video walkthroughs and VR experiences in a single click. These can also be sent to clients as a self-extracting executable, without the need for an Enscape licence, or turned into a WebGL-based website.\nWith an aggressive upgrade cycle, this is the second release of the year. Its capabilities have continuously improved and expanded. The latest release does not disappoint.\nEnhanced lighting and reflections\nEnscape is different to most other real time renderers in that is uses elements of ray tracing. The results approach photorealism but are not as accurate as those from a dedicated ray trace renderer like V-Ray.\nFor the new release it has overhauled and enhanced the lighting and reflection algorithms. This delivers speed improvements and also improves the product\u2019s reflection fidelity, meaning geometry which is offscreen or obscured by other objects can reflect in high detail, within the scene.\nTexture maps are now more accurately displayed, as well as cutout materials, which now appear reflected in mirrors and other reflective surfaces, together with any corresponding shadow. Light sources are also better represented in reflections. Interior lighting has been tweaked to produce more natural results, with less light leakage and more pronounced indirect shadows.\nNew ArchiCAD Material Editor\nEnscape added support for ArchiCAD in June 2018 (2.3). This release brings a major improvement for users of the BIM tool, now supporting the Enscape Material editor, enabling access to, and editing of, the full spectrum of material parameters. New controls for bump maps, which were previously not supported in the native ArchiCAD Surface Editor and a roughness editing tools can help finish off a scene by making it looked lived in.\nAsset Library Expansion\nScenes need more than just high-quality materials and lighting to give a feeling of reality. It\u2019s important to be able to add assets quickly and easily to dress the scene. With Epic Games acquiring TwinMotion earlier this year, competition is heating up and Enscape has responded by dramatically adding to its asset library.\nWith this release come over 300 new models, including people, tropical plants, home accessories, bathroom assets, street signs, vehicles, food, vegetation, furniture and more.\nAdditionally, Revit users now have expanded placement options: on a surface, on an active work plane or linked to an existing Revit family. There are standalone files for each of the categories, so users can browse the asset groups before placing them in a project.\nEnhanced Settings In previous versions of Enscape, the scene settings were stored separately to the BIM model. This meant that when moving data from machine to machine, the Enscape Settings for the model did not match the original intent.\nIt\u2019s now possible to save the settings within the CAD file, so you can share your model with other Enscape users, who can repeat the same render and get the same results. It\u2019s a small feature but very useful for teams. It\u2019s also worth noting here that in settings there is now support for numerical input in addition to sliders.\nNvidia RTX\nNvidia\u2019s new GPU-accelerated physically-based RTX rendering technology looks set to revolutionise high resolution, real-time photorealism. Those of you that attended NXT BLD this June will have seen a number of talks demonstrating the potential benefits of RTX-assisted rendering on desktop and mobile workstations.\nWe were fortunate to have Enscape\u2019s co-founder Moritz Luck, showing an early \u2018work in progress\u2019 version of 2.6 on an RTX-enabled laptop, where Enscape smoothly rendered a very large and reflective building model in real-time, while the camera flew about the site. It was interesting to see the quality improvements delivered by RTX, specifically around reflection.\nHowever, before you get too excited, at the time of going to press, the RTX acceleration in 2.6 was disabled, due to some stability issues being sorted out by Nvidia. We have been told this will be enabled in an update soon.\nThere are already some enhancements to the existing ray tracing engine in 2.6, but RTX will take ray tracing in Enscape to new levels of quality through physically accurate reflections and diffuse, indirect lighting.\nThere is an array of other smaller features for this 2.6 release. It\u2019s possible to reduce the memory usage by down-sampling some of the larger textures, there have been improvements to all sun shadows. Oculus Rift S controllers are now supported and are visible during VR sessions. Enscape 2.6 is now available for download and comes with support for Revit 2020.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/the-future-of-construction\/","title":"The future of construction","date":1520208000000,"text":"New design software is needed for an age in which modular buildings are built for off-site fabrication and then assembled on-site, according to Autodesk\u2019s Jim Lynch\nTo many working in AEC, Autodesk may just be \u2018the company that makes AutoCAD.\u2019 For those involved in BIM and the 3D world, their familiarity might extend to Revit, Civil 3D and BIM 360. However, the company is becoming a leading player in the manufacturing space, too \u2013 not just in design tools, but in its more physical aspects. This may even extend to helping to cut metal.\nAutodesk spans a number of creative industries and its research and development activities have explored everything from biotech to aeronautics. With the overview this has given it, the company is in a strong position to see how digital design and digital manufacture are converging, in order to radically change the way we design and fabricate.\nMechanical engineering and plant design were the first industries to adopt 3D modelling and drive machines directly from design data \u2013 but it\u2019s taken another 20 years for the AEC industry to begin adopting 3D-based workflows. In that time, new modelling tools, new digital fabrication processes and new types of automation have become available and, according to Jim Lynch, vice president of Autodesk\u2019s Construction Products Group, AEC and manufacturing are now converging to change the way that buildings and infrastructure are designed, fabricated and assembled.\nTowards DFMA\nIn the next 20 years, there will be two billion more people on the planet, as Lynch himself pointed out to attendees at Autodesk University back in December. \u201cImagine the infrastructure and buildings required support this size of population,\u201d he said. \u201cWe have to ask ourselves, will the same old methods the best to meet this new demand?\u201d\nLynch cited as an example a project from his own home town of Boston, Massachusetts: the Big Dig, more formally known as the Central Artery\/Tunnel (CA\/T) Project. It was meant to take six years to compete and instead took over two decades. The cost overrun was in the region of 190%. \u201cWe need to consider new ways of designing, fabricating and constructing,\u201d he said. \u201cHere, progressive thinking, innovation and determination are critical.\u201d\nThe automotive and consumer products sectors, for example, must produce large amounts of high-quality products consistently and efficiently. The building industry should learn from that, according to Lynch \u2013 but for the convergence of manufacturing and construction to happen, there will need to be changes in the way components are designed, how they are assembled and how processes and data are managed.\nThese changes come together in the concept known as Design for Manufacture and Assembly, or DFMA. It focuses on improving throughput and efficiency in manufacturing and is already well-established in markets such as automotive.\nBut Autodesk believes it will work just as well in AEC, where mass customisation is often the name of the game. \u201cThink of hospitals, hotels, schools, data centres,\u201d said Lynch. \u201cToday, we just don\u2019t have a smart way of dealing with those repetitive structures in the design process nor in the construction process,\u201d he said.\nAutodesk\u2019s thinking, therefore, runs along these lines: in order to meet rising demand without sacrificing quality or harming the environment, the building design process must consider more modular and standardised approaches and the Autodesk software tools used in building design must therefore support DFMA, \u201cthrough codification, intelligence and logic\u201d, so that buildings can be optimised for offsite construction.\nIn effect, designers will have a greater impact on how buildings are made and constructed \u2013 but they will need to adopt new methodologies, along with other projects stakeholders, including engineers, fabricators and contractors.\nFuture design and fabrication\nAs an example of new methodologies in modular construction, Lynch pointed to a hospital designed by the Oslo-based practice, Nordic Office of Architecture. This has a high degree of repetition in terms of floors, rooms and components. \u201cImagine if the architect could have a \u2018smart assistant\u2019, to focus on the design with regard to the way the buildings are built and delivered \u2013 an analytical tool that is based on input and previous experience, which helps designers create a library of standard, yet configurable, modules,\u201d said Lynch.\n\u201cThe process would start with conceptual parametric placeholders that are simple in geometry but extremely rich in data. This would play out through various different design scenarios and would also take into account estimation and procurement,\u201d he explained. In later stages, the components might be further developed, to become more precise and detailed in terms of geometry and also supply chain data; but, Lynch insisted, \u201cThis doesn\u2019t mean that all modules must be the same. On the contrary, modules can be mirrored configured and adjusted.\u201d\nFabrication will change, too, with AEC factories producing multi-material, multidiscipline panels and modules. Machining processes will be automated, following \u2018recipes\u2019 and fabrication logic supplied directly from DFMA-empowered BIM data. Assembly instructions will be derived from these models, too, and manufacturing of modules will be sequenced in order to optimise factory efficiency while still meeting project deadlines.\nThere are numerous advantages to be had here, according to Lynch. \u201cCompared with working on-site, off-site construction is safer and more efficient. It\u2019s also easier to manage quality and ensure precision. Components created in these factories will have individual identifiers, such as QR codes, that allow for tracking and monitoring, from design to production to delivery, through installation.\u201d\nThis, Lynch reckons, might all be done through a collaborative cloud solution. He pointed to Autodesk partner Manufacton as a case in point \u2013 a software-as-a-service (SaaS) platform where construction firms plan, track and manage prefab elements across all supply chain members, including detailers, procurement, shop foremen and field superintendents.\nFactory-like building sites\nEventually, these modules must arrive on-site \u2013 but here too, Lynch imagines a far more factory-like environment than your typical building site, complete with assembly lines and robotic automation. At Swiss technical university ETH Zurich, for example, researchers can demonstrate bricklaying robots. Meanwhile, research into 3D printing of building structures or even whole buildings is coming along, as new materials such as concrete clay and polymers become available. And self-driving technologies are coming to heavy equipment, thanks to the work of companies such as Built Robotics.\nWhile some of Lynch\u2019s predictions may seem fantastical, it\u2019s fair to say that all the elements for this vision are in place \u2013 if not in the AEC space, then certainly in manufacturing.\nMoving to BIM and 3D modelling will be a step in the right direction for companies in the AEC space, because the digital model is the foundation of almost every downstream process, but it\u2019s already clear there will be far steeper hills to climb ahead.\nNot least of these are the \u2018soft\u2019 issues that need to be addressed in terms of design approaches. Modular and component- based design comes with constraints and the need for discipline. For some, they may cramp creative freedom.\nLikewise, demand must be there in order for setting up new factories to be a worthwhile exercise. The lumpy boom and bust cycles that plague real estate could make it hard for such factories to remain consistently busy and profitable.\nStill, despite the barriers, Lynch was keen to present DFMA as \u201cthe future of construction\u201d and a source of huge opportunity. He even went as far to issue a warning to those who don\u2019t embrace it, hinting at a future of irrelevance.\n\u201cJust think how robotics and automation have changed automotive production and the resulting improvements in the cars we drive today. Think about the automotive manufacturers that embrace the new technology, who are reaping success, and those that haven\u2019t, who are not.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/design-for-the-new-generation\/","title":"Design for the new generation","date":1580947200000,"text":"In 2015, Autodesk released Project Fractal to investigate generative design in AEC. It has now evolved into Project Refinery and Lawrence Hooker, technical consultant, Excitech explores the Beta\nGenerative design is, in essence, a collaborative effort between human beings and machines. The process takes perfect advantage of the merits of each contributor; computers excelling at working with enormous datasets and algorithms and humans being really good at selecting rational, realistic designs, and evolving these designs into a real project.\nWhen creating project schemes, the human designer only has time to generate a few design alternatives. Typically this is a very time-consuming process and hence expensive. The ideal design may be missed or, not even explored due to time and cost restrictions. This is a perfect application of generative design. The human can then select the best designs and refine these further if required.\nThe process starts with the designer defining a number of inputs that are important to a particular design. For example, if we are designing a high-rise conceptual building, we may want to set a series of inputs such as total required floor area, maximum height, the maximum footprint, fa\u00e7ade area, and so on. The generative design software can then use these inputs and iteratively generate numerous outputs using variations of each input. The software can potentially generate hundreds or thousands of design alternatives. Many of these will be inappropriate but some will likely fall within the parameters of a good design.\nThe key part of the process is ranking the large amount of results into a few possibilities that the designer may want to evaluate further. In order for the software to rank the designs, the designer will need to define a series of ideal outcomes. Using the example of the high-rise building design, the outputs may contain parameters such as cost of construction, height to width ratio, core to floor ratio and fa\u00e7ade area. This is achieved with the use of specialised algorithms.\nDynamo and Revit\nProject Refinery relies heavily on the use of Dynamo to define inputs, outputs, perform calculations and generate any model geometry that may be required. Dynamo is a visual programming tool that is included with products such as Revit, Civil 3D and Advance Steel. It aims to unleash the power of programming to the masses making use of its visual programming interface, which is far easier to learn and quicker to implement than traditional coding.\nAccess to specific Autodesk application tools and commands are provided within the Dynamo interface, as well as a raft of tools to create custom geometry, manipulate and sort data and define custom logic. You can also use Iron Python and C# to extend the power of Dynamo further and also link data and geometry to other software applications.\nIn figure 1 you can see a very simple Dynamo program to create a range of Revit levels from 0 to 12,000mm with a floor to floor height of 3,000mm. The Level name is generated by concatenating a string \u201cLevel \u201cwith a list of numbers from 1 to the count (i.e. how many items occur in the list).\nA typical Dynamo script for Project Refinery will have a series of sliders with ranges set to define inputs. The bulk of the Dynamo script will then build geometry and data based on these inputs. The geometry will initially be defined in Dynamo but can also be generated in Revit. For example, a basic tower massing model can be built with Dynamo geometry and then Revit floors, walls, structural columns and curtain walls could be added to build a basic model. A series of Dynamo watch windows are created to show various outputs.\nIn our example of a high-rise massing study the outputs may be data such as Core to Floor ratio, Floor area and material volumes, as seen in figure 2. These watch windows are used by Project Refinery to rank the models.\nProject Refinery\nProject Refinery can be run directly from the interface of Dynamo for Revit or Dynamo Sandbox, Dynamo now having a menu dedicated to Generative Design. Once inside the menu you can export your project for generative design. You are prompted to write a simple description for your project and add an image.\nThe Dynamo graph is evaluated within the Export for Generative Design dialog and, if no errors are found, is ready to go! Typical errors will be if the outputs cannot be passed, i.e. the output is not an Integer or a number or the Dynamo Graph has not been saved.\nThe next stage is to create a study on one of your published designs. The image and description are helpful and aid you when selecting a particular study.\nOnce a study is chosen then a solver must be selected from a list, as shown below, to determine how Project Refinery generates and presents the models. Inputs are then selected to drive the design, typically these will be sliders that have been defined within Dynamo. For example, the inputs for a high-rise tower may be tower height, floor to floor level, core to floor ratio, mechanical levels and structure type. Some outputs will typically need to be constrained, for example the slenderness factor may not want to exceed 1:10 and the core to floor ratio will not want to be exceeded to keep efficiency of the tower design. The next stage is to control the number of generations that Refinery will produce; the more generations you create, the slower the process.\nFor example, if you are using the cross-product method with a large number of inputs the number of models generated can multiply quickly. If you are using a random generation then you can directly enter the number of models that you want to create.\nOnce the models are generated you can then use a variety of methods to visualise the results and rank these by certain criteria. figure 3 shows the design grid view with the models ranked by total floor area, the scatterplot graph below is plotting the tower height vs the slenderness ratio. The size and colour of the circle can also be assigned to various parameters.\nAnother method is to visualise the data by using a parallel coordinate graph. figure 4 shows all the inputs and then the values are plotted along the graph, the line is highlighted in blue when you move your cursor over the image in the grid view.\nWhen a solution has been selected you can open this in Dynamo which will then build the relevant model in your design application \u2013 in the case of the high-rise tower, Autodesk Revit.\nConclusion\nProject Refinery currently runs on the user\u2019s local computer and uses some interesting technology to solve multiple solutions with a good amount of speed. For example., I generated 200 random versions of a tower massing study in just under 18 seconds! I would imagine in the future that the services will be running on the cloud to further increase speed. Autodesk is also releasing a number of toolkits that can be used within Dynamo to aid generative design processes. One challenge is the optimisation of space within a building or applications such as housing site layouts. These are very high on the wish list for generative design and are currently quite challenging issues to solve.\nGenerative design will, year on year, become more and more important in all sorts of industry sectors. We have already seen real world examples of it being used within the manufacturing sector to refine, optimise and create stronger, lighter products. The AEC industry will surely have huge gains when reducing materials, making better use of space and, of course, increasing efficiency.\nIf you want to use generative design processes in your next project, then you will need to make sure that your Dynamo skills are polished and ready to go!\n\u25a0 autodesk.com\/campaigns\/refinery-beta\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-aec-summit-2017\/","title":"Autodesk AEC Summit 2017","date":1502236800000,"text":"Autodesk invited the press to the new headquarters of its AEC Division in Boston, USA, for three days of presentations, customer site visits and technology talks on everything BIM. Martyn Day was there to report\nAs Autodesk is now deep into its transition to become a \u2018subscription\u2019 company, the old annual releases are now a thing of the past.\nAutodesk\u2019s AEC division is seemingly on a biannual cycle of delivering major improvements to its core products. These currently happen in March \/ April and September \/ October timeframes, with perhaps some other news items coming out in November \u2013 in time for Autodesk University in Las Vegas.\nSubscription allows software developers to be much more dynamic in the delivery of content and be more open about their development objectives. With the Autodesk portfolio seemingly ever-growing to deliver on BIM, collaboration and now fabrication, Autodesk held a press event to provide a snapshot of its products and give some indication of current and future development.\nTechnology trends\nOur introduction and first talk was by Nicolas Mangon, Vice President AEC Strategy and Marketing at Autodesk. In an overview of the industry, Mangon laid out some interesting stats, with predictions that the 3.5 billion people who live in cities today will swell to 10 billion by 2050. This means there will be a huge need to build new homes and infrastructure to manage this increase in these megacities. However, AEC makes up 6% of the global GDP and is low margin, high risk and highly fragmented.\nHistorically the sector has not invested heavily in technology but a move to digital constriction could save $1.2 trillion in efficiency \u2013 10% of the total money spent globally on AEC today.\nMangon highlighted several technology trends that are set to revolutionise the industry. High definition surveys, compute power (the cloud) enabling sophisticated analysis of big data, Virtual Reality (VR) and Augmented Reality (AR) immersive environments, mobile technology, the convergence of manufacturing and AEC to digital processes and Artificial Intelligence (AI) driving generative design and machine learning. Combined, these will revolutionise the design, construction and Facilities Management (FM) of the built environment.\nThe future of project delivery\nVikram Dutt, Senior Director, Building Business Line at Autodesk gave the next talk concerning Autodesk\u2019s focus on \u2018Connect BIM\u2019. Autodesk wants to use the cloud and BIM to provide an environment which creates a single place to access the right information in the right context on the right device, from design to maintenance.\nAutodesk has not had a glorious history in document management, against products such as Bentley ProjectWise for instance, but when files become redundant and people are more concerned with model sharing and accessing live data, Autodesk sees it has a chance to offer an alternative.\nBIM models are getting larger, models are being accessed more often and teams are becoming more geographically distributed. With 20 million AEC professionals worldwide and the next 30 years requiring a herculean effort in infrastructure delivery, the process needs to change.\nAutodesk feels it is in a unique place to offer this as with Revit market share, it feels it has BIM leadership. Dutt added that Autodesk has developed the only native cloud platform for AEC, with scalability and a well-established AEC ecosystem. With an eye on industry compliance and now developing a deep cloud portfolio, Dutt proposed BIM 360 as a cloud solution for the AEC space from design, to build and operate.\nAutodesk Forge\nAutodesk Forge is a significant development for Autodesk and its ecosystem of developers. It provides a cloud infrastructure for all of Autodesk\u2019s technologies and serves them up as building blocks for third-party developers to create new solutions or develop \/ merge with their own. All this is made possible because BIM 360 and Forge operate in a single Common Data Environment (CDE) enabling new ways to process, distribute and see AEC information.\nIn the past, developers have had to acquire expensive licences of OEM Autodesk products, or through APIs develop plug-ins for Autodesk\u2019s desktop products. Forge is a fundamental and major change to third-party developers.\nProject Quantum\nWith a hectic schedule, a number of issues got passed over relatively quickly. Unfortunately, one of those issues was Project Quantum which promises a whole new world of collaboration between participants in AEC workflows using the cloud. We covered this in great depth in the January \/ February 2017 edition.\nThe main new piece of information that we gathered on Project Quantum is that Autodesk expects to have at least a couple of customers talk \/ demonstrate their early experiments with the formative technology at Autodesk University in November.\nThere has been much speculation as to how far along Autodesk is with the development of Quantum. With this information, I think it\u2019s safe to assume that Autodesk is further along than most expect.\nWhile collaboration is the primary goal of the first tranche of Quantum technology, it is especially important to Autodesk\u2019s commitment to try and better connect modelling with fabrication and to participate in the industrialisation of construction. The first realisation of this, is acknowledging that Revit is not best suited to drive fabrication machines directly but it\u2019s essential that BIM interfaces with digital construction tools.\nDigital Construction\nSarah Hodges, Director, Construction Business Line at Autodesk took to the stage next and concentrated her talk on a specific area of construction software, collaboration and connectivity \u2013 with BIM 360 Docs acting as the central hub, connecting to Glue, Field, the newly named BIM 360 Ops and Project IQ.\nThe task of managing construction data and post occupancy facilities management through the cloud offers Autodesk some fresh markets to address and, with mobile access, the BIM model really starts to leverage the Information component.\nThroughout Hodges\u2019 presentation, the new BIM 360 cloud-based User Interfaces really stood out as providing very clear views of multiple data types and worked well on the screens of mobile devices, although these also respected the fact that 95% of all data required on site is still preferred in 2D at the moment.\nInfrastructure re-imagined\nTheo Agelopoulos, Director, Infrastructure Strategy & Marketing Design and Build went through Autodesk\u2019s thoughts on the market and developments within its product portfolio on Infrastructure. Refreshingly, this year InfraWorks was not the only product Autodesk seemed to want to talk about and really pushed the company\u2019s capabilities on Reality Capture with Recap and Remake.\nAgelopoulos stated that Autodesk doesn\u2019t want to be a GIS firm (and so partners with the likes of ESRI and others for that component) but it does want to sit at the intersection of BIM and GIS.\nAutodesk customers are starting to really use drones, laser scanners, even LiDAR for much more than surveying and are building aggregated models for advanced uses such as measuring material removed from site, security, and design optioneering.\nInfraworks was demonstrated in advanced bridge design and water services design, all looking very slick and turning civil engineering into the best version of SimCity there\u2019s ever been.\nThe cloud was the big story repeated throughout the day; cloud simulation and collaboration with BIM 360 Glue were shown and even Civil 3D being able to upload and publish to the cloud.\nUK vs US\nThe next session included talks from Phil Bernstein, Independent Strategy and Technology Consultant (former Autodesk AEC VP) and Mark Bew, Chairman at BIM Task Group in the UK.\nBernstein is interested in what factors drive the adoption of technology. In the UK and the US, Revit has done well and BIM is becoming the normal way of working, but he identified that this success was for different reasons.\nIn the US, where Government mandates are not particularly liked, BIM succeeded as it\u2019s a highly competitive market. Nobody forced BIM on any firm, the tool was taken up to \u2018bludgeon the competition\u2019. The net result is a market where no two firms use BIM in the same way.\nMeanwhile, the UK has taken a policy-based approach with an alliance of the supply chain with mutually agreed objectives and prescribed common practices.\nWhile this is perhaps a rose-tinted view of the challenges and acceptance of BIM standards, the general difference in approach is certainly well observed. Americans will fly by the seat of their pants. In the UK, we are more likely to buy ours from Marks and Spencer, as they just work.\nBernstein finished off with a prediction that he feels that, as the construction market is high commoditised with low margins and even final costs can\u2019t be guaranteed, the market may change to seek engagement for more guaranteed outcomes, over lowest cost, as a larger policy objective.\nMark Bew, Chairman at BIM Task Group from the UK did a sterling job of taking the audience through the Government\u2019s drive to improve the UK\u2019s construction industry\u2019s woeful productivity issues, which employs 3 million people and generates \u00a3100 billion a year, of which 40% of the projects are public works.\nAs the single biggest client, the Government kicked off a strategy in 2011\/12 to push a data-driven process to improve program delivery and drive costs down. CrossRail is the first end-to-end project delivered in that manner but is only one of 15 worldwide projects around the world that is using the UK-derived standard.\nBew stated that Level 3 focus has now started, with an estimated implementation in 2025, with \u00a315 million in backing from the Government (Level 2 had \u00a35 million) but solving the Level 3 problem is much bigger than \u00a315 million and so he is in the process of planning how to get more funds.\nLevel 3 is all about transactional BIM with much more sophisticated models. It\u2019s the move from files to objects, single buildings to communities of buildings with infrastructure (could be termed smart cities), and of course, data security is a major issue.\nBew thinks we have lost the skill of generating decent briefings, and through better understanding of feedback from real world information in space usage, human interaction wants to focus on creating detailed digital briefings for Level 3 projects.\nRevit and Connect BIM\nIt has been 17 years since the launch of Revit. Autodesk has owned it for 15 years of its life and so has done all the heavy lifting. Oddly enough Revit was originally a subscription-based product before Autodesk bought it. In 2005, we had the first version of Structures and 2006 came MEP. By 2013 they had all been rolled back into one product.\nThere wasn\u2019t a lot of time allocated to in-depth features but Autodesk highlighted the updates to the new multi-storey stairs command in Revit. Storeys can be different heights and unique edits can be made with auto-updates to the railings. The railings can also be used as fences.\nNavisworks models can now be taken into Revit and users get the same performance, allowing Navisworks to draw inside Revit. As a test, they took the whole city of Moscow inside for a dynamic fly-through.\nFormit and Revit now play better together, so early concept designs don\u2019t lose information when they move downstream.\nAutodesk Insight is a cloud-based energy and daylight performance tool which can be used through the design process, from Formit concept models, all the way to final sign off. It has a really nice simple graphical interface and it\u2019s easy to understand the results.\nAutodesk Revit Live is Autodesk\u2019s first iteration of delivering Revit models to VR. In the past, you had to use multiple packages to do it; now it can be uploaded to the cloud for automatic preparation and pre-flight.\nThe main problem here is that any changes require the whole file to be re-uploaded and re-processed. I suspect this will be improved quickly as \u2018Revit to VR\u2019 applications from third-party developers, such as Enscape, can already handle small delta changes without having to shunt big Revit models around. (see our Revit to VR article).\nBuild Space\nOne of the key areas in Autodesk\u2019s new offices is the Build Space, a factory floor with a suite of massive fabrication tools and robots for customers to come and experiment on designs using cutting edge digital fabrication and materials.\nThere is also a space for industry-related incubators to operate in-house and use the machines, while sharing their experiences and challenges with Autodesk to help improve software product development.\nThe area is run by Rick Rundell, Technology and Innovation Strategist, Start-up Mentor and Senior Director at Autodesk and the facility has really delivered on connecting Autodesk with local academia and innovative developers and has become an in-demand space where digital and physical design technologies can meet and be tested.\nWe visited the space last year and it was still early days; now there are a number of interesting projects being run in the space, from innovative mould making for large scale forms, using robots to 3D print \/ extrude and stretch tetrahedron-based lattice forms to 3D printing components for historical facades which are being refurbished in the city.\nLeica BLK + Autodesk ReCap\nThere is been a lot of hype around Autodesk\u2019s co-developments with Leica for its new portable laser scanning system the BLK 360. First shown at Autodesk University 2016, the BLK 360 is possibly the sexiest laser scanner ever made, if there should be such a thing.\nThe device is incredibly small and will fit into a bag and is the first ultraportable laser scanner ideally suited to scanning the interiors of buildings. Autodesk and Leica have worked together to create a relatively low-cost combination of portable scanner with Recap running on Apple\u2019s 12\u201d iPad Pro. The combination enables firms to arrive at location and quickly carry out multiple scans while collecting the data and checking scans on the iPad Pro. This helps firms ensure the laser scan data is complete with no holes in the model before anyone leaves the site.\nTo see the BLK 360 and Recap work together we visited wework in downtown Boston, a company which hires office floors or whole buildings in major cities around the world and converts them into shared co-working and hot desk environments. This impressive company uses Revit, standardised fit-out with laser scans to rapidly assess buildable space and return on their investment prior to signing any lease. The company regularly sends out employees with large laser scanners to capture potential new sites with one employee saying they made over 200 site visits last year to scan and collect 3D data. The new Leica BLK 360 and iPad combination will save considerable luggage space and effort, and they\u2019ve been very happy with their trials so far. The accuracy of the Leica is to within 4-6 mm, which is good enough for modelling. While it was hoped that the machines would be available by now, they are still under pre-order on the Leica website, so there is still some work to do. We can\u2019t wait to get hold of one of these to test.\nwework is a great example of how BIM can be used to provide tactical advantage over competitors. BIM isn\u2019t a technology, it\u2019s a process and the company has continued to refine its process to enable rapid modelling of potential new sites. This not only identifies the buildings with the best business potential but also brings forward the ordering of components, accelerating fit and finish.\nThis rapid turnaround has enabled the firm to grow quickly in and beyond the major centres of business. The company aims to build communities of workers in their very funky spaces and I was especially won over by their supplying of free beer!\nSkanska 121 Seaport\nNear Boston\u2019s North End, the Seaport region has become a very busy construction site for many office buildings and condos. Since the city courts relocated to the area there has been an explosion of investment and interest in creating a masterplan to turn this previously neglected harbour area to mixed use.\nThe net result of this is 2.8 million ft.\u00b2 of residential, 1.2 million ft.\u00b2 of office space, the same again for retail, together with 860,000 ft.\u00b2 of additional hotel space and a quarter million ft.\u00b2 for civic and cultural spaces.\nOne of the companies heavily involved in redevelopment of the area is Skanska and we visited its headquarters to hear about some of the projects the company was involved in \u2013 as well as a little about their BIM process.\nA case in point was 121 Seaport, the 17 storey 400,000 ft.\u00b2 office building, a platinum LEED design which features outdoor spaces such as rooftop terraces, a pedestrian-only 50,000 ft.\u00b2 retail promenade, a fitness centre and access to the Boston Underground. To talk about the process, Skanska had assembled a team which worked on all aspects of the design. The owner and commissioning firm was Skanska itself and, unsurprisingly, it used Skanska to build the office block, with architecture firm CBT doing the design work. #\nThe building site was itself an issue as in the top corner it overran the Boston subway, which meant that it was impossible to drive deep piles in parts of the site. CBT decided that the form of an ellipse would help in overcoming some of the site\u2019s limitations, but in the process it discovered that the form factor also benefited, spreading wind loads on the structure. It also became a sales point in a very rectilinear part of the Boston skyline.\nThe teams talked about how they collaborated and used BIM tools such as Revit, Navisworks and VR to drive the process and expedite completion.\nSkanska\u2019s expertise in 4D has enabled and accelerated construction time which drives an expedited opportunity to rent the office space.\nMEP and Fabrication\nThe Boston Seaport area is not only a regeneration project for offices and houses but also for fabrication and industry. We visited \u2018Building 16\u2019 to meet one of the largest local mechanical construction firms, Cannistraro, headed by CEO, John Cannistraro, which had acquired the lease on the historic building originally constructed in 1940 by the U.S. Navy.\nCannistraro is in the process of consolidating four fabrication units into one, with this giant 157,000 ft.\u00b2 building on the waterfront bringing everything under one roof and close to where a lot of the building work is taking place.\nThe first part of the process though, is to rehabilitate the building and bring it back from decades of neglect. Built by the US Army Corps of Engineers, it\u2019s impressively over engineered.\nCannistraro began utilising BIM in its earliest forms in the early 2000s, and expanded its use each year. The use of BIM for off site prefabrication became prevalent across the organisation and on all projects by 2008. The company became known in the industry as an early adopter and advocate for the use of BIM, early collaboration and alternative delivery methods such as design assist and IPD.\nToday Cannistraro employs over 30 coordinators and designers in its BIM group as well as an in-house VDC (Virtual Design & Construction) group for file \/ model management, maintaining BIM standards, as well as quality assurance.\nCannistraro currently uses Autodesk fabrication CAD MEP, as well as Navisworks and BIM 360 Glue, and is increasing its training and usage of Revit. The company also owns various tools that supplement its BIM-enabled fabrication efforts including Trimble total stations, 3D laser scanners, a HoloLens headset and manufacturing \/ scheduling tools including FabPro and Touchplan.\nWhile we were in Building 16, Jim Lynch, Autodesk\u2019s Vice President, BIM Product Line Group, gave a presentation on Autodesk\u2019s commitment to connecting BIM to digital fabrication. In the manufacturing space this has already been a great success but that revolution has yet to come in AEC.\nBy making a move to creating accurate models, the downstream benefits have yet to be realised. Autodesk has technology in both AEC and in manufacturing and is committed to sharing that expertise between verticals. The obvious meeting point for that is in MEP fabrication, modular construction \u2013 any job that requires complex shapes, material cutting or shaping.\nThrough Project Quantum the data will be made available in whichever application is applicable to the task, with the data seamlessly moving between trades providing the level of detail required at each development stage.\nPerkins + Will\nThe final session was given by Dr John Haymaker, Director of Research at architecture and design firm Perkins + Will (P+W) who evaluates new technologies and heads up its incubator program, along with 9 research labs around the world.\nHaymaker feels that we are at a watershed moment for architecture and data is changing everything. It\u2019s not just helping P+W better refine and design towards objectives like cost and schedules, but also water and energy consumption. It\u2019s becoming performance-based design and the risk is being shifted to contract and delivery.\nHaymaker said, \u201cWe have to decide if a building is the best it can be along many different performance metrics. We are reaching the human cognition, we are at our limit of how many objects we can consider \u2013 alternatives, systems, materials. We need computational assistance. We aren\u2019t using computational design to replace any of our 2,000 designers, we want to enable cyborg super designers and design teams! \u201d\nP+W has built up in-house research labs with experts in process, efficiency, material performance, urban water issues, design resilience, mobility (transport), society, and human experience.\nThe many competing issues which face designers fascinate Haymaker. Performance-based design needs to trade off these competing issues to maximise the best outcome that meets the most important criteria. The traditional route is to come up with various resultant designs which are then analysed for how best they trade off against the competing criteria.\nToday\u2019s methods are just not good enough at defining what all the alternatives are, or how well we measure them up.\nHaymaker thinks in terms of Design Spaces, which he started while at Georgia Tech. He defines them as being BIM models plus the environment around them and multiple BIM models that might fulfil the brief, as well as the stakeholders and decision makers and the objectives, preferences for trade-offs, the constraints.\nAfter formulating the outlying parameters, it is easier to derive computational algorithms which can then create thousands of alternative designs within the defined trade off.\nP+W is also experimenting with new fabrication technologies, such as composites, 3D printing and robots, using CNC milling to make moulds for complex geometrical panels. The idea is to show contractors the design intent to provide guidance. Thermoplastics are another area of interest where, after heat is applied, robots can pull and stretch the material to produce precise deformations \u2013 meshed panels with structural characteristics.\nOn the subject of materials, Haymaker talked about \u2018Tall Wood\u2019 structures and a project that\u2019s intended for Chicago. It\u2019s led to experimentation with modular designs, material sourcing and onsite fabrication. P+W has spent some time in Autodesk\u2019s Build Space to create test modular components for the building.\nConclusion\nWith so many talks and site visits, the Boston event really was an incredibly intense three days. The message was consistent and strong: while Autodesk is improving its desktop tools, all destinations for that data will be on Autodesk\u2019s cloud and that is where a huge effort is being placed.\nThird-party developers are being encouraged to develop for Autodesk\u2019s cloud and for customers to get the promised benefits of a common data environment \u2013 collaboration and powerful simulation \/ mobile access and lots of compute power \u2013 having data on a desktop is not going to cut it.\nHowever, for customers that are not happy with a subscription-only option or having to rely too much on a single vendor, this could be less than brilliant news \u2013 as Autodesk won\u2019t just own your design weapon of choice but will own your company\u2019s design process.\nThe question will be if the benefits provided outweigh the concerns. I have to admit that Autodesk\u2019s vision of BIM-cloud-utopia does sound compelling and it has seemingly identified the key problems. The devil will be in the details and one wonders how generous an eco-system will be in adapting to different processes of customers. Then there\u2019s also the fear of future price hikes and no way out.\nWhile a lot of BIM is about software development and usage it\u2019s clear that, with advances in mobile computing, the cloud and big data, software and hardware are becoming better integrated and seamless.\nFor such a long time we have been concerned with moving from 2D drawings to making models and enjoying the advantage of co-ordinated output of drawings when edits are made. This truly pales into insignificance when one considers the revolution of a move to true, end-to-end digital construction and fabrication \u2013 design construction firms are driving this development forward at considerable pace.\nOut of all the software developers, Autodesk is perhaps best placed to deliver on that digital \/ physical connection, being the only significant AEC developer with feet in both the construction and the manufacturing sectors.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bim-technology-protocol-for-archicad-v2-0\/","title":"NEWS: BIM Technology Protocol for ArchiCAD v2.0","date":1459209600000,"text":"Guide aids the practical implementation of the unified UK BIM standard for ArchiCAD users\nThe AEC (UK )BIM Technology Protocol For ArchiCAD v2.0 has now been released.\nVersion 1.0 of the ArchiCAD supplement was first published in March 2013. Version 2.0 brings the documents into line with the updates made to the main AEC (UK) document (published in June 2015), aligns with the latest industry standards, and incorporates lessons learnt and software developments that have been made by Graphisoft over the past few releases.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nLondon Gatwick creates new geospatial platform\nNEWS: Nvidia launches \u2018VR Ready\u2019 program for professionals\nSkema beta: design automation\nGr\u00e4bert - towards drawing automation\nEsri UK partners with Tetra Tech on indoor mapping\nBOXX pushes CAD performance with AMD Ryzen 5000 workstation\nSkema to streamline SketchUp to Revit workflow\nNEWS: Enscape launches SketchUp plug-in\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesks-2012-line-up\/","title":"Autodesk's 2012 line-up","date":1305331200000,"text":"Autodesk has a multitude of products to offer the AEC industry. With the new 2012 range, the company has decided to create a number of product bundles, or suites. Martyn Day investigates.\nAutodesk has been executing its yearly release of updated CAD products like clockwork for the past decade. In the last five years the AEC product range has mushroomed, with the company unleashing a deluge of new and evolving point solutions around the March timeframe. This year there was a significant difference to the launch, in that it was not just about the new features \u2014 in fact, the new features have very much taken a backseat. The big news for Autodesk\u2019s 2012 AEC range is the availability of product suites.\nSuites are the culmination of the AIRMAX (AutoCAD, Inventor, Revit and Max) internal development program at Autodesk: to standardise the User Interfaces of the products, to improve data compatibility between the platforms and to share underlying graphics subsystems to provide a homogeneous look and feel in the display textures and colours. Because Autodesk\u2019s products are all built on one platform, this has been a Herculean task that has taken years to complete. With the \u2018under the hood\u2019 work done, Autodesk felt it could produce bundles of related applications, with considerable savings, expanding the capabilities of its customers across complete workflows, from project design, visualisation and simulation to documentation and construction.\nAutodesk has suites for AutoCAD workflows, Manufacturing and AEC. The AEC solution is called the \u2018Autodesk Building Design Suite\u2019 and is available in three editions: Standard, Premium and Ultimate. All three editions include AutoCAD 2012, AutoCAD Architecture 2012, AutoCAD MEP 2012, AutoCAD Structural Detailing 2012, Autodesk Showcase 2012 and Autodesk SketchBook Designer 2012. The Premium edition, in addition, includes Autodesk Revit Architecture 2012, Autodesk Revit MEP 2012, Autodesk Revit Structure 2012 and Autodesk 3ds Max Design 2012. The Ultimate edition expands this huge dollop of software with Autodesk Navisworks Manage 2012, Autodesk Quantity Takeoff 2012 and Autodesk Inventor 2012.\nSo the Standard, which is an AutoCAD-based offering, has six products, the Premium suite is for BIM (Building Information Modelling) and offers ten and Ultimate, which addresses simulation and analysis, weighs in with thirteen applications. The amount of professional software in these suites is really quite mind blowing. On one hand I am compelled by software gluttony at the veritable gourmet banquet of applications and the cost savings \u2014 which range up to 60%. However, on consideration, all these applications are professional level, mature products that require training. If you don\u2019t use the applications regularly, then it\u2019s easy to forget how to drive the stuff. With the separation of specific tasks within firms, would a single user actually use many of these? Would a design architect ever load up AutoCAD MEP?\nIt is true that designers these days don\u2019t stick to just using one application but will mix and match from a number of \u2018hot\u2019 tools on the market. While I have seen this in action, it has tended to be with products like SketchUp, Generative Components and Rhino, which are more at the conceptual end of the process. However, I do realise that most designers would love to get their hands on 3ds Max, because it\u2019s \u2018way cool\u2019.\nThe suites support Autodesk\u2019s cascading network management system but there are limits. In a mix of licenses, the cheap seats are allocated first. So in a network mix of two AutoCADs and one Building Suite, those opening AutoCAD will be allocated the single licenses first leaving the Building Suite able to provide any one of its applications next. However if three people need AutoCAD, then none of the Building Suites\u2019 additional applications can be opened until the last AutoCAD is shut down.\nFrom this, I can see how for large firms, the suites offer a good way to buy in a lot more breadth of product at a discount. Depending on the type of single user, a suite may also be a great way to save money, if two or more of the products are of use. However there is also the ongoing subscription pricing which is a slight increase on a single product.\nWith so much software included, Autodesk has dispensed with DVDs and ships the suites on a single \u2018funkily\u2019 designed USB stick, which I have been told is 32GB in size and can\u2019t be overwritten.\nFeature updates\nThere are too many products and too many features to go into detail here but here\u2019s a round-up of what\u2019s new in the core building products:\nAutoCAD Architecture 2012 is definitely the poor cousin of Revit in terms of development but, as it\u2019s a pure AutoCAD play, it is still popular. 2012 comes with a number of enhancements and a few new features. Doors, Spaces, Structural Members and Windows have all been enhanced, together with IFC support and Point Cloud. Also new is built-in access to Autodesk Exchange and collaboration with AutoCAD WS. AutoCAD MEP gains similar new capabilities together with some other enhancements.\nAutoCAD Civil 3D 2012. The new release focuses on three key areas: support organisational standards, enhanced transportation design, extend visualisation and analysis.\nCivil 3D 2012 includes new functionality for styles and settings management, as well as improvements to labels. There\u2019s additional integration with Autodesk Vault. There\u2019s new functionality for geometry creation for corridor editing and superelevation modelling as well as new tools for surfaces, water analysis and civil visualisation.\nVault Collaboration AEC enables access to data management from within the user\u2019s design environment. Built-in data replication keeps the most up-to-date versions of project drawings, even if they\u2019re thousands of miles apart. When there\u2019s no web connectivity, changes will be kept and re-synced when connection is restored.\nRevit Architecture 2012 is a substantial release with plenty of new features and enhancements. The most striking things are: support for point clouds is built-in (like AutoCAD), the excellent 3Dconnexion 3D mouse is now supported out of the box (see page 26 for a full review), and there is a powerful new wiki help.\nAutodesk has woken up to the fact that BIM is great and really valuable to those doing the construction. This release sees additional functionality to appeal to those that want to make construction BIM models, which are different to an architectural BIM model. New parts and assemblies together with conceptual energy analysis takes Revit to the heart of construction.\nThe new Revit server and Citrix support will greatly enhance workgroup usage, as well as remote access. Other than that, the majority of existing functions have been enhanced.\nThere have been many people asking if there is going to be an Apple Mac version of Revit. I can clearly state that Autodesk currently has no plans to write a specific OSX release. As part of a major behind the scenes reworking, Autodesk is breaking down its products and rewriting them to be platform independent, i.e. not based on Windows. In time, Revit will be ready to be delivered over the web or via a number of other ways, which will mean it will run on any platform. For now, Autodesk\u2019s AEC team considers that this will be the way that Revit will appear on the Mac.\nRevit MEP 2012 was probably the weakest of the Revit range of BIM tools but is now ramping up at considerable pace. The Revit server, Citrix, energy analysis and 3Dconnexion support is also added here, as are new features of for Pipe Placeholder Layout, Duct Placeholder Layout, Parallel Pipe Layout, Parallel Conduit Layout. In documents its also possible to add 3D tags.\nNavisworks 2012 is actually two products, Navisworks Manage and Navisworks Simulate. These are used to co-ordinate BIM models for construction and management. The interface of both finally gets brought up to spec with the other Autodesk products and directly links to Revit\u2019s new construction modelling and Vault.\nSimulate gets an interactive Gantt chart with 4D simulation, schedule tasks and a new timeliner API.The most important feature, Clash detection, is only available in Manage and this release sees a massive improvement in the management and tracking of clashes. File support has been expanded and improved to add DWF, Primavera, Faro (laser scans), RVM, Pro\/Engineer, SketchUP, and Bentley DGN.\nBuzzsaw 2012, the online document distribution and management service, is now compatible with Autodesk Vault, allowing documents to be automatically kept up to date and linked to the drawing office. The addition of Buzzsaw for Android, iPhone and iPad also now expand the system into a truly mobile document system. Add AutoCAD WS, now for iPad and Android tablets, into the mix and drawings can be accessed, edited and commented on in a pretty seamless way. By connecting up these standalone solutions Autodesk is building a very compelling, low-cost mobile solution. Autodesk is calling this vision \u2018BIM 360\u2019 and is well worthy of evaluation.\n3ds Max Design 2012. There are have considerable improvements this time around with the new Nitrous accelerated graphics viewports, improved compatibility with Alias products, inclusion of the ray renderer, some great new non-photorealistic \u2018stylistic\u2019 rendering styles, new rigid body dynamics and a range of other enhancements.\nProject Galileo. During the launch event we had a few glimpses of Project Galileo, which is currently in Autodesk Labs (labs.autodesk.com\/utilities\/galileo\/overview\/), giving the impression that it might actually be near to launch. Galileo is a concept modelling tool for city-scale projects, being able to handle multiple datasets in the context of a huge 3D model. The project brings CAD, BIM and GIS together to enable collective decisions based on having all the necessary information in one place. This is certainly a product to watch.\nWatch this space?\nIn conversations with the Autodesk AEC team, I mentioned the good value of Graphisoft\u2019s ArchiCAD SE product, which is a cut-down version for full ArchiCAD that retails at under \u00a31,000. There was some agreement that to get the mass of users to migrate to BIM, cost was an issue and that the \u2018all singing and dancing\u2019 tools in the Revit suite were perhaps not an easy \u2018in\u2019.\nThe market has also yet to see Dassault Systmes launch its BIM modelling tool, Live Buildings, based on Catia, possibly this year. On the subject of a possible Revit LT product I was told to \u2018watch this space\u2019 and our conversation naturally went on to Autodesk\u2019s Project Vasari (labs.autodesk.com\/utilities\/vasari), which is an \u2018expressive design tool for creating building concepts with integrated analysis for energy and carbon\u2019.\nWhile currently in \u2018Labs\u2019, the product has a powerful modelling engine and hooks directly into Revit for post rationalisation. I may be putting 2+2 together and getting eight but I would not be surprised if Vasari ended up being Autodesk\u2019s entry-level BIM tool, to get people into modelling and analysing their designs. It\u2019s a very impressive product and I would strongly suggest giving it a try.\nConclusion\nFor Autodesk, 2012 is the year of the Suite. For customers, on one hand, it\u2019s now possible to own a lot more products for considerably less.\nHowever, due to the delivery mechanism, under a single user licence, the skills-level required for an individual to derive that value has to be considered. The majority of tools in the bundle are mature, professional applications that require training and if not used frequently, rust sets in. Also the ongoing increase in subscription per seat has to be considered.\nThat said, larger firms using a cascading licence system will find the Suites more useful, adding in some more exotic applications into the mix.\nLooking ahead, I am wondering what this will mean for the development of Autodesk\u2019s AEC products?\nIn a presentation from a different division of Autodesk, a snippet of information on plans for 2013 indicated that \u2018the cloud\u2019 will have an increasing role within our design tools. This isn\u2019t about running CAD over the web but concerns specific built-in functionality using web-based services to perform tasks.\nAt the moment subscription is mainly about buying the next release; in the future it will give you access to powerful web-based analysis, rendering and collaboration tools, all from within your design application. To a small degree this has already happened with Ecotect which, when coupled with a subscription, gives customers access to the online Green Building Studio (GBS) energy analysis service.\nFor this year, the 2012 crop of Autodesk products continue to demonstrate the steady evolution approach that the division has taken to adding functionality in an industry that makes changes slowly. Continuing to harmonise the various interfaces and expand data compatibility between products are always welcome improvements.","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/magnetek-joins-esris-business-partner-program\/","title":"Magnetek joins ESRI's business partner program","date":1060300800000,"text":"Magnetek, Inc. announced that it has joined the Business Partner Program of ESRI, a GIS software field. Magnetek, a manufacturer of digital power supplies, systems and controls used in many industrial, commercial and consumer applications, will integrate ESRI\u2019s Arc View GIS software with Magnetek\u2019s HIQgrid digital control system. This system monitors, controls and predicts the life of electrical equipment such as streetlights and distribution transformers that are powered by or integral parts of the electric utility grid. Installed on remote IBM eServers, ESRI\u2019s Arc View software enables the HIQgrid system to pinpoint locations of equipment in need of maintenance, repair or replacement anywhere on the utility grid, making it easy for technicians to respond to fault and failure signals.\nMagnetek\u2019s HIQgrid system is the power monitoring and control systems capable of transferring complex data streams on the utility grid via power-line modems. The system consists of digital control units positioned at numerous points along the power line. These units capture and relay coded data to collection \u201cnodes\u201d utilizing proprietary software algorithms that \u201cisolate\u201d the digital data streams from the high-voltage electric current being carried on the same conductors. The collection nodes, in turn, communicate the data via ISDN, Ethernet, GSM, GPRS or microwave to remote servers for central monitoring and control. The HIQgrid system has been successfully demonstrated on streetlights in the city of Copenhagen with collection nodes at 1.5-kilometer (0.9-mile) intervals. Deployed on a citywide basis, the HIQgrid system with Arc View is expected to reduce street lighting costs substantially. It is applicable anywhere in the world, and it may be used in other utility grid applications such as distribution transformer monitoring and automated meter reading.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/review-graphisoft-archicad-20\/","title":"REVIEW: Graphisoft ArchiCAD 20","date":1467676800000,"text":"With its stripped-back interface, new emphasis on data management and the introduction of a live link to Rhino+Grasshopper, ArchiCAD 20 looks set to open new doors for the software company, writes Randall S. Newton\nBack in the 1980s, Graphisoft was the first company to put a 3D architectural modeller on a personal computer, the thennew Apple Macintosh. Today, it\u2019s a BIM thought leader even if, globally, it\u2019s not the top seller of architectural software. That leadership continues in ArchiCAD 20, with several key enhancements introduced with a view to extending the product\u2019s value.\nAccording to the company\u2019s marketing slogans for this release, ArchiCAD 20 offers \u201ca fresh look at BIM\u201d and will \u201cput the I in BIM\u201d for customers. We\u2019ll get to that second claim later, but that \u2018fresh look\u2019 will be obvious to the experienced ArchiCAD user the moment version 20 boots. The user interface has been completely redesigned to be, in the words of Graphisoft\u2019s Tibor Szolnoki, \u201cmodern, minimalist, fresh and friendly.\u201d\nIts interface designers have stripped the screen down into three elements: background, icons and text. \u201cThe old interface had too many buttons of varying size and emphasis,\u201d Szolnoki explains. That led to what designers refer to as \u2018interface noise\u2019, but borrowing from best practices for mobile apps, ArchiCAD 20\u2019s rebuilt interface is more comfortable and less cluttered. In the past, it presented users with two line thicknesses, three line types and three colours. By contrast, ArchiCAD 20 now gives them thin lines, uniform button sizes and only the most commonly used commands appear by default. At the same time, there are 7,000 all-new command icons, with another 13,000 library icons to be updated on an ongoing basis.\nVisual updates extend beyond the user interface. A new two-point perspective option allows the 3D view to shift to a two-point perspective in any camera position; all 3D vertical edges become vertical in perspective projection.\nThe \u2018eye candy\u2019 side of architecture has not been forgotten. ArchiCAD 20 runs the latest version of CineRender, based on Cinema 4D. Changes include improved surfaces, reduced render times and more realistic contact shadows. A new reflectance channel allows multiple reflection layers and fine-tuning control over blurriness and distance dim.\nMetadata matters\nA new emphasis on data management explains why Graphisoft executives are claiming that ArchiCAD 20 \u201cputs the I in BIM\u201d. Much of the new technology in this version comes with an enhanced approach to importing, managing and displaying the diverse forms of metadata associated with any architectural model.\nThat\u2019s important, because information relating to item cost, manufacturing details, building code compliance, fire ratings and hundreds of other issues typically comes from multiple sources. Little of this data is generated by the architectural team itself; most comes from third parties who are not BIM users. In addition, much of the data arrives in Microsoft Excel spreadsheets, although more manufacturers are adopting the IFC open standards for architectural information.\n\u201cWe had too many ways to work with metadata prior to Version 20,\u201d says Szolnoki. IFC was one of many sources of information and the various information formats were not well connected.\nVersion 20, by contrast, leaves behind the tags and categories, files and folders approach to storing and using metadata. Architects were afraid of this complexity and left it to BIM managers. Much information was only found attached to library models or imported from add-ins when needed.\nNew in ArchiCAD 20 is Property Manager, a dedicated framework for nongeometric data. Most frequently used properties are put into groups; users can create new categories and groups. The existing Find & Select tool now works inside Property Manager to find metadata details.\nMost metadata arrives without a readymade visual footprint, so ArchiCAD offers one or allows the user to create it. Using fire rating as an example, ArchiCAD automatically provides visual cues to see whether a wall has a 30-minute, 60-minute, or 90-minute rating. The designer can quickly create a view that presents fire rating.\nMuch of the metadata that architects work with is from manufactured elements, with doors being the most common example. Every door manufacturer provides data, but using it creates a broken workflow. The data arrives in an Excel spreadsheet and must be placed into the BIM model. There\u2019s no two-way information exchange between BIM model and Excel file. The designers who use this information often limit their door choices to a few beloved options, to minimise the tedium of metadata use.\nArchiCAD 20 directly imports this data into ArchiCAD Elements for storage and use. The data is automatically populated into schedules and to the BIM model. Unique ID numbers are attached to incoming metadata, allowing a twoway path to be created between spreadsheet and BIM. ArchiCAD 20 users may also create a new spreadsheet from data that was created in the BIM model.\nPlaying favourites\nArchitects don\u2019t want to be data managers. They want to design \u2013 and they want the process to be like having a Lego kit, with all the information already attached to the modelled element. To improve the process, Graphisoft has reworked Favourites, a tool it says has been under utilised in previous ArchiCAD versions, especially among small firms. It was seen by many as hard to use and not graphic. ArchiCAD 19 had a workaround \u2013 the option to pick parameters from another object \u2013 but it was easy to misuse the process.\nArchiCAD 20 offers a new twist, Geometric Favorites. This works as a palette, offering thumbnail views instead of names as in previous versions. Shift-hover changes the view, on the fly, between 2D and 3D. Graphisoft says the new Favorites Palette is better equipped for helping designers manage data. Pop-ups put Favourite options at the mouse point. When a designer\u2019s favourites add up to hundreds, the Edit Favorites command can rearrange which ones should appear first. To organise these easily, there is a tree structure and a folder structure. Users can search by properties, as well as by item names. The company claims this new approach to using library elements is how the average architect will use information management inside ArchiCAD.\nA related feature is Graphic Override, a new tool based on the Renovations tool in previous versions. In past versions, it would overwrite data based on renovation status. Now, the command is used to update and display metadata. It will overwrite visuals and overwrite criteria. In a demo, Graphic Override instantly painted all 60-minute fire-resistant walls to orange; the view can be switched on and off as needed. Such views are available for both 2D and 3D.\nArchiCAD 20 can accommodate and maintain IFC workflows, but also offers more advanced and more flexible model and visualisation functions. Getting all load-bearing walls to change their appearance, for example, is quick work. FM View can instantly colour rooms by function; Lease View can highlight a tenant view of a floor plan. Graphic Override also offers \u2018cardboard\u2019 and white views for early sharing of models with a client, when it is easy to get lost in the details.\nAlgorithmic alliance\nAlgorithmic design for initial concept has become the hot new technology in architecture. Originally known as \u2018generative design\u2019 and first brought to market by Bentley Systems, it has been adopted by many signature architecture firms as a fast way to get to hig\nh-quality \u2018blobby\u2019 architecture in a short time. Algorithmic design allows pre-selecting geometric elements that act as parametric constraints. With those set, it becomes possible to explore unlimited options rapidly. The most popular product for this today is the combination of the 3D modeller Rhino from McNeel and Associates and its Grasshopper add-on that specialises in algorithmic design. This is seen as a bestof- breed solution and is used around the world as an initial design tool in architecture. The standard workflow here has been to conduct initial concept development in Rhino+Grasshopper and then send the results to the BIM team, which creates the schematics and construction details. It is a silo approach to design, with no live connection between products.\nWorking closely with McNeel and with international architectural studio BIG (click here for more details), ArchiCAD has developed a two-way live link to connect Rhino+Grasshopper and ArchiCAD 20. The link solves the major challenge in using Rhino with an architectural modeller: 3D data formats.\nRhino uses NURBS (non-uniform rational basis splines) to create and store geometric data, good for modelling complex 3D curved objects with extreme precision. ArchiCAD, by comparison, uses its own mesh-based 3D format for geometry representation. The link is a two-way communication channel to import\/export Rhino data and a plug-in to save data to ArchiCAD GDL (Geometric Description Language, the format for creating library objects.)\nThe ArchiCAD team makes much of its GHAC Logic \u2014 or, in other words, Grasshopper\/ArchiCAD Logic. This integrates the Grasshopper visual scripting interface into the ArchiCAD workflow. The logic of Grasshopper is to define or assign a geometric parameter with vectors to algorithmically define and control design. Graphisoft executives know that customers will come up with their own use cases, but primarily see the GHAC Logic as suitable for three workflows:\n1. Starting in Rhino+Grasshopper to design and then extending the data to ArchiCAD 20 for BIM.\n2. Complementing ArchiCAD with algorithmic functions, using simple 2D as reference, for control of geometry and to control attribute values.\n3. Starting with a \u2018geometric container\u2019 and then setting and controlling geometry from within ArchiCAD 20, to include attribute values and GDL parameters and take advantage of new libraries in ArchiCAD 20. \u201cIt is a very powerful tool,\u201d says product manager Erik Havadi.\nFinal thoughts\nMakers of best-of-breed software are not usually market leaders, but do tend to be the pioneers of new tech. ArchiCAD is considered a best-of-breed BIM modeller, but it takes the middle ground in this release, combining the best of internal improvements with a key technology alliance in the spirit of \u2018if you can\u2019t beat them, join them.\u2019 The live link to Rhino+Grasshopper, in particular, is a game-changer that will open doors for Graphisoft.\nRandall S. Newton is a business analyst and journalist with more than 30 years experience in the AEC and manufacturing design industries.\nClick here to read the related article \u2018Graphisoft thinks BIG to move BIM forward\u2019\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/quadro-rtx-at-heart-of-new-zbook-mobile-workstations\/","title":"Quadro RTX at heart of new ZBook mobile workstations","date":1559001600000,"text":"HP also rolls out new entry-level desktop workstations that aren\u2019t so entry-level anymore\nHP has released details of its new HP ZBook 15 G6 and ZBook 17 G6 mobile workstations which feature 9th Gen Intel Core i9 processors (and equivalent Xeons), plus Nvidia Quadro RTX GPUs. Both models offer tool-less expandability to upgrade memory and storage \u2018in seconds\u2019, a feature that was introduced with the G5 editions.\nWhile the aluminium alloy CNC machined chassis looks to have remained largely the same, the move from G5 to G6 marks a significant increase in performance.\nUsers now have access to up to 8 CPU cores with the new 9th Gen Intel Core i9 processors. This continues a trend which started with the G5 editions, stepping up from 4 to 6 CPU cores. 8 CPU cores will be of significant interest to those who rely on CPU-based ray trace renderers like Luxion KeyShot or simulation tools like Ansys Mechanical.\nThere\u2019s also been a significant boost in GPU performance. The ZBook 15 G6 offers up to the Nvidia Quadro RTX 3000 and the ZBook 17 G6 up to the Quadro RTX 5000.\nFor these new GPUs, the focus is on faster ray tracing with dedicated RT cores (for ray tracing) and Tensor cores (for AI denoising). To take full advantage, applications have to be RTX-enabled and this should be coming in the next releases of V-Ray, Unreal Engine, Enscape, Autodesk VRED and many more.\nSignificantly, the move to Nvidia Quadro RTX also means the ZBook 15 G6 becomes HP\u2019s first VR Ready 15-inch mobile workstation. But for high-end VR workflows you\u2019ll still need the 17-inch HP ZBook 17 G6.\nFinally, for colour-critical workflows, the ZBook 17 G6 it features what HP says is the world\u2019s first 17-inch mobile workstation display with 100 percent DCI-P322.\nHP has also updated its entry-level workstations, with four new models \u2013 the HP Z1 Entry Tower G5, HP Z2 Mini G5, HP Z2 Small Form Factor G5, and HP Z2 Tower G5.\nThe major focus is on the HP Z2 Tower G5, which has been redesigned to provide \u2018two times the graphics power\u2019 of the previous generation. Thanks to the new 650W power supply, it can offer up the Quadro RTX 6000 graphics, a significant step up from the Quadro P5000 in the HP Z2 Tower G4. To support the new 9th Generation Intel Core processors, HP has also introduced a proprietary motherboard which it says unlocks an extra 8% boost. Entry level isn\u2019t so entry level any more.\nFor real budget users, the HP Z1 Entry Tower G5 is a new sub entry-level workstation which looks to be an updated version of the HP EliteDesk 800 Workstation edition.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/graphisoft-thinks-big-to-move-bim-forward\/","title":"Graphisoft thinks BIG to move BIM forward","date":1467676800000,"text":"With the launch of ArchiCAD 20, Randall S Newton visited Graphisoft\u2019s HQ in Budapest to hear how fresh thinking and a powerful partnership are shaping the company\u2019s approach to open BIM and collaboration\nGraphisoft, the maker of ArchiCAD, is a division of the larger Nemetschek Group, which recorded record revenue of \u20ac285.3 million in 2015, up 30% on the previous year.\nThe newest version of ArchiCAD is now out and offers some great new features, but to listen to Graphisoft vice president for Europe, Andras Haidekker, you would think that the company faces major challenges. Of course it does \u2014 but no more than any other forward-looking software company in this market.\nGraphisoft\u2019s leadership team knows that, in order stay relevant, the company has to move BIM forward, not just as a modelling system, but also as an information management system. That\u2019s a challenge when almost half of your potential audience are still scratching their heads and only just starting to think about maybe trying this new BIM thing.\nWith its new ability to work with Rhino data, and additional new features relating to import and use of metadata, Graphisoft says it\u2019s on a mission to put the \u2018I\u2019 in BIM.\n\u201cInformation is power,\u201d ArchiCAD implementation team leader Tibor Szolnoki told me on a recent day-long visit to Graphisoft\u2019s headquarters in Budapest. \u201cInformation is an asset architects can sell.\u201d\nThe \u2018M\u2019 in BIM, meanwhile, is now about management, not models, as I heard several executives say on my trip to Hungary.\nTo put it another way, Graphisoft executives believe that few architects are using the information management capabilities of either BIM or the open IFC construction data model. \u201cBetween the desktop and the mindset, there is a big gap,\u201d says Szolnoki. Increased mandates from government for IFC-compliant documentation is changing things, but it isn\u2019t just customer reluctance that Graphisoft needs to tackle. It also has to sell its customers on new workflows.\nPower of partnership\nHere, Graphisoft believes it has found a great partner in Bjarke Ingels Group (BIG), a progressive architecture firm with headquarters in Copenhagen and a major office in New York. BIG prides itself on being \u2018edgy\u2019 in its technology choices and design instincts. Its principals talk of \u2018sustainable hedonism\u2019 as a guiding philosophy.\nBIG uses Rhino 3D as its go-to CAD for early stage conceptual design.early stage conceptual design. Like other \u2018rock star\u2019 architectural firms already using Rhino, BIG went nuts over the arrival of Grasshopper, an add-on module to Rhino for algorithmic design (also known as \u2018generative design\u2019 or \u2018computational design\u2019, depending on which vendor you speak to, because Bentley and Autodesk also offer these tools).\nBIG\u2019s edgy Scandinavians are also shrewd technologists. They saw that their elegant, algorithmic designs were being tossed over the proverbial wall to the schematics team for complete detailing in BIM, with no live connection between the concept data and the schematics, and knew this needed to change. The information link was as broken as if they had delivered their ideas on paper.\nLuckily for them, Graphisoft had spotted the same problem among its existing client base. In 2015, the company started sharing an early beta release of a live link between Rhino and ArchiCAD that could provide two-way data exchange.\n\u201cThey came to us because it was taking too long for their innovative designs to become construction documents,\u201d says Haidekker.\nBIG found out, and asked to take part in the testing process. To cut a long story short, BIG\u2019s Copenhagen office is now standardising on ArchiCAD 20 as its software for schematics and construction detailing. The read-write of Rhino+Grasshopper data was a big reason, but so was the full information management package ArchiCAD 20 offers.\n\u201cBIG is very design-oriented,\u201d comments Haidekker. \u201cThey wanted to reduce time on documentation. They see with ArchiCAD for BIM they can work three to four months on concept design and then create project documentation in one to two months.\u201d\nInformation management drove BIG into the arms of Graphisoft, and this seems to have triggered a domino effect. Graphisoft\u2019s dealer network is adding Rhino to its portfolio and BIG\u2019s competitors are now looking at ArchiCAD with fresh eyes. \u201cWe are now invited to companies who use Rhino,\u201d says Haidekker. \u201cSome big players, the so-called \u2018blobby architects\u2019.\u201d\nMany architects prefer to use best-ofbreed \u2018point\u2019 tools, as opposed to being limited to a single vendor\u2019s product line. Rhino has become the point tool of choice for initial concept design. McNeel and Graphisoft worked together closely on the implementation of the live link between Rhino+Grasshopper and ArchiCAD to advance schematic design and construction detailing for their mutual clients.\nMore than meets the eye\nFull compliance with IFC 4 in ArchiCAD 20 opens the door to new, more streamlined workflows to keep information live and useful throughout a project. Two new workflows in particular, Design Transfer View and Reference View, allow architects to work more closely with other disciplines while utilising the same BIM. They allow custom, discipline-specific views to be created. The combination of these new data-oriented workflows with the new graphical representation tools Smart Filters and Graphical Favorites (see here for more details) means increased visibility for full data about doors, pumps, curtain walls and all other manufactured or custom-crafted elements.\nArchiCAD 20 also supports the latest iterations of gbXML (for analysis data) and PHPP (Passive House Planning Package) for exchanging data with engineers and consultants.\nTo work better with data that is still being stored in those ubiquitous Excel spreadsheets, ArchiCAD 20 offers Mass Property Value Import, a tool for the semi-automated import\/export of data. It isn\u2019t push-a-button yet, but it does remove the tedium of manual entry of product data into BIM schedules, and reduces the inevitable mistakes that this process so often introduces.\nRandall S. Newton is a business analyst and journalist with more than 30 years experience in the AEC and manufacturing design industries.\nClick here to read our review of Graphisoft ArchiCAD 20.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-richard-harpham-katerra\/","title":"Video: NXT BLD 2019 \u2013 Richard Harpham, Katerra","date":1565049600000,"text":"Silicon and Sawdust \u2013 Deconstructing Construction \u2013 NXT BLD London, June 2019\nDarwin suggested that, \u201cIt is not the strongest of the species that survives, nor the most intelligent, but the one most responsive to change.\u201d And things are changing. The construction industry has been mistrustful and adversarial , often resulting in a messy \u2018blame the customer, don\u2019t trust, don\u2019t share\u2019 environment. However, the language of carpenters, brick layers, electricians and plumbers is being replaced by the language of factories, robots and supply chain management. During this session,we learn how Katerra went from 50 to 5000 employees in under 4 years and understand how it is developing the technologies to keep pace with its meteoric growth.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nTal Friedmanl, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-forges-ahead-with-construction\/","title":"Autodesk forges ahead with construction","date":1542844800000,"text":"Autodesk\u2019s refocus on construction has never been so clearly evident than at this year\u2019s annual user event in Las Vegas. Using all possible technologies, Autodesk is seeking to digitise design and fabrication, while easing collaboration and enhancing the process with machine learning. By Martyn Day\nUnder its previous CEO, Carl Bass, Autodesk doubled down on development in the highly competitive manufacturing area, producing products such as Fusion and embracing digital fabrication. Of course, as this is Autodesk we are talking about, the company covers multiple markets, so this wasn\u2019t the only area which developed \u2014 the ground was laid for cloud and the move to subscription.\nThe new CEO, Andrew Anagnost, now in his second year in the role, has made significant changes, de-focusing from products that are not yet commercial, culling ones that were not revenue generating and investing in the products that are currently being used by customers. The commitment to the cloud has continued and the company has made a number of key acquisitions.\nThe one area Anagnost has always highlighted is the potential for the construction market to revolutionise the way it works, and benefit from digital fabrication and automation. From what we, at AEC Magazine, have seen on our travels this month, he is not alone in this thought. Bentley Systems, Microsoft, Dassault Syst\u00e8mes, Siemens, Trimble and others have all placed construction as their key market. Next year promises to be a real-life Game of Thrones, as we will witness the battle of the CAD giants for the collaboration hub, the construction BIM and the best connection to digital fabrication. Winter is coming \u2014 well it is November, what do you expect!\nAnagnost keynote\nAs if to prove automation will not take all our jobs, the keynote opened with people dancing with a robot arm. Anagnost started with automation and concentrated on how it can help with making things \u2018better\u2019.\nIn 1900 40% of the USA worked in farming; today it\u2019s 2%. The value chain for food extends well beyond farm to market, but we now have more access to food than ever before \u2013 cooking shows, plant-based meats, gluten free everything, etc. Farming is making better use of land, producing great yields and repetitive jobs have been replaced. Now we have organic farmers as well as industrialised food production.\nTransportation is going to change. The US currently has 4 million people employed as drivers, and while in the future this will decrease, there will be more opportunities, moving more people, more efficiently than ever before. How many BIM managers were there ten years ago? Industries change, new roles are created.\nAnagnost took a look at his home town of LA and how it will build necessary infrastructure to host the Olympics in 2028, while leaving a positive legacy for the city\u2019s inhabitants. Anagnost looked at technologies like Autodesk BIM 360, which can provide collaboration with real time feedback, showing constructability, cost and risk, building a Digital Twin. This also works for manufacturing, with everyone working together, looking at the options for a design, with the computer choosing those that are easiest to make, replace etc.\nWe operate today on designing and manufacturing in constraints which are self-imposed. Using automation we can free ourselves from this, as the computer will stop design hazards before they ever happen and customers can design these tools themselves.\nBorduin keynote\nOver the summer, Autodesk lost a number of long standing execs, including Jeff Kowalski, the company\u2019s CTO. His replacement, Scott Borduin, is actually a long-standing Autodesker who once held the role of CTO.\nBorduin talked about how data flow and automation can bring benefits. But historically, data flow is hard when the data is getting bigger and every firm has unique workflows.\nBorduin said that Autodesk, on its journey to the cloud, realised that moving a product like Revit to the cloud would only change the location of the application. It needed to develop a solution that would allow customers to define their own workflows and provide a pipeline for that data to flow and for tools to plug into, even those of its competitors. Part of this is Forge, Autodesk\u2019s development platform, which enables applications to access and manipulate design data held in Autodesk\u2019s cloud.\nDr Erin Bradner, Autodesk\u2019s robotics expert, then gave an example of a concept for a landing vehicle, designed by JPL to investigate one of Jupiter\u2019s icy moons. Using Autodesk Generative design tools, JPL removed one third of the weight in the design, but also designed for additional competing issues, such as manufacturability and cost.\nBy analysing dozens of designs, optimising for 3-axis and 5-axis milling, making the generative design manufacturable with today\u2019s technologies, the JPL engineers were only presented with design options that fitted their criteria. They didn\u2019t waste time looking at designs that failed to meet their performance envelope. This was pretty amazing stuff. Looking at a single part, Autodesk\u2019s generative software can give geometry solutions which fit the manufacturing resource available: additive (3D print), cast, 5-axis or 3-axis CNC.\nMoving on to buildings, Autodesk has been helping Daiwa House Group (daiwahouse.com\/english) solve challenges around Japan\u2019s chronic shortage of build space. The Japanese company builds houses, care homes, hospitals, shopping complexes \u2013 pretty much anything \u2013 and is focussed on automation and BIM to fabrication.\nWhen spaces become available, Daiwa House uses generative design tools to identify the best small pockets of land and develop speculative uses. They then approach the land owners to see if they are interested. The software assesses plots quickly and runs through many iterations until the best design and return on investment is identified. As each project is done, machine learning is applied to enhance future proposals.\nAt the end of the presentation, Anagnost announced that the company will expand from offering product-based training to role-based certifications, so in addition to Revit, customers could also qualify as a BIM manager.\nAnagnost felt that upskilling for careers is more important than how to run a specific individual product. Autodesk will move from offering physical training to more digital training, with a new lefarning engine in development. The company is working with four US labour unions and firms such as Village Capital, Nexus Edge, blendoor and sorcero.\nAnagnost concluded that automation will need new skills, bring better jobs and there will be a scarcity of skills, not a scarcity of jobs. The talk finished with three new terms, Adaptability, Resilience and Community, saying he and the company felt they had a moral and social responsibility to address the challenges and help its customers.\nAEC keynote\nGame engine visualisation and Virtual Reality (VR) is becoming big business in AEC. At this year\u2019s AU, Autodesk announced a strategic partnership with Unity, which will provide the developers of the real time 3D development platform with sub-level access to Autodesk\u2019s applications to integrate its VR tools. This will lead to seamless integration with Unity, allowing geometry, meta data, textures to flow into VR environments without any need for end-user translation.\nEventually the data flow will be both ways with designers being able to change the designs in Unity and have that flow back to update products such as Revit. With a target date of October 2019, the company says that some of the links will be delivered earlier.\nThis is an unusual move for Autodesk, as it has a suite of VR\/AR developers working on tools for its platforms, including Epic Games (Unreal Engine), IrisVR, Enscape, Lumion and Twinmotion. Autodesk also had\/has its own game engine viz \/ VR tool called Revit Live. Selecting one developer to gain special access isn\u2019t usually in Autodesk\u2019s playbook, unless it has bought them. With Unity that is highly unlikely.\nUnity does have two advantages over the others in that, for the last year, it was the first developer to access the Autodesk FBX SDK for Max and Maya, seamlessly sharing assets for in-game iteration. It\u2019s possible the success of this brought Unity closer to the development teams. The other being that Unity supports 28 different interactive target platforms.\nIt seems Autodesk\u2019s efforts in game engine viz and VR for the AEC sector will be phased out, as they were never that competitive to the third party partner applications in the market. This is perhaps the first sign of Autodesk choosing to vacate the market, to concentrate R&D dollars on something they have better competency on, or greater opportunity. However, it does somewhat leave Unity\u2019s competitors at a disadvantage.\nTraditionally Autodesk has bought the one developer to own that space. I can\u2019t help but feel that this is some kind of dereliction of duty of a software developer. However, with ESRI and now Unity, Anagnost\u2019s tenure looks to be different to any previous Autodesk CEO, in that a strategic partnership is sometimes good enough. Unity\u2019s Pro offering does come in at the high side of its competitors.\nJim Lynch, Autodesk\u2019s vice president & general manager, construction business unit, talked about the benefits of adopting a manufacturing-led mind set to defeat the industry\u2019s well documented inefficiencies. Added to those problems, 95% of firms can\u2019t find enough skilled workers, 20% of the industry\u2019s work force is about to retire and 35% of architects are over 55 years old. Part of the answer is deploying new technology and workflows. Autodesk is concentrating on collaboration tools, automation tools and predictive tools to look ahead and identify trouble before if happens.\nA raft of new features related to Autodesk BIM 360 was shown on stage:\n\u2022 A new BIM 360 \u2018site collaboration for Civil 3D and Revit\u2019 was shown, linking civil data with Revit dynamically.\n\u2022 Model co-ordination in BIM 360 can now run automatic clash detection between models.\n\u2022 A new PlaceMe tool lets you place an icon within a 2D drawing and call up a 3D view of where that is in the 3D model.\n\u2022 New timelines for tracking project progress and collaboration.\n\u2022 New approval workflow process in BIM 360, initiate and sign reviews, organised in sets.\n\u2022 Assemble Systems for quantities and building elements in BIM 360.\n\u2022 Cost management model added to BIM 360 linked with change orders.\n\u2022 A new freeform rebar tool to automate mundane tasks.\n\u2022 Customisable check lists for issues, identify causation and resolution.\n\u2022 Dashboards all now in one place.\n\u2022 Reports can be sent to anyone.\n\u2022 Machine Learning to analyse the data.\n\u2022 Predictive analytics for projects now ready for use.\nAutodesk customers \u2014 Mott MacDonald, Lera Consulting Engineers and Paric \u2014 all gave examples of how they used Autodesk technology to innovate within recent projects.\nProject Plasma\nRemember that big article we did on Project Quantum, the future of Revit and the cloud? (tinyurl.com\/ quantum-AEC). Well, after going into stealth mode, the project has been re-branded Project Plasma and is back on track. Our understanding is that some of the technologies that were being developed were going to be so useful to all Autodesk customers and products that some of that technology needed to be examined in a platform context, as opposed just to Autodesk\u2019s Quantum sphere.\nBy all accounts the development work is now looking really good. We hope to bring you a deeper insight into where the project has got to and the basic benefits of collaborative working and connecting design to digital fabrication.\nESRI partnership\nOne of last year\u2019s biggest announcements was Autodesk\u2019s strategic agreement with ESRI to co-operate and work together. The development teams have been hard at work and the fruits have been first seen in the release of \u2018Autodesk Connector for ArcGIS\u2019, which links Autodesk InfraWorks with the ArcGIS online system. This capability improves BIM workflows by directly connecting to GIS data in a more seamless way, versus an import with no ability to refresh if GIS data is updated. All attributes of the GIS data are added to the InfraWorks model, making the 3D context model more robust and accurate.\nESRI will also be linking ArcGIS\u2019s online, vector, image and map data to Civil 3D, Map and AutoCAD. ESRI will add support for Revit at some point too. This ability was demoed earlier this year at ESRI\u2019s user event in London. Both companies are working on an integration between ArcGIS and Autodesk BIM 360. The partnership will be really exciting for those wishing to design in context or access geographically sorted data. There is a special offer for Autodesk customers to have a complimentary subscription to Esri ArcGIS till the end of 2019 (tinyurl.com\/GIS-free-AEC).\nRobots in Construction\nOn day one, AU hosted a special \u2018Connect & Construct Summit\u2019 which featured a whole day of future gazing talk topics, together with hands-on workshops.\nThere was quite a bit of content looking at current experimentation with automation onsite and in factory settings. On the show-floor, some of the work Autodesk has completed with customers was being demonstrated, including a shipping container with two robot arms which had been used to experiment in creating and assembling structures.\nAutodesk is bringing its knowledge from the manufacturing space into the AEC world and working with R&D teams from large architecture firms to experiment and build using new materials and production methods. Autodesk\u2019s Build space facility in Boston featured heavily and it now seems to be recognised by architectural firms in the US as a place to go and try out the impossible.\nAutodesk Forge\nOne of the key messages of the conference was that Autodesk web development platform, Forge, was for developers, dealers, customers \u2014 anyone \u2014 to develop and create their own workflows and products. It\u2019s taken quite a while to get where it is today but there are now a number of examples of customers and developers who have deployed Forge components to share, analyse or collaborate their designs.\nDevelopers have wanted to run with some of this technology but have been frustrated with the rate of development of certain components, together with a meandering business model. A key component being Revit.io \u2013 a cloud version of Revit which developers can call on to access Revit functionality. When this stabilises, I expect we will see a raft of commercial applications based on Forge.\nAutodesk has hired ex-Google cloud platform guru Sam Ramji as VP cloud platform, to come in with deep cloud experience to stir up Autodesk\u2019s development and ensure Forge is an enterprise level, robust cloud API. Ramji was incredibly impressive on stage and can be watched here (tinyurl.com\/ramji-AEC)\nRebel dealers\nIn recent years, Autodesk has been reducing the number of dealers it has worldwide and the margins on product sold and subscriptions. The net fallout has been approximately 600 resellers have either sold up or closed up shop. As Autodesk drives the subscription model and increasingly goes direct, the days of the small Value Added Reseller (VAR) have been numbered.\nWhat we have seen is a number of very large resellers have continued, usually offering expanded services, but many of these are now struggling with the loss of margin on commodity product, especially AutoCAD LT. One large and respected European reseller and developer, Graitec, broke ranks just before the show to offer its own, in-house developed IntelliCAD-based clone called Advance CAD, as an alternative to AutoCAD LT for DWG editing at just \u00a3300. (graitec.com\/advance-cad). This is a significant departure for a channel that has traditionally been very loyal. We expect others to follow.\nPerhaps the 2D drawing market will, after decades of domination, now see increased competition, with perpetual licences costing a fraction of a full equivalent Autodesk subscription. There are a large number of mature DWG work-a-likes available including BricsCAD, Graebert ARES, Dassault Syst\u00e8mes Draftsight (created by Graebert) and ZWCAD.\nWe understand that Autodesk\u2019s planned significant price hike in maintenance to \u2018persuade\u2019 customers that \u2018Subscription makes more sense\u2019, comes into effect in March 2019. This will see 20% added to the cost of maintenance for non-subscribed applications. In general, this is to be seen in the context of a general trend of increased cost of ownership for Autodesk\u2019s products, for those outside of subscription all the way to those with enterprise licences.\nIt will be interesting to see if there is any dent in the subscription rate with customers seeking alternatives or if the stickiness of customers and their AutoCADs proves durable.\nConclusion\nAutodesk University this year was all about BIM 360 and Forge. A monumental effort is being put into both those elements as Autodesk pushes itself to deliver enterprise quality platforms for collaboration and application development. Just as we were going to press, Autodesk announced a breathtaking $875 million acquisition of PlanGrid, a developer of construction productivity software that delivers blueprints, project docs, markups, RFIs etc. on synchronised mobile devices. I suspect this is the biggest acquisition in Autodesk\u2019s history. If there was any doubt about the company\u2019s focus on construction, here\u2019s 875 million reasons to believe them.\nThe construction market is certainly heating up with big developers hoping to get a slice of the market. Oddly, everyone seems to be going for the same core areas \u2013 cloud-based management, co-ordination and collaboration. I guess when everyone identifies inefficiencies in a federated or fractured process, it\u2019s the go to destination to look for a solution. However, there will also be battles over the construction BIM and how to connect design to digital fabrication as well as interoperability \u2013 CAD companies have inherently failed to be truly open while preaching that they are. As the battle moves from desktops to the cloud, it will be interesting to see if access to APIs and data is or isn\u2019t shared amongst competitors.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/interview-huw-roberts-bentley\/","title":"Interview: Huw Roberts, Bentley","date":1165449600000,"text":"AEC Magazine\u00dds publisher, Martyn Day, managed to get some talk time with Bentley\u00dds Global Marketing Director, to get his opinion on BIM, IFCs and current vents in the industry.\nThere has been a radical change in the interface of MicroStation. How has this altered the building vertical products that are based on the new version?\nHuw Roberts: These new versions of Bentley Building benefit dramatically from the new XM 3D interface. There\u00dds also a load of enhancements to the vertical products functionality and easy of use, as we use the task-based interface to great effect. So, there are some domain specific changes to the interface that are contextural in nature, they change with each discipline too. This simplifies and organises the design process for users.\nIf a company has a particular way they want to design in one for the disciplines, they can develop their own or modify the product to match their workflow or standards. With XM, the interaction with the software has been made much simpler, which is good. If you look at Bentley Architecture or Structural, or any of the Bentley verticals, in relation to Autodesk\u00dds Revit, Revit is easy to use and understand and navigate but kind of falls down when you go beyond its capabilities, which you do fairly quickly. The flip-side is that the MicroStation-based applications can pretty much create anything you can think of, but they are hard to get up and get started on. I think the new MicroStation interface, the new task based-design and heads-up technology, radically addresses that. It\u00dds basically a whole new interface for our vertical products.\nThe second thing is that while our integration between all our applications is already pretty good, that\u00dds improved again, with a lower entry point for document management. We have bought a number of analysis companies and are continuing to develop partnerships with other analysis products in all the disciplines and we have also increased our support for IFCs up to 2\u00d73.\nAutodesk has been talking a lot recently about IFC compliance in Revit and the decision of the General Services Adminsitation (GSA) to require submittals in IFC format. What\u00dds your take on Interoperability?\nHR: The differences in approach to IFCs really reflects the differences between the companies. Interoperability is basically the ability to take information from software, one datasource, one person, one activity to give to someone else in a way that they can use it for their own purposes and maybe bring it back. So, it\u00dds an exchange based approach. You should be able to do that in a predictable and managed way. Now that\u00dds good and has been very important to Bentley for a long time, but don\u00ddt mix with that the concept of integration as integration is better than interoperability, because \u00d9interoperable\u00dd is only a snapshot of your data from your native environment and giving it to someone else as a reflection at some point in time as to where you were. All those exchanges have to be managed and synchronised and, whichever way you do it, you are working in a disconnected way.\nIn an integrated workflow you can act on and work with the information as it appears. In the Bentley BIM applications, if you are an architect working with a structural engineer and a building services engineer, obviously you are all using Bentley applications to do that and working on your own data but all that information is shared, viewable and editable (within reason) by anybody on the team using those applications. You can actually fire up Bentley Architectural, Structural, Mechanical, Electrical all at the same time, within one session of MicroStation or use reference files and all the data and information is accessible. They all speak the same language. The support for IFCs that Revit has, is really only a small part of the definition. The General Services Administration (GSA) here in the US (a government body) required that preliminary designs came with an IFC model and that was really just spaces and a few other elements, it\u00dds not a very robust model definition at all. What Autodesk has done is included an export, and only an export, and include only that definition. Autodesk has no IFC for structural, mechanical or details of architectural design and they don\u00ddt have IFC import. We offer the full definition of IFCs in all of our building disciplines.\nThe IFC as a technology means of interoperation is now getting pretty good for exchanging information and there are lot of downstream analysis tools that can use the IFCs now, so it\u00dds become a really well adopted technology and that\u00dds really good for the industry, no one software vendor can dominate. The latest release has really taken it to another level. And the IAI has gotten really smart about promoting it to the architects and their whole \u00d9building smart\u00dd campaign has taken their message, made it less technocratic and more understandable.\nI was quite amazed to see the demonstration of Autodesk Revit Structural running as a separate database to the Architectural Revit model. I thought the whole idea about BIM was about a single database?\nHR: I actually think it\u00dds hilarious. Autodesk talks out of both sides of its mouth. On one side they say, the only way to do BIM is to have a single database. Revit\u00dds whole concept is that you all have to be in one database and the only effective way to access that database is through that tool. So, you can\u00ddt deal with a Revit Building model and have it aggregate information from various sources, like AutoCAD, MicroStation and SAP and Excel. Yet at the same time, if you are talking about another discipline, like mechanical, there\u00dds a separate database and you can\u00ddt put the two together anyway. I think they are finding that the federated model is the only way to go \u2013 a distributed team using different tools, running different information sources. There\u00dds a whole lot of information that\u00dds needed in buildings that has nothing to do with CAD.\nSo what\u00dds happened to the Single Building Model concept?\nHR: I think the industry is a victim of old marketing. The notion of a single building model is because people want a place to go where everything is correct and up to date; everyone who needs to get to it can get to it. How that\u00dds technically managed, whether that be one file, or a number of federated files, isn\u00ddt really what people care about, it\u00dds the result. I think in order to achieve that, the \u00d9SBM\u00dd or whatever you want to call it, it has to be a federation of databases, files, sources and connections to support the nature of the industry.\nIn our Bentley architecture you can do everything in one file if you want to \u00b1 that\u00dds a single building model, and it can get huge. That\u00dds the approach Revit takes. What happens if you are in different cities? What if you use different tools? It has to be federated and you can do that with our integrated tools.\nAutodesk is quoting some serious numbers for Revit sales, upwards of 140,000 seats?\nHR: You have seen that game before, the Revit Series, is a bundle of AutoCAD and Revit. I think that\u00dds what that is. They are including educational in there too. We could do the same thing but throwing in all our seats but that\u00dds not a useful fact, it\u00dds not realistic in the market. There are firms that have been doing BIM very consistently and proving the technology, like BDP and Whitby Bird in the UK and the rest of the market is doing it at a slower place. Almost everyone is looking or investigating BIM. There is virtually no demand that doesn\u00ddt at least have a pathway or roadmap to BIM. A lot of companies are realising that a move to BIM is a change, it\u00dds all about the discipline specific capabilities.\nEverything that we can see is a neck and neck race in BIM, between Bentley and Revit. We have some AutoCAD users coming over when they go to BIM and in fairness we have some mixed shops that are trying Revit too. But we have Revit customers coming to us after they used it, and they didn\u00ddt like it so came over to our side of the house. There is a horse race in BIM and it is us two. Graphisoft seems to be looking at the construction space, their worldwide marketshare is less than 2% now, but the CEO there, Dominic Gallello is doing a good job, he\u00dds turned them around and is finding new outlets for the company.\nWe have a lot of online resources and we are pursuing events and activities where we can tell the story and that\u00dds working. Our BIM applications are accelerating and it\u00dds getting out there to the AutoCAD base.\nBentley has purchased a number of solvers and analysis packages for structural design over the years. Autodesk\u00dds recently purchased its first, a French analysis company called Robobat. What\u00dds your opinion on this?\nHR: The notion of using BIM and tying it into analysis is one of the great payoffs of the approach. I think it\u00dds interesting because Robobat is a French company that is well adopted in France and Poland, a little bit in South East Asia and South America, but almost not on the map in North America. It\u00dds fairly high-end analysis for purists, it\u00dds more like STAAD, it can handle the complex stuff well but not a great match for your typical structural engineer, doing concrete masonry and steel.\nIt\u00dds interesting that Autodesk bought them. We are guessing Autodesk will probably fold this into Revit Structure and it\u00dds clear they were having trouble with the links between the program and the analysis applications. If you look at how the links work, Revit Structure doesn\u00ddt have sloping columns, sloping beams, connection types, so the analysis programs don\u00ddt know what to do but there\u00dds nowhere there to put it. I think both sides were having a lot of frustration, so Autodesk wanted to bring some of it in-house.\nI know Autodesk makes the pitch that structural engineers use lots of different applications and Revit Structure is a common interface but I think that\u00dds off track. Structural engineers in general, \u00d9don\u00ddt do CAD\u00dd. There are engineers that do analysis and design and then there\u00dds production teams that do the documents or model work. Engineers use STAAD etc and they are thinking about totally different things.\nRevit structural is clearly oriented at making a drawing. They are tying to get into the Structural engineers through the documentation space, we are going through the tools that the actual engineers use and connect those to whatever is required, that\u00dds more of a reflection of what engineers want and how they work today.\nAEC Magazine is a bit of a fan of Bentley\u00dds Generative Components (GC). How could GC technology impact BIM?\nHR: The connection of GC to the rest of our vertical programs is really strong, thanks to the underlying architecture of the XM version. GC is now closely integrated with BIM applications, so the model being manipulated by GC could be a Bentley Structural model or a Bentley Architecture model, not just \u00d9dumb\u00dd geometry. So you could have a GC design that is changing the curve of building and heights of floors and is also driving the steel frame. This could be linked to analysis programs so you know what impact your changes had on any of the disciplines. You could have the rules of GC linked to a discipline specific rule, other than just geometry. Some of those constraints might be structural properties of the material or size of glass panels you can buy, so with BIM, that takes you beyond form finding to making real construction decisions by using GCs. There\u00dds already a generation of users out there using it on real live projects.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/tekla-uk-bim-awards\/","title":"Tekla UK BIM Awards","date":1352678400000,"text":"Tekla\u2019s UK BIM Awards, an annual celebration of collaborative workflows and model based processes, honoured some impressive projects this year.\nWinner Arup \u2014 National Museum of Qatar\nThe National Museum of Qatar, in Doha, is the flagship in an important series of cultural and educational projects being commissioned by the Qatari government. It is currently on site, having been in design since 2008.\nArchitect Ateliers Jean Nouvel drew inspiration from the desert rose, a crystalline formation found below-ground in saline regions of the desert. When imagined as a building, the result is a four storey, 300m by 200m sculpture of intersecting disc shapes up to 80m in diameter.\nThe structural solution settled on radially and orthogonally-framed steel trusses, supporting fibre-reinforced concrete cladding panels to create the required aesthetic and performance of the building envelope.\nThe key challenges for the design resulted from the highly complex geometry of the disc interaction. No two discs are the same; no two discs intersect each other in precisely the same way. The galleries and other key spaces in the building are created by the interstices between the discs; any alteration to the architecture involves moving discs and thereby moving the structure within the discs.\nThis has led to an evolution of systems and processes required to handle, manipulate and develop geometric ideas from the architect, and establish engineering solutions, before communicating them in their most useful form to the wider community.\nStructural modelling\nThe structural team developed a parametric Generative Components script-based tool to automatically create wire-frame geometry in the correct position within the architectural Rhino envelope. The basic wire-frames were further populated with property and loading data using spreadsheet-based automation.\nCustom-designed spreadsheet macros were further used to combine separate disc models into larger combined models for structural analysis. Element strength checking was also automated as far as possible, to make practicable the design of the 250,000 separate steel elements.\nThe analysis models were used as the basis of the production Tekla model, directly translating geometry, section data and key annotation such as disc-to-disc interface nodes.\nThe Tekla model was issued to the contractor at tender stage for accurate pricing. The model has been kept up to date and reissued as the design has been completed, and contains the live drawing files needed to communicate information in 2D.\nDuring the site phase, the contractor has adopted and developed this model to a fabrication level of detail. Requests For Information (RFIs) raised result in either a sub-model being issued, which is incorporated in the contractor\u2019s live model, or a proposal from the contractor incorporated back in the original design model. In the latter instance, the proposed model is exported and used directly to modify the analysis and design, to confirm acceptability. The design model makes use of Tekla phases and classes to keep track of issue status of recent updates.\nAlso at tender, the Tekla model was used to interrogate concrete volumes in order to produce accurate steel reinforcement figures.\nSuperstructure steelwork connections\nThe scheme for the building involves many intersecting steel framed discs and therefore results in numerous connections between discs with largely different geometries. Advice was taken from external steel fabricators and their design teams (who might ordinarily carry out the detailed design of such connections), to establish an effective normalised solution where possible.\nThough many bespoke connections could not be avoided, a significant number could be based around a connecting circular hollow section (CHS), which could tolerate a large range of incoming member arrangements and thus standardise the supporting steel arrangement. In order to communicate this to the contractor, a number of connections were designed, detailed and illustrated on drawings with 3D perspectives and 2D sections. The nature of the each connection was such that the geometry and therefore design could not properly be understood without interrogation of the intersecting members in three dimensions, for which Tekla was invaluable.\nWithin discs, families of connections were defined to cover typical arrangements such as the bracing connection to the chord of a truss. Although more simple than the connections between discs, it was also useful to understand these in 3D to investigate how the subtle changes in geometry between discs altered the nature of the connections. Typical internal connections were again represented by both 3D and 2D information on the drawings with tables of parameters such as plate thickness and number of bolts defined for a range of section sizes and incoming member angles.\nProject participants\n- Project owner: Qatar Museums Authority\n- General contractor: Hyundai\n- Architect: Ateliers Jean Nouvel\n- Structural design: Arup\n- Mechanical engineering: Arup\n- Steel detailing: Arup and Eversendai\n- CIP\/reinforcement detailing: Arup\n- Other structural detailing: Arup\n- HVAC\/MEP detailing: Hyundai\n- Steel fabrication: Eversendai\nPublic choice winner Bourne Engineering \u2014 Reading station\nThe huge redevelopment of Reading station involved a collaborative approach with Lakesmere (cladding) for an alternative proposal for the design, manufacture and installation of canopies and their associated support members using an offsite, modular approach.\nThe canopies at Reading station are based on the integration of support cross beams, gutter beams and purlins. The main cross beams were fabricated from UB sections, and bolted together with intermediate beams to suit specific lifting points on the module. Purlins & soffit rails were installed between the canopies to support the Kalzip and soffit sheets.\nSome 450 number canopy modules (3,000 pieces of steelwork), spanning across 15 platforms and 1.3km were designed, detailed and co-ordinated with the design team. With each module featuring slightly different geometry, following the platform radii, the steel modules had to be matched fabricated. This was achieved by the trial construction of the canopy modules on the production line.\nInteroperability\nThe importance of collaborative and co-ordinated work was essential in ensuring the primary steelwork and the secondary steelwork (cladding) was set out correctly ensuring best fit when interfacing with other works (transfer deck bridge). Canopy fixings to the transfer deck were co-ordinated by the exchange of IFC files between the two software applications, enabling efficient workflows and ensuring best fit. The complexity of the purlin\/soffit rail setting out was co-ordinated by the exchange of DWG\/DXF files imported\/exported between Tekla and AutoCAD.\nReducing site labour\nThe modular canopies were constructed on a pre-planned production line, where primary steels\/secondary steels (purlins\/rails) and cladding could be installed at varying stages and checked for alignment prior to being dispatched to site \u2014 all of which was planned\/designed\/detailed with Tekla.\nOther challenges faced were the logistical difficulties of delivering the steelwork to site in a safe manner while maximising the number of deliveries to speed up site installation. The canopies were delivered to nearby Forbury, before being craned by RRV over the track and freighted the final mile to the station platforms.\nWhere possible, deliveries took place at night to ensure the highest levels of passenger safety were maintained. Close co-ordination between the station\u2019s signalling staff and Bourne\u2019s project team also ensured disruption was kept to a minimum, which was vital given Reading station is a major UK transport hub. Trailer support frames were designed\/drawn in tekla to enable this to be accomplished.\nThe modular approach reduced site labour by approximately 3,000 site man hours in favour of reduced work periods carried out in a factory environment, thereby capturing benefits of increased productivity, improved logistics and quality control. The reduction of the overall construction programme was achieved by running the factory activities concurrently with site build activities.\nIntegrated design\nTo the north and south of the platform canopies at Reading station, the Western gate line building and the Northern entrance building were constructed. Both structures were of huge complexity comprising of large bent jumbo sections supporting the roof and vast amount of secondary steels that formed a geometrically challenging support grillage, to the varying cladding systems on both the roof and elevations.\nThe Jumbo sections on the roof had to be sourced from Japan, which meant adhering to a programme was of upmost importance in ensuring these steels arrived in the UK on time ready to be bent\/fabricated. Tekla Structures provided Bourne with the necessary tools to enable accurate setting out for fabrication and erection purposes.\nThe key to success with both these buildings was down to the collaborative approach between Bourne and Lakesmere that ensured the co-ordination of the secondary steel sections (via model exchange) resulting in an integrated design solution that met the client\u2019s requirements.\nProject participants\n- Project owner: Network Rail\n- General contractor: Costain & Hochtief (JV)\n- Architect: Grimshaws\n- Structural design: TATA\n- Steel detailing: Bourne Engineering\n- Other detailing: Lakesmere\n- Steel fabrication: Bourne Engineering\n- Other participants: Lakesmere","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/elecosoft-achieves-supplier-status-on-g-cloud-11-framework\/","title":"Elecosoft achieves supplier status on G-Cloud 11 Framework","date":1563494400000,"text":"Public sector bodies can now choose Powerproject SaaS, IconSystem and ShireSystem\nElecosoft\u2019s cloud-based project planning, project management, BIM and maintenance management software and services have been listed on the G-Cloud 11 framework, the Crown Commercial Service\u2019s digital marketplace.\nIt means public sector bodies are now able to choose the following Elecosoft software tools: Powerproject SaaS, a cloud-hosted version of Elecosoft\u2019s Powerproject software for project planning; IconSystem, a cloud-based BIM system; and ShireSystem, the maintenance management software.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE\nRelated articles:\nSamsung launches 2TB Solid State Drives (SSDs)\nNHS Foundation Trust creates smart estate with digital twin\nOn the right track\nMassMotion\nNEWS: Karalit CFD to host AEC and built environment webinar\nCAD in the cloud\nBentley Systems brings carbon assessment to iTwin Experience\nSolibri signs long term deals with major AEC firms\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-sandeep-gupte-nvidia\/","title":"Video: NXT BLD 2019 \u2013 Sandeep Gupte, NVIDIA","date":1565049600000,"text":"Re-imagine cities of the future with next gen visualisation \u2013 NXT BLD London, June 2019\nExciting new technologies have emerged that are transforming AEC design workflows. From real-time ray tracing and engineering simulation, to virtual reality and AI-enabled applications, visual computing is becoming increasingly critical to AEC firms for optimizing and speeding building and infrastructure design. In this session, NVIDIA describes how these technologies are empowering users and helping the industry re-imagine cities of the future\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/stewart-international-forms-new-land-projects-division\/","title":"Stewart International forms new land projects division","date":1081900800000,"text":"Stewart International has formed a new Land Projects Division to work with municipal and national governments on developing real estate infrastructure including land registry, mapping and taxation systems. Stewart International (Stewart Information International, Inc.) is a wholly owned subsidiary of Stewart Information Services Corp.\nThe new division, led by Director Tevfik Turel, combines Stewart International\u2019s former projects division with the international mapping group from Stewart Geo Technologies, another Stewart subsidiary. The division will continue Stewart International\u2019s pursuit of projects to consult with developing countries on their real estate infrastructure. The division also will work with municipal and national governments to improve land registry, cadastral mapping\/GIS and property taxation systems.\nThe Land Projects Division currently is working on the development and implementation of Stewart International\u2019s new e-government technology for land information management, Landfolio. This solution provides governments a flexible automation toolbox addressing the three core components of land information management: land maps, ownership information, and property taxation.\nLandfolio can operate as a fully integrated land information system, or depending on the needs of the local government, each of its three components \u2013 land cadastre, land registry and land tax \u2014 can operate as a stand-alone application or in combination with any other component.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-trimble-crewsight-to-track-construction-site-workers\/","title":"NEWS: Trimble CrewSight to track construction site workers","date":1424995200000,"text":"Automated, RFID-enabled site tracking system helps contractors improve visibility into the costs of labour on construction projects\nIt may sound a touch Orwellian, but Trimble reckons its new CrewSight System will help contractors increase efficiency, safety and security on the construction site.\nThe system can be used to automate the management of workers entering or leaving a site, allowing tighter tracking of progress to pinpoint discrepancies and identify opportunities to optimise the work process. Trimble says knowing who and when workers are on site allows project managers to validate project costs for labour, provide proof of compliance and maintain safety on the site.\nThe system consists of three main components: Trimble CrewSight, the application that enables contractors to collect, view, analyse and store the workers\u2019 data; a choice of hardware sensor devices that collect information for worker entry and exit on the site; and an enrollment station and RFID-enabled badges that allow companies to verify and enroll each worker in the system prior to entry, eliminating the need for on-hand personnel to check-in workers manually.\nDepending on what type of hardware sensor devices are chosen for each site, contractors have an option of using guard-assisted worker check-in, worker self check-in using a manual check-in\/check-out device called \u201cVIS Box\u201d or hard access using a turnstile or vehicle gates. This access control is designed to prevent unauthorized workers from being on the site or in areas of the site without the proper credentials. In addition, CrewSight allows contractors to monitor every zone of the site and supports monitoring multiple sites from the same user interface.\nCrewSight provides up-to-date data that is accessible from any Web-enabled device and will be integrated with existing scheduling software such as Trimble\u2019s Prolog project management and scheduling software to enhance the implementation with real-time field data.\n\u201cControlling labour costs is a major consideration for all construction managers,\u201d said Mark Sawyer, general manager of the GC\/CM Division of Trimble Buildings. \u201cTrimble CrewSight provides accurate reporting on the actual hours contractors and subs are working to ensure that projects stay on track. The planned integration with scheduling software, such as Trimble Prolog, gives a more complete picture of the site, the workers and the entire project.\u201d\nTrimble CrewSight is available in North America through the BuildingPoint network and other Authorized Distribution Partners of Trimble GC\/CM solutions.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-revit-linked-to-trimble-connect\/","title":"NEWS: Revit linked to Trimble Connect","date":1451952000000,"text":"New plug-in gives Revit users direct access to Trimble Connect, the collaboration platform based on GTeam by Gehry Technologies\nTrimbleConnect-for-Revit from AMC Bridge provides bi-directional data exchange between Autodesk Revit Architecture and Trimble Connect server.\nThe Revit plug-in enables Revit users to collaborate with project participants and exchange project information.\nUsers can browse documents in Trimble Connect from Revit Architecture, select and Import models to a Revit session in IFC or RVT file formats and publish and update active Revit models on Trimble Connect server directly from the Revit Architecture UI.\nThe plug-in supports Autodesk Revit Architecture 2015-2016.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nV-Ray 7 to get support for gaussian splats\nNEWS: ClearEdge3D acquires construction verification software firm\nNEWS: Collaboration to enhance building energy efficiency\nTopcon launches MC-Max machine control solution\nKeeping your data safe\nProject Soane: design viz winners\nKolega harnesses AI for site feasibility\nChaos and IES to 'democratise building performance analysis'\nAdvertisement\nAdvertisement\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/applanix-introduces-pospac-air-v4-4\/","title":"Applanix introduces POSPac AIR v4.4","date":1171843200000,"text":"Richmond Hill, Ontario, Canada, 17 February 2007 \u2013 Applanix has introduced POSPac AIR Version 4.4, the latest release of the company\u2019s post-processing software used to extend the accuracy potential of the POS AV system.\nDesigned to maximize data quality and optimize workflow from project planning through to project completion, POSPac AIR 4.4 is specifically configured for the airborne environment and is compatible with a variety of aerial sensors.\nNew processing features are a big part of the new POSPac AIR Version 4.4 release, including a new dual frequency single point carrier phase processing algorithm for Precise Point Positioning (PPP). PPP is a Global Positioning System (GPS) processing technique that uses precise ephemeris data to help solve for the floated carrier phase ambiguities without the need for base stations. This is ideal for remote areas that do not have dense reference station coverage. Typical accuracies range from 10cm to 50 cm, depending upon mission conditions.\nPOSPac AIR Version 4.4 also includes updated integer GLONASS processing capabilities that enables both GPS and GLONASS satellites to be used in Kinematic Ambiguity Resolution (KAR) for faster and more reliable fixes with cleaner trajectory processing. GLONASS processing allows a user to process GLONASS L1 and L2 observables along with the L1 and L2 GPS observables to produce the final integer carrier phase DGPS solution.\nFor airborne surveying, this can provide an advantage by reducing periods of poor dilution of precision (DOP), particularly at high latitudes, thereby extending the survey window for highest accuracy surveying.\nPOSPac AIR 4.4 combines maximum direct georeferencing accuracy with the exact tools needed for efficient application to remote sensing, LIDAR, and photogrammetric map production.\nPOSPac AIR 4.4 is expected to be available early in the second quarter of 2007 through the Applanix sales network.\n\u2013 About Applanix Corporation\nEstablished in 1991, Applanix (www.applanix.com) is a wholly owned subsidiary of Trimble, develops, manufactures, sells and supports precision products that accurately and robustly measure the position and orientation of vehicles operating in dynamic environments. Applanix\u2019s Position and Orientation Systems ( POS) are used in a variety of applications, including road profiling, GIS data acquisition, aerial surveying and mapping, railroad track maintenance and seafloor mapping.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/news-spotscale-s-global-service-captures-buildings-in-3d-using-drones\/","title":"NEWS: Spotscale\u2019s global service captures buildings in 3D using drones","date":1499644800000,"text":"Firm uses network of local drone operators to capture 3D models for urban planning, inspection, or design development\nSwedish firm Spotscale has launched a web service where AEC professionals can order a detailed 3D model or \u2018spot\u2019 of anywhere in the world. Customers simply draw an area of interest on a map to get an estimated price point, which is then followed by a formal quote. A local drone operator, selected by Spotscale, will take care of capturing the imagery, which is used to create the 3D model.\nSpotscale works with local drone operators with multi-copters and high-res cameras to sweep along every street, fa\u00e7ade and roof to photograph all angles possible of the area. The imagery is then processed on the Spotscale cloud, where algorithms optimised for buildings and urban environments are used to calculate detailed 3D models. Models can be delivered in a varity of formats suitable for iPads, touchtables, CAD applications, 3D printers, AR- and VR devices.\nThe Swedish firm also offers a processing service for firms that have their own camera and drone. Customers upload their images and Spotscale will deliver a 3D model in multiple resolutions as well as provide tools for construction, inspection or urban planning.\n\u201cWe have been pushing the limits of drone capture, processing methodology and visualization in order to achieve our 3D models. Today it still requires some drone flying skills, but when drones are allowed to fly autonomously this kind of urban drone 3D mapping will explode\u201d says Ludvig Emg\u00e5rd, Founder and CEO of Spotscale.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/bim-what-your-government-wants\/","title":"BIM: what your government wants","date":1327708800000,"text":"The message is out: Building Information Modelling (BIM) will be mandated on all UK government building projects by 2016. AEC Magazine reports on what that means for the construction industry.\nWhatever your view of Building Information Modelling (BIM), it seems that the UK Government has bought into the vision hook line and sinker. As a result by 2016 all major projects carried out with public money will have to have a BIM-relevant process and data delivered in additional file formats.\nThe BIM mandate is to be phased in over five years beginning this summer. At the moment there is no preconceived notion as to a limit in the value of projects that it is not applicable to, but with such a major client demanding a change in working methodology the idea is that it will become the industry-wide norm for all projects. Herein lies the fear for the vast majority of architects that have not yet bought into a 3D collaborative approach.\nBoth Autodesk and Bentley ran BIM conferences recently. Autodesk repeated its excellent day of talks from the previous year, although this time the Autodesk messaging was stronger and the \u2018warts and all\u2019 presentations seemed to have less warts than before.\nBentley\u2019s first BIM event was smaller and not necessarily 100% about BIM, with an example of the benefits of Generative Components thrown in and a comprehensive description of the Bentley product suite, which actually was very useful.\nBoth events shared two industry speakers, chief construction adviser to the government Paul Morrell and chief information officer, Business Systems, at URS Scott Wilson and chair of the BIM working group Mark Bew. While Mr Morrell put the government\u2019s decision in context, Mr Bew spelled out the process of deliverables and the timeline for standards, guides, classifications documentation and delivery, commonly referred to as the B\/555 roadmap.\nMorrell\u2019s home truths\nPaul Morrell\u2019s guiding statement is that BIM is not about a specific technology or product, but a process to give clients all the data that is of use to manage the facility after hand over.\nTo do this the client must be very specific about what the deliverables are and the government is currently defining what information it wants at each stage of design and construction and in what format. These protocols will not stipulate a specific BIM system or proprietary file\/database format.\nMr Morrell is a great speaker and frames the decision for mandating BIM by going back to first principles. For the UK that started with the financial crash of 2008. David Smith, chief economic reporter at the Sunday Times, ran an article at the time stating that, according to the UK Treasury, it would take until 2031 \u2013 2032 to get back to borrowing figures that were previously known as prudent (40% of GDP). That would only happen if a generation of excessive debt could be wiped out \u2014 and this was before the problems in the Eurozone countries kicked off.\nIn a reference to the popular refrain \u2018up shit creek without a paddle\u2019, to describe a hopeless situation, Mr Morrell said that the government was looking for \u2018paddles\u2019 and that BIM was a big paddle.\nHowever, despite this healthy dose of cynicism, Mr Morrell does believe that there are opportunities through mandating BIM. Population growth is demanding investment in infrastructure globally. Many of these projects will need to manage and efficiently use finite resources \u2014 offering potential work for UK companies.\nWe are living in a world of \u201ctoo little cash and too much carbon\u201d, according to Mr Morrell. We have to \u201cthink our way out of these new metrics of design and try new ways of working\u201d.\nThe UK undoubtedly has world-class design in the AEC space and the government considers that this should be part of an integrated offer of construction, as opposed to the traditionally fragmented approach to design and construction that we have suffered from for far too long.\nMr Morrell typified the UK approach as being \u2018the least possible work, at the last possible moment at the highest cost\u2019 and that clearly is not fit for purpose\nMr Morrell typified the UK approach as being \u201cthe least possible work, at the last possible moment at the highest cost\u201d and that clearly is not fit for purpose.\nWe now need to take leadership at delivering projects that \u201cmodernise the world\u201d, he said. This means we have to do the basics well and be able to repeat it \u2014 a major challenge. Mr Morrell cited the London Olympics as an example of how it could be done. However, he tempered that praise by saying that other countries do the same, more frequently, citing a major road repair in Japan after the tsunami, which only took six days \u2014 less time that the project scoping investigation would take in the UK.\nMr Morrell said that the UK still takes a \u201cvery 19th century\u201d hierarchical approach to projects, where everyone knows their \u201cplace\u201d, but disputes are commonplace and data is not shared, is lost in the process and ultimately not delivered to the client for downstream use. Because of this, Mr Morrell sees BIM as being a way to fix the process as it forces collaboration and data sharing and turns the hierarchy upside down.\nOn top of that, clients are fed up of getting buildings that do not perform as they were original specified to do. In some studies, 40% of new buildings still miss their original efficiency targets. It seems very little thought is given to maintenance in the design phase, increasing the cost throughout occupancy. This can equally be applied to existing \u2018as built\u2019 assets \u2014 with 25 million homes in the UK needing energy retrofitting by 2050.\nMr Morrell admitted that the government needs to build but has no money and that the industry wants to build but has no work. That should lead to a \u201cgreat moment of change\u201d, according to Mr Morrell, who urged the industry to take full advantage.\nThe government sees BIM as a way to drive efficiency during the economic downturn so that UK companies are best placed in five to 10 years time to compete on the international stage. \u201cBIM is hard\u201d recognises Mr Morrell. \u201cBut never mind how hard it is to build a digital model and to get all the uses out of it and to get the systems to talk to one another \u2014 it must be surely easier than what we do now, which is a 1:1 model, out in the wind and the rain, loaded with risk.\u201d\nPrevious governments have tried to push the industry to change its ways, most famously with the 2004 Latham and 2008 Egan reports into driving construction industry efficiency improvements, and spending 20 million in the process. But nothing has changed.\nMr Morrell decided that, in order to change a business you have to change its drivers, not just preach to them; so the decision to adopt BIM moves the debate from preaching to practicing. Now, both clients and practitioners have to change and Mr Morrell\u2019s task is to build the structure to support this with steering committees from within the industry, meaning that the private sector is assisting in deriving government policy.\nHowever, Mr Morrell did admit that this move to BIM was akin to JFK\u2019s \u2018We are going to the moon\u2019 speech \u2014 we have a destination, but still have to work out a way to get there.\nMark Bew\nMark Bew, chair of the BIM working group, is the \u2018details man\u2019 in the drive to get UK practicing BIM in the next four years\nThe working group actively chose to not define what BIM is; but to work out what the needs of the government were in mandating BIM. It was decided that BIM is all about data and where data could be shared, and commercial targets.\nThe group is not allowed to specify any one technology from a vendor for BIM software. The process has to be open and observe the various BS standards. However, the group has carried out benchmarks on BIM technologies and if significant benefits were not seen then the technology would be retested.\nThe government sees BIM as a way to drive efficiency during the economic downturn so that UK companies are best placed in five to 10 years time to compete on the international stage\nMr Bew believes we are very much at the thin end of the wedge when it comes to fully understanding the working processes that will be required in four years\u2019 time.\nThe first move is the mobilisation of the industry to consider the task in hand, then in phase 1, the early adopters deal with the cultural issues, derive consistency through experience and become familiar with the \u2018packaging\u2019. Here COBIE, a US spreadsheet formula, is the main BIM deliverable on top of traditional drawings.\nPhase 2 will take us to the five-year mark and will include delivering a data repository of some kind with COBIE enriched data on live projects.\nPhases 3 and 4 call for web data and process driven deliverables, with a possible ISO standard for BIM.\nMr Bew\u2019s roadmap looks to be very complicated, as one would expect with such a Herculean task and there is no single \u2018file\u2019 that will do the job. BIM will mean that data is \u2018filtered\u2019 out of various systems depending on the requirement.\nConclusion\nMr Morrell makes a great case and I think most people agree that the industry could work together better. Heads needs to be cracked together in order to change the culture of the industry, and perhaps this latest move by the government will be the one to achieve that. However, talking to a few architects at the Autodesk and Bentley events, many still admitted to being there out of fear. On current margins and with current work levels, the cost of buying new software, hardware and training is simply not an option for them, but they fear their clients will demand BIM more out of hearing the buzzword than actually wanting the deliverable.\nSimilarly, in the US I have met architects who are told by the government to use Revit, but all the client wanted was 2D drawings. This clearly puts a huge economic strain on many practises.\nEducation and mandates cannot be just for one side of the equation.\nI have said it before and I will say it again: one of the main benefits being sold here is the advantage of silo removal and collaboration; but the core BIM tools from the various vendors do not play well with one another. It took 20 years to get 2D CAD systems to sort out data sharing and most of it was done by reverse engineering Autodesk\u2019s 2D DWG standard. Now the software companies have an opportunity to sell a completely new CAD system or enhanced capabilities to the same customers again (at a hefty profit) and in the process data interoperability is going to be bombed back to the stone age.\nCOBIE is a lowest common denominator \u2014 it is the \u2018I\u2019 in BIM but not the geometry. Industry Foundation Classes (IFCs) are too limited and not guaranteed to be anywhere near 100%.\nThe UK is a nation of builders and designers. This was a great opportunity to get the software vendors to actually work together and improve a problem that everyone can see coming. At the moment it seems the only way to remove the problem of interoperability is for everyone to use the same system.\nAs such, architects I have talked to are worried about a) investing in the wrong system until a dominant player comes to the fore, and b) investing full stop.\nIn the meantime we will keep watching and seeing how the roadmap develops.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-cost-benefit-analysis-linked-to-bim\/","title":"NEWS: Impact Infrastructure links cost benefit analysis to BIM","date":1421280000000,"text":"New tool allows AutoCAD Civil 3D users to more easily measure Triple Bottom Line \u2013 economic, social and environmental \u2013 returns\nA new tool from Impact Infrastructure allows users of Autodesk AutoCAD Civil 3D, part of the Autodesk Infrastructure Design Suite, to input 3D model spatial information directly to the AutoCASE cloud-based Triple Bottom Line [TBL] analysis service.\nTBL analysis goes beyond exploring only the engineering aspects of a design to embrace an accounting framework with three parts: social, environmental and financial.\nAccording to Autodesk, providing clients with TBL analysis to evaluate a design has been prohibitively costly and very limiting when it comes to exploring design options. The company says the traditional approach involves hiring an outside analyst service; with fees as high as $100,000 for a single project, resulting in static reports that don\u2019t support exploring \u201cwhat if\u201d alternatives on-the-fly.\n\u201cAutodesk\u2019s support has helped us achieve our goal to automate business case analysis by combining two global standards \u2013 Cost-Benefit Analysis with Building Information Modeling,\u201d said John Williams, Impact Infrastructure Chairman and CEO. \u201cAutoCASE will help dramatically reduce the cost of comprehensive business cases making them routine components in project planning, design and development.\u201d\n\u201cWith direct access to AutoCASE from within AutoCAD Civil 3D, it\u2019s like bringing an economist, ecologist and public health expert to your desk to help prioritize alternative planning scenarios based on societal value,\u201d said Emma Stewart, Head of Sustainability Solutions, Autodesk. \u201cUsers will be able to plan, design and finance proposals, placing emphasis on achieving optimal financial, economic, social and environmental returns. AutoCASE generates TBL business cases quickly, so when changes occur, teams can see the impact immediately and use this information to increase their chances to win financing and buy-in from community stakeholders.\u201d\nImpact Infrastructure is starting with a stormwater module, which will allow infrastructure professionals to analyse an Autodesk AutoCAD Civil 3D project design for such factors as public benefits of improved water quality, increased recreational, and property value.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nIdeate Software enhances Revit add-ins\nNvidia Blackwell GPUs at heart of new Z by HP workstations\nNew utility designed to automate repetitive tasks in Revit\nNEWS: Cidon Construction halves estimating costs\nRevit 7\nIntel launches 28 core, 4.3GHz Xeon W-3175X processor\nIdeate Automation links to Autodesk Construction Cloud\nMADCon Virtual Convention for Architecture Students\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-florian-frank-herzog-de-meuron\/","title":"Video: NXT BLD 2019 \u2013 Florian Frank, Herzog & De Meuron","date":1565049600000,"text":"User Defined Software \u2013 NXT BLD London, June 2019\nThe past has shown how the evolution of software influenced architects in their design and obviously manufacturers in their product spectrum. This talk puts an emphasis on the workflow influenced interaction design for software development, to unchain the user\u2019s needs. Starting a journey from digital fabrication over machine control, towards user interaction it highlights principles for a general approach in software development. Examples show digitally driven roof constructions of the elephant house Zurich over exhibition booths, free software projects and custom software solutions working at Herzog & De Meuron.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedmal, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-evolution-part-ii\/","title":"Autodesk Evolution part II","date":1392681600000,"text":"In the second part of our exploration of the changes in Autodesk\u2019s AEC development, Martyn Day reports on the company\u2019s forays into cloud, big data and programmatic design\nOut of all the software vendors in this industry, Autodesk is by far the most fixated on moving to the cloud. Every product the company has, or rather, is going to keep, is in transition to having a cloud-based equivalent. For now there are fully featured mature Autodesk desktop products and relatively new cloud variants, deemed \u2018360\u2019, as well as some intermediate \u2018virtual\u2019 technologies, such as the recently launched Autodesk Remote. Autodesk firmly believes that customers want, and would benefit greatly from, their data and applications being available everywhere.\nAutodesk Remote is a standalone application for Autodesk subscription customers using Windows 7\/8 (32-bit and 64-bit) and is compatible with all Autodesk products. Remote is installed on a target PC and enables Autodesk software installed on the main workstation to be run from a remote computer over a standard network.\nThere is a companion iPad version of Remote, still in development, which enables Inventor or Revit to be run from the workstation virtually on the iPad, with the Remote iPad version featuring optimised touch interface, dependent on the speed of the wi-fi connection.\nThere is an element of smoke and mirrors to this technology as it is \u2018virtual\u2019. Autodesk has not rewritten Revit to run on an iPad, it is being manipulated by the compressed graphics over a streamed network.\nVirtualisation is something that many AEC firms are currently investigating as project teams work across the world on common Building Information Modelling (BIM) data sets. Using technologies like Autodesk Remote, it would be possible to host the data centrally and have teams log in remotely to licenses hosted on powerful centralised workstations. This would secure the design data, as the model information never actually leaves the building and aids offshoring or 24\/7 project work. The main drawback is the reliance on the Internet and bandwidth, as it is essential to access both the data and authoring application.\nHowever, Autodesk Remote is just the start of the application of this technology. Autodesk is also experimenting with delivering applications like Revit, direct to web browsers, bypassing the need for a workstation with a corresponding software license.\nCombining HTML-5 with a Java-based streaming technology from OTOY, with Nvidia\u2019s Keplar GRID GPUs running on Amazon\u2019s servers, Autodesk can deliver demanding applications like Revit to a Chrome, Firefox or Safari browser at speeds in excess of 35 frames per second, which is seamless to the human eye. Thus enabling Windows, Macintosh and yes, iPads\/tablets to run any of Autodesk\u2019s applications. There are rumours of a 13-inch iPad next year, perhaps we are not far away from design computers being just large thin touchscreens?\nMake the connection\nWith Amazon\u2019s server farm, it would certainly enable customers to make savings, purchasing low-power, low-cost workstations. However, this necessitates the best possible, stable Internet connections.\nReliance on the Internet as essential infrastructure is something that users are exceptionally wary of. In many developed countries around the world, bandwidth is variable and connections are constant. Take that to emerging countries like India where the majority do not have the Internet, let alone fast broadband Internet and the concept of the cloud and virtualisation ring hollow.\nAt Autodesk University at the end of last year a press member from India asked how Autodesk\u2019s cloud vision would work in a country of Internet \u201chave nots\u201d. Autodesk chief executive Carl Bass could only suggest that mobile phone networks could perhaps pick up the data slack.\nBig data\nOne of the most prevalent messages in Autodesk\u2019s sales pitch on the future of design computing is the immense amount of processing power available by moving to a cloud-based infrastructure. And what do we do when we have excess capacity of CPUs? Why we quickly evolve ways of using it.\nBrian Mathews, VP \/ AEC Group CTO, head Reality Capture & Design Viz teams, has made it clear that Autodesk is positioning its portfolio of products to be able to handle huge datasets. While we may consider a multi-gigabyte Revit model to be today\u2019s big data, advances in the pricing and adoption of 3D laser scanners mean we soon could well be dealing with terabytes or petabytes of highly accurate, captured data. With advances in point cloud handling and meshing, Autodesk is developing a new generation of tools that will change the way we use reality-captured data.\nBefore we create new buildings, roads and bridges, we have to capture the existing condition, which can be time consuming and expensive. Terrestrial laser scanners can measure millions of 3D points in seconds and multiple scans can be combined to cover huge areas. The prices of these devices has dropped from $100,000 to $35,000 in the last few years, leading to increased usage and interest in the potential of using point cloud data. Nearly all the main CAD systems now support point clouds as a native format out of the box.\nWith increasing capabilities to apply \u2018knowledge\u2019 and allocate object properties to shapes within a \u2018dumb\u2019 cloud of points, scan-to-BIM will eventually become much easier and less of a manual process.\nMr Mathews\u2019 team is developing Autodesk Recap and Project Memento. Recap is Autodesk\u2019s reality capture software and looks to be revolutionary in many ways. Recap can sit on a laptop and take streamed point clouds from laser scanners. As the data builds up, Recap gives live feedback as to where possible occlusions may be (gaps in the point cloud where perhaps an object has shadowed the laser scans).\nRecap can also dispense with the targets that surveyors use to geo-reference the point cloud datasets to accurately merge them. Instead it uses real-world objects to map and merge multiple scans. According to Mr Mathews, under test conditions Recap is at least as accurate as traditional target-based surveying, if not better.\nProject Memento is aimed at downstream users that have to handle huge meshes and textures that are created from laser scans or photogrammetry processes. Despite being able to handle huge data sets, the software uses \u201chardly any RAM\u201d, according to Mr Mathews and provides all sorts of mesh diagnostic and analysis tools with auto-fix and sub-sampling capabilities.\nRecap has since been quietly launched with Memento finding a home on Autodesk Labs. Both these products have to be seen to be believed and places Autodesk in an interesting position when laser scanners drop to the sweet spot for volume adoption\nMr Mathews\u2019 team has previously developed 123D Catch, which can stitch multiple camera views together and create a 3D meshed model of a building or object. With 3D sensors such as Microsoft\u2019s Kinect, Leap Motion being embedded in games consoles, phones and laptops, our whole computing world is going to become 3D aware.\nNaturally, Autodesk\u2019s cloud vision also plays a part here as the datasets involved can be many times larger than the average hard drive and so storing point clouds remotely on Autodesk360 servers and streaming these to anyone, anywhere could be a massive benefit.\nDynamo\nDynamo is a visual programming environment for Revit and Inventor and probably all Autodesk tools in time. Dynamo was originally created by ex-Buro Happold engineer, Ian Keough in his spare time. Autodesk has since snapped it up and thrown a lot of resources at it, to make it an official product. Dynamo is essentially Autodesk\u2019s version of McNeel (Rhino) Grasshopper, borrowing heavily on the plug and play visual scripting interface that Grasshopper championed.\nHowever, Dynamo appears to be a wider playpen, with Autodesk, unusually deciding to make it open source as well as enabling Dynamo to mix and match scripts from a number of sources: Autodesk\u2019s own DesignScript, Python (which means it can automatically take a lot of stuff scripted for Grasshopper) as well as its own scripts.\nWhile predominantly emanating from the AEC team, its links with Revit are being added to constantly and it can be used to create complex geometry or defined object behaviours (such as pannelisation).\nThere is still plenty of work to do. I wonder what would happen if Dynamo was controlling Revit, but Revit had parametric constraints applied to the design that contradicted Dynamo\u2019s script? Those kind of conflicts are not yet catered for but I suspect that Dynamo is going to spark its own fan-base community and possibly appeal to those of competitive systems.\nSadly, it seems that Dr Robert Aish, father of GenerativeComponents at Bentley Systems and DesignScript at Autodesk, had left the company at the time of Autodesk University, with the Dynamo team getting all the \u2018computational design\u2019 focus.\nI suspect that the new programming language that Dr Aish created at Autodesk will lack promotion and development as a result.\nFormIt\nAnother relatively new code stream, Autodesk FormIt, seems to have survived its first year in the wild with Autodesk now having big plans for this conceptual modeller. There will finally be a desktop variant to go with the original iPad version.\nIt seems that much of Autodesk Vasari\u2019s analysis capabilities is due to be added into the product, making it Autodesk\u2019s first coherent conceptual design for architects, enabling massing, modelling and analysis.\nThere was even talk of FormIt being added to Dynamo at some point in the future to create a standalone scripting and analysis conceptual tool, capable of lightweight generative designs which could be brought into Revit for the detailed design and documentation phases.\nConclusion\nHistorically, Autodesk has never been hard to keep up with, as it was so AutoCAD-centric. Now, it is all topsy-turvy, as products appear from nowhere (and some vanish without so much as a goodbye).\nAutoCAD is the one product that rarely gets mentioned by an Autodesk employee, yet its revenues still make up a hefty chunk of the balance sheet. I guess the good news is there are still a lot of conversion sales to be made. Autodesk is now brought to you by the word \u2018cloud\u2019 and number \u2018360\u2019.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nWith the continuation of this article, which was based on my visit to Autodesk San Francisco HQ, I have subsequently attended Autodesk University where some of the products I had seen were released, updated or more information was made available\nRead Part one of this article HERE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-tal-friedman-foldstruct\/","title":"Video: NXT BLD 2019 \u2013 Tal Friedman, Foldstruct","date":1565049600000,"text":"Between the folds \u2013 Towards a material revolution \u2013 NXT BLD London, June 2019\nThe age of digital information is rapidly evolving into its next stage- the transformation of matter. How can digital information give life to novel applications and uses of industry standard materials? A complete paradigm shift of the relationship between man, machine and material is needed as we enter a new industrial revolution. The talk shares some of the office\u2019s work using unique algorithms for design-to-fabrication workflows that try to challenge the classic perception construction. From mathematics of Origami to dynamic finite element analysis, the office works to bridge between the digital and physical with a mission to bring freeform design and customization to the mainstream construction sector.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/social-distancing-at-london-st-pancras-tracked-in-real-time\/","title":"Social distancing at London St Pancras station tracked in real time","date":1586390400000,"text":"Digital twin platform uses AI and computer vision to monitor distance between passengers at London station\nA digital twin platform that was originally designed to alert station managers to overcrowding at St Pancras station in London is now being used to monitor social distancing due to the coronavirus pandemic.\nDeveloped by UK startup OpenSpace, the technology uses IoT and AI to detect and visualise the distance between passengers in real-time. The resulting data can then be used to compare historical weekly and daily information for trend analysis. It will provide a useful indicator of public adherence to Government guidelines, especially when lockdown measures are lifted in stages.\n\u201cOur technology is designed to detect real-time passenger separation to alert station managers to current and future overcrowding, and suggest interventions. But the unexpected events of the past few months have revealed a new application \u2013 monitoring social distancing. If our data can help better inform government strategy on COVID-19 to help save lives, then we want to do our bit,\u201d said OpenSpace CEO Nicolas LeGlatin.\n\u201cThe platform detected a 90 per cent drop in passenger numbers after lockdown measures were introduced on Monday 23rd March, compared with a weekday in January this year. This represents the scale of travel demand change due to COVID-19 in one of the UK\u2019s busiest stations.\u201d\n\u201cOur purpose is to use technology to help make the passenger experience better for everyone, including protecting privacy. Cameras with computer vision technology are key to measuring passenger flow rate, travel patterns and social distancing. The data collected is anonymous, and doesn\u2019t use facial recognition. Through the use of Virtual Reality (VR) headsets, operators can put themselves in the shoes of passengers in real-time. Like those approaching a crowded area, to see and feel some of what customers are feeling, driving improvement strategies.\u201d\nThe project at St Pancras went live in 2019 and was funded by the Department for Transport through the First of a Kind Round 2 competition, delivered by InnovateUK. Other project partners include High Speed 1, Govia Thameslink Railway, Network Rail \u2013 High Speed, and Birmingham Centre for Railway Research and Education.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/concept-design\/conceptual-design\/","title":"Conceptual design","date":1459382400000,"text":"Technical products for detailed design & documentation are well developed, and yet conceptual design products remain in the shadows. But this is changing. By Martyn Day\nHistorically, conceptual design has been poorly served by CAD software developers. Many Building Information Modelling (BIM) design tools were first sold to improve the process of creating co-ordinated detail drawings, with features like massing and simple geometry manipulation failing to appear for years.\nThere is one notable exception \u2014 a product called SketchUp, launched 15 years ago by a small company called @Last Software. SketchUp was low cost and with an easyto-use \u2018push \/ pull\u2019 3D modelling environment it built up a solid user-base.\nGoogle acquired the software in 2005 and made the base product free, which helped get many architects into modelling and subsequently BIM. It was a necessary and welcome industry catalyst and it is still one of the most popular 3D modelling tools used by architects.\nIn 2012 Google sold SketchUp to Trimble. The company is currently fleshing out the modelling and documentation tools and offers an HVAC add-on, together with a range of third party apps in its store.\nIn some ways the success of SketchUp and its low cost dissuaded competitive development until relatively recently.\nWith the rise in use of mobile devices and more \u2018push \/ pull\u2019 modelling interfaces there have been a number of recent innovations in sketching and concept modelling.\n2D and 3D sketching\nWith the new generation of powerful tablet computers, specifically the Microsoft Surface Pro and Apple iPad Pro, software developers are now starting to explore sketching and modelling tools with more intuitive interfaces than those of their desktop cousins.\nAutodesk has a number or 2D tools, the best being SketchBook Pro. Others include Adobe Illustrator Draw, Paper by fiftythree and Bamboo\u2019s Paper to name a few. One of our favourites is Morpholio, which offers a suite of eight applications for the budding 2D artist.\nGravity Sketch develops 3D modelling tools for complex shapes. It also links to the Shapeways 3D printing service and models can be shared on 3D content portal Sketchfab \u2014 all from a tablet. The software looks quite interesting for form finding and fabrication of small models but currently lacks industry-specific tools.\nMassing and modelling\nMost of the systems we have mentioned are predominantly aimed at replicating the 2D sketching process. Autodesk, however, has a mobile and desktop architectural concept tool, Formit 360, which is a Revit-friendly modelling tool with added analysis. It could be seen as Autodesk\u2019s attempt at competing with SketchUp.\nFormit 360 Pro offers building energy analysis, solar analysis, shadow studies and building cost indicators. Teams can also collaborate and share models for critique. By linking it with Revit these models can act as useful start points for the complete design and detail process, thus preventing BIM managers from getting frustrated with architects that keep submitting design changes in the form of SketchUp models that need to be remodelled in the detail design system.\nOf course another option is to do massing inside the core modelling and detailing environment. While early \u2018BIM systems\u2019 lacked the capability to do this, complex geometry creation has improved, and most tools now have the ability to easily create complex forms beyond mere rectilinear.\nAutodesk Revit, Graphisoft ArchiCAD and Bentley AECOsim all have massing tools to rough out volumes and levels. This saves the hassle of converting data from proprietary conceptual modelling tools.\nHowever for advanced users this is still not good enough. When building components have complex forms or intricate parent \/ child relationships with surrounding components, such as glazed curtain walls, guttering or sun shades, editing a 3D \u2018BIM\u2019 model is often just too complex.\nMcNeel Rhinoceros (Rhino), another thorn in the side of the larger AEC software developers, is a very popular choice for conceptual design, especially for buildings that feature complex surfaces. It is widely used by London\u2019s internationally famous architects, relatively inexpensive, and has its own ecosystem of advanced, yet affordable, add-ons.\nNearly every BIM tool has a Rhino import capability. However, it is not just its native surface-modelling functionality that impresses; Rhino is leading the most exciting developments in script-based modelling as well.\nComputational design\nWith computational design the computer has the potential to take some of the workload away from the designer. It handles all of the geometry complexity and component relationships, while at the same time potentially optimising the design through analysis.\nEssentially, computational design applications reduce the risk of handling complexity by allowing the computer to update models based on user-programmable rules, which define the layout of a building.\nShould a complex model with dependencies and relationships need editing then the computer would regenerate the design ensuring these codified rules were obeyed.\nThese tools are so powerful that they are enabling new architectural vocabularies that can be seen in the works of practices such as Zaha Hadid and Foster + Partners. Rules-based computer-driven geometry lowers risk in design, links well to CNC (Computer Numerical Control) machines for fabrication and can enable late-term changes to projects, driving details and documentation without human intervention.\nThe first company to champion these kinds of tools in the mainstream was Bentley Systems with its Generative Components (GC) scripting environment.\nThis was followed by Grasshopper for Rhino, which quickly became the mainstream computational design solution. Recently Graphisoft started to support links to Grasshopper within ArchiCAD to drive its BIM geometry. Grasshopper is the de facto standard for college students and is heavily utilised in many conceptual teams as a way to explore designs and drive complex forms. In the beginning Bentley\u2019s GC tool was driven by writing scripts, so Grasshopper made a huge ease-of-use lead by providing a plug and play visual interface which would create the scripts in the background.\nAutodesk has also launched Dynamo Studio, which has a similar visual scripting interface to Grasshopper and can be used to drive Revit, Navisworks, Robot Structural Analysis Professional, Excel, Rhino and other applications.\nDynamo\u2019s ability to link to and drive geometry in Revit is getting a lot of positive attention as those firms that want to use Revit for documentation and modelling now have a much more capable and complex modelling system.\nAs Dynamo also supports the Python language, scripts written for Grasshopper can also be re-used in Dynamo.\nComputational design versus BIM\nNearly all BIM systems were originally designed as a better way to create 2D documents that the industry relies on and is still largely contractually obligated to deliver.\n2D drawings are inherently dumb \u2014 they may not be accurate, may miss information, and could become uncoordinated through a change-driven design process or on-site feedback.\nThe BIM model with its single version of the truth driving the 2D sections, elevations and details should, in theory, be co-ordinated and always reflect the state of the actual live design. This link dies once drawings derived from a BIM system are dropped into a \u2018dumb\u2019 2D CAD system and edited.\nImproved 3D for complex geometry is a relatively recent enhancement to BIM modelling tools, thus Rhino was always finding a use in projects. In fact many computational purists feel that today\u2019s BIM systems are merely \u2018Lego CAD\u2019 and are limited to \u2018best fits\u2019 to create functional rectilinear mundane buildings like offices or schools \u2014 not \u2018real\u2019 architecture. However the available systems have become a lot more flexible in the last five years and there are ways to model pretty much anything.\nToday\u2019s BIM tools still do not play well with manufacturing systems out of the box. Rhino, for example, has a much better direct manufacture processes.\nA number of expert practices are finding the 2D documentation part of the process is looking increasingly archaic as sharing the 3D model with a fabricator is much more explicit than sending a collection of 2D drawings.\nArchitect Frank Gehry had trouble getting his buildings made as he used to supply 2D drawings to contractors who would always add a large fee to the bids as they had to try and work out how the complex forms fitted together. Once he started sending a 3D model, the contractors\u2019 costs all came in lower and to within 1% of each other.\nAs computational design systems lower the risk in complex designs, and as new architectural forms become part of the visual vocabulary, traditional architects are going to have to grasp 3D tools. With advances coming in robotics, prefabrication, 3D printing and modular construction, AEC design systems will add direct manufacturing design capabilities and drawings will increasingly become less relevant to the process, as is happening in the engineering world.\nThis article is part of an AEC Magazine Special Report into the Future of Building Design, which takes a holistic view of the technologies and processes, which are set to change and enhance the AEC industry in the coming years \u2014 from concept design all the way to construction.\nClick to read the other articles that make up the report.\n1) Introduction New technologies are empowering architectural firms to improve quality, capabilities and process.\n2) Conceptual design There are a whole host of digital tools for early stage design experimentation.\n3) Rapid site design The rapid capture of site topology is being aided by new technologies.\n4) Benefits of 3D design Evolution, not revolution when making the move to 3D CAD.\n5) Moving to model-based design How to get from 2D to 3D, how to roll out training and how to overcome common issues encountered along the way.\n6) Design viz Advanced new rendering technologies are opening the door to design realism in architectural workflow.\n7) Design, analysis and optimisation Once you have a 3D CAD model, optimse your design for daylighting, energy performance and much more.\n8) Collaboration and model checking How to share models with clients, contractors and construction firms and test the quality of your model.\n9) Workstations What to look out for when choosing a workstation for 3D CAD.\n10) Virtual Reality New technologies are now available to support powerful new design workflows.\n11) 3D printing Architects are 3D printing architectural models with impressive results.\n12) Fabrication As building time gets compressed what will revolutionise fabrication and construction time?\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-doxel-uses-reality-capture-and-ai-to-boost-construction-productivity\/","title":"NEWS: Doxel uses reality capture and AI to boost construction productivity","date":1516752000000,"text":"Autonomous drones and robots capture site conditions, then deep learning algorithms recognise objects in real time.\nA new Artificial Intelligence (AI) and computer vision-based system from Doxel promises to deliver a major boost to construction industry productivity.\nThe Doxel system uses autonomous drones and robots to visually monitor a site, both indoors and outdoors, with LiDAR and HD cameras, then applies proprietary deep learning algorithms to recognise objects. Based on this information the system can assess the quality of installed work and quantify how much material has been installed correctly. A cloud-based dashboard can then provide project managers with real-time feedback on productivity, as well as how actual costs and time spent are comparing to the original budget and schedule.\n\u201cYou can\u2019t improve what you can\u2019t measure. Without real-time visibility into quality and progress, managers simply can\u2019t boost productivity. Our turnkey solution literally tracks progress for hundreds of thousands of line items in project budgets and schedules, comparing actual performance to original plans,\u201d said CEO and Co-Founder Saurabh Ladha. \u201cThis is transformative for an entire capital project team. With Doxel\u2019s system in place, project managers can react in minutes, not months, increasing productivity by fifty percent and bringing in projects twenty-five percent under budget.\u201d\nAt the heart of Doxel\u2019s solution is what the company describes as a major breakthrough in computer vision software; one that allows AI to understand the world in 3D.\n\u201cConstruction projects involve millions of similar-looking components, packed tightly together in a dark environment. It\u2019s a Molotov Cocktail of challenges for computer vision software,\u201d said CTO and Co-Founder Robin Singh. \u201cUsually, the amount of training data such an algorithm would need would be enormous, and even then, the results may not be reliable. With our proprietary 3D semantic algorithms, we\u2019ve been able to get more reliable results with a fraction of the training data. Our algorithms recognise and contextualise objects not just based on colour, but also based on shape, location and size.\u201d\nDoxel says its system can detect errors almost immediately. Using a design-intent 3D model that shows the intended location, size and shape of every component, the system can alert managers if site work doesn\u2019t match designs.\nIn addition, Doxel announced a $4.5 million investment led by Andreessen Horowitz, with Alchemist Accelerator, Pear Ventures, SV Angel and Steelhead Ventures participating.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-connected-iot-tools-to-help-optimise-next-generation-buildings\/","title":"NEWS: Connected IoT tools to help optimise next generation buildings","date":1458604800000,"text":"Smart buildings of the future showcase a new home for Dassault Syst\u00e8mes technology\nA landmark smart office building project is set to introduce Dassault Syst\u00e8mes\u2019 aims to introduce more sustainable management of homes, commercial buildings and factories to smart cities of the future.\nDesigned in cooperation with Rhomberg Group, Zumtobel Group, Bosch Software Innovations, and Modcam AB, the pilot project at the LifeCycle Tower (LCT) ONE building in Dornbirn, Austria was presented during the 2016 Bosch ConnectedWorld event in Berlin, Germany.\nThe modern building, owned by Rhomberg Group and equipped with a state-of-the-art Zumtobel lighting solution and smart controls system, is already an ideal site to turn into an innovative connected building targeting the highest standards of sustainability and user comfort.\nIn this context, Dassault Syst\u00e8mes\u2019 3DEXPERIENCE platform aims to offer a \u201cunified virtual environment for design, simulation and seamless exchange of information between electronics, mechatronics and sensors of each system in smart objects, buildings or vehicles\u201d.\nThe real-time monitoring and analysis of Internet of Things (IoT) components and systems operations is in this sense expected to help optimise the next generation of design.\nIn the LCT ONE project, Dassault\u2019s platform provides real-time insights into the building\u2019s usage and technical health to optimise energy efficiency and occupancy, including energy usage and savings per luminaire, per floor or for the entire building.\n3D visualisations of presence data and a heat map that shows occupancy to help track space usage, while maintenance insights include luminaire failure notifications, operating hours and the usage history of the lighting system.\nAdded extras to such a system, like saved knowledge of preferred lighting scenarios, help increase end-user satisfaction.\n\u201cThe IoT is evolving into the \u2018Internet of Experiences,\u2019 where devices are digitally connected to the physical world around them to become part of a living experience shaped by interactions among people, places and objects,\u201d said Monica Menghini, executive VP and chief strategy officer, Dassault Syst\u00e8mes.\n\u201cBy cooperating with Bosch Software Innovations and other innovators in their respective industries, we can demonstrate how sensor information can be easily harnessed from any big data repository in real time and linked to the 3DEXPERIENCE platform\u2019s realistic representation of a virtual environment. \u201cSustainable cities can become a reality sooner than we think.\u201d\nDassault Syst\u00e8mes will present this and other 3DEXPERIENCity projects during its \u2018Design in the Age of Experience\u2019 conference in Milan, Italy on 12-13 April.\nThe event, as part of the greater Milan design week is a first-of-its-kind Dassault Syst\u00e8mes Global User Conference, presenting the opportunity to discover and exchange ideas around a global, human-centric approach of Design across Mechanical Engineering, System Engineering, Industrial and Creative Design, Styling and Program Management.\nAll these disciplines will be represented by Dassault Syst\u00e8mes\u2019 brands, with keynote presentations from the likes of consumer electronics designers Innodesign; the Delft Hyperloop Team, and Genta Kondo, CEO of EXIII, a Japanese company developing robotic prosthetic arms.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/windows-xp-x64-start-of-a-new-era\/","title":"Windows XP x64: start of a new era?","date":1124841600000,"text":"Rob Jamieson revisits an area which is very important for the design sector, but not necessarily as important to general computing. 64-bit Windows is shipping but does it work for mainstream CAD?\nIf you are a Unix user there have been many ways to get access to more RAM than is made available in Windows due to its imposed limit, but for most users who today rely on Microsoft\u2019s OS it has not been so easy. So what is the limit? I often get asked by CAD users, \u2018why do they need Windows XP x64 when their 32-bit Windows XP system has 3GB or 4GB of RAM?\u2019\nUnder Windows XP a single process can only allocate 2GB of RAM to this one process, this being your CAD application. The CAD or Design Program can only address this amount of RAM regardless of the physical amount in your machine as it\u2019s an OS imposed software addressing limit. You can cheat and use the \/3GB switch in the boot.ini file, but only certain applications can understand this and see 3GB. However, other applications can become \u201cflaky\u201d as this moves the allocated resources for graphics etc into other areas which these other applications might not understand. There are one or two exceptions to the rule with Microsoft email packages but that\u2019s out of our area of design.\nWorking with XP x64\nI installed Windows XP x64 on several systems and added some 32-bit CAD software to see how it works. I\u2019m not going to talk about relative performance issues as this is the reserve of MCAD labs. Instead, I\u2019m going to look at it from a user point of view to see how viable it is to use this in a working environment today. Unfortunately today there is little CAD software that has been recompiled into 64-bit code and some of the solid modelling kernels are going to take a long time. There is (or will be soon) some 32-bit CAD software, which is 64-bit aware from the point of view from memory addressing, so this is what we are really looking at today. This gives you the ability to load larger files with a 32-bit program imposed limit of 4GB. This is double what you have today with Windows XP!\nThe first machine I had was a dual Fujitsu-Siemens Opteron with 6GB of RAM. Windows XP x64 Edition installed OK so I added ATI\u2019s FireGL 64-bit graphics driver and some CAD software. Once you have XP x64 up and working don\u2019t expect a massive leap from XP, it looks just the same. You will have to wait for Windows Vista \u2013 previously referred to by its internal code name Longhorn \u2013 with its new 3D interface, before you will see any changes. Now I don\u2019t favour any CAD vendor but I do have access to some software quicker than others so I put on Inventor 10 and 3dsmax 7.5. Inventor installed after I manually downloaded Netframework but the installer for 3dsmax crashed every time. If you went into the folders and manually installed each module 3dsmax did install OK.\nWell there\u2019s no point in having all this memory and not putting it to good use so I created a new assembly and added in two large Mastenbroek trench diggers. This took the memory straight up to 2.25GB. It\u2019s no good patterning the model as this does not add weight i.e. use any more RAM. After adding more models I got to 3.4GB but I think this was putting a strain on the software and it was slowing down the file allocation with hard disk etc. It did not crash though!\nI ran 3dsmax at the same time and the first thing it did was run on the same processor as Inventor. I could pattern large files but it became a little unstable. I wasn\u2019t very fair on max as I was running another intensive application at the same time.\nAfter I had created a large dataset I copied it off onto a portable USB drive (it was very slow) and sent the Opteron system back where I had borrowed it from. Now it was time to have a look at a workstation in the interesting price point of \u00fa1,500, a P4 EM64T with PCI Express and 4GB of RAM from RMT. I added an extra drive so I could keep the Raid 0 with XP intact. Now this new drive had a beta of x64 so I bravely did an upgrade (I never normally recommend you do this as you end up with all the guff you had last time) and it worked fine! There were a lot more unknown devices in \u201cdevice manager\u201d so a quick trip to the manufacturers\u2019 web site and I was able to pull down and install these. The interesting thing is they were all labelled \u201cbeta drivers\u201d but they all worked fine. Manufacturers do this when they are not sure if everything is going to work OK; it gives them a free \u201cget out of jail\u201d card. On went the ATI FireGL graphics 8.1.33 driver followed by Inventor and 3dsmax as before. I started to copy the CAD files off the USB drive but they were all corrupt, other files on the drive were fine so it was the USB drivers on the Opteron that were faulty. I checked the site and no updates for that system were available. Three hours down the line after I had rebuilt and re-migrated the files I tested loading up two trench diggers. The performance was similar to the Opteron but 3dsmax was more reliable.\nInventor reported in its \u201cabout\u201d file that it could see 4GB of RAM. This is very important as when you create drawing views you need extra memory to find all the files before all the hidden line algorithms can work out what should be in the front and behind etc.\nThis system was the older 3.6GHz Pentium 4 EM64T motherboard (925 chipset) so it lost some allocated memory but acquitted itself fine. The newer 955 chipset RMT workstation would not lose this memory. As I type away now in Word 2003 on the x64 workstation I\u2019m tempted to see what else works OK.\nAfter a frustrating hour I could not get AutoCAD 2006 to install \u2013 there were problems with Demoshield 7.5 (similar to 3dsmax installer problems but you could get around it). Utility software such as Nero and DVD players were fine. Only the latest virus checkers would install, earlier ones complained.\nThe main conclusion that can be drawn from this exercise is that x64 is very new. You need \u201cbranded\u201d hardware with good driver support, patience and a good understanding of how to install device drivers. Your CAD vendor will need to say that their application is supported. Inventor 10 still has a slight problem with content centre under x64 so it\u2019s still not officially supported. If you are reaching your limit in model size or it takes ages to produce drawing views because you are out of RAM and it\u2019s mission critical to get a solution, Windows XP x64 Edition is here now, just be careful!\nRobert Jamieson works for workstation graphics specialist, ATI.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-martin-mcdonnel-soluis-sublime\/","title":"Video: NXT BLD London conference \u2013 Martin McDonnel, Soluis \/ Sublime","date":1502150400000,"text":"VR, MR, real time viz and the Augmented Worker \u2013 NXT BLD London, June 2017\nMartin introduces the three \u2018Rs\u2019 \u2014 Augmented, Mixed and Virtual Reality \u2014 and how these technologies and others are driving a number of projects at Soluis and Soluis subsidiary, Sublime. Two projects that stand out in this compelling talk are a design viz project for Radisson RED that used Unreal Engine to instantly render images and movies, whilst making edits in real time, and the Augmented Worker, a project that explores the tools needed for effective use of AR on the construction site for digital job guidance, on site visualisation and more.\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Paul Nichols, Skanska\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Arthur Mamou-Mani, Mamou-Mani\nConstructing (and deconstructing) buildings with cable robots\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/next-generation-optimisation-with-structural-insights\/","title":"Next generation optimisation with Structural Insights","date":1449446400000,"text":"A new service from Bentley will allow engineers to optimise designs for construction as well as tap into valuable engineering knowledge. By Greg Corke\nBentley Systems ProjectWise Scenario Services Connect Edition has finally arrived as an Early Access technology preview. The Microsoft Azure cloud-based \u2018optioneering\u2019 service uses large-scale simulation to help engineers find the best performing design options.\nRather than just using analysis in the traditional way to validate a handful of design options, Scenario Services is all about helping engineers explore many more alternatives and then easily compare their relative performance.\nIt uses the vast computational power of the cloud to run simultaneous analyses of tens, if not hundreds of different designs. Then, once complete, engineers can use the software\u2019s web-based visualisation dashboard to compare and contrast alternatives against key \u2018performance metrics\u2019 and very quickly hone in on one or more solutions that they want to explore further.\nThe first analytical applications to be supported by Scenario Services will be STAAD, for structural analysis, and SACS, for analysis of offshore structures. Next year Bentley will launch an Early Access program for AutoPipe, for pipe support optimisation and RAM Concept, for post-tension geometry optimisation.\nWith AutoPipe and RAM Concept the software becomes even more powerful, putting engineers on the path to semi automated design. Rather than the engineer manually defining a handful of design options for evaluation, the software will use so-called genetic algorithms to automatically generate a large number of alternatives.\n\u201cIdeally what\u2019s going to happen down the road is you will come up with valid ranges of each performance metric that you want your design to achieve and then you are going to tell the cloud to go ahead and optimise your design to achieve those goals,\u201d explains Raoul Karp, vice-president Structural and BrIM at Bentley Systems.\nIn the long term Bentley plans to roll out Scenario Services to a much wider range of applications and not just limited to structural design. Optimisation will be everywhere and the performance metrics against which designs are assessed will span multiple disciplines.\nStructural Insights\nFor structural design, Bentley has a much bigger vision for Scenario Services. A new service in development called Structural Insights will extend the scope of the service in two key ways.\nFirstly it will give the engineer access to information typically held downstream that should help lead to better design decisions upstream.\nSecondly, it will allow engineers to compare the performance metrics of new designs against historical projects.\nEngineers typically optimise designs for strength, deflections and materials, but are not generally concerned with reducing costs relating to constructing and maintaining the building.\nStructural Insights will give firms the ability to input all sorts of ancillary information that could lead to a more efficient or cost-effective design. This could be real time information about lead times on concrete and steel, but could also include more detailed information relating to construction costs, as Mr Karp explains.\n\u201cWhen you build a steel structure you need to use cranes to lift the material. Cranes up to a certain height can carry a certain sized beam, or certain load. Beyond a certain height you need a different kind of a crane,\u201d he says.\n\u201cWouldn\u2019t it be interesting if you could feed some crane information into Structural Insights. It could run through your model and tell you \u2014 hey do you know if you were to downsize these two beams at this level, or change the framing a little bit, the contractor wouldn\u2019t have to get a whole new crane just to lift those beams?\u201d\nThe big question is would the engineer really care about this? As Mr Karp explains, with an increase in design \/ build and private public partnerships, the engineer is now incentivised on the cost of construction and not just that of design.\n\u201cYou\u2019re going to see more and more engineers interested in understanding when is \u2018this\u2019 material that I\u2019m speccing in my design actually going to be available to be constructed,\u201d he says. \u201cThis beam is going to take me two months to get from the fabricator. I\u2019ll just swap it out and use this different beam and it can then be built quicker.\u201d\nMr Karp adds that in a private public partnership the team also makes money from the ongoing operations \u2014 to the point that they might go for a completely different material if they know they are going to have to maintain it for 20 years.\n\u201cThey might choose concrete if they know it is going to be more resistant in this environment and only have to do maintenance every five years, versus \u2018I\u2019m going to use steel here because it\u2019s quick to get up, it\u2019s cheaper to get up but I\u2019m going to have to come back every three years to paint the thing.\u2019 It changes your mentality on design.\u201d\nStructural Insights is also designed to help firms tap into engineering knowledge that exists inside (and outside) the organisation. This will help firms learn from past experiences in order to improve designs. Traditionally, this kind of knowledge is locked inside the minds of experienced engineers and often lost.\n\u201cThere are a lot of younger engineers that love their software but don\u2019t have the experience,\u201d explains Mr Karp. \u201c[With Structural Insights] you\u2019re capturing the experience held in the designs and you then can feed it back to the user to the point of time that you need it the most [conceptual to mid-design phase].\u201d\nAs with Scenario Services, Structural Insights will allow engineers to easily visualise the performance of different designs, current and historical, through a simple web interface.\nDesigns can be compared against different performance metrics. Then, if any one project shows significant savings, the engineer can open an i-model of the design and investigate what it was about that specific project that helped it get a better \u2018score\u2019.\n\u201cUsers can compare on what they like,\u201d says Mr Karp. \u201cSay, for example, let\u2019s just look at hospital projects, maybe the ones that are concrete flat slab and only over ten storeys. And then you\u2019ll be down to four or five projects anyway.\n\u201cThen you can look at the other things: weight, construction time. And then you might see \u2014 this project is really doing a lot better than ours, let\u2019s open it and see why. And then you\u2019ll see, oh they are using a shell of a slab with post tension instead of a regular sized slab. So that would be how I would expect engineers would start to learn.\u201d\nIn order to get the most out of the system, firms will obviously need to invest time populating it with historical data. Users will be able to add information, such as project type, the function of the building, geographic locations, as well as structural data. Obviously this needs to be easy to do, so Bentley is currently looking at ways to help streamline the process.\nBentley has an ambitious vision for Structural Insights in that it hopes it will become an industry benchmark. \u201cThe expectation and desire is that at some point we will get our users to commit to saying I don\u2019t want to just compare to my organisation\u2019s projects, I want to compare to every other project done by every other organisation in this area,\u201d he says.\nKnowledge sharing on this scale is unprecedented and it will be interesting to see how the open market embraces it. Collaboration between local government agencies, however, will be a much easier sell.\nShould Bentley get firms to buy into its vision, the benefits to the construction industry as a whole could be huge. Structural Insights will be available in limited access in March 2016.\nThis article is part of an in depth technology feature on Bentley Systems.\nRead Bentley merges design with reality.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/review-vabi-dashboard\/","title":"REVIEW: Vabi Dashboard","date":1431561600000,"text":"A new range of low-cost, add-on design analysis applications aim to provide architects and engineers with a very graphical dashboard for performance feedback early on in the design process, writes Martyn Day\nIt has proven exceptionally hard to sell building analysis to architects. The vast majority still believe it is a specialism outside of core architectural design. This has not been from a lack of trying. IES gives away some low-level functionality to SketchUp and Revit customers, and Autodesk provides some access via its Suites and Subscriptions, but without widespread success.\nOne reason for a lack of take-up is that building analysis tools have nearly always failed to integrate well with design tools. Results are given in measurements that only a building services engineer can understand and even progressive Building Information Modelling (BIM) architects do not prioritise a performance-based process.\nAnd yet, a BIM design process has the potential to offer architects an incredible amount of feedback on expected performance. Spatial, material and geographic design information can all be analysed to assess design choices.\nThere are many players in the building analysis software market. Specialist firms such as Integrated Environmental Solutions (IES) (iesve.com) and Sefaria (sefaira.com), together with solutions from Bentley (which acquired Hevacomp) and Autodesk (which acquired Ecotect and others).\nWith this competitive environment to crack, we did not expect to hear from another building analysis company wanting to join the fray. However, Dutch developer Vabi is approaching the market in a novel way, looking to provide a broader range of design analysis tools, through low-cost subscription, right inside the BIM design environment.\nVabi\nVabi dominates in its home market of Holland, with 95% of the country\u2019s building performance analysis market. Founded in 1972, it started life as part of the Dutch Research Institution (TNO), as the centre of Expertise on Building Service Calculations.\nIn 2010, after 38 years, Vabi became a privately held company and continued to benefit from its roots in government-mandated building performance. The company\u2019s Vabi Elements software is pretty much the de facto standard in Holland, from small practices to large blue chip companies.\nVabi employs over 65 people and has a methodology of training users on its building analysis tools for free at its HQ in Delft, while providing its software free to students and universities in Holland.\nDutch building regulations have been deeply encoded into the core Vabi Elements product, so when it came to producing new, internationally applicable tools, it meant starting from scratch and applying decades of know-how to new broader applications.\nVabi has come up with a much broader suite of analysis tools with a very unique vision for application delivery, payment and usage for potentially all BIM products, but concentrating on Revit first.\nThe company has launched its new Revit add-on products as a low-cost subscription model (between $10-$30 a month per app). These bite-size analysis applications are delivered inside Revit\u2019s menu system, and within its own building performance dashboard to give instant feedback on design performance over a wide range of criteria.\nOver time, Vabi plans to develop scores of these applications, over a range of topic areas which can be \u2018pick and mixed\u2019 and displayed in the dashboard. It is not just a range of new applications, it is a whole ecosystem.\nDashboard\nThe most instantly obvious benefit of the new Vabi applications is the dashboard display, which is in the Revit ribbon menu and gives \u2018at a glance\u2019 design performance information. Each application has a \u2018fuel\u2019 type indicator providing feedback on how well a design matches the criteria specified.\nI have seen many demonstrations from Autodesk showing what they think BIM will look like in the future, providing feedback, using on-screen dashboard feedback, for example, but this is the first time anyone has delivered such a system within Revit. It is really simple to understand and does not need explaining.\nMarket changes\nThe whole design tool market is changing quite rapidly and not just the move from 2D drawing to BIM. Industry silos are coming down, with more firms opting to become multi-disciplinary and benefit from the rich data that BIM provides.\nDesign tools are moving to cloud delivery and subscription. Vabi has looked ahead and designed a development, deployment and subscription payment system that works with the current desktop environment and anticipates the future changes coming to the market, such as fewer dealers, more cloud-based delivery and micro payments.\nBy keeping the pricing low and enabling short term per-month use, Vabi hopes to attract many Revit users to dip in and out of its software suite and use them on demand, perhaps keeping long term subscriptions to the most regularly used applications.\nModules\nThere are currently 10 modules available in five core topic areas: Financial, Comfort, Functional, Green Building and Secure & Safe. Each application is initially available on a 15 to 30-day free trial followed by a monthly price of $10 to $30 \u2014the difference in price indicating the perceived benefit or complexity of analysis. Apps can be used on a monthly on \/ off basis and are available in 135 countries.\nThe initial ten applications available are:\nFinancial: Financial Simulator and Energy Assessor.\nComfort: Thermal Comfort Optimizer, Ventilation Optimizer, Daylight Ratio Evaluator and Lighting Organizer.\nFunctional: Spatial Requirements Assistant, Daylight Ratio Evaluator and Accessibility Evaluator.\nGreen Building: Energy Assessor.\nSecure and Safe: currently unpopulated but expect to see additional applications appear over the coming weeks and months. We have seen a list of modules in the works and there is a lot more to come.\nFinancial Simulator\nFor all those \u2018what if\u2019 moments, Financial Simulator pulls together and displays financial performance related data and reveals the P&L, Balance Sheet, Discounted Cash Flow and Return on Investment (ROI) impact of design decisions. It enables the consideration of design alternatives using comparisons of alternate design options from impact on overall building value, to profitability and is a useful discussion tool with owners and asset managers.Financial Simulator provides the following: Accepts inputs such as rental rates and marketing costs for different types of occupancy. Uses floor areas and assigned occupancy types of rooms to calculate returns. Saves all financial indices in the BIM model Displays all financial results. Contributes various performance indicators in the Vabi BIM Performance Dashboard.\nThermal Comfort Optimizer\nThe Thermal Comfort Optimizer allows the user to set optimal design temperatures for each room in the BIM model. This can increase understanding of energy usage and occupancy requirements. The software takes into account occupant satisfaction, metabolism, expected clothing insulation level, relative humidity, air velocity, and room heating\/cooling setpoints for indoor temperatures in winter and summer. The Thermal Comfort Optimizer features: Allocation of the thermal comfort values per room based on room function, space and occupancy type. Analyzes 3D spaces in the BIM model and uses Fanger model predictive mean vote (PMV) and percentage of people dissatisfied (PPD) calculations in conformance with ISO 7730 (2005) Annex D.Saves thermal analysis data within the BIM model Displays thermal comfort results. Creates a Thermal Comfort Performance Indicator in the Performance Dashboard.\nLighting Organizer\nLighting Organizer automates the task of ensuring the right amount of light is available in a design. As different design options are investigated it enables decisions on lux levels, lumen, lighting, brightness, electrical installation, light fittings, ceiling fittings, lighting plans, fixed and variable lighting, lighting comfort, living and working spaces, etc. The Lighting Organizer features: Accepts as input the minimum illuminance per room area and space type. Scans the BIM model room by room for area and available lighting. Calculates and saves the lighting values per room in the BIM model. Displays Lighting results. Creates an artificial Lighting Performance Indicator in the Vabi BIM Building Performance Dashboard.\nVentilation Optimizer\nThis application checks against building codes and client needs, considering clean air volumes, ventilation capacity, airflow capacity, air handling units (AHU), ventilators, supply, exhaust and return ducts, vents, detectors etc, as prescribed by room occupancy or space type and number of people capacity. The Ventilation Optimizer features: Scans the BIM model room by room for volumetric space data. Calculates and saves the ventilation flow per room in the BIM model. Displays Ventilation results. Creates a Ventilation Flow Performance Indicator in the Vabi BIM Building Performance Dashboard.\nAccessibility Evaluator\nThis application assists in checking the accessibility compliance of a building design for wheelchair or other disabled access. The software will check corridor widths, room access, ramps, door widths, multi floor or level access via lifts and lift dimensions, disabled toilet and shower facilities, etc. The Accessibility Evaluator features: Calculates and saves the accessibility values per room in the BIM model. Displays Accessibility results. Creates various Accessibility Indicators in the Vabi BIM Building Performance Dashboard.\nDaylight Ratio Evaluator\nChecks daylight area per room inside the BIM model and assesses illumination, energy use and solar glare\/ reflection regarding modelled floor areas, positioned glazing, glass area, artificial lighting, comfort and brightness The Daylight Ratio Evaluator features: Scans the BIM model and reports daylight ratio compliance against required minimums. Accepts input of required minimum daylight area ratio per room area. Uses floor areas of rooms and glass areas of windows, doors, and curtain walls. Automatically adds and saves the daylight area ratio values per room in the Revit model. Displays your daylight ratio results. Creates a Daylight Performance Indicator in the Vabi Building Performance Dashboard.\nSpatial Requirements\nTo ensure a design meets the functional requirement of the client\u2019s brief and building regulations. Spatial Requirements Assistant automates the checking of the Revit model against set criteria. The Spatial Requirements Assistant features: Accepts various functional requirement inputs based on Revit\u2019s room occupancy types and user defined rule sets for custom requirements such as occupancy types, room and other space sizes, toilets, circulation areas, parking spaces, etc. and Building Code rule sets such as minimum access widths, circulation space requirements, and the number of toilets or parking spaces. Scans the BIM model and reports compliance of the current design iteration against all the functional requirements. Creates various Functional Performance Indicators in the Vabi BIM Performance Dashboard App.\nEnergy Assessor\nMinimizing energy, meeting regulatory requirements and occupant satisfaction is a modern design essential. Vabi Energy Assessor, comes with a great pedigree and works inside the Revit BIM model to assess monthly and yearly energy costs, electricity use, gas use, renewables use, CO2 emissions, green building rating, EPC, energy performance, heating, cooling, HVAC, thermal insulation and airtightness. The Energy Assessor features: Accepts various decision inputs on building data, grouped by building discipline. Uses Revit building envelope construction data and room data. Automatically adds and saves the energy use values within the BIM model. Displays energy use results. Creates various Energy Performance Indicators in the Vabi BIM Building Performance Dashboard.\nConclusion\nVabi\u2019s new dashboard is more than just a building analysis tool and its approach to the delivery of its applications is truly unique. The relatively low cost of access as well as flexible subscription for third party add-ons are also untried in the Autodesk developer community. While these are all inventive it will be interesting to see just how popular they prove, there should be at least one or two listed here that appeal to most people.\nVabi\u2019s masterstroke is linking the apps to a unified digital performance dashboard display, which is a highly desirable capability. It is now possible, at a glance, to see how well the current design conforms to many performance criteria, no training required. The benefits for building performance feedback in the iterative design process are also clearly obvious, however some of the analysis tools do require users to \u2018prepare the model\u2019, such as allocating room spaces within the Revit model, to provide important spatial information for downstream analysis.\nThe market will have to decide if the \u2018portion size\u2019 of the Vabi applications is right for the incremental subscription fee it is charging. It will depend on how much benefit can be derived from their use. It strikes me as a sign of the company\u2019s confidence in its products that the apps can be extensively trialled before purchase and turned on \/ off on a monthly basis. Vabi is actively encouraging feedback and wants customers to help drive development of its modules.\nThe buy \/ download system appears to be set up for individual or small firm access, for large firm global access to application suites, it is best to contact Vabi directly.\nPrice: From $9.99 per month Website: vabisoftware.com\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bentley-releases-aecosim-building-designer-connect-edition\/","title":"NEWS: Bentley releases AECOsim Building Designer CONNECT Edition","date":1505865600000,"text":"New release of multidisciplinary BIM tool benefits from the comprehensive modelling environment of Bentley CONNECT Edition\nBentley Systems is presenting the new AECOsim Building Designer CONNECT Edition as a BIM application for use on building projects of significant size and\/or engineering complexity. These buildings, says Bentley, typically have the challenge of combining vertical construction and horizontal infrastructure (like roads, railways, utilities, etc.), while design and project delivery firms have broad responsibilities for multiple project delivery disciplines and across subcontractors and joint venture organisations.\nAECOsim Building Designer CONNECT Edition integrates the work of architects, structural engineers, and MEP engineers in a single application. Importantly, it also shares a comprehensive modelling environment with all of Bentley\u2019s CONNECT Edition applications, which gives it \u2018seamless, translation free, intra-operability\u2019 with Bentley\u2019s multi-discipline portfolio of design, analytical, and construction modelling applications.\nWithout a comprehensive modelling environment, says Bentley, engineers and architects have had to struggle with complex data exchange, resulting in information loss and repeated translations, or even resort to force-fitting a BIM application beyond its intended use to model geometry, which is lacking in BIM intelligence.\nAll CONNECT Edition applications, including MicroStation CONNECT Edition, include deliverables production automation through the CONNECT Edition\u2019s documentation centre. According to Bentley, the documentation centre greatly improves the consistency, quality, and efficiency of drawing set organisation.\nAECOsim Building Designer also benefits from reality modelling, by which existing conditions are captured through photos and\/or scans and converted into engineering-ready reality meshes with Bentley\u2019s ContextCapture, bringing the actual geo-coordinated context directly into the AECOsim design environment.\n\u201cThis is an exciting time for Bentley as we complete the delivery of our CONNECT Edition applications. AECOsim Building Designer CONNECT Edition enables our users to improve their productivity and to collaborate across multiple disciplines by sharing content and data in a seamless workflow through a comprehensive modelling environment,\u201d says Santanu Das, Bentley Systems\u2019 senior vice president, design modelling. \u201cAs we continue to make advancements in BIM technology, this version of AECOsim Building Designer helps them to collaborate on projects of any scale or multidisciplinary complexity, and automate the creation of deliverables for all stakeholders. And now that CONNECTservices are made available to Bentley application subscribers, we bring the collaborative advantages of ProjectWise to every user of AECOsim Building Designer.\u201d\nEiPM, a Barcelona-based engineering and architecture firm, is an early adopter of AECOsim Building Designer CONNECT Edition, and is using the application to deliver complex projects. Xavier Coll, BIM manager and project manager, research and development for EiPM, commented, \u201cIt\u2019s a quantum leap. The new interface is totally clean and user-friendly with spectacular improvements in interoperability and usability of the interface in all tools. It now has more intuitive workflows, which contribute to increased productivity and reliability. Undoubtedly, this new version is the best comprehensive BIM modelling software on the market.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/vercator-point-clouds-in-the-cloud\/","title":"Vercator - point clouds in the cloud","date":1586304000000,"text":"Laser scanning has been threatening to go mainstream for a number of years, but the high cost the of hardware and software has kept it in the realms of the specialists. A new UK start-up combines the power of the cloud with new algorithms to break the hegemony, writes Martyn Day\nLaser scan data is transitioning from the realms of the surveying and specialist reality capture firms to the workstations of architects, fabricators and builders. It had been hoped that the increased level of commoditisation would drive down prices but laser scanners where bundled with the associated point cloud software remain stubbornly costly. While the data capture devices may well remain at a price premium, the software which registers and processes the point clouds, is certainly open to competition.\nTo date, nearly all the software solutions for processing point clouds have been desktop-based (Leica Cyclone, Faro Scene, Trimble Realworks etc.), with some cloud services to stream \/ view captured sites. A new UK start-up, Correvate, aims to use the combination of cloud processing power with a new advanced registration algorithm to speed up work\ufb02ows and broaden the deliverables from scanned data. The company has also implemented a token-based pay-per-use system, removing the high-ticket entry price of its competitors.\nAt the heart of Correvate\u2019s Vercator software lie patented algorithms which automatically align 3D point cloud datasets. These were originally conceived within the University College London\u2019s (UCL) Department of Electronic and Electrical Engineering but have now been exclusively licensed to Correvate which is leading their development programme.\nVercator\u2019s registration is not target-based, but works by fnding natural features within an uploaded series of overlapping scans and calculates vectors across the whole dataset.. There are typically millions of these features, which are used as reference points to perform horizontal and vertical alignment between the overlapping scans, processed by the Vercator registration engine. By opting to use so many vector features to align the point clouds, instead of a handful of targets, the software can increase the accuracy of the result. And by using the power of the cloud, provide the beneft of automation and signifcantly reduce registration time. Correvate estimates its software is anything between 60% and 80% faster than a typical work\ufb02ow (manual registration, plus processing time) and the larger the dataset, the bigger the beneft. Whilst the registration process can be target free, therefore enabling users to modify their site methodology to ultimately become quicker and more effcient during data capture. Targets can still be processed should the user wish to implement site geo-referencing.\nIn field registration\nVercator software is hosted in the cloud, which means it\u2019s everywhere, on desktop or mobile. Scan data can be uploaded directly from site or when returned back to base. As scan fles are notoriously big, upload times are a potential drawback vs a traditional workstation approach. Nobody wants to upload 100GB+ of fles over a 4G mobile. However, many city-based building sites do have decent WiFi these days and the imminent roll out of 5G will be a game changer for big data on the move.\nThe Vercator Field App runs on a mobile device and allows the mapping of scanned positions relative to the building layout. Site plans can be sketched out and images uploaded to provide a backdrop to place scan markers onto. A network of scan locations is built up along with the corresponding referenced pairings, which can be read by the registration software. This reduces the initial processing as the software knows which locations overlap thus giving it a pre-fltered head start.\nOn a workstation, Vercator is accessed via a browser. Users log into their account and start by creating a new project. Files can be uploaded to Vercator\u2019s cloud storage or a company\u2019s own hosted service. Scans are selected and added to the project and then the network is added, indicating the sequence and relationships of the fles. Vercator supports ptx, pts, e57, Faro \ufb02s, Z+F zfs, Riegl rdbx formats. If the Field App is not used, perhaps when working with older scanned data, a \ufb02oor plan can be added and the scan nodes moved to their relative places.\nThere are a number of user defned preprocessing settings which drive the quality of the output: max distance threshold, resampling, coarse and fne registration settings. Jobs are then submitted to the cloud server and you will receive an email on completion. The system frst produces a coarse registration and then a fne one. For those that want a bit more information on what\u2019s happening, there are progress bars that show what the software is currently doing on the job, conversions, preprocessing, registration etc.\nCompleted jobs can be viewed online with a range of useful interactive tools. It\u2019s possible to change the colours of the scans, measure, create clipping planes, isolate scan sets. After the coarse scan, if the result is correct, then proceed to the fne processing and viewing.\nOnce the job is completed the resulting registered scan or scans can be exported in a number of industry standard formats: xyz, ply, e57, pts, las, laz and rcs. Vercator will provide a link once any conversion has been processed. It\u2019s also possible to simply download the matrices required to align the scans which may be held locally, negating the need to go for a big download.\nBeyond registration\nCorrevate\u2019s vision goes way beyond registration but this is the low hanging fruit of the existing laser scan market. The company realises that by being in the cloud, it can connect its services via APIs to the growing number of popular cloud-based collaboration systems which companies like Autodesk have developed (Autodesk Construction Cloud which includes BIM 360). Correvate has plans in place to utilise Forge components in its software development. While Construction Cloud is adept at handling BIM and drawing data, the special needs of laser scan information and increasing demand for its use will bring all sorts of possibilities.\nThe biggest development opportunity for Correvate is applying various Artifcial Intelligence (AI) applications to the real-world data to solve key issues, such as \u2018Scan to BIM\u2019. As the system is inherently built on vector-based feature recognition, the company has set its sights on addressing the gnarly problem of extracting intelligence out of a dumb fle consisting of 3D points \u2013 recognising windows, doors and ceilings and creating respective solid entities. As the software is based on the cloud and fully automated this could be all part of the same process. Imagine a series of unregistered scans being uploaded to Vercator and the resulting native Revit BIM model placed into BIM 360, untouched by human hands! Similarly, you could scan an entire building and get an asset management model from the items identifed within the scan. The company is already hard at work at developing this technology.\nPricing\nVercator is paid for by tokens. One registration is 1 token = 1 scan. A registration process between 100 uploaded scans is 100 tokens. You can \u2018pay-as-you-go\u2019 or buy through a monthly discounted auto replenish system. The more tokens you buy, the bigger the discount. For comparison, 500 tokens on PAYG is $630, while on auto replenish it\u2019s $560. There are special deals for academic and educational users.\nConclusion\nCorrevate has developed a unique cloudbased solution which is frst aimed at assisting the traditional scanning market by automating and speeding up registration with massive parallel processing, bypassing many of the current desktop application barriers, such as high cost of ownership, time to get accurate, highly usable results and bypass throughput limitations, should they be man or machine. While it won\u2019t replace products like Leica\u2019s CloudWorx for the post-production work of manual creating 2D drawings or performing surface analysis, it can get you to that point faster. In the future (hopefully September) Vercator will automatically create the surfaces e.g. walls etc and then from these surfaces create takeoffs for 2D linework e.g. \ufb02oorplans, sections and elevations.\nVercator\u2019s approach to feature recognition puts it in good stead to develop the next stage of AI-derived models that go beyond the current survey deliverables from point clouds. The aim to automate the Scan-to-BIM process from individual unregistered scans would truly be revolutionary in the construction industry. Watch this space.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-autodesk-s-aec-collection-expanded-at-no-cost\/","title":"NEWS: Autodesk\u2019s AEC Collection expanded at no cost","date":1504742400000,"text":"Autodesk bundles game engine VR, structural analysis, computational design and more\nIn our recent interview with Autodesk CEO Andrew Anagnost, when confronted with feedback we\u2019ve heard from customers and the disapproval of the value for money in subscription, the company head pleaded for customers to give him one year to hold off judgement.\nToday, coinciding with Revit\u2019s birthday, Autodesk has gone some way to providing a lot more functionality for exactly the same subscription price as before, adding a number of applications to the Architecture, Engineering and Construction (AEC) Collection for BIM and Revit-based workflows.\nThe new additions include Revit Live (game engine viz and VR), Robot Structural Analysis Professional, Structural Bridge Design, Dynamo Studio Advance Steel and Fabrication CADmep. This builds on the core applications of Revit, AutoCAD, AutoCAD Civil 3D, Navisworks Manage, Infraworks and 3ds Max.\nAccording to Vikram Dutt, Sr. Director, Building Business Line at Autodesk, the move now enables Revit models for virtual reality, fabrication, computational design, and analysis supporting BIM processes from planning and design through pre-construction.\nThis is obviously the addition of a considerable tranche of new capability and welcome that there is no additional subscription fee. The question is, will firms actually use the additional products, with associated training and staffing costs, as well as the flexibility of licenses with a design department? It will also mean more difficult decisions in working out the cost \/ product balance between existing perpetual licensed products on maintenance, individual product subscriptions and these new mega Collections.\nIt\u2019s wise to keep one eye on the future and consider how underlying subscription costs may change over time and could negatively impact department budgets.\nThere is certainly enough capability now included to explore emerging and cutting edge uses of design technology such as VR, Analysis, Computational Design and Digital Fabrication. Obviously with owning most of the applications, Autodesk hopes the benefits of the BIM 360 cloud backbone will become even more apparent.\nWith the announcement Autodesk released a video to give customers a feel for how the tools in the Collection can impact project development, highlighting the Italian design and engineering firm Open Project which worked on Bulgari\u2019s new factory in Valenza, Italy, taking BIM workflows to the next level and using integrated software to leverage their models for visualisation, fabrication, and multidiscipline coordination.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/get-ready-for-real-time-ray-tracing-with-nvidia-rtx-and-volta\/","title":"Get ready for real time ray tracing with Nvidia RTX and Volta","date":1522281600000,"text":"Nvidia ushers in a new era for design viz with Nvidia RTX technology and Volta-based Quadro GV100 GPU\nNvidia today gave a glimpse of what design visualisation might look like in the near future, introducing a new \u2018cinematic quality\u2019 ray trace rendering technology that can deliver \u2018photoreal visualisations of massive 3D models in real time.\u2019\nThe Nvidia RTX technology uses deep learning to help predict what images will look like and runs on Nvidia Volta GPUs, including the professional-focused Quadro GV100, also announced today.\nReal time ray tracing has always been the holy grail of rendering, which is why Nvidia is calling this the biggest advance in computer graphics in 15 years. Ray tracing has historically been slow, taking minutes or hours to render a single frame, so being able to get results in \u2018real time\u2019 promises to have a huge impact on worfklows in product development, design and architecture, as well as in movies and games, of course.\nRay tracing simulates the way light behaves in the real world. As the computational demands are huge, it has primarily been done offline, often in render farms.\nNvidia estimates that over 6 billion rendering hours take place every year with current ray tracing technology. \u201cThis is primarily all batch rendering based on CPUs, so the market potential for rendering is truly enormous, explains Sandeep Gupte, senior director for Nvidia\u2019s Professional Solutions Group.\n\u201cWe\u2019re excited to bring this combination of GV100 and RTX technology to really make a dramatic impact on these workflows.\u201d\nYet Nvidia RTX technology isn\u2019t just about making current workflows faster: It\u2019s about delivering interactive rendering inside the viewport, as Gupte explains.\n\u201cThis has been always the desire of artists and designers when they are working on their projects, on their scenes,\u201d he says. \u201cIf you can see the work you are doing with all the correct lighting and shading and reflections and so on, you can make so many decisions, so much faster.\u201d\nCurrent viewports in CAD and other 3D software rely on rasterisation, a rendering method that takes vector data and turns it into pixels (a raster image). It\u2019s very fast, so it means huge models can be rendered instantly and manipulated smoothly, but in order to increase the realism in the viewport it has to fake real world effects like ambient occlusion.\nRay tracing, on the other hand, delivers physically-accurate results as it precisely simulates light and calculates how thousands of rays bounce off some objects and refract through others, thousands of times.\nTo suddenly go from a process that has historically taken minutes or hours to one that is \u2018real time\u2019 is an incredible leap.\nThe gaming focused demonstrations shown in the videos below are stunning, but we expect the quality inside the viewport of professional 3D software to sit somewhere between current rasterisers and full ray trace renderers when using accessible workstation hardware.\n3D software developers will likely combine traditional raster techniques with select ray tracing techniques to deliver the required performance for typical end users.\nNvidia says RTX technology supports area shadows, glossy reflections and ambient occlusion.\nNvidia RTX technology does not simply deliver \u2018real time\u2019 ray tracing through brute force with faster GPUs. It relies on the architecture of Volta, Nvidia\u2019s new generation GPU technology.\nOlder Nvidia GPU architectures, such as \u2018Pascal\u2019 and \u2018Kepler\u2019, only feature CUDA cores. In contrast, the new Volta-based Quadro GV100 has Tensor cores as well, a new type of core that is optimised for Artificial Intelligence (AI) applications and plays a very important role in Nvidia RTX.\nFor example, the Nvidia RTX OptiX AI denoiser is designed to dramatically reduce the time it takes to render a high-fidelity image by training a neural network to reconstruct the image using fewer rays.\nNvidia\u2019s RTX software rendering stack sits on top of Volta. It can be accessed through a few different APIs, including Nvidia OptiX (that ray trace technology behind Nvidia Iray and other GPU renderers) and Microsoft DXR (DirectX Ray Tracing), an extension of DirectX 12. In the future it will also support Vulkan, the open API from the Khronos Group, the not for profit organisation that is also responsible for OpenGL.\nSo, when might we see \u2018real time\u2019 ray tracing appear in professional 3D applications? \u201cThe immediate way you can access RTX is through OptiX,\u201d said Gupte. Nvidia says RTX is supported by more than two dozen design and creative applications, specifically mentioning Chaos Group (developer of V-Ray), Epic Games (Unreal Engine), Dassault Syst\u00e8mes (Catia and SolidWorks Visualize), Optis, Autodesk and Isotropix.\nThe technology also paves the way to greater visual quality inside VR applications, presumably with a nod to Nvidia HoloDeck.\nNvidia says designers can use physics-based, immersive VR platforms to conduct design reviews and explore photoreal scenes and products at scale.\n\u201cPeople don\u2019t want to work with datasets where they have to dumb down the quality of the data, they want to work with full fidelity,\u201d said Gupte. \u201cThey want to use VR as part of design process.\u201d\nBut what about the new GPU hardware? The Quadro GV100 looks to be a phenomenally powerful GPU.\nIt draws up to 250W and boasts 32GB HBM2 (ECC) memory, 7.4 TFLOPS of double precision performance and 14.8 TFLOPS of single precision performance. It looks to be a significant step up from the Pascal-based Quadro GP100 (16 GB HBM2, 5.2 TFLOPS, 10.3 TFLOPS).\nIt also supports NVLink interconnect technology, which means two Quadro GV100 GPUs can be connected together to essentially make one giant GPU.\nThis will scale up the performance and double the memory to 64GB, so it can handle very large datasets. We would expect designers would need to use multiple Quadro GV100 GPUs to get the most out of Nvidia RTX.\nThe GV100 is not limited to ray trace rendering. There are a couple of other areas where it is being pitched, including simulation, where it can accelerate CAE solver performance, and in AI development. \u201cThe power of Volta on these Tensor cores means all of this gets accelerated dramatically,\u201d says Gupte.\nPrices were not disclosed but the GV1000 will almost certainly not be cheap. We predict somewhere in the region of \u00a310,000. However, it\u2019s important to note that this is the first professional GPU based on Volta. The Nvidia Titan V, which sits somewhere between consumer and professional, is also based on Volta.\nNvidia is not the only technology provider to introduce \u2018real time\u2019 ray tracing. At DEVELOP3D LIVE last week AMD unveiled a new hybrid viewport technology that is not only designed to deliver higher quality visuals inside CAD and other 3D applications but increase 3D performance in general by using the Vulkan API.\nYou can read more about it in our in-depth article and see AMD\u2019s DEVELOP3D LIVE presentation below.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-alexander-le-bell-tridify\/","title":"Video: NXT BLD 2019 \u2013 Alexander Le Bell, Tridify","date":1565049600000,"text":"The impact of automated web VR workflows and streamlined collaboration \u2013 NXT BLD London, June 2019\nFor the digital transformation to happen, focus needs to shift from catering only to professionals to a broader base of decision makers, influencers and end-users. A web shareable, mobile friendly and game-like experience that is easy to use, will help non-technical professionals and end-users understand the role of architectural BIM models in the design and construction process. This allows them to participate more effectively and better contribute to the workflow. An immersive dynamic web environment is a widely adopted distribution channel where users can be creative and share their ideas. This is the key for change to happen. During the presentation: We will discuss how the web can be harnessed so it adds value to the construction industry in a cost-efficient way. We will demonstrate how to publish BIM straight to VR online so everyone can interact with 3D models from anywhere (mobile and desktop), as well as show use case examples that would help companies to gain strategic edge.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/tripod-data-systems-cuts-prices-for-new-tds-ranger\/","title":"Tripod Data Systems cuts prices for new TDS Ranger","date":1058400000000,"text":"Tripod Data Systems announced an immediate across-the-board price reduction on its entire line of new TDS Ranger rugged handhelds. The price reductions accompany TDS\u2019 recent re-launching of the Ranger line, which now features color screens on all models, increased memory and a wider range of extreme-weather operation. New Rangers with 128 MB of Flash RAM have been reduced $400, while new Rangers with 32 MB of RAM are priced $300 less than the 32 MB monochrome models they replaced. And customers can upgrade to a new Ranger with 512 MB of Flash RAM for just $400 more than the 128 MB unit. The price reductions include stand-alone Rangers as well as Rangers bundled with TDS survey software, including Survey Pro. TDS continues to give the customers greater value by improving the Ranger while making it more affordable than ever. They have produced the 20,000th Ranger on the market, proof that the customers continue to rely on Ranger\u2019s rugged construction and proven reliability. Every Ranger features a lightweight carbon fiber and polyurethane exterior that exceeds MIL-STD-810E standards for drops, shock and vibration and is rated IP 67 for protection against dust and water. All Rangers feature a 206 MHz Intel StrongARM processor, a 320 x 240 backlit reflective color display, 64 MB of SDRAM and a 57-key tactile-action keyboard with separate navigation, alpha and numeric keypads plus touch-screen input. The Ranger uses the Microsoft Windows CE .NET operating system. Pricing and Availability: The Ranger is available from TDS-authorized dealers. Manufacturer\u2019s Suggested Retail Price for the Ranger 200C is $2,199 with 32 MB RAM, $2,599 with 128 MB RAM and $2,999 with 512 MB RAM. Software-bundled models range from $3,199 for the Ranger 200C with 32 MB RAM and Survey Standard to $6,299 for the Ranger 200C with 512 MB RAM and Survey Pro MaxT software that includes GPS and robotics modules.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/opinion\/transitioning-to-bim-pt2\/","title":"Moving from AutoCAD to Revit: part 2","date":1431561600000,"text":"Sean Bryant gives more practical advice on how a CAD manager in a medium-sized, multi-disciplinary practice can help smooth the path from AutoCAD to Revit.\nThis is the second in a series of three articles about making the move to Revit.\nUsing the scenario from part one of the series:\nA medium sized, multi-disciplinary CAD practice, involved in architecture, structure and services, which often works with external contractors. Based in London, UK, it has fifteen core users, with anywhere up to twenty-five users when contract CAD personnel are brought in to make up capacity. A CAD manager is in place who acts as liaison between management at director level and the users in the CAD team. The team is currently using Autodesk AutoCAD for all of its work and is up to date with the latest version, due to an active Autodesk subscription agreement.\nThe practice has decided to use Autodesk Revit as its BIM tool of choice. There is a need to manage both the implementation and training required to make the practice both effective and efficient.\nThe article is written from the CAD manager\u2019s perspective.\nAddressing the Revit implementation\nCAD manager to director level\n\u201cRevit is now our tool of choice. We need to invest in our IT and server capacity to ensure we have the server space to handle our Revit central models, and make sure that our users have enough local space to work with local worksharing models.\u201d\nCAD manager to the CAD team\n\u201cAs we move forward with Revit in the practice, we will be working with much more capacity, both locally and on the servers. However, this does not allow us to be lazy. We will have to implement new strategies that allow us to be economical with this new space provided, as we will be working with much larger file sizes than we were with AutoCAD.\u201d\nRevit project files (RVT files) are bigger files. The CAD manager is making sure that the CAD team is aware that new working practices will be needed to manage this on a day to day basis, and that at director level, they are fully aware that investment is needed in the IT infrastructure. On an operational level, this has to be managed to ensure effective use of the IT infrastructure, to maintain Revit productivity. The last thing the CAD manager wants is a heavy Revit server that slows down local models and hinders the CAD team\u2019s productivity on live projects.\nFrom an IT standpoint also, the IT department managing the servers do not want slow servers overloaded with repetitive Revit data, hence the new working practices must include suitable housekeeping policies that keep the servers quick and lean, so as to handle the Revit 3D models.\nTraining Needs Analysis (TNA)\nCAD manager to director level\n\u201cWe will need to ensure that all Revit users undergo a Training Needs Analysis (TNA) to assess their training needs and requirements. This will give us a picture of exactly what they need and allow us to use the training effectively and get the most out of our incumbent training provider. It is imperative we allow the team to work with their strengths but also get trained up on areas of weakness, so that we have fully rounded Revit users that are effective and productive.\u201d\nCAD manager to the CAD team\n\u201cWe need you all to undergo a Training Needs Analysis (TNA). This is to assess your existing Revit knowledge (if you have any) and what areas you need to work on to make sure you are fully trained on every aspect of Revit you need to perform your role within the practice effectively. We need you to make sure that you include everything in your TNA so that we can get the best training for you from our training provider.\u201d\nEffective training on any CAD product is imperative. The CAD manager is using the TNA to ensure each Revit user is trained to their strengths and that any areas where their product knowledge is weak is thoroughly assessed and appropriate training given. The TNA is done individually per user to make sure that each user gets training tailored to them. It also provides the user with the reassurance that with the new CAD product, in this case Revit, they will be fully trained and prepared to use the product on live projects that the practice is, or will be, working on.\nTraining delivery\nCAD manager to director level\n\u201cWe will be using our incumbent training provider to provide all of our Revit training based around the findings of the TNA exercise. We will be making sure that all training is scheduled around projects and staffing levels for those projects. We have to make sure that key project staff are trained sequentially, and to allow for at least one key member of project staff to be in the office at any given time when the other project team members are being trained.\u201d\nCAD manager to the CAD team\n\u201cYou will be receiving individualised training based on the findings of your completed TNA. We need each of the project teams to build a training roster that allows for one key project team member to be in the office at any given time. We have to make sure that our existing projects are covered at all times. This will be even more important once Revit and BIM are implemented, as we will need to make sure that our central Revit models are fully co-ordinated with external contractors, partners, engineers and the client.\u201d\nThe CAD manager has a responsibility to manage all of the ongoing projects in the practice. Training will disrupt the smooth running of those projects unless it is managed properly. The TNA process gives exact results on what training is required per user, so the CAD manager can ask the project teams to build a training roster per team, ensuring project coverage, and making sure there is at least one project team member in the office when training is ongoing.\nAs CAD manager, this management is extremely important to the efficient running of the current projects. There is also a responsibility on the CAD manager to make sure that each team is trained in turn, rather than one member at a time. You want each team to be fully trained up, so that they can hit the ground running. But there will always be one key project member remaining (the one that stays in the office when the others are being trained). The solution here is to train that key member when another team is being trained. For example, Key Project Team A Member gets trained with Key Project Team B and vice versa. That way, full project coverage is maintained but all teams get trained with sensible timescales.\nFurther training and certification\nCAD manager to director level\n\u201cOnce the CAD team is bedded in and trained on Revit, we need to think about intermediate and advanced training and build a budget and a schedule for this. It is imperative we keep the CAD team lean, trained and efficient on the latest versions and methodologies. The only way to do this is regular scheduled training. We also need to benchmark their Revit knowledge by way of Autodesk Revit Certification. All of our Revit users should become Autodesk Revit Certified Professionals.\u201d\nCAD manager to the CAD team\n\u201cNow that we have Revit set up in the practice and training is ongoing, we need to think ahead towards intermediate and advanced training. We would like you to keep notes of all aspects of Revit that you feel you will need to perform your role within the practice. You will also be offered the opportunity to gain Autodesk Revit Certified Professional status through our training provider.\nIn order to obtain this qualification, you will need to get approximately four hundred hours of active Revit usage under your belts. Keep a note of your hours and when you hit the four hundred mark, please inform us. We can then get you an examination place for certification.\u201d\nHere, the CAD manager is, in essence, future-proofing the practice\u2019s investment both in Revit and the staff. In order to get a return on that investment, the CAD manager is already looking forward to future training for the CAD team. Staying on top of a CAD product once it is in place is imperative, to stay lean, and make sure the staff are fully aware of new version upgrades and methodologies that come with it. With Revit, though, a new version updates all of the Revit data to that particular version, and there is no backward compatibility. All of the project team, including external contractors, partners and the client would need to upgrade as well. Everyone involved with that particular project has to be on the same version of Revit.\nConclusion\nIn this second installment about transitioning to BIM, we have looked at the implementation of Revit and training of the practice staff to use Revit. The physical implementation of the Revit software is fairly simplistic and Autodesk provides full documentation on how to do this. What is paramount here is the server infrastructure, and how it is set up to the advantage of the CAD team using Revit.\nQuite often a new server is set up from scratch to allow for the worksharing of Revit projects, along with a professional cloud storage account for use with external parties and for archiving. This would be individual to each and every organisation that uses Revit, but it must be set up to ensure that Revit is used to its full advantage by way of central and local Revit models.\nTraining is also imperative. For any organisation to get full return on investment on any software that is purchased or subscribed to, the staff must be trained. The training should be mandatory for all appropriate staff, along with (possibly, in this case) Autodesk Certified Professional status. The Certification qualification then provides a staff benchmark to competency levels, plus it will also highlight any further areas of training required.\nOverall, a transition to a BIM application such as Autodesk Revit should not be undertaken lightly. It requires organisation, planning and a will to move forward and utilise the tools Revit provides. With BIM 2 compliance on the horizon for January 1, 2016 in the UK, it is in every organisation\u2019s best interests to make the move to BIM and its associated CAD applications as soon as possible.\nThe first part of this article can be read here.\nThe third part of this article can be read here.\nAbout the author\nShaun Bryant is an Autodesk Certified Professional with twenty-six years total industry experience using AutoCAD and Revit.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/making-light-work-part-iii\/","title":"Making light work: part III","date":1175212800000,"text":"Following on from last month\u00dds look at Global Illumination with mental ray, the third part of Darren Brooker\u00dds four-part comparison of the interior lighting methods available within 3ds Max takes us through three-point lighting.\nOur first two articles on the various interior lighting tools available within 3ds Max both looked at Global Illumination; first in context of the radiosity renderer, and then with mental ray. This month we\u00ddll leave GI algorithms behind and look at how to fake the look using the standard lights and a technique called three-point lighting.\nThe convention of three-point lighting is one that is firmly established in cinematography, and has become one of the main foundations for CG lighting too. One of the principal reasons for this is that the technique helps to emphasise three-dimensional forms within a scene using light.\nUnsurprisingly, there are three lights involved in three-point lighting, and each has a specific function. Providing the main illumination in a scene, the key light is the dominant light, or the one that casts the most obvious shadows. The job of fill lights is to model the indirect lighting that is produced by direct light bouncing off an environment\u00dds surfaces. Helping to separate the subject from the background, the backlights give a scene depth.\nWe\u00ddll start by placing the key light, which represents sunlight coming through the windows to the right of the interior. Create a Target Direct light and once created right-click the Transform button to bring up the transform dialog and type in X:-13m, Y:-17m, Z:12m. Rename this light DirectKey01, then place the target at X:3m, Y:0m, Z:1m. Turn on Raytraced shadows, as these will need to penetrate the raytraced glass material, and give the light a Multiplier of 8.0.\nIf you render now you will see a largely black interior, because there is no indirect lighting component being calculated (see Figure 1). All you should see is the direct component of this one single light, as well as the reflective elements within the scene, such as the lighting fixtures, which also have a Self-Illuminated material applied. The next task would be to place the secondary key lights within each of these fittings. Rather than do this manually, choose File > Merge and select the 03threePointLightingCeilingSpots.max file. Select all the spotlights and choose OK.\nSelect any one of these lights and within the Modifier Panel take a look at the way the light is set up. Firstly, you can see that shadow maps are being used here rather than raytraced shadows, which is primarily because we don\u00ddt want sharp shadows from these lights. You should also notice that the light has Decay enabled and set to start at 2.0m, as the decay from interior light fittings is very noticeable, compared with sunlight which does not decay over the few metres it travels into a room. Additionally, the light has Far Attenuation set to Start and End at 2.0m and 8.0m.\n{mospagebreak}\nIf you render now you\u00ddll see that the ceiling lights now also cast light, but that again these only emit direct light and that we\u00ddll need to mimic the indirect light that would be bouncing off the surfaces of the interior (>see Figure 2).\nThis is where fill lights come in. Fill lights are used to mimic this indirect lighting component and are coloured to match the surfaces they represent. It\u00dds a good idea to start with the most dominant surface in the scene in terms of indirect light, which in this case would be the floor. If you open the 03threePointLightingFill1.max file and select the FspotFill01 light, you can see that it is positioned underneath the floor, casting light back up into the scene and is coloured brown to mimic the light that would bounce off this surface. You can also see that the light has attenuation set to keep the illumination local and has no shadows, as this type of light would not cast noticeable shadows. Furthermore, the Specular component of the light has been turned off, as you can see in the Advanced Effects rollout, because bounced light does not consist of a specular component. Finally, you can see within the Spotlight Parameters rollout that the light is set up as rectangular, which is set to match the room\u00dds proportions, as you can see in the viewport labelled FspotFill01.\nIf you take a look at the remaining fill lights in this scene, you can see that these lights represent the indirect light from the primary wall surfaces. FspotFill02 models the light bouncing off the yellow wall to the left of the scene. FspotFill03 is coloured blue to represent the indirect light from the rear wall, whilst FspotFill04 models the light bouncing from the pools of direct light on the floor. Finally, FspotFill05 represents the light bouncing off the right-hand wall with the windows.\nIf you render now, you can see that our scene is really shaping up, when compared with any of our Global Illumination techniques from the two previous tutorials (see Figure 3). There are a couple of problems though, the area underneath the glass stairs does not look quite right and the shadows require attention. Furthermore, the rear-right portion of the room is also too dark.\n{mospagebreak}\nTo correct the first of these issues, unhide the two lights in the scene and turn them both on. If you take a look at the parameters for these lights, you can see that one of them has a positive Multiplier value and has shadows turned on, whilst the other light has a negative value with no shadows. These two lights work in combination, with the negative Multiplier value of one light cancelling out the positive Multiplier value of the other light, leaving just the shadow rendered, as this is only on one light. This is a trick that can be very useful, particularly when you have a large area that needs illuminating, but the objects casting shadows are scattered around, like cacti in a desert environment for example. Rather than attempting to stretch your shadow map right across the environment, it\u00dds best to keep the shadow map (which is expensive in terms of memory at render time) tightly focused on the shadow casting object, making the rendering as efficient as possible.\nTo correct the second of these issues, open the 03threePointLightingFinished.max file and you will see that there is an Omni light in the affected corner of the room. With this light selected, in the top view you can see that it has been scaled along the length of the room to provide some fill light which has been stretched to accommodate the proportions of the room. This is a very handy technique for providing very localised fill light.\nOther things to note about the final solution include the fact that there are extra lights that provide very subtle contact shadows between the tops of the columns and the ceiling. Once you have reached this stage of fill lighting, any further lighting will likely be refining your solution in very subtle increments, as these lights do. Furthermore, there are some things that are best cheated into materials rather than lights. In this final solution you can see that the top of the right-hand wall has been selectively darkened by putting two versions of the original diffuse map into a Mix map, with one of these versions darkened using its Output controls. Within the Mix map, a Gradient Ramp defines the mix between the two diffuse maps and with the gradient focused along the top of the wall, the wall is selectively darkened along this top edge, which mimics how it would be darkened here due to ambient occlusion. (see Figure 4)\nAs you can see, the result of all of these lights is a fairly complicated lighting solution, but one that has no sampling issues and that will consistently take the same time to render no matter whether objects are moving within it or not. For approaches to rendering based around radiosity or photon mapping, the solution would have to be recalculated every time something moved within the environment. Sometimes it\u00dds better to fake the radiosity look. However, the flipside of this is that when you are asked to change the look of the lighting scheme, the process of setting this up again can be equally as involved. However, knowing each of these approaches and how to get the best out of each is essential to using 3ds Max\u00dds extensive lighting tools in the most effective manner.\nDarren Brooker is a BAFTA award-winning lighting artist who has worked at many top UK studios. He works for Autodesk as a product specialist. His book, Essential CG Lighting Techniques with 3ds max, is published by Focal Press\nClick here for Part l , Part ll orPart llll of this tutorial","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-bim-360-layout-connects-bim-model-to-the-physical-world\/","title":"NEWS: Autodesk iPad app connects BIM model to the physical world","date":1415750400000,"text":"Autodesk partners with Topcon for new BIM 360 Layout app that guides users to physical locations on site.\nAutodesk is targeting contractors with a new iOS app that connects BIM models to the construction layout process.\nThe Autodesk BIM 360 Layout app uses the BIM 360 cloud service to connect digital designs with the physical world by controlling a robotic instrument from Topcon. With a direct connection to the Topcon LN-100 Robotic Instrument via WiFi, users can be guided to physical locations on the site \u2013 based on points created in the 3D model that are represented in the BIM 360 Layout app.\nThe idea behind the technology is to replace error-prone manual layout techniques with what Autodesk describes as \u2018laser-guided precision\u2019. Accuracy, productivity and delivery of projects on-time and on-budget are the end goals.\nThe workflow is as follows. Contractors start with Autodesk Point Layout in connection with AutoCAD, Navisworks or Revit, and then import their model with field points into the BIM 360 Glue web service which can then be synced with the BIM 360 Layout app on the iPad.\n\u201cAs the layout process is directly connected to 3D models on our construction projects, rigor is placed on managing and leveraging the up-to-date, coordinated information by the entire project team for construction layout,\u201d said Ondrei Poliak, Virtual Construction Manager, PCL Denver District. \u201cIn our use of the BIM 360 Layout app which is connected to BIM 360 web services, we have streamlined the process of checking and verifying our sub trades\u2019 staked and installed components resulting in time and money efficiencies never before realised on this level.\u201d","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/trimble-awarded-development-contract-from-raytheon\/","title":"Trimble awarded development contract from Raytheon","date":1021334400000,"text":"Trimble has announced that it has been awarded a $2.1 million military development contract to supply Global Positioning System (GPS) technology for Raytheon Company\u2019s Miniature Airborne GPS Receiver (MAGR) 2000 GRAM SAASM (M2KGS) Program.\nUnder the contract, Trimble will incorporate its Selective Availability Anti-Spoofing Module (SAASM) technology in the ForceTM 5 GPS Receiver Application Module (GRAM-S) for use in Raytheon\u2019s MAGR 2000.\nTrimble\u2019s Force 5 GRAM-S is an embedded, dual-frequency GPS receiver module with Precise Positioning Service (PPS) capability. Designed in compliance with the NAVSTAR GPS Joint Program Office (GPS JPO) GRAM guidelines, the Force 5 provides unequaled performance for airborne and high dynamics applications. Raytheon\u2019s MAGR 2000, utilizing the Force 5, has been selected for use in a variety of U.S. and Allied military aircraft including the MH-53E Sea Dragon, F-16 Fighting Falcon, F\/A-18 Hornet, AV-8B Harrier, ASTOR and E-2C Hawkeye.\nSAASM is the architecture selected by the Joint Chiefs of Staff, which implements the next generation security functions for all GPS PPS military users. The use of SAASM significantly enhances and ensures the U.S. and Allied military user the ability to obtain precise GPS position, velocity, time, and other GPS sensor information in all types of environments. Trimble\u2019s SAASM technology was granted GPS JPO security approval on May 1, 2000.\nLike the current Force 5, the SAASM-equipped GRAM-S will provide the features necessary for Instrument Flight Rules (IFR) operation including all-in-view tracking, Fault Detection and Exclusion (FDE) and step detector in accordance with RTCA\/DO-229, predictive FDE in accordance with FAA Notice 8110.60, and European B-RNAV aircraft operations. When operated in the PPS mode, it will also provide Navigation Warfare (NAVWAR) features including Anti-Spoofing and high Anti-Jamming protection including integration with Controlled Reception Pattern Antennas.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/bam-and-autodesk-construction-iq\/","title":"BAM and Autodesk Construction IQ","date":1563840000000,"text":"Construction is a highly complex and fast paced activity to project manage. All too often, errors and delays on site can lead to the most expensive cost over runs, safety issues and can quickly become an exercise in fire-fighting problems. What if there was an AI system that had your back?\nThe use of artificial intelligence for vertical tasks might still be embryonic, but AI is working behind the scenes in nearly all of the digital services we use every day \u2013 email filters, social media, job matching, relationship matching, image recognition, chatbots, face detection, web searches, music recommendations, mobile banking, maps, flights and car rides!\nMuch has been written about the potential use of AI in the design of engineering components and buildings, helping signature architects generate complex forms, but much less has been said about how it could be applied to the discipline of construction, in both safety and project management.\nConstruction IQ\nAutodesk Construction IQ is a service for applying AI to building project management information. A long time in development, it\u2019s gone through quite a few name changes along the way, seemingly to avoid falling foul of other (non-competing) similarly named business applications. It was formerly known as Project IQ, then Construct IQ, and now under the Construction IQ brand is part of Autodesk\u2019s BIM 360 cloud platform.\nUp until July 2019 it was only available to companies that hosted data on the BIM 360 servers in the US. This has now been expanded to all BIM 360 hosting locations and can be added as an additional service to BIM 360 subscriptions.\nFor its development, Construction IQ has literally been fed data from tens of thousands of real-world historical projects, covering a wide range of building types: residential, university housing, office buildings, towers and airports. It has learnt from millions of issues and checklists and it continues to learn all the time from every new project it analyses.\nUsing this past knowledge, the system acts like an old hand that knows to look out for specific warning signs which might be imperceptible to a human. Construction IQ is looking out for a wide range of risks, from issues that would jeopardise project delivery times, to those that would impact health and safety. This can be at a high overview level or at a granular level, identifying contractors, employees which carry the most risk. As projects are run through BIM 360, Construction IQ subscribers get a configurable dashboard where a broad overview of the analysis across multiple projects is displayed. Obviously as the system is cloud-based it can be run on a desktop or mobile device, such as a phone or tablet.\nA traffic light system indicates the quality risk for today\u2019s planned activities, safety risk, the number of sub-contractors at risk, high risk issues, water risk issues, checklist and overdue issues. This can also include a live camera feed from the site, weather information and a timeline of risks. By selecting a topic area, the usercan drill down into more detail on who, why and what. The software will even prioritise what it thinks are the most critical issues and will learn the risk profiles of individuals.\nThe BAM experience\nAt Autodesk University London last month, Simon Tritschler, technical development specialist and Michael Murphy, digital construction operations manager at BAM Ireland gave a very candid talk about their experiences with deploying Construction IQ and the lessons learnt about their project management and construction processes.\nBAM Ireland was established in 1958 as Ascon Contractors and rebranded as BAM Contractors around 2008. The company was initiated as a Dutch-Irish joint venture and today it\u2019s the country\u2019s largest civil engineering contractor. BAM currently employs over 2,000 people directly and indirectly and its development focus is on the digitalisation of construction, increasing standardisation and continuing the development of its pre-construction management.\nOut of all the BAM groups, BAM Ireland seems to have a keen appetite for deploying the latest technology and seeing how it can improve its digital construction processes. It puts this down to the fact that it\u2019s a small part of the operation and can therefore be more digitally agile.\nThe company is also heavily into lifecycle. As BAM builds and operates many facilities for clients for up to 25 years, the incentive to manage risk and maintenance in the short and long-term gave additional impetus to join the Construction IQ beta in 2016.\nWhile BAM Ireland is obviously a big proponent of BIM and construction simulation it also wanted to rid itself of the paper process so adopted BIM 360 to host project data and digitise its data col lection and distribution workflows alongside the model.\nOnce BIM 360 had been implemented and initiated with Construction IQ, the team started to run the insight capability on projects to identify which contractors were the greatest risk to project success. The answer came back that it was clearly BAM Ireland itself! Construction IQ \u2018red flagged\u2019 inconsistent document issues in existing projects and this turned out to be project members who were raising issues but failing to close a large percentage of them in the system once they had been fixed.\nIf this system is taking a lot of heavy lifting away, it\u2019s giving us a laser sharp focus in terms of what the genuine health and safety issues are Michael Murphy, BAM Ireland\nMichael Murphy, BAM Ireland\nThis meant projects appeared to have many overdue issues, even though they didn\u2019t. This undermined the whole digital process and so BAM Ireland\u2019s first task was one of internal education and house- keeping, as a lot of its collated data was \u2018garbage\u2019. This dramatically brought down the number of open issues in projects. However, the system still identified real and critical issues within the huge haystack of project documents, of which there can be 10,000 \u2013 15,000 per building.\n\u201cA huge problem here for us is overdue issues. If we fix these problems early, they\u2019re cheaper to fix,\u201d explains Murphy. \u201cIf we start with a $25 issue that could be fixed in design, if that gets to construction that increases to $250 to fix. If it\u2019s spotted during snagging that will be $2,500. If it gets into operation it could cost $250,000. Knowing where the issues are early on is essential.\n\u201cIf this system [Construction IQ] is taking a lot of heavy lifting away it\u2019s giving us a laser sharp focus in terms of what the genuine health and safety issues are. We don\u2019t have to explain how it works to the team; it just happens! Not only is it pointing at major issues but it\u2019s giving us more time.\u201d\nSince adopting Construction IQ, the company has realised a 20% improvement in on-site quality and safety and a 25% increase in staff time spent on high- risk issues, both of which are products of improved decision-making, resulting from increased visibility into issues and risks. There is also comfort in the knowledge that as Construction IQ continues analysing every BAM Ireland project, it\u2019s refining its prediction capability, improving its accuracy.\nFor now, Construction IQ has been used on new construction projects, but the team is also interested in applying the AI to span the entire project lifecycle for maintenance data, which could identify trends in longevity of supplied components and help define better long-term building design.\nThe team wants to use Construction IQ to help standardise efficient processes across the BAM Group. From the early exposure, it\u2019s clear that the application of AI and machine learning is providing extremely pertinent information about current and potential risks to the project portfolios.\nThe future\nConstruction IQ hasn\u2019t yet allowed BAM to look at how lessons learnt can be used to impact project scheduling, which is something the team would like to investigate, how it could impact managing sub- contractors and sequencing for example, which remains a manual process.\n\u201cThe reality is that construction projects tend to have gone badly wrong before anyone has realised it,\u201d concludes Murphy. \u201cIt is being built or has been built completely wrong. What we are seeing now is that we get early indicators, what the major issues are, [and with] which subcontractors.\n\u201cYou can identify and remedy issues earlier, no more floating under the radar. We have more time to have better oversight and, as the general contractor on projects, we have got to own it and anything that can help us \u2018own it better \u2018has got to be a good thing and we realise we are just scratching the surface of what is possible.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/harness-the-power-inside\/","title":"Harness the power inside","date":1550707200000,"text":"Nobody ever thinks their workstation is too fast \u2013 it can always go quicker. Speed is usually addressed with bigger processors, but developers are now looking at smarter ways to harness the power inside, writes Greg Corke\nFor years, BIM and design visualisation software has relied on a brute force approach to boosting performance. If you want renders back quicker or models to move more smoothly in the viewport, simply throw more processing power at the problem.\nAnd why not? In many cases it works extremely well. You only have to see what AMD\u2019s 32-core Threadripper CPU can do for render times to know there\u2019s plenty of mileage in this \u2018bigger equals better\u2019 approach.\nBut it doesn\u2019t always work. Take, for example, the long-standing issue of poor 3D performance when working with large models in mature CAD and BIM software: no matter how powerful your workstation\u2019s GPU is, frame rates will simply not go up. Much of this is down to the fact that the code is old and simply not designed to take advantage of modern GPU hardware.\nRather than tackle the issue head on, many CAD software developers have chosen to simplify the way models are represented in the viewport. But things are now changing. The new beta graphics engine in SolidWorks 2019 allows users to move huge 3D models smoothly without compromising visual quality. It works by putting the GPU centre stage.\nFor years, the GPU took a supporting role when it came to 3D graphics. When modern CAD software was first developed, the GPU didn\u2019t have the power to take on so much responsibility, nor did the graphics APIs exist to allow it to do so.\nWith the new OpenGL 4.5 graphics engine in SolidWorks 2019, assemblies that previously only used 5% to 10% of the GPU\u2019s processing resources are now maxing out high-end graphics cards. The performance increase is quite phenomenal, breathing new life into old hardware. We hope to see BIM software developers follow suit.\nWhile this is a case of taking good advantage of modern APIs, other software developers are exploring new ways to get more out of a workstation\u2019s GPU. In 2017, Ansys shook up the world of simulation software with Ansys Discovery Live, a new tool that promised \u2018instantaneous simulation results\u2019 by accepting a small but manageable trade-off in accuracy. Importantly, it used the GPU in a different way to other simulation tools. Instead of focusing on absolute precision, the development team asked the question \u2018what would be good enough for design exploration?\u2019, and harnessed the power of the GPU accordingly.\nSimilar developments are happening in design visualisation. Nvidia has built \u2018AI denoising\u2019 into its Optix ray tracing engine. So instead of having to wait for the GPU to compute thousands of passes, it conducts a few passes and then uses deep learning to remove the noise. It essentially gives a best guess as to what a fully resolved image would look like. This technology has already been implemented in Chaos Group\u2019s V-Ray NEXT.\nNvidia is now taking ray trace rendering one step further by actually changing the architecture of its GPUs. AI denoising in V-Ray NEXT works on most modern Nvidia GPUs, but the new Nvidia RTX graphics cards are specifically architected for ray trace rendering. They include three different types of processing cores \u2013 RT cores for ray tracing, Tensor cores for deep learning and CUDA cores for shading. Together with software specifically written to take advantage, RTX promises to massively reduce the time it takes to deliver ray traced images. Indeed, Chaos Group has demonstrated this happening in \u2018real time\u2019 in Project Lavina. We hope to see commercial applications appear in 2019.\nOf course, we can\u2019t expect things to change overnight. It took DS SolidWorks two years to develop its new graphics engine using an API that works with most graphics cards six years old or less. Nvidia RTX demands a change in both software and hardware.\nBig transformations like this often need to happen for our industry to truly advance, but the challenges Nvidia faces with RTX adoption are nothing like those of CPU manufacturers looking to introduce revolutionary new technology.\nFor years, we\u2019ve stored information in bits \u2013 either a 1 or a 0 \u2013 but quantum computing introduces the notion of qubits, which can represent both 1 and 0 at the same time. By offering multiple states inside the CPU, the technology promises to deliver computers that are thousands of times faster than those currently available and we expect exciting applications for simulation in particular.\nWhile IBM has just unveiled its Q System One, the first commercial quantum computer, to much fanfare, don\u2019t go holding off on your workstation purchase quite yet. A shift in CPU architectures will likely take decades and there\u2019s still plenty of life in the x86 architecture. All eyes are already on Intel\u2019s monster 28-core Xeon CPU, which is due to launch soon, and there\u2019s also AMD\u2019s 64-core Epyc to come.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-leica-simplifies-reality-capture-with-blk3d\/","title":"NEWS: Leica simplifies reality capture with BLK3D","date":1528761600000,"text":"Handheld device features stereo camera and electronic distance measurement technology for real-time, in-picture 3D measurement\nThe Leica BLK3D from Hexagon is a new 3D handheld imager that allows users to take immediate and precise 3D measurements from any 2D image it captures. The compact device, which is similar in size to a smart phone but weighs 480g, is intended to replace manual measurement and documentation processes using tape measures, distance meters, photos and written notes.\nAbove all, the BLK3D is designed to be very easy to operate, so architects, builders, site managers, civil engineers, window fitters and others can all use the device with very little training. \u201cBasically, if you can take a photo you already have all the skills you need,\u201d says Tobias Heller, Product Manager at Leica Geosystems.\nThe BLK3D follows in the footsteps of the BLK360, the push-button compact 3D laser scanner AEC Magazine reviewed last month, but the BLK3D is a different device entirely. It combines a calibrated stereo camera with electronic distance measurement technology and advanced algorithms to allow users to take in-picture measurements with \u2018professional-grade accuracy\u2019.\nOnly one shot (two pictures) is needed but to improve 3D accuracy, up to three shots (six pictures) can be used. All images should be of the same composition but taken from different angles to increase the baseline.\nSmart snapping algorithms can help users select the right pixel on the image. Once picked, the software can automatically snap to the same point in other images.\nImportantly, all of the processing with the BLK3D is done on the device, so the results are instant, allowing users to make fast, informed decisions while still on site. Other systems that use photogrammetry to capture reality rely on powerful desktop workstations or cloud processing. \u201cIt\u2019s a massive simplification of the stereo photogrammetry process,\u201d says Heller.\nLeica is claiming the device offers millimetre accuracy in short range, by which it means anything under 5m. All images are geolocated using the built in GPS.\nThere are many different use cases for the BLK3D. One of the main applications is documenting the location of assets during construction. For example, electrical cables or other MEP installations, before they are hidden behind dry walls. Here, the device can also be used to build up simple 2D plans on screen, with captured images \u2018stitched\u2019 to specific walls and multiple images captured over time.\nThe BLK3D can also help reduce the number of people needed for simple survey. For example, a civil engineer documenting cracks on a building can now do everything by themselves. Previously, it may have meant one person holding a tape measure up against the crack while another takes a photo.\nOther potential applications include, window fitters quickly taking measurements for fast, accurate quotes; structural engineers monitoring and documenting progress of structural damage; and architects measuring site conditions for both renovation and new build projects.\nIn the future, the BLK3D could be fitted to a drone for applications such as roof inspection. Heller admits that adding this capability is on the development plan and the only thing that would need to be written into the software would be remote control firing of the photo, over WiFi.\nThe beauty of the device is that measurements can be taken live while on site. However, once back at the office, the device can Be synced with a laptop or desktop computer over USB and more measurements taken. Data can be stored as a digital asset for retrospective measurements. In the future this capability will be available as a cloud service, which would be useful for as-built facility documentation.\nBattery life is pretty good. Leica told AEC Magazine that around 1,000 pictures can be taken on one charge. For a standard user, this should last the whole day, but some of its beta test customers have been taking around 5,000 pictures a day, so batteries are easily replaceable with spares carried in a bag.\nPricing has not yet been finalised but Leica told AEC Magazine it was looking to make the device available for around 3,000 \u2013 4,000 Euros.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/artificial-intelligence-ai-frontiers-in-construction\/","title":"Artificial Intelligence (AI) frontiers in construction","date":1550534400000,"text":"Mariusz Gorczyca, R&D structural engineer at Kingspan, wonders if we are ready for the level of digital ingenuity promised by AI and neural networks\nAs part of the Kingspan research team, my passions lie with the development of structural mechanics and how we can further enhance the technological development of the built environment. As a part of my masters thesis I was working on the applications of Artificial Intelligence (AI) and Machine Learning (ML) in the AEC industry. My research looked at how AI and ML are shaping the way we work, how projects are managed and delivered and most importantly, the question of whether the industry is ready to embrace this level of digital ingenuity.\nAI for architects and engineers\nIt\u2019s no secret that public attention on AI has rapidly increased recently, despite the fact that the technology has been slowly developing for the past 70 years.\nIf we consider that structural mechanics has been developing accurate theoretical models for predicting strain and stresses for the past few decades and that these theoretical models require a fixed set of input parameters such as material properties, boundary conditions etc. to produce results such as deflection, stresses etc. \u2013 it comes as no surprise that this is a pretty complex and time-consuming process. Therefore, because of these complexities, experienced engineers are often needed to interpret the results for other parties. This is where AI can support this level of development, by using ready collated data to predict behaviours and streamline research processes.\nWe can also look at how the iterative nature of architects, engineers and contractors working together with a limited project budget has its complexity. Proven solutions for overcoming project hurdles are generally preferred over innovative and ground-breaking ideas, however such situations could be changed by introducing tools which would offer fast yet accurate predictions of actual results \u2013 once again, a job for AI.\nModels which give fast yet accurate results are termed Reduced Order Models or Metamodels and can be based on AI methods. Metamodels typically require less input parameters and offer results in shorter time. Some examples of metamodels could include predicting wind loads on complex building geometries [1], estimating remaining structural service life [2] or predicting concrete strength based on mixture ingredients proportions [3].\nIntroducing such methods will without doubt generate project efficiencies on both time and cost. Whilst still in their early adoption stages, it won\u2019t be long before this becomes a proven preferred solution.\nAI and neural networks\nThe most crucial element for any AI-based solution is the underlying data used for the process. Data contains examples of inputs and outputs which can be both collected or generated. What\u2019s important, is the data must be representative of the task in order to create a reliable model. Out of several different types of models, Artificial Neural Networks (ANN) have been proven to outperform other models in most cases.\nArtificial Neural Networks are computing systems inspired by biological neural networks that effectively replicate elements of a living brain and are one of the most commonly used tools in ML as they are designed to replicate the way humans learn.\nSuch systems \u201clearn\u201d to perform tasks by considering examples of activity and measuring repeated outputs \u2013 they are excellent at finding patterns which are often too complex for a human programmer to extract and teach the machine manually.\nThe architecture of an ANN contains neurons organised in a three-layered structure \u2013 the three groups of layers are generally named: input layer, hidden layer(s) and output layer.\nNeurons are connected between these layers by weights which are unknown at the beginning of the process. Those weights are adjusted in the learning process.\nSo far in the industry, using ANN, only \u201cexclusive\u201d metamodels have been created. This means that a metamodel built for a specific structure can not be reused for a similar structure or any other altered asset (this limits the practical value of the metamodel).\nHowever, research by M. Nourbakhsh of the Georgia Institute of Technology, proposed a set of 25 parameters describing any structural element within any structure with 1D structural elements. This approach led to a metamodel which could be applied to a wide range of different structures and thus is \u201cgeneralisable\u201d [4]. The potential of this solution is enormous. Such a metamodel could work as an intelligent assistant helping designers make better decisions. The assistant could run in the background and provide near instant feedback on each decision on multiple projects.\nWork environment and data flow\nThe idea of building \u201cgeneralisable\u201d metamodels was introduced only recently. Therefore, before any practical application it needs to be exercised and tested since nobody in the AEC industry can rely on a black-box mechanism! Thus creating a perfect opportunity and place for my research to start.\nThe lack of publicly available data meant I had to acquire new data for the purpose of my study. Creating structural models and extracting data manually would have been extremely time consuming and probably impossible within a given time frame. Instead, the required data was generated in a semi-automatic manner with the help of Autodesk Dynamo and its connectivity with other applications. The software use and how I exercised the connectivity between different pieces is shown in figure 1, hence creating a parametric environment.\nThis new parametric work environment allowed for a highly automated process, for which, in total, 12 different geometries of example structures were created using three unique parametric models, each controlled with a single parameter. My models contained 3D space frames and 3D space trusses (examples of these structures can be seen in figure 2) with the same boundary conditions assigned to structures of the same type.\nThe number of structural elements in the models ranged from 250 to 1,600. In total, more than three million members were created, analysed and eventually used in model training. To ensure variability in data, each member was assigned a random cross section before running the structural analysis. With that in mind, you can see that there was literally no chance to generate two models exactly the same.\nTraining data \/ building metamodels\nOnce the environment was set up, data generation could begin. It took approximately 48 hours on an average laptop to generate 500MB of data saved in .csv files. Each data point contained a set to 25 input parameters with information about location of the member in space, applied load and supports, cross section characteristics and summarised stiffness of members close to the analysed member. For each data point one additional feature was saved, namely maximum von Misses stress in the member.\nFurther work continued outside of Dynamo with a help of the python programming language and available external libraries. These libraries are already well developed and make working on AI solutions easier. It is highly recommended to get an intuition about the data by checking different statistical properties at the very beginning.\nIn order to build a metamodel with help of ANN the data needs to be split into two different sets: training set and testing set. Further ANN models should be defined and trained, here a single hidden layer with 30 neurons was used. After which the model can then be tested with \u201cunseen\u201d data from a saved test set.\nPredicting stresses with AI\nOne of the tests we carried out verified that metamodels can be used to predict results of \u201cunseen\u201d structures but within the same class of structures (dome, slab, wall, etc.).\nThis means, for structures in two classes high quality predictions were achieved showing the R2 score as high as 0.75 (where the maximum is 1.00). On the average, predicted stresses in members varied by 10% to 20% from true values. For one class, where less complete structures were included in the training data set, predictions were arbitrarily wrong. It demonstrates how the quality of data is important for AI-based solutions.\nAnother test checked if a single metamodel combining knowledge from 12 various structures can predict results of \u201cunseen\u201d structures. The result showed that the amount of generated data was not sufficient for the model training and the ANN algorithm was not able to recognise patterns in the training data. It shows the importance of the data quantity against model generalisability. This means that a metamodel for general application would need a data set bigger than available to any individual but can be generated in a cumulative effort of the industry.\nThe application of AI in the field of structural engineering brings as many opportunities as challenges. Civil responsibility of engineers sizing structural systems requires accuracy and confidence much higher than it was achieved with metamodels in this work. At the same time results predicted with metamodels give a good indication of structural performance which otherwise would need to be consulted with a structural engineer.\nThis is but one element of my research, intended to show benefits from AI-powered solutions in the hands of engineers and architects. The above demonstrates a potential for a new generation of engineering tools which will help work smarter and more efficiently. It will help engineers and architects shift their attention to the creative parts of work while computers complete the remaining mundane parts.\nMariusz\u2019s research is the basis of his talk at BIM Show Live 2019, on Thursday 28 February; Application of artificial neural networks in static structural analysis where further examples will be shown on how AI and ANN are effective solutions for providing efficiencies on construction projects from the initial concept stages, and enabling soft landings.\nReferences\n[1] S. Wilkinson, Approximating Computational Fluid Dynamics for Generative Design, University College London, 2015\n[2] W.Z. Taffese, E. Sistonen, Machine learning for durability and service-life assessment of reinforced concrete structures: Recent advances and future directions, Automation in Construction, 77\/2017\n[3] M. S\u0142onski, A comparison of model selection methods for compressive strength prediction of high-performance concrete using neural networks, Computer and Structures, 88\/2010\n[4] M. Nourbakhsh, Generalizable surrogate models for the improved early stage exploration of structural design alternatives in building construction Georgia Institute of Technology, 2016\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-graitec-customers-given-free-revit-tools\/","title":"NEWS: Graitec customers given free Revit tools","date":1437436800000,"text":"Advance Powerpack 2016 designed to deliver increased functionality, better control and more automation for Revit\nGraitec is offering the Advance PowerPack for Autodesk Revit 2016 as a complimentary benefit to all its subscribed Revit customers.\nThe recently released Revit add-on includes a range of productivity tools for modelling, smart annotation and auto-dimensioning. There is also a Revit family manager.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nHP launches new remote graphics subscription\nCupix brings virtual job site navigation to Procore\nNEWS: Rhino users introduced to GPU rendering\nKapio Cloud construction platform is \u2018BIM, AI and Big Data ready\u2019\nAllplan 2005\nRevit importer added to SketchUp 2023\nZotac ZBox Pro External GPU Box launches\nFaro releases Hybrid Reality Capture\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/how-ai-is-impacting-building-simulations\/","title":"How AI is impacting building simulations","date":1537488000000,"text":"Dr Parag Rastogi, lead building physicist at proptech firm arbnco, explores how the power of Artificial Intelligence (AI) and machine learning can be harnessed to optimise energy efficiency in buildings\nArtificial Intelligence and machine learning are increasing in prominence in the building, architecture and construction sectors. One area that is ideally suited to take advantage of these powerful new technologies is building energy simulation.\nThere are various reasons why building energy modelling is a vital resource. Whether it is to demonstrate compliance with legislation such as the Minimum Energy Efficiency Standards (MEES) through EPCs, increase energy efficiency, or investigate poor performance, the ability to simulate what-if scenarios is crucial to informed decision-making.\nThe current applications of AI on the market now do not replace human judgement. Rather, where AI comes into play is the ability to run millions of calculations in seconds to ensure feasible options are not disregarded for lack of time or resources for analysis.\nRemoving laborious manual calculations frees up energy assessors to use their time to greater effect, creating more value for the end client. It would be impossible to build a perfect energy model of every individual commercial property, and every renovation option. Collecting and analysing data at scale is an expensive and slow undertaking, even if the data exists. Ultimately, therefore, AI enables scalability.\nThe promise of AI is that it solves the bulk of the problem \u2013 processing 80% of the options can allow the user to focus in on the 20% \u2013 ultimately increasing efficiency and using their expertise to greatest effect. AI will be an important lever to unlock the potential of building physics to improve design, engineering, and architecture. Digitising the shelves and shelves of documents about buildings will mean that every element of a building becomes more searchable and accessible, making problems easier to detect, locate and solve.\nConscious & unconscious bias\nGiven the hundreds of variations and thousands of combinations to consider in any given building, energy managers use their experience and knowledge to make recommendations on retrofit strategies. When doing so they are making judgements based on unconscious and conscious bias \u2013 for example, making the assumption that a particular retrofit strategy will succeed because it had a positive impact in a previous similar project.\nBy running a very large number and combination of options and strategies, AI and machine learning can help remove that bias, suggesting options the assessors themselves may not have thought of. Every facilities manager and energy assessor has different knowledge and experience, and with that comes blind-spots, which this process could help reduce.\nSo, is AI really coming for our jobs?\nAI isn\u2019t perfect, it certainly doesn\u2019t replace judgement or engineering education; often the first piece of advice given to engineers exploring AI is that if you put garbage in, then you will get garbage out. Data quality and quantity are both crucial.\nWhere its strength lies is in identifying patterns faster, with potentially better accuracy. Ultimately, AI and machine learning are new mathematical tools that must be adapted for purpose. It isn\u2019t a case of simply introducing them into the analysis and expecting a miracle. In any case, the smartness and appropriateness of the recommendations is irrelevant if they are not acted upon in the physical world!\nApplying machine learning to retrofit exploration\nMy interest in applying machine learning to building simulation stems back to my extended research stay at the RIKEN Institute for Advanced Intelligence Project in Tokyo, Japan, where I collaborated with Dr Mohammad Emtiyaz Khan, a world-leading researcher in data science and machine learning.\nIn August 2017, I was given the opportunity to build upon our experiments thanks to funding from The Data Lab, an innovation centre focused on helping Scottish industry collaborate with public sector and universities to promote the use of data science and artificial intelligence.\nThe nine-month research project enabled building energy simulation company arbnco to work with the Department of Architecture at the University of Strathclyde to understand how best to introduce AI in building simulations.\nThe project focused on the arbn consult platform software which provides the ability to quickly and accurately assess the Energy Performance Certificate (EPC) rating of a commercial property, producing bespoke and fully-costed retrofit packages for delivering improvements in energy performance.\nThe software, based on the industry-standard Simplified Building Energy Model (SBEM), tracks regulated energy loads, fixed building services such as heating, cooling, lighting, controls, ventilation, hot water, fuel and refrigerants.\nUsing this energy model, the platform delivers an analysis of commercial real estate improvements to determine retrofit strategies for mitigating risk and improving financial performance of commercial real estate.\nProject outcomes\nOur team of researchers combined expertise in building science, machine learning, automation in construction, software, and building simulation. Over the course of the project we explored ways that machine learning can be used in tandem with building simulation software to make quicker, more informed decisions.\nThe research project enabled arbnco to upgrade its existing retrofit-exploration software arbn consult. The arbn consult software offers users the possibility of a Multi Objective Optimisation, taking into account both EPC ratings and costs to retrofit commercial properties. Users can input constraints such as cost or payback period to determine the optimum retrofit strategy.\narbnco\u2019s product offering is currently based on UK compliance software. Going forward, it intends to apply this research to dynamic energy modelling software such as Energy Plus, developed by the US Department of Energy, and ESP-r, developed by the University of Strathclyde, to increase global usage.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/bentley-strengthens-its-grip-on-reality\/","title":"Bentley strengthens its grip on reality","date":1495670400000,"text":"Bentley has been showing its point cloud and photogrammetry capabilities for several years. With growing interest and adoption, it\u2019s clear that new workflows and benefits are now there for the taking, writes Martyn Day.\nBentley Systems has been a keen acquirer of hot technologies and firms over the past five years. While cornering much of the analysis, simulation and document management arena, Bentley turned part of its acquisition strategy to focus on laser scanning\/ point clouds and addressing its core tool\u2019s ability to accurately handle mixed raster and vector formats.\nLaser scanning has always faced an uphill struggle. Scanners have been expensive to buy, complex to operate and generate huge files to manage, so have remained the preserve of the expert surveyor. When in 2011 Bentley acquired PoinTools, the company had the best tool on the market for managing and editing point clouds. The deal also brought the company\u2019s well-respected CEO Faraz Ravi into the Bentley fold.\nFast-forward to 2015, and Bentley acquires Acute 3D, a company with a technology that converts photographs into highly accurate 3D models through the use of complex algorithms. As a test case, we first saw Bentley use this application to create a highly detailed model of downtown Philadelphia from 28,000 photographs, which were uploaded to the cloud for processing and used to make plans for the Pope\u2019s visit to the city.\nRavi explains that now, without the need of expensive laser scans, anyone can create a reality mesh. All they need is a series of photographs, which can be taken with an iPhone, DSLR camera or even a drone.\nThe reality mesh can be used throughout construction, commissioning and operation. Regular scans can detect changes and deviations from the design model to the as-built and be used to impact changes to construction documentation.\nExpanding issues further, reality modelling also has a role to play in operations, enabling visual operation and inspection of virtual assets, and introduces new potential for product training and simulation in maintenance, performance monitoring, decommissioning and renewal\/replacement.\nHigh fidelity\nOne would assume that the most accurate survey you can carry out is with a high density point cloud, collecting millions of measured points. Historically speaking, this was the case, but with the new algorithms driving photogrammetry methods the difference between the two is becoming smaller. Mr Ravi explained to AEC Magazine that the results from their own tests of laser scanning versus photogrammetry prove that there is not much between them anymore in terms of accuracy and, for those times when both types of surveys can be performed, the Bentley ecosystem can handle a combination of both LiDAR (point cloud) and photogrammetry (reality mesh).\nProduct development\nThese technologies have driven internal product development to create Bentley\u2019s ContextCapture range of offerings for reality modelling, which link surveying and engineering. ContextCapture\u2019s new capabilities include cloud processing services, a mobile app and photo planning.\nA helicopter was chartered to obtain the aerial images that were used to produce the detailed model of downtown Philadelphia. Today on construction sites, advanced drones can do this job, but they need to be told what to do. New photo planning capabilities in ContextCapture allow engineering or survey professionals to plan the optimal camera positions and flight paths for UAVs to achieve required levels of accuracy.\nReality can also be captured easily back on earth and a new ContextCapture mobile app is designed to make it easy for anyone to create 3D reality meshes. Simply take photos with an iPhone, then upload them to the new ContextCapture cloud processing service, which sends a 3D reality mesh back to the phone or office.\nDesktop users can also take advantage of the power of the cloud to process reality meshes. ContextCapture uses GPUs to process the hundreds or thousands of photos needed to create a reality mesh, so to speed processing time for very large models (typically involving more than 30 gigapixels of imagery), users can use the dedicated ContextCapture Center in the cloud, which is architected for grid computing with multiple GPUs.\nContextShare is another new development that extends Bentley\u2019s ProjectWise connected data environment to securely manage, share, and stream reality meshes, and their input sources, across project teams and applications. Navigator Web is a new web application that delivers highperformance streaming of very large reality meshes through the browser to desktop or mobile devices.\nNew workflows\nBentley envisages reality modelling being carried out, used and accessed at the start of the process with mapping and early planning, through design, construction and operation. With new technologies come new workflows, and Bentley has been quick off the mark to flesh out point solutions that can be brought together to extend the benefits of using Reality Mesh throughout a design process. ContextCapture processing and meshing can be done either in the cloud or on-premise, stored centrally on ProjectWise and then distributed everywhere.\nConclusion\nBentley\u2019s foundation is the MicroStation platform, on which all the technologies it acquires or develops sit. The speed at which Bentley has managed to incorporate the new capability and include support throughout its product ecosystem should not be ignored. By having a single product stack, Bentley took an acquisition which placed it in the game later than Autodesk with its 123D Catch development, but provided it to all its customers in less than a year with enterpriselevel infrastructure to back it up. This is nimble footwork and demonstrates the commitment and belief that reality modelling is going to make a big difference. It also shows the benefit of having a single development platform.\nA year ago at the SPAR conference in Houston, CEO Greg Bentley explained that he believed there was significant potential for reality modelling in improved infrastructure engineering, in particular, from continuous surveying enabled by simple digital photography and UAVs, but admitted that he expected resistance from survey professionals, who were heavily invested in expensive laser scanning equipment. However, in just one year, Bentley said it was clear that reality modelling had \u201cgone mainstream\u201d globally, supporting projects and assets of every domain and scale in \u2018going digital\u2019. AEC Magazine agrees.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/tekla-structures-2019\/","title":"Tekla Structures 2019","date":1558483200000,"text":"With its latest Tekla promises to make engineers more productive. Accelerated production of GAs is the most obvious benefit but the end goal is to build confidence in the 3D model, so data can flow freely from design office to site\nWith Tekla Structures being one of the most mature structural BIM tools on the market, by now one would think it would have 2D drawing production nailed completely. But the latest 2019 release shows there is still room for innovation. It also demonstrates how important the humble 2D drawing continues to be. Tekla Structures might excel at delivering construction ready 3D models, complete with every nut, bolt and weld, but the 2D drawing is still considered to be the bread and butter deliverable for the AEC industry.\nFor some time now, Tekla Structures has offered optimised drawing production for fabrication drawings, including pre-cast, cast in situ and steel. Simply create a model, hit a button and the software automatically runs off all the drawings. All the user needs to do is a little bit of tidying up and sometimes none at all. This level of automation is hardly surprising, as much of the work done by fabricators or contractors is based on project-specific templates and is highly repetitive.\nFor engineers, things are a bit different, as they spend a lot of time producing GAs, and as each project varies, often working with different architects, it\u2019s hard to build true automation into the process.\nWhat Tekla has done for this new release is add tools to help reduce the amount of manual clean up needed on GA drawings. Users can now clone annotations, such as dimensions and marks, as well as linework and hatching styles. So, once things have been laid out correctly on one part of the drawing, they can then be applied to other similar areas \u2013 bit by bit or all in one go.\nWith a staircase, for example, simply annotate on one floor, adding info for risers, angle, tread, stringer, fixings etc., then clone those annotations to the second, third or fourth floor. Previously each floor had to be done manually, which was very time consuming (and monotonous). It\u2019s important to note that this isn\u2019t just copying and pasting dumb geometry. All cloned annotations are made fully associative to the model objects in the new location. For example, part marks and information about levels and heights will be automatically updated. Then, if any changes are made to the model, the annotations will update accordingly.\nAs one can imagine, this works best with repetitive structures, such as foundation plans, anchor plans, rebar placement drawings or elevation drawings. But the geometry doesn\u2019t have to be identical, only similar. The software is smart enough to handle variations. Of course, for this feature to work reliably, annotations do need to be associated with the correct objects. For example, a single point on a 2D drawing could relate to many things \u2013 a plate, a beam corner point, or even a grid. Now with the new version, users can specify exactly which objects dimension points should follow.\nThe drawing environment also gets the kind of search and edit capabilities you\u2019d typically find in the modelling environment, with an enhanced version of the Drawing Content Manager. When first introduced in 2018, it allowed you to check and edit objects and drawing content in the current drawing.\nFor example, you could easily check which objects were missing marks, quickly highlight objects of a certain type, or globally change the way they were displayed. This was limited to a default set of properties but now you can add any property that exists within Tekla Structures, even custom properties. For example, you can now quickly identify items that need special notes, in order to draw attention to them \u2013 beams over a certain length, for example, or rebar over a certain weight that will require special lifting on site. You can also use it to quickly add notes to custom objects like lock nuts or objects with special paint finishes.\nThere are a few other drawing related updates. GAs can now be rolled back in time, so if someone has made an error when editing a drawing, you can revert to any number of previous versions. You can also compare versions by overlaying one over the other.\nThird-party data\nAccording to Tekla, its users are increasingly working with third party data, including BIM objects and those exported from mechanical CAD systems in formats like IGES and STEP. However, the quality of these objects can vary. Some can have faces missing, which means they are not solid. Previously, these may have looked OK in certain views, but in other views, including details and section, the objects may have completely disappeared. Now, the software ensures that these objects are displayed in all views regardless. Tekla says this tool will prove particularly useful when importing complex formwork models for cast-in-place concrete construction.\nThe way third-party objects can be positioned within the software has also been improved, using direct manipulation handles and interactive preview. In the past this type of control was only possible for native Tekla custom components, so users first had to convert the third-party object and add it to the custom component library, which took time. Again, this is especially useful when working with cast-in-place concrete structures, says Tekla.\nThe way Tekla Structures manages IFC files has also been enhanced. When importing a revised IFC file, you\u2019ve always been able to compare old and new to see what\u2019s changed. Now, rather than having to go through every change, you can tell the system to look out for specific things like changes in length or area. You can also set tolerances, so it doesn\u2019t pick up those which, in the grand scheme of things, are insignificant.\nImprovements have not only been made to data import, but export as well. While Tekla is very much committed to IFC for sharing data, some clients specify RVT files as the project deliverable, so the new Revit Geometry Export tool allows firms to comply without having to invest in a license of Revit Structures. The extension, which is powered by technology from the Open Design Alliance, is available through the Tekla Warehouse.\nTrimble Connect enhancements\nTrimble\u2019s collaboration platform, Trimble Connect, has also seen some significant enhancements. First, it is now much better at viewing colossal IFC files, which can be hundreds if not thousands of megabytes. Rather than trying to open a heavy IFC file directly which could take minutes, or might not work at all, a new viewing technology converts the data into a much lighter format without losing fidelity. This feature, currently in beta, should be particularly beneficial when taking data on site on mobile devices.\nThe workflow between Tekla Structures and Trimble Connect has also been enhanced. It\u2019s now possible to upload NC files and drawings (assembly, part or GA) to Trimble Connect and link them to the respective objects within the model, all in one go. This is a neat feature for collaboration as it gives the whole project team easy access to in-context documentation. In short, the 3D model becomes the index for project deliverables. Simply click on a beam, for example, and you can instantly view the associated data.\nThe new release also allows you to overlay 2D information on the 3D model, which can further aid checking, validation, and communication.\nTaking data on site\nOnce data is inside Trimble Connect it can be pushed out to many different devices for use on site. This could be a standard tablet for viewing 3D models and drawings or an Augmented \/ Mixed Reality device to see construction ready models in the context of where they will be built.\nTrimble is investing heavily in augmented \/ mixed reality and has developed two hardware devices. The first is Trimble SiteVision, a handheld system that combines a 2D display with Trimble\u2019s centimetre- precision Catalyst GPS. The second is the Trimble XR10, a wearable mixed reality hard hat built around Microsoft\u2019s HoloLens 2. Both are in early development.\nVisual enhancements\nFor the new release, Tekla has turned to game engine visualisation and made two new visualisation extensions available through the Tekla Warehouse. The Tekla Structure Visualizer and Trimble Connect Visualizer are both based on Unity and allow users to visualise large structural models much more realistically than can be done in the viewport and also create stills and animations for project presentations and marketing.\nThe push button workflow for Tekla Structures Visualizer is very simple. A base set of materials \u2013 steel, concrete, grass, timber, etc. \u2013 are automatically mapped. Users can create animations simply by placing the model in key positions and the system then smoothly transitions between them. The animation can be played in the software or, if a video file is required, captured using third-party software, such as the Xbox technology built into Windows 10.\nThe Trimble Connect Visualizer is similar but opens up the model for sharing and also works with referenced models, such as IFC or RVT.\nBridge Creator\nThe Grasshopper-Tekla live link, introduced in 2018, gave engineers access to powerful algorithmic modelling tools, which could be used to good effect for bridge design. Now Tekla has given bridge designers their own \u2018Bridge Creator\u2019 tool that is designed to make early phase design faster and easier.\nIn essence, users can take a road\/rail alignment from a civils design tool in LandXML format, then automatically apply key sections along that alignment to define the bridge deck.\nSteel modelling enhancements\nTekla Structures now has enhanced modelling tools so it can better handle bespoke steel work such as bent plates or conical plates. This could be useful for highly architectural staircases, for example, or ductwork corners and flues.\nIt\u2019s also now possible to add self-tapping screws and nails to the model without creating a hole. Previously, this was done automatically, regardless of the type of fastener. Now you can ensure that the model is completely accurate, which is particularly important when generating Bills of Materials or NC files.\nWelds have also been enhanced to allow users to more accurately define them within the model. Previously, fabricators would often define welds simply by adding manual marks to drawings. Now, there\u2019s a growing demand to share more accurate models, improve traceability and use robots, which rely on precise information.\nPre-cast concrete\nVersion 2019 introduces a new design-to-cost tool for the pre-cast industry that allows engineers to compare different alternative designs and find the most cost-effective solution. Users define the unit costs for different element types in the model and the system then costs up the whole model or selected parts. Tekla says that because you can immediately see the cost impact of the changes in the model, you can more easily find the most economical precast solution for your projects.\nThere are also enhancements to the tools that automate modelling and detailing for double wall structures. New Half Slab Reinforcement tools let you automatically add optimised, detailed reinforcing and braced girders to half slab floors.\nConclusion\nStructural engineering continues to be heavily reliant on 2D drawings. Even if an engineering model is available, most contractors or fabricators still prefer to build their own from scratch.\nIt\u2019s easy to extol the virtues of model-based deliverables where data flows seamlessly from design office to contractor\/ fabricator, but for this process to be fully embraced, everyone needs to have confidence in the accuracy of the data. But why should they, when far too often the 3D engineering model is just a starting point and 2D drawings become disconnected when polished in a 2D CAD tool like AutoCAD?\nTekla Structures is renowned for its ability to generate detailed fabrication drawings quickly. Now, by delivering new tools that can dramatically accelerate the production of GAs, it is looking to build a stronger presence in the engineering sector. The big challenge will be getting engineers into the mindset that because the model and the drawings are inextricably linked, both assets can be shared. Then, as a next step, if you remove the drawings altogether, you remove the traditional time-consuming approval process.\nOf course, for some firms this is already becoming a reality. There are engineering consultancies and fabricators effectively working on the same Tekla Structures model via Tekla Structures Model Sharing and Trimble Connect. So, everything just goes straight through and design changes are also dealt with much quicker.\nThe next step is to take those models on site with data flowing seamlessly, so what you model in Tekla Structures is exactly what is built. Trimble is putting things in place for what it calls the Constructible Process. And then, of course, the industry needs to join the dots.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/workstations\/review-armari-magnetar-x64t-g3-fwl-64-core-amd-threadripper\/","title":"Review: Armari Magnetar X64T-G3 FWL [64-core AMD Threadripper]","date":1582156800000,"text":"With its new 64-core Ryzen Threadripper 3990X, AMD has completely transformed the design viz workstation. Ray trace rendering and CAD have never been such good bedfellows, writes Greg Corke\nA few years ago, it was almost unthinkable for a CPU to have 64 cores. And, even if it could, the frequency of those cores would be so low you wouldn\u2019t really want one inside your workstation. Lots of cores are great for rendering, but if that comes at the expense of single threaded performance, which is what makes CAD and many other design and engineering applications tick, then it\u2019s a compromise few would be willing to make.\nUntil recently, advances in CPU technology had become quite predictable, but it\u2019s amazing how quickly things can change. In summer 2017 AMD launched Ryzen Threadripper. The first-generation CPUs featured up to 16 cores and were great for multi-threaded workflows but lacked the all-important single threaded performance to make them a serious threat to Intel. Now two and a half years later, and with AMD\u2019s 3rd Gen Threadripper rollout complete, that couldn\u2019t be further from the truth.\nEarly February saw the release of Ryzen Threadripper 3990X, a 64-core, 128-thread beast of a CPU which hardly compromises on frequency at all. It has a base clock of 2.9GHz and a max boost of 4.3GHz, but with sufficient cooling in place it can even get close to 4.0GHz on all 64 cores. Viz specialists, architects, engineers and product designers can really have their cake and eat it too. It\u2019s a phenomenal proposition for anyone that uses a CPU renderer.\nAs far as multi-threaded performance is concerned, Intel simply can\u2019t compete. The closest it has is the Core i9-10980XE (18-cores) ($1,000) and the Xeon W-3175X (28-cores) ($2,999). If you want more cores, you\u2019d need the server-focused Xeon Platinum 9282 (56-cores) ($50,000) or two Xeon Platinum 8280 (28-cores) ($20,000). And you still wouldn\u2019t be able to beat Threadripper 3990X.\nOf course, with limited competition on the desktop workstation front, AMD can charge a premium, and the Threadripper 3990X doesn\u2019t come cheap. The 64-core CPU will set you back $3,990, matching its model number precisely. This might seem expensive but, when you consider the huge impact it can have on design viz workflows, many will consider it money well spent. After all, there\u2019s only so many coffee breaks you can have in one day, as you wait for a render to complete.\nThe render god\nArmari has a long history of developing high-performance workstations that are both extremely well-built and well-tuned. The UK firm was one of the first to get on board with 1st Gen Threadripper and now offers AMD CPUs across its entire Magnetar range, from single socket Ryzen and single socket Threadripper, to single and dual socket EPYC, which is AMD\u2019s official enterprise CPU.\nFor the Threadripper 3990X, Armari has designed and manufactured a completely bespoke chassis to handle its extreme demands. The 3990X is rated at 280W Thermal Design Power (TDP) but, unlike AMD EPYC, it can actually be pushed much higher. And when more power gets pumped into the CPU, the brakes come off and Threadripper really starts to fly in multi-threaded workflows.\nTo do this, Armari uses AMD Precision Boost Overdrive, which will essentially continue to push the frequency of the CPU as long as the workstation can cool it adequately. And the Magnetar X64T-G3 certainly can. Its Full Water Loop (FWL) cooling system is impressive and comes with a giant radiator with nearly three times the surface area of those used in its other workstations.\nOn average, Armari reckons it can sustain 550 \u2013 650 Watts of power in real world applications, with momentary boosts in excess of 800 Watts. In practice, this means the machine can maintain very high clock speeds over long periods of time; not just in single threaded workflows, but when rendering as well.\nWe left it rendering in the design viz focused KeyShot for well over an hour and it maintained a phenomenal 3.90GHz on all 64 cores. Fan noise was noticeable, but not too distracting. However, it\u2019s important to note that this is a prototype system and, when the machine hits production, Armari says the radiator fans will be tuned back to around 50-60% duty cycle maximum.\nRendering in KeyShot 3990X from DEVELOP3D on Vimeo.\nThe machine completed our 4K, 128 pass test render in a record breaking 38 secs. That\u2019s nearly twice as fast as a 32-core Threadripper 3970X and more than ten times as fast as the six-core Intel Xeon E-2176G, the kind of CPU you\u2019d typically find in a CAD workstation. It was also streets ahead of the competition in the V-Ray NEXT benchmark with a result of 93,436 ksamples.\nBut this isn\u2019t just about numbers on charts. A CPU like this can have huge impact on workflow. With high-quality 1,280 x 720 resolution renders literally taking a few seconds and 1,920 x 1,080 resolution renders not much longer, there\u2019s no more stop and start. You really can iterate in real time without having to compromise on quality or resolution. Although, naturally, this depends on the complexity of your scene.\nSystem memory\nMemory also plays a very important role in rendering. 3rd Gen Threadripper can support up to 256GB, spread across eight slots, which is important if you work with very large scenes. This is the theory, at least. Armari tells us compatible 32GB modules are currently quite expensive, which is one of the reasons why our test machine was configured with 64GB (4 x 16GB Corsair Vengeance LPX DDR4-3600 C18 SDRAM modules) \u2013 the other being that 64GB is a good amount for mainstream viz workflows.\nBut it\u2019s not just about capacity. 3rd Gen Threadripper also features a new memory architecture which gives every single core fast and equal access to memory. In contrast, with 2nd Gen Threadripper not all cores had direct access to memory, so sometimes had to ask other cores for data and then wait for it to arrive. Chaos Group\u2019s CTO Vladimir Koylazov explains this in more detail below.\nChaos Group on 3rd Gen Threadripper\nChaos Group\u2019s CTO Vladimir Koylazov shares his thoughts on the Threadripper 3990X and what 64-cores and the new memory architecture of 3rd Gen Threadripper means for rendering\nThe third generation Threadripper CPUs are great for ray tracing \u2013 and there is one crucial breakthrough that makes it possible. The Threadripper 3990X CPU implements uniform memory access for all cores, which gives a huge performance boost for rendering. Here is a short explanation.\nUsually the main bottleneck for many-core machines is RAM \u2013 especially with ray tracing, different cores usually need different parts of the scene geometry or shaders. Scenes these days can be very large, measuring hundreds of gigabytes. Making sure that each CPU core gets the data that it needs from the system RAM as quickly as possible is a fairly difficult task.\nTo make it somewhat easier for hardware manufacturers, the so called \u201cNUMA\u201d architecture was introduced (where NUMA stands for \u201cNon-Uniform Memory Access\u201d). For multi-CPU systems, this means that each CPU only has access to a portion of the system RAM directly, and if it needs data from the other portions, it needs to ask another CPU to fetch it. Within a single CPU it means that only certain cores have direct access to the memory, and other cores must ask them to fetch the data they need. For ray tracing specifically, this adds quite a bit of overhead and typically NUMA configurations affect performance in quite a big negative way. Unfortunately, there is no easy way to optimise the software around this hardware peculiarity. This is one of the reasons why many-core dual-CPU systems have sometimes not performed as expected for our customers, especially with large scenes that are far larger than the CPU caches. We have profiled many such systems with CPUs from different manufacturers and, barring bugs or other multithreading problems, we have found that invariably the main bottlenecks occur when the CPU cores wait for data to arrive from the system RAM. Anything that slows that operation, like a NUMA architecture, has an adverse effect on performance.\nIn the newest [3rd Gen] Threadripper CPUs, all cores have equal access to the system RAM without additional delays like asking another core to fetch the data. This allows each CPU core to move through the compute operations a lot faster than previous NUMA-based architectures. What you get are 64 CPU cores operating closer to their maximum potential \u2013 which has not been possible with any other CPUs previously. This means that, on the whole, we have not had to do much to optimise the V-Ray and Corona code for the new Threadrippers.\nWe did have one piece of code in V-Ray (the light cache calculations) that was limited to 64 threads and which we had to rework a little bit in the latest V-Ray builds so that we can use all 128 logical threads, but beyond that, we only had to make sure that each CPU core can operate as independently as possible from the rest.\nBeyond rendering\nOf course, designers, engineers and architects aren\u2019t only interested in ray trace rendering. Putting BIM and CAD to one side for a moment, there are several multithreaded tools that can benefit from multiple CPU cores, although very few apart from video encoding and editing that can use all 64 cores as efficiently as a ray trace renderer. Many simulation and point cloud processing tools, for example, are limited to a dozen or so cores, or offer very little additional benefit if your workstation CPU has more. With some applications, processing times can even go up once you hit the CPU core sweet spot for the software or dataset you\u2019re working on.\nWith this in mind, unless you know for certain that your design and engineering focused software will benefit from 64 cores, we wouldn\u2019t really recommend the Threadripper 3990X for anything other than ray trace rendering. Instead, money would probably be better spent on the 24-core AMD Ryzen Threadripper 3960X, which is less than half the price, or even the 16-core AMD Ryzen 9 3950X.\nIn saying this, it\u2019s important to note that Threadripper beats Ryzen hands down when it comes to memory bandwidth and cache, both of which can be really important for memory intensive workflows like point cloud processing or simulation. 3rd Gen Threadripper can also support much more memory \u2013 256GB compared to 128GB in 3rd Gen Ryzen.\nFor single threaded applications like CAD or BIM the 64-core 3990X is never going to beat a top-end eight core Intel CPU like the Core i9 9900K. But it\u2019s not that far behind. It completed our Solidworks 2020 IGES export test in 84 secs, only 9 seconds slower than an overclocked 4.9GHz Core i9 9900K, which is still one of the best CPUs out there if you want a workstation that is 100% focused on CAD.\nInside the Magnetar X64T-G3\nArmari\u2019s 64-core Threadripper workstation is a serious piece of engineering and one of the heaviest workstations we\u2019ve ever reviewed, thanks in part to its hefty cooling system. To make it easier to carry there are two handles on top that flip around 180 degrees, so they sit flush when not in use. Next to the front handle you\u2019ll also find two USB 3.1 Gen 1 ports and (on the production version, at least) a USB 3.2 Gen2x2 port. There are plenty more USB ports on the rear of the machine, as well as two Ethernet ports (2.5Gb\/s and 1Gb\/s).\nTo handle the big power demands of the 3990X, Armari uses the ASRock TRX40 motherboard. With limited on-board M.2 sockets, it comes with a Hyper Quad M.2 PCIe add in board that can host up to four M.2 NVMe SSDs. In our review machine it\u2019s populated with a pair of 1TB Corsair MP600 M.2 NVMe SSDs configured as a 2TB RAID 0 array.\nThe MP600 is based on PCIe 4.0, which offers twice the bandwidth of PCIe 3.0, so is already a fast SSD. It boasts sequential read and write speeds of 4.95GB\/sec and 4.25GB\/sec respectively but configuring it as RAID 0 takes it to new levels. In the CrystalDiskMark benchmark it clocked 9,062MB\/sec read and 8,298MB\/sec write and copied a 90GB zip file in just over 50 secs.\nOf course, SSD performance like this will only truly benefit those working with colossal datasets typically used in workflows such as high-end viz, 8K video, point cloud or simulation. Anyone with more mainstream viz workflows will likely be more than happy with a single MP600, backed up by up one to four 3.5\/2.5-inch SATA\/SAS HDDs\/SSDs.\nIf you\u2019ve forked out $4k for a 64-core CPU, the chances are you\u2019ll only really want to use the GPU for 3D graphics or VR. The Magnetar X64T-G3 came with a single AMD Radeon Pro W5700 (8GB), which is a decent choice for mainstream design viz. We review this pro GPU in detail in this article.\nFor more demanding 3D workflows, the machine can handle one or two Nvidia Quadro RTX 5000, 6000 or 8000 GPUs, but if you want three or four GPUs, perhaps for GPU rendering, then you\u2019re best off talking to Armari. With a different motherboard (the ASRock TRX40 Creator) and a 2000W PSU (instead of our test machine\u2019s 1300W EVGA SuperNOVA G2 GOLD Modular) the workstation can support four GPUs. However, it may not be able to deliver the same power to the Threadripper 3990X, so all core clock speeds may be lower.\nConclusion\nIn all the years I\u2019ve been reviewing workstations I can\u2019t remember a CPU ever impressing me as much as the 64-core Threadripper 3990X. It really is a phenomenal feat of engineering, giving the best of both worlds for single threaded CAD and multi-threaded ray trace rendering. If you use a design viz tool like V-Ray or KeyShot then it\u2019s completely untouchable. Intel has nothing that gets remotely close.\nBut Intel Core or Intel Xeon aren\u2019t the only competitors to AMD Threadripper. In the last couple of years, the GPU has also become a serious challenger for rendering. This is especially true for Nvidia RTX GPUs which feature dedicated cores for ray tracing and AI denoising, and more memory on the high-end Quadros. RTX is also supported by an increasing number of viz tools including V-Ray, KeyShot and Enscape.\nGPU rendering has certainly been gaining traction, but now with AMD Threadripper delivering genuinely huge leaps in performance and offering quick access to up to 256GB memory, the battle is far from over.\nOf course, a 64-core CPU isn\u2019t for everyone. Designers, architects and engineers who only use CAD or BIM software will likely fare better on Intel, which still offers faster single threaded performance with an eight core CPU like the Core i9-9900K. But if you\u2019re into ray tracing in any shape or form, any one of the 3rd Gen Threadrippers, including the 24-core 3960X and the 32-core 3970X, should serve you well. And Armari is proving to be one of the best to get the most out of this exciting new platform.\nProduct specifications\n\u00bb AMD Ryzen Threadripper 3990X (2.9GHz, 4.3GHz Boost) (64 cores) CPU\n\u00bb 64GB (4 x 16GB) Corsair Vengeance LPX DDR4-3600 C18 SDRAM\n\u00bb ASRock TRX40 Taichi motherboard\n\u00bb 2 x 1TB Corsair MP600 PCIe 4 M.2 NVMe SSD (RAID 0)\n\u00bb AMD Radeon Pro W5700 GPU (8GB)\n\u00bb Full Water cooled Loop system (FWL upgrade) \u2013 includes 1 Free service (Coolant change, O-Ring inspection)\n\u00bb Microsoft Windows 10 Pro for Workstations 64-Bit\n\u00bb Armari Magnetar S\/M\/R\/X Series \u2013 3 Year RTB workstation warranty\n\u00bb \u00a36,664 (Ex VAT)\nCPU benchmarks\nCAD (Solidworks 2020 IGES export) \u2013 84 secs (smaller is better)\nRendering\n(KeyShot 8.1) \u2013 38 secs (smaller is better)\n(V-Ray Next Benchmark) \u2013 93,436 ksamples (bigger is better)\nGraphics benchmarks\n(frames per second @ 4K res) (bigger is better)\nViz (Enscape) Museum \u2013 15\nViz (Lumen RT) Roundabout \u2013 17\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-trimble-looks-to-simplify-layout-for-contractors\/","title":"NEWS: Trimble looks to simplify layout for contractors","date":1421712000000,"text":"New Trimble Rapid Positioning System combines Trimble RPT600 Layout Station with Trimble Field Link 2D software\nThe Trimble Rapid Positioning System is a new simplified layout solution for building construction professionals. It includes the Trimble RPT600 Layout Station to layout points and capture as-built measurements and Trimble Field Link 2D software running on a performance tablet to control the layout station. The solution is designed to enable contractors to record and find positions for layout points including foundation layout, anchor bolts, walls, ceilings and hangers on the construction site.\nTrimble says the new Rapid Positioning System is the first layout solution specifically designed to meet the demands of interior and exterior building construction applications.\nThe Trimble RPT600 Layout Station features what is described as an easy single-button set-up workflow\u2014Trimble Autostationing\u2014that enables the system to self-locate to automatically determine its position relative to the site with no leveling required. Trimble says this feature allows contractors to get started faster and with less technical expertise.\n\u201cBuilding construction has unique needs and unique users,\u201d said Bryn Fosburgh, vice president responsible for Trimble\u2019s Construction Technology Divisions. \u201cThe system was designed from the start to enable the construction community to work with precision and reduce the need for rework, but without the specialized knowledge required to operate a solution designed for geospatial applications. This can give contractors more flexibility in staffing layout and as-built data collection tasks.\u201d\nRunning on the tablet, Trimble Field Link 2D software enables contractors to view their 2D design files and layout points via Trimble VISION technology that allows users to remotely see and measure through a live video feed from the instrument on their data controller. According to Trimble, this enables the contractor to set-up the tool in the most convenient location.\nTrimble Field Link 2D introduces a new, simplified user interface and workflows designed for the building contractor. Trimble says this new system provides increased flexibility to the layout team to use layout points created in the office or easily create points in the field.\nTrimble is also offering what is described as a seamless round-trip workflow from office to field by combining Trimble Field Link 2D with Trimble Connect, a cloud-based construction collaboration platform. Up-to-date files are downloaded and uploaded through Trimble Connect enabling collaboration throughout the project from model coordination meeting to delivering final design files for the owner.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nNHS Foundation Trust creates smart estate with digital twin\nNEWS: Autodesk to scrap perpetual licensing system\nOasys enhances pedestrian simulation tool MassMotion\nPart3 to give architects control over construction drawings\nAutoCAD 2006 review\nNEWS: Asta Powerproject enhanced for construction project management\nReality Cloud Studio to \u2018democratise reality capture\u2019\nEnscape integrated with BricsCAD BIM to enhance viz capabilities\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/technology\/the-rise-of-the-robots-2\/","title":"The rise of the robots","date":1459728000000,"text":"Martyn Day evaluates the latest projects and technologies involved in deploying robots and 3D printing in construction\nWhile the general media seems happy to stir up fears of mass-unemployment and social instability brought about by the infiltration of robotics into traditional middle-class jobs, the construction industry remains relatively untouched by automation, despite decades-old flagellation over market inefficiencies.\nBIM has been touted as a way to reclaim some of this inefficiency but, looked at a different way, it is also the entry point to robotic fabrication.\nIn the manufacturing world, the move to 3D modelling enabled Computer Numerical Controlled (CNC) machining and 3D printing, the core asset of which is the creation of a 3D model to drive the software, to drive the machines.\nAs a model-centric approach becomes more mainstream in AEC, it will inevitably drive the digital fabrication of components or complete buildings. However, today\u2019s attitude towards creating BIM models is more about documentation than driving fabrication and will need another step-change to enable model-driven fabrication.\nWhile much of this work resides in the research labs of universities, there are companies like Laings, which is actively seeking to deploy rapid fabrication technologies that hitherto have been the preserve of the automotive industry. Laings is doing so with its Design for Manufacture and Assembly (DfMA) approach to modular construction.\nA number of other highly-publicised projects are also seeking to demonstrate that robots and 3D printing can be utilised effectively on large-scale projects.\nModular and Prefab\nModular design and prefabrication has long been a popular subset of AEC. It has been proven to work for \u2018protocabins\u2019, McDonalds restaurants, emergency shelters and caravans, which need to be \u2018manufactured\u2019 rapidly and deployed in weeks.\nHowever, despite many attempts, prefabrication has generally failed to get much traction in construction until relatively recently. There are now a number of firms, such as China\u2019s Broad Sustainable Building (BSB) company, which are working out how prefabrication can have benefits without the old drawbacks. Autodesk VP strategic industry relations Phil Bernstein recently examined how technology changes are pushing the AEC industry towards embracing prefabrication. (lineshapespace.com\/futureof- construction).\nMr Bernstein envisions that buildings will be \u2018assembled\u2019 and then mass customised, enabling sophisticated design changes even though components are configured within a production line environment.\nMr Bernstein gave a number of examples that, he said, prove that it is possible to utilise digital fabrication and have a unique end result.\nFor example, BSB built a 57-storey tower, with 800 apartments in 19 days (producing an amazing three storeys per day) by using prefabricated components.\nThe B2 Pacific Park building in Brooklyn\u2019s Navy yard had a very complex design that used prefabricated components to build 32-storeys, 363 apartments, and 930 modules.\nFacit Homes has developed a unique way to employ digital fabrication within its BIM process for domestic dwellings. Using Revit, customers work with the firm to design their individual home.\nThe Revit model is then used to generate GCODE to run a CNC milling machine, which is shipped to the construction site in a shipping container. The building is assembled from insulated wooden box sections, which are cut fresh each day wherever the building site is and controlled back at base in London.\nFacit Homes managing director Bruce Bell explained to AEC Magazine why the traditional view of prefab buildings from factories will not work in the residential sector.\n\u201cThere is a direct correlation between factory fabrication and repetition because you can\u2019t have factories sitting idle due to the overheads. So, as soon as you have a factory, you need turnover and in order to have turnover, you need standardisation and you end up producing the same thing over and over again.\n\u201cIf you build on-site, which the vast majority of buildings are, the constraints are completely different and fabricating on demand has benefits such as having no heating, storage costs etc, as running a factory would.\n\u201cThe economics (of prefab) just don\u2019t stand up. It leads to standardisation and people don\u2019t want the same, and every site has its own requirements. There is no one size fits all.\u201d\nThe robots are coming\nRobotics use in construction remains embryonic, with today\u2019s plinth-located manufacturing robots making Doctor Who\u2019s Darleks look positively advanced.\nHowever, there are a number of projects that aim to teach robots to weld and lay brick. To do this it is important to overcome platform immovability, limited arm reach, onsite spatial awareness and real time clash detection.\nSafety is also a major concern as these robots will be more than likely working alongside humans.\n3D printed buildings\nGiven that 3D print technology has been around for such a long time, there remains considerable hype around its application. This is probably, in many ways, due to its fall in price to address the emerging \u2018maker\u2019 and consumer markets.\nNow that hoopla has died down, the reality is dawning on AEC professionals that those with engineering knowledge and CAD skills can manufacture with a growing range of desktop machines. While the print technology progresses slowly, there has been a great leap forward in materials that can be used.\nI have seen 3D printers that use chocolate, mud with seeds, plastic, cake mix, candy, ceramic, rubber, colours, UV curing liquid and various metals. It was only a matter of time before concrete and clay became available on the 3D print menu.\nMany 3D printed buildings currently come out of China, although many do not appear to live up to the 3D printed label. For example, Zhuoda Group claimed to have produced a 1,100 square metre \u2018neoclassical mansion\u2019 featuring multi-storey (five floors) and decoration in just 10 days. However, further investigation reveals that 3D print was used to generate components in a factory, which were delivered on site, not \u2018cast\u2019 in situ from a roving 3D print head.\nWinSun has developed its own system \u2014 a 3D printer array that stands 6.6 metres high, 10 metres wide and 40 metres long.\nThe \u2018print engine\u2019 sits in WinSun\u2019s factory and fabricates building parts in large pieces. These are shipped and assembled on-site.\nWinSun claims the process saves between 30%-60% of construction waste, can decrease production times by between 50% to 70%, and labour costs by between 50% and 80% percent.\nYingchuang New Materials claims to have \u2018printed\u2019 up to ten buildings in 24 hours. Each \u2018house\u2019 was made for less than \u00a33,000.\nSo far the Chinese company has spent 20 million yuan (\u00a32 million) and taken 12 years to develop its additive manufacturing device. The only sections not produced by the printer were the roofs.\nChinese companies are also keen to find new materials, such as using recycled concrete from unwanted buildings to produce new 3D printable concrete. However mixing concrete with fibreglass and different resins could lead to health issues should anyone actually live in these buildings. The materials science of 3D printed buildings is still some way off.\nAmsterdam\u2019s Dus Architects has been experimenting with plant-oil based materials to create a 3D printed house on its open source KamerMaker (room maker) 3D printer. Again, due to build size issues, 3D printing is used to create 2 x 2 x 3.5m high sections of the design, which are stacked up like Lego bricks to create a 3D printed equivalent of a Dutch gabled canal house. The project started in 2014 and is set to last three years.\n3D printing concerns\nThere are many challenges for 3D printing buildings. Physically there is a need to have a huge frame around which the 3D print head can move, otherwise it will remain as print sections and assemble. The materials need to be durable and fit for purpose and consistent \u2014 you do not want air bubbles or material weakness in supporting loads, for example.\nThere are also many problems with printing 3D buildings in concrete. The first problem is the model has to be constructed in a way to get the best fabrication success rate, which will certainly not be the same as producing a BIM model to produce drawings. In addition to the BIM model for architecture and a BIM model for construction, at the moment it would require a model for digital fabrication too.\nConcrete curing times have to be taken into consideration. The print head needs to travel as fast as possible and the material deposited needs to solidify and harden within minutes.\nSuddenly architects will find themselves being faced with questions that engineers and industrial product designers face every day when designing cars, planes and consumer products. When buildings are made of prefabricated components or 3D printed, they become more like machines, more an assembly that needs to be durable and repairable.\nAEC professionals need to consider how to build in structural elements, reinforcement and lighten non-supporting walls. Should the walls be fabricated in one long continual \u2018print\u2019 or be broken down? Should ducting or spaces for ducting be included in the 3D model and what would that mean to later refurbishment or alterations\/repairs?\nHow will the material consistency change over the time of the 3D print? Will the weather negatively impact cure times? How long is the material guaranteed for? What\u2019s the toxicity of the material? Can one material fulfill all criteria for each part of the design? What are the legal issues?\nThere are also fundamental problems with devising shapes for manufacture in today\u2019s AEC tools, which quite frankly were never designed with 3D print or direct manufacture in mind.\nThis area should improve over time as cement companies like LafargeHolcim experiment with extrudable and quick curing materials.\nComplex forms Signature Architects like Zaha Hadid and Foster + Partners find themselves drawn to the possibilities of the technology.\nWe are seeing an increasing use of nonstandard fabrication materials and methodologies to achieve stunning forms that could previously never have been built for an acceptable budget. As many of these shapes are derived from generative and computational methods, connecting them to automated fabrication machines sounds like a good idea.\nMany of Frank Gehry\u2019s designs could not be built because the cost estimates from fabricators had huge \u2018risk\u2019 fees included as it was not totally clear from the 2D drawings how components could be manufactured. When Gehry\u2019s practice started using the CAD tool Catia to produce detailed 3D models his contractors and fabricators better understood the design and could reduced costs by using the model to cut the steel and aluminium. Gehry is still proud that he can have a sculpted wall for the same price as a straight one.\nFoster + Partners is part of a consortium set up by the European Space Agency to explore the possibilities of 3D printing lunar habitations. As it is prohibitively expensive to ship heavy materials to the moon, Foster + Partners is looking to process and print a lunar soil-based material into an inflated dome. Simulated lunar soil has been used to create a 1.5 ton mock-up using a D-Shape printer.\nSOM, together with the Oak Ridge National Laboratory have been working on research for a 3D printed structure made of C-sections called AMIE, which generates solar energy and has a symbiotic power sharing relationship with a 3D printed electric car.\nConclusion\nDigital fabrication is undoubtedly coming to the construction industry. With so many active research projects and investments being made in materials and robotic technology, some will eventually stick.\nHowever, it is going to take a while for the various dots to get connected, as changes are required to software, hardware, contracts and mindsets. The idea of a machine or robots creating a building in a single 3D print still seems like science fiction still to me.\nAt best, it may work in space, for quick military fortifications, or in emergency shelters or homes. But single continuous pour does not seem to make much sense. I also seriously doubt that 3D printing a flat wall is actually any faster or better than a traditional block wall, unless you have severe labour shortage. This is a misapplication of the technology.\nMuch of the 3D printing hype from China seems to really be a story about prefabrication and assembling it on site and most just look like concrete sheds.\nThere also needs to be considerable technological advances in all fields to make this work. 3D print industry guru Terry Wohlers said: \u201cWhen considering the time and cost of constructing an entire building, the skeletal walls are a small part of the project. You also need floors, ceilings, roofs, stairs, and kitchen and bathroom fixtures. Consequently, I cannot see how the use of 3D printing technology could save any time or money.\n\u201cWhen you factor in the added cost of a very large, expensive, and not very portable 3D printer, the cost of these walls are likely far more expensive and time-consuming than conventional walls. The use of 3D printing may be good for marketing and attention, but that\u2019s all.\u201d\nFor now, prefabrication and assembly on-site can lead to incredible productivity benefits, if perhaps not to stunning architecture.\nRobots work best in environments that are controlled and predictable. They are therefore much more likely to be of use in a factory fabricating components.\nTo effectively employ these methods, designers will need to understand the limits of the actual construction, materials and fabrication technologies far better than they do today.\nEven in engineering, which is typically much more connected to fabrication, engineers still create designs that cannot easily be produced digitally or otherwise.\nThis is an edited version of an article that first appeared in the Nov\/Dec 2015 edition of AEC Magazine. To read the full article, which includes more in-depth information about construction robots and 3D printers, click here.\nThis article is part of an AEC Magazine Special Report into the Future of Building Design, which takes a holistic view of the technologies and processes, which are set to change and enhance the AEC industry in the coming years \u2014 from concept design all the way to construction.\nClick to read the other articles that make up the report.\n1) Introduction New technologies are empowering architectural firms to improve quality, capabilities and process.\n2) Conceptual design There are a whole host of digital tools for early stage design experimentation.\n3) Rapid site design The rapid capture of site topology is being aided by new technologies.\n4) Benefits of 3D design Evolution, not revolution when making the move to 3D CAD.\n5) Moving to model-based design How to get from 2D to 3D, how to roll out training and how to overcome common issues encountered along the way.\n6) Design viz Advanced new rendering technologies are opening the door to design realism in architectural workflow.\n7) Design, analysis and optimisation Once you have a 3D CAD model, optimse your design for daylighting, energy performance and much more.\n8) Collaboration and model checking How to share models with clients, contractors and construction firms and test the quality of your model.\n9) Workstations What to look out for when choosing a workstation for 3D CAD.\n10) Virtual Reality New technologies are now available to support powerful new design workflows.\n11) 3D printing Architects are 3D printing architectural models with impressive results.\n12) Fabrication As building time gets compressed what will revolutionise fabrication and construction time?\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/a-civilised-discussion\/","title":"A civilised discussion","date":1243900800000,"text":"In recent years Autodesk has reinvigorated its focus on Civil Engineering. Martyn Day talked to Adam Strafaci, senior manager, Civil Engineering Product Marketing and Dana Probert, Civil Technical marketing manager at Autodesk.\nThere is no doubt that civil engineering design is going 3D. Model-based design is revolutionising all aspects of the building and construction market. To find out more about Autodesk\u2019s 3D products and ambitions in the Civil Engineering market AEC interviewed Adam Strafaci and Dana Probert of Autodesk\u2019s Civil team.\nMartyn Day: Could you explain the transition from Land Development Desktop to today\u2019s civils products?\nAdam Strafaci: Land Development Desktop came out of an acquisition called Softdesk, and was originally built for the US market. It wasn\u2019t really built with a global audience in mind and the original specifications didn\u2019t cater to different project types, such as transportation or environmental. As the business started to grow, it didn\u2019t make sense, from a technological standpoint, to expand LDT. Customers were looking for something that could handle multiple formats and they wanted a model-based environment. So we went back to the drawing board and started developing AutoCAD Civil from scratch.\nThere are two variants, AutoCAD Civil and AutoCAD Civil3D. About a year ago, as we were adding more and more functionality on the analysis side of Civil 3D, making the system very rich and capable for engineers, we found there were many users on the document side of the process that didn\u2019t need tools like storm water analysis. So we created a version that didn\u2019t have all the analysis tools for the drafter and designer but it still uses the same model. Currently it\u2019s only available in the USA but we are working out better product differentiation to make it applicable to different geographies.\nMD: Why build a 3D product on AutoCAD instead of developing something like Inventor?\nAS: Within the Civil Engineering community everything is either MicroStation or AutoCAD-based. We stuck with AutoCAD because it\u2019s the tool that civil engineers are used to using and are most comfortable with. We kept all the native AutoCAD drafting capability but we built this rich parametric 3D model on top of AutoCAD.\nEssentially we have created a number of custom objects. In highway design we have a corridor custom object, which is a 3D parametric object, which you can abstract information from. With our latest release of Civil 3D we have added quantity take offs from. So the transportation engineer can design the road and then extract all the quantities,such as the volume of pavement, from that model.\nDana Probert: Standard AutoCAD commands can be used as a foundation to build Civil 3D objects, such as polylines. Another example of custom Civil 3D objects in our 2010 release of Civil 3D are intersection models, we have a parametric object to generate intersections saving lots of drawing time.\nMD: Different countries have different drawing standards. How does Civil 3D handle this?\nAS: We have these things called \u2018Country Kits\u2019, which include drafting standards, different types of representations and some custom functionality. For instance, in China we just added sub-assemblies for non-linear designs like highways, to make cross-sections. In fact with 2010, all the country kits are shipped with every copy of Civil 3D, as we recognise our customers work on global projects.\nMD: What are the key areas of success and focus for Civil 3D development?\nAS: We have three primary focus areas. One of them is undoubtedly transport, including roadwork, highways, railways and harbours. Then we have land development, which could be residential development, shopping centre, industrial. The third area is environmental and we are specialising in water resources like channel design, storm water management or dams. However, the product is being used in many different situations. We have made some significant progress in the last couple of years.\nMD: The recent announcement from Autodesk and Bentley concerning the swapping of file format technology, so both firms can read and write each other\u2019s files, is great news for the industry and I would have thought would help Autodesk\u2019s civils products?\nAS: Indeed, file formats have been a major barrier, a lot of the agencies require DGN file format delivery \u2014 they don\u2019t care how the design was done, they just want it delivered in that format. Compatibility is a huge plus for us, engineers can use whatever design tool they want and deliver in the right format for the agency. PDF is also important and leveraging the new PDF features in AutoCAD 2010 will be great for our customers. It\u2019s important because a lot of government agencies require PDFs that are searchable and we know the support for TrueType fonts which makes that possible.\nMD: Model-based design seems to be really taking off in transportation. What benefits does this provide over traditional working practices?\nAS: One of the big selling points for getting our products into transportation is its support for machine controlled grading, so they can take the 3D model that is developed in Civil 3D and use that directly for machine guidance on-site. With some of the other tools out there, models need to be created after the design process, taking the 2D construction documents to build the 3D model after the case. With Civil 3D the model is a direct result of the design process. Our models remove the guess work so you can get very precise volumes.\nMD: There is a lot of talk about Building Information Modelling (BIM) in civil engineering now, which started off as a buzzword on the architectural side. Can you have BIM in civils?\nAS: We are seeing a couple of things. Civil engineers that are working closely with architects and structural engineers, where BIM has had a lot of traction, are being asked to participate in the BIM process, so that is creating pull-through demand in civil engineering. We are also starting to see the use of the term in transportation from large construction firms in the sense of allowing designers and contractors to share information like elevations, cross-sections, quantities of earthworks, so co-ordinated design changes can be made within the process. So we think it does apply; the terminology is not widely used but when we explain it a light bulb goes on and people get excited.\nCivil engineers can embrace a BIM workflow by starting with getting the land use data into the design process to help inform design decisions early on. That then acts as the foundation of the BIM model. Then, after more detailed design work, you build out that model, and it becomes a foundation to collaborate with other disciplines, such as a structural engineer, using Revit Structure to design a bridge, now that we have interoperability between those two products. Taken further that model can then be used for simulation and analysis. There are some tools within the products as well as a very extensive network of partners that are building analysis and simulation that leverage the Civil 3D model.\nMD: There is a big focus on Green and Sustainable design. What\u2019s happening in the Civil division to support this?\nAS: When people hear the words sustainable design, they usually first think about what the architect can do with the building but the civil engineer plays a major role in sustainability. There are three main areas, the first is choice of location to reduce environmental impact. The second big area is around deploying design processes that minimise the construction area and prevent soil erosion. And thirdly is storm water, minimising storm water run off, reducing the impact of water quality in the surrounding area. Our technology uniquely facilitates this in two ways: the model means you can very quickly evaluate many different design options, and the native geospatial and storm water analysis.\nAs to the future, we are hearing a lot from customers about the LEED1 credits that rely on the civil engineer and are looking for automation. So they can go through their designs and see the impact on LEED credits as they alter the design. Of course, the problem with this is that the standards are still emerging, so it\u2019s a long term project but it\u2019s certainly doable. 1[Ed \u2014 The Leadership in Energy and Environmental Design (LEED) is a green building rating system developed by the US Green Building Council (USGBC)]\nMD: How do you go about handling workflow and process?\nAS: It really depends where you are. Japan, Korea and to some extent China, tend to have a couple of engineers designing and they may well not be hands on using software and they will be supported by hundreds of drafters. It\u2019s very 2D focussed. Then you look at the USA and Western Europe and it\u2019s completely different, with far fewer 2D drafters and the engineers that are adopting this model-based process, whereby the documents are a product of the model. We are trying to support both of those models and have introduced a product that is for Japan only and leverages AutoCAD LT with some civils functionality and a standards checker to meet Japanese drafting standards.\nDP: Civil 3D is very flexible, a lot of the functionality that is included in our country kits allows customisation to meet many, many drafting standards without having to have the knowledge of programming. MD: Autodesk has changed its subscription offering, from a yearly update to delivering more regular add-ons. Will this be the same for Civil 3D?\nAS: Absolutely, we are always adding extensions and delivering them via subscription. Just in the last day we posted two new extensions to the subscription centre. One of them is a tool specifically for France, to do with their requirements for roundabout design, matching the country standards. The second piece of software is particularly useful for Northen Europe, it\u2019s an import\/export tool for MX files, so data can be exchanged between Civil 3D and Bentley MX.\nMD: With the popularity of Revit, are there any plans to deliver a Revit Civil?\nAS: We don\u2019t have any plans to develop a Revit Civil but we are really concentrating on developing better interoperability between the Revit products and Civil 3D in a number of areas. One of them is around bridges and Revit Structure where engineers can take an alignment, a roadway centre-line, produce a surface model and bring that into Revit Structure for modelling and pass that information back. There is a lot of demand for bringing Revit models into Civil 3D and not just a dumb block. You would want to know the square footage of the roof area, for drainage design, or the building connection points for MEP pipes to hook up with pipes on the site. So 2010 is just the first cut at providing some level of interoperability between Revit Architecture and Civil 3D.\nMD: The need to share data and adoption of 3D appears to be slowly changing the how companies work together?\nAS: It\u2019s exciting to see this level of collaboration.There is a big project in upper west-side Manhattan and due to the complexity of the project, the owner stipulated that it be done in BIM. The Civil Engineering firm involved didn\u2019t really understand what BIM would mean for them and our reseller showed them the benefits of modelling and what really sold them on it was when they put everything together in NavisWorks \u2014 the architect\u2019s model, structural model and pipe work done in Civil 3D. They were quickly able to find some serious conflicts between the tie backs that were part of the slurry wall and the utility work that would not have been found till they had got to construction. There are definitely some tremendous benefits.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/bim-show-live-lays-down-gauntlet-for-digital-construction\/","title":"BIM Show Live lays down gauntlet for digital construction","date":1551916800000,"text":"Delegates challenged to open their minds to new ways of working and build a better future for the construction industry\nWe\u2019re ready for a \u201cconstruction revolution\u201d creating opportunities for young people to gain new skills and develop new capabilities, explained Simon Rawlinson, Arcadis, as he opened proceedings at BIM Show Live in Newcastle last week. With improved productivity this is achievable, he explained. \u201cYou all have a responsibility to make this happen, we can all do better and we must challenge ourselves.\u201d\nThis statement rang true throughout out the next two days as people laid down challenges for others to open their minds to new ways of working and build a better future for the construction industry.\nBIM Show Live \u2013 as much as BIM is at the heart of the AEC industry \u2013 has grown beyond BIM in the traditional sense and explores a greater expanse of digital construction methodologies, technologies and design.\nThe event\u2019s co-founder Rob Charlton said that in the eight years the show had been running adoption of digital construction has accelerated, but we have also become very good at criticising our sector. \u201cThe skills shortage will continue to grow and the impact of Brexit is likely to mean we lose some experience from the sector,\u201d he said, before focusing on the many positives.\n\u201cProjects such as Crossrail, Kings Cross or pretty much any major development in London demonstrates the impact we have on society. The future will see us move from BIM to twin with the increase in adoption of the digital twin where relationships and interactions between people, places, and devices can be applied to any physical environment.\u201d\nProfessor James Woudhuysen told delegates to forget what we are used to and be open to new ideas. That innovation doesn\u2019t just come from the super-brains in our industry, and we need to look at the \u201cB\u201d in BIM rather than focusing on the \u201cI\u201d. The time of the building is now, he said, and it\u2019s what we do with the information that counts as much as how we acquire and process it.\nThis sentiment was echoed by James Austin of Autodesk, whose insightful delivery of where the industry is now, where it has arrived from and where it is headed captivated the audience. \u201cTo stand still would be catastrophic,\u201d he said. \u201cYes, we are in a good place right now, but we need to strive for continual improvement and continue to look for innovation in new places, not just what we are used to. Evolution is the enhancement of the next generation of construction.\u201d\nAdam Ward of BIM Technologies discussed the need to industrialise construction and take the lead from Design for Manufacture Assembly (DFMA). The most attended seminar, an exploration of the new ISO 19650 standard, was delivered by one of the standard\u2019s authors Paul Shillcock, accompanied by Emma Hooper and Andy Boutle of the UK BIM Alliance. ISO 19650 was explored further in an all-female panel debate on how the new standard is impacting on the various networks within the supply chain.\nIndustry giant Jay Zallan cognitively challenged the audience with his exploration of the opportunities and implications of connected technologies on the construction ecosystem. \u201cIn just a small number of years we have advanced so much technologically it is astounding, but we sometimes need to think for ourselves and not rely on technology to provide the answers,\u201d he said. \u201cWe need to drive the technology and not let the technology drive us!\u201d.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/unreal-engine-becoming-part-of-design\/","title":"Unreal Engine - becoming part of the design process","date":1537315200000,"text":"Greg Corke talks to Epic Games Enterprise about the potential of Unreal Engine in AEC and its ambition to make the real time game engine an essential tool for the design process\nUnreal Engine is making big strides in architectural visualisation. The CGarchitect survey from February 2018 revealed that 21% of viz artists were using the real time game engine in production, double that from 2017.\nNext year we expect these numbers to grow again, fuelled in part by the recent beta launch of V-Ray for Unreal. By offering a push-button workflow from the leading arch viz rendering engine, barriers to adoption are being removed.\n\u201cYou can actually use Unreal without learning Unreal, which is what customers told us they wanted,\u201d says Marc Petit, General Manager, Epic Games Enterprise of the workflow that automatically transfers the entire scene \u2014 materials, lights and all \u2014 into the real time engine. \u201cSuddenly, for the same amount of effort, they get to interactive content. They can stay in their favourite environment, do what they do, but they can now get interactive and immersive and VR for free.\u201d\nThe future of Unreal Engine in creative agencies and viz studios certainly looks bright, but Epic Games is not happy for it to simply be a showcase tool for polished real time and VR experiences. It has much bigger ambitions for AEC.\n\u201cThe big prize is to become part of the design process,\u201d explains Petit, \u201cand offer everything the platform does well \u2014 which is ingest super complex geometry, create high fidelity rendering, animation, physics, multi-user, immersion, VR support and AR support.\u201d\nWorkflow is king\nIn architecture and engineering, designs continually change, so a workflow was needed to support this fluid process. If users had to re-apply textures, materials and lighting every time there was a new design iteration, Epic knew it would be fighting a losing battle. The cost of data preparation had to be eradicated.\nTo support its \u2018enterprise\u2019 users, Epic developed Unreal Studio, a suite of tools designed to make it easier to work with CAD and other 3D software. Unreal Studio is currently in beta and is free to download. The beta was recently extended to run until September 2019, when it will officially launch for a monthly subscription fee of around $50.\nUnreal Studio includes a number of components, including templates, assets (materials), learning tools and professional support, but it\u2019s the Datasmith workflow toolkit that is most important, as it streamlines the flow of data from CAD into the Unreal Engine Editor.\nDatasmith works with a number of applications, including SketchUp Pro, Rhino, 3ds Max and SolidWorks, either through plug-ins or direct import. Support for other applications, including Revit, are also in development, as Petit explains. \u201cRevit is such a closed environment, so it\u2019s a tough nut to crack, but we\u2019re working on it now.\u201d\nUnreal Studio 4.20, released in July this year, took a massive leap forward with the introduction of non-destructive workflows. Now, when a design model changes and is re-imported, any investment that has been made in Unreal Engine is maintained.\n\u201cThere\u2019s always a new version of the MEP model, a new version of the architectural model \u2014 and so the non-destructive workflow means that all the edits and decisions that you\u2019ve made in Unreal get reapplied automatically when you reimport,\u201d explains Petit. \u201cOr we optimise \u2014 I mean if you re-import a hotel and you only change the door knobs, we\u2019ll figure it out. We don\u2019t need to re-import the doors and the walls, we\u2019ll just re-import the door knobs. All of this optimisation helps you be more productive.\u201d\nBuilding on the foundation of nondestructive workflows, the next step for Unreal Studio is to support live streaming between the design tools and Unreal Engine. This capability, which is earmarked for release later this year, should have a massive impact on how Unreal is embraced throughout the design process. Architects, engineers or clients alike will be able to instantly visualise the impact of design changes in the visually rich Unreal environment, a feature that has made tools like Enscape so popular with architects.\n\u201cWe already do it for Maya \u2014 live streaming \u2014 and we are now going to bring it to products like SketchUp. So, you can actually have someone in the workflow modelling in SketchUp, streaming to the engine,\u201d explains Petit. \u201cWe needed the non-destructive workflow first because it\u2019s a form of live update. You don\u2019t read from a file, you read from a stream.\u201d\nData optimisation\nIn 3D games, datasets undergo an incredible amount of optimisation to ensure good performance on screen and in VR. And, as long as the scene delivers the right look and feel, then geometry can be simplified dramatically.\nAEC is different in that geometry generally needs to be very accurate, so designs are not over simplified. Furthermore, with designs continually evolving, firms don\u2019t have the luxury of spending hours optimising every new dataset. The process is made more complicated if assets need to be simultaneously pushed out to different platforms \u2014 PC, mobile, VR or AR \u2014 and at various Levels of Detail (LoD), with different polygon budgets for quality and performance.\nIn short, the process needs a significant level of automation.\nWith Unreal Studio 4.20 users are now able to prioritise which objects can be heavily optimised and which should not be optimised, using a combination of metadata and Python scripts.\nPetit recalls an issue on a recent customer project, \u201cThere was a big problem with the urinals in a s tadium because, for some reason in VR, it was not performing. We ran an analysis and we realised that there were a lot of urinals [with complex geometry] that were getting processed, not fully rendered, but processed.\n\u201cYou can now tag those as second-class citizens. You can write a script that says, oh my god, this is not an important piece, I\u2019m going to decimate the hell out of it.\u201d\nIn many BIM workflows, metadata is prevalent, so standard classifications and scripts can be used across projects to help manage polygon budgets more effectively. However, Unreal is also exploring ways to harness the power of machine learning so Unreal Engine would be able to automatically \u2018recognise\u2019 objects and process them accordingly when metadata is not present or in the right format.\n\u201cRight now, a human would have to interpret the metadata but ultimately, you could have a very simple machine learning layer on top of it and say \u201cIt\u2019s a screw, I don\u2019t care about a screw, it\u2019s going to be a cylinder. Or it could be used to add behaviours, like \u2018this is a door of a certain type, and this is how it should open\u2019.\n\u201cThere are two things we are looking at,\u201d adds Petit. \u201cOne is, of course, recognising shapes. You can tell that a cup is a cup, and you then have strategies to optimise a cup. We could also start to develop semantics around some metadata and start to automate decision making based on that.\u201d\nPetit is keen to emphasise any discussions on machine learning are very prospective and acknowledges that object recognition is very complex, \u201cYou can train a machine to recognise a screw, but you need a lot of data, you need to show it a lot of screws and we haven\u2019t worked out who we partner with to do that aspect.\u201d\nThe learning curve\nUnreal Engine is a very powerful tool; the rendering quality is superb and due to get even better with the promise of real time ray tracing using Nvidia RTX technology. But it is also known for its complexity and steep learning curve. It is also important to understand that Unreal Engine is a software development platform, for creating a game or an application, and not an off-theshelf software tool. This makes it very different to something like Enscape, which is designed specifically for architects and laser-focused on ease of use. And while large architectural practices have the luxury of in-house viz teams with the advanced programming skills needed to get the most out of the software, smaller firms need more hand holding.\nEpic Games acknowledges this and is working hard to help newcomers get up and running quickly. With SketchUp, for example, providing that the SketchUp assets are well modelled and the materials applied correctly, the plug-in automatically translates cameras, materials, and geometry into Unreal Engine assets.\nUnreal Studio also includes templates to use as a starting point for architectural projects into which users can simply drop their model. Then, by using Blueprint, Unreal\u2019s visual scripting language, users can go on to add life to a scene, as Simon Jones, Director, Unreal Engine Enterprise explains. \u201cBlueprint is modular, so we can have libraries. If you want to create some function in a scene, let\u2019s say you want the door to swing and you want it to make a noise while it swings and maybe cast light into the scene as it reflects light, now, that could be a blueprint that exists somewhere in a library that you just lift to drop into your scene and apply it to that object, that door.\u201d\nJones is adamant that Epic Games is not going to introduce multiple versions of Unreal Engine, say one for architectural visualisation, another for auto design. \u201cWe can never genuinely predict what features people will use,\u201d he says \u201cIt would be arrogant of us, probably foolish of us, to eliminate features that other industries are using,\u201d citing virtual production TV and film techniques, such as green screens, as examples of features that are increasingly being used in design viz.\n\u201cWe may introduce a templated front end, so you can select, as you fire up the engine, that you\u2019re an architect \u2014 and maybe later you can say what type of architect you are, maybe how advanced you are \u2014 and then we\u2019re going to present to you the way to use the engine in the way that you understand it.\n\u201cFor example, we might start you off with a scene that looks like a room, and you can bring in your own content and adapt it. We might pre-load some materials that are addressable for you \u2014 a library. But you\u2019ll always be able to go back [and use all the features if you wish].\u201d\nEven with this hand holding, some will still find Unreal Engine too complicated for their needs. And it\u2019s here that thirdparty developers look set to play an increasingly important role.\nTwinmotion, the real time visualisation and VR tool designed for architects, is a prime example. The latest release, built around Unreal Engine, is designed to be easy to use and offers live links to Revit and ArchiCAD.\nEpic Games is actively looking for more third-party developers to bring industry-specific workflow expertise to the game engine and makes the source code freely available. \u201cWe acknowledge that every market segment has real problems [workflow challenges] and they deserve to have dedicated applications,\u201d says Petit. \u201cWe want to partner with the people that are going to bring those dedicated applications.\u201d\nThe obvious value for third party developers is access to an extremely high quality, high performance game engine, but the benefits don\u2019t end there. Unreal Engine offers out of the box support for VR (with HMDs and CAVES) and AR for use on the construction site. There is also a huge potential for collaborative design across distributed teams, as Petit explains, \u201cIt\u2019s a game engine and has all the features for collaboration \u2014 voice chat, multi user.\u201d\nUnreal Engine could even be used to perform roles traditionally carried out by engineering-focused products like Navisworks, \u201cWe don\u2019t have clash detection out of the box, but our physics engine is really good,\u201d says Petit, adding that there are also prototypes for cutting planes \/ live sections.\nThe aggregation platform\nEpic Games has a long-term ambition for Unreal Engine to become a hub for all AEC data throughout the design process and beyond, \u201cWe\u2019re seeing this in construction already, people are assembling MEP models, architectural models, structural models and using Unreal Engine as an aggregation platform.\u201d\nPetit also believes the co-ordinated model should not be limited to visualisation. \u201cRight now, you aggregate in 3ds max and do a beautiful picture and aggregate in Navisworks to make all the decisions.\n\u201cPeople will soon realise they can do this in the same platform; it will generate a lot of saving.\u201d\nAnd those savings could potentially be huge. The notion of using a single asset that could be adapted to different workflows as and when required, supporting engineering and aesthetic decision making throughout the design process, is incredibly powerful. The visual fidelity of the engine may grab the headlines, but the way in which it can handle colossal datasets in multi-user environments promises to deliver a strong foundation for collaborative AEC workflows.\nPetit acknowledges the engine is not yet ready to take up the mantle as the allencompassing multi resolution, multipurpose generator of \u2018digital twins\u2019, \u201cWe need more data types, we could be better at terrain, we could be better at point clouds, we still have work to do, but the premise and the promise are very good. The potential is overwhelming,\u201d he concludes.\nUnreal Engine \u2013 how much does it cost?\nThere is a lot of confusion surrounding licensing for Unreal Engine and how much it costs. In short, anyone can use the engine for free, but if you make a product that you sell \u2013 a game, for example \u2013 you pay a royalty on those transactions.\nAs design viz doesn\u2019t involve the creation of commercial products, no royalties are applicable and architects and creative agencies needn\u2019t pay a dime if they don\u2019t wish to.\nHowever, those who place a value on professional ticketed support, viz templates, materials, tutorials and the nondestructive CAD-to-Unreal Datasmith workflows, will need an Unreal Studio subscription.\nUnreal Studio launches officially in September 2019 and will start from $49 per month, which seems money well spent. The beta is currently free. Epic\nGames Enterprise also offers bespoke licensing agreements for larger firms that may need special help with pipelines, custom workflows, support plans or simply need changes to the Unreal Engine Studio EULA (End User License Agreement).\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/construction\/ai-to-help-predict-embodied-carbon-in-construction\/","title":"AI to help predict embodied carbon in construction","date":1606694400000,"text":"Innovate UK funded project to help drive down industry carbon emissions and meet Government\u2019s green targets\nA cross-sector multidisciplinary research group in the UK is developing an embodied carbon analytics AI system that predicts the carbon output on building and infrastructure projects, based on BIM data, materials carbon data and lessons learnt on past projects.\nThe consortium, which comprises Winvic Construction, the University of the West of England (UWE Bristol), Edgetrix and Costain, has secured \u00a3800,000 of Innovate UK funding to deliver the ASPEC [AI System for Predicting Embodied Carbon in Construction] project over the next two years.\nAccording to main contractor Winvic, while construction organisations continue to make advances in how they address both embodied and operational carbon emissions, current calculation methods for the former are onerous; with no design support calculation solution, there is no efficient way for design teams to proactively drive down the embodied carbon and carbon footprint of projects.\nThe ASPEC technology, adds Winvic, will therefore be crucial in propelling constructors and material manufacturing firms to meet the UK government target removing 10MT of carbon dioxide by 2030. Furthermore, a 50 per cent reduction in carbon emissions has been set through the Construction 2025 strategy and the target for 2050 is to bring all greenhouse gas emissions to net zero.\n\u201cFollowing the announcement of government targets paired with BREEAM sustainability requirements, construction organisations and clients have worked hard to reduce emissions, but the task certainly isn\u2019t simple,\u201d adds Tim Reeve, Winvic\u2019s technical director and project lead. \u201cThat\u2019s why we believe that utilising the most up-to-date AI and advanced big data analytics techniques in a way that has never been done before will be transformational for Winvic\u2019s green agenda and pave the way for significant changes across the whole industry.\n\u201cHaving the ability to optimise schemes and see embodied carbon costs as real-time design and material changes are applied will naturally lead to quantifiable reductions in greenhouse gas emissions, and having technology that automatically estimates the embodied carbon cost of any digitally designed project will make the delivery of many project tasks much faster.\u201d\nWinvic and its project partners will develop a Common Data Environment (CDE) to eradicate the challenges that arise from having multiple, siloed datasets. The huge amounts of data that are generated from different phases of construction projects will be documented and stored in one system; it is this data nucleus that will facilitate the machine learning technologies, providing industry professionals with the insights to make more eco-friendly decisions on materials used.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/the-case-for-open-sourcing\/","title":"The case for open sourcing","date":1574380800000,"text":"Jaimie Johnston, head of global systems at Bryden Wood explains why hoarding IP won\u2019t save construction (and what we need to do instead)\nIn 1956, Dr. Mervin J Kelly, an American physicist and engineer, enters a courtroom in New Jersey for the verdict in one of the most important antitrust cases in history.\nKelly is president of Bell Labs, the main provider of telecoms services in the US and his company is the subject of a long-standing lawsuit to break up the Bell System, which is (allegedly) monopolising the market.\nThe outcome of the trial is dramatic. Bell Labs is forced to license all its patents royalty free and is simultaneously barred from entering any industry other than telecommunications. So, almost 8,000 patents in a wide range of fields (1.3% of all the then current US patents) become freely available to anyone. The patents cover technologies created by the most innovative industrial laboratory in the world \u2013 the lab that invented the transistor, the solar cell, and the laser \u2013 and suddenly all their secrets were out. However, the innovation, competition and collaboration that followed the judgement changed the industry and then the world.\nWe now recognise that this very moment was the birth of Silicon Valley.\nThe case for open sourcing in design and construction is no less compelling. We know the industry can\u2019t continue in its current form. We won\u2019t meet the UK\u2019s need for housing, education, healthcare and transport infrastructure by designing and building traditionally. Change is essential.\nA new, open source solution\nWe can\u2019t adopt a sophisticated digital design process while using Victorian construction methods. We can\u2019t tack design for manufacture and assembly (DfMA) on to a traditional design approach.\nBoth the digital and physical processes must transform simultaneously, and both must be open source.\nFragmentation in the design to delivery process has created narrow but deep specialist roles \u2013 for instance the \u2018architect\u2019 may now comprise a concept architect, an executive architect, a fa\u00e7ade architect and an interiors architect. Consequently, an asset\u2019s design undergoes multiple handovers from concept to detail to fabrication, and so on. At every stage, design gets diluted and \u2018value engineered\u2019. Current procurement methods encourage everyone to limit their scope of work to well-defined and defensible pieces, and pass risk down the line. The result is a protectionist environment with little scope for innovation. When innovation does happen, it\u2019s in small pockets and focused on one aspect of the process.\nWe think a number of things are needed to reverse this trend.\nFirstly, a new, holistic, design process that simultaneously considers client values, design, procurement and assembly (on and off site). It needs to be technology-based and highly digital but that\u2019s not all. It must be intrinsically linked to (and maximise the benefits of) DfMA.\nSecondly, we must recast the sharing of knowledge, and create common solutions that everyone can unite behind and progress. This will only happen if we open source best practice for the collective good. The issue facing the industry is not a lack of potential work \u2013 UK government is investing \u00a3600 billion in the built environment over the next 10 years \u2013 it is our ability to deliver high performing assets with our current capabilities. Easy to say, harder to action. But at Bryden Wood we\u2019ve begun by developing a number of open-source innovations.\nCreative, technology-led design\nThe vision for BIM was to enable designers to use data from existing assets to make better informed decisions at the front end of the design process. However, because these tools get deployed once the concept design has already been developed, there is a missing link that would allow a truly data driven design process.\nBryden Wood has created and launched three web-based, free, algorithmic tools to allow the digital design process to start much earlier \u2013 and go much faster \u2013 before feeding into existing BIM workflows. The result is a complete digital process from design to delivery.\nPrism is our open source app to accelerate the design process for precision manufactured housing in London. It\u2019s free, easy to use, and combines spatial planning rules with precision manufacturer expertise to quickly determine viable housing options for a given development. Designs, produced in hours, can then be fed into existing BIM workflows, and in turn into platform-based construction solutions.\nOur Seismic tool accelerates the design of primary schools, this time embedding operational requirements from the Department for Education (DfE) in an app that has successfully been used by children to design their own schools.\nFor Highways England we developed a rapid engineering model that uses data from an existing asset\u2019s performance to produce 200+ design options as opposed to one option, heavily iterated. The result is a motorway designed in days, not months.\nIt\u2019s our view that free, web-based apps like Prism and Seismic will remove current barriers to entry, speed up design, and enable many more people to design the buildings and infrastructure we badly need. We\u2019re hoping to create a community who can adapt the apps and boost their functionality. Meanwhile, tools like those we developed for Highways England put new technology directly into the hands of our clients to help them monitor and run their asset as efficiently as possible.\nA Platform approach to DfMA\nPlatforms are sets of components, with designed in compatibility, that combine to produce highly customised structures, highly efficiently. Just as the chassis revolutionised car manufacturing by providing a common platform for customisation, the Platform system is based on repeatable forms and standardised connections, enabling many different kinds of space to be built with a single \u2018kit of parts\u2019. In the words of the Infrastructure and Projects Authority (IPA), \u2018a single component could be used as part of a school, hospital, prison building or station\u2019.\nThis point is crucial because sector-specific thinking within the industry has long been a constraint. Many people view housing, for instance, as being separate from and different to anything else, which forces a reliance on demand in one particular market sector. As a result, a manufacturer of a residential system may find it difficult to achieve the full benefits of a manufacturing approach because the housing pipeline is inconsistent.\nA universal Platform system, with interoperable components that work across sectors, will allow manufacturers to expand their capability efficiently across building types to be able to service a much larger share of the overall construction market. In turn, creating consistent demand and sustainable pipelines in different sectors.\nInteroperability will mean companies don\u2019t have to work in isolation. With Platforms, the industry can work collectively, in a model of continual improvement, to accelerate the pace of change.\nOur work on Platform systems has been published, for free, on our website. And, through The Construction Innovation Hub, we have launched an open call for Platform-based solutions to encourage anyone, from within or outside the construction industry, to get involved. This is part of the largest ever Government-backed R&D programme in our sector and closes at the end of this month (constructioninnovationhub. org.uk\/platform-design-open-call).\nBy combining digital design tools with sophisticated DfMA, we can create a new, technology-led process with increasing benefits for all through complete interoperability. Open sourcing both the digital and physical solutions could mean the powerful network effect we have observed in numerous other sectors finally comes to construction.\nThe skills gap in our sector is well documented and worsening, and understandably Generation Z view a career in construction as bleak and unrewarding. However, a tech-led career in a fast-moving sector creating high performing assets, is a totally different prospect and one we must foster.\nWhat is certain is that open sourcing technology will create a much more diverse workforce across all aspects of the built environment who will challenge and ultimately change the sector.\nAlthough the 1956 judgement was a painful one for Bell Labs, the impact was lasting and positive. The question is, how much more powerful could the outcome for construction be if we voluntarily share solutions, starting now?\nIt could be the transformative moment we\u2019ve all been waiting for.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bluebeam-enhances-revu-workflow-tool\/","title":"NEWS: Bluebeam enhances Revu workflow tool","date":1489017600000,"text":"2017 release delivers enhanced PDF-based take-off and workflow automation tools\nRevu 2017 is the latest version of the AEC-based, PDF-centric workflow automation and collaboration tool from Bluebeam. The new release features several new features and enhancements including new take-off tools for estimators for creating higher quality, PDF-based bids and expanded batch processing tools to allow architects and engineers quickly apply digital signatures and professional seals across a batch of multiple files.\nOther new tools include Automatic Form Creation, which is designed to increase efficiency across a broad set of needs including RFIs, submittals, contracts and permits and streamlined Measurement and Count Tool enhancements to help construction professionals eliminate redundancies, utilize take-off data downstream and manage project documents.\nThe software also includes enhanced 3D navigation and functionality to help visually convey complex concepts to project partners, while the ability to embed 360 photos is said to allow teams to quickly assess and document jobsite progress.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/automated-stockpile-volume-monitoring\/","title":"NEWS: automated stockpile volume monitoring","date":1407110400000,"text":"3D Laser Mapping automates volume measurement of stockpiles for mining industry\nSiteMonitorSV is the latest addition to 3D Laser Mapping\u2019s suite of SiteMonitor solutions, combines advanced 3D laser scanning technology with software to provide up to date inventory that re\ufb02ects current stock levels.\nVolume measurements require regular surveys of the same area followed by a comparison to a reference plane or surface, which through conventional methods is resource intensive and therefore costly. The automation enabled through the SiteMonitorSV system is designed to allow repetitive daily, weekly or monthly volume surveys of stockpiles to be completed faster than when using traditional survey methods.\n\u201cOne of the great things about this as a solution is its versatility \u2013 SiteMonitorSV can be mounted on conveyor structures, light poles or any existing infrastructure,\u201d says Dean Polley, Regional Director for Africa at 3D Laser Mapping. \u201cIt really is a cost effective, low maintenance and easy to use solution.\n\u201cThe visual representation resulting from multiple scanners working together makes SiteMonitorSV easy to measure and manage stockpile volumes, creating an accurate inventory\u201d.\nThe system, which uses a multi-axis laser architecture to generate an accurate point cloud will capture the full range of information and regularly updates itself to help ensure an accurate record of stock and movement.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-martyn-day-aec-magazine-nxt-bld-2\/","title":"Video: NXT BLD 2019 \u2013 Cobus Bothma, Kohn Pedersen Fox (KPF)","date":1565049600000,"text":"Accelerating design decisions with rapid visualisation \u2013 NXT BLD London, June 2019\nWithin the design process there are both internal and external consumers of visualisation deliverables. With short deadlines, fast turnarounds are essential. Bothma will look at the technologies KPF deploys to accelerate the process and how the firm uses visualisation to inform all stakeholders in complex design decisions.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/physical-research-laboratory-honours-scientists\/","title":"Physical Research Laboratory honours scientists","date":1068854400000,"text":"Physical Research Laboratory honoured eight distinguished scientists at a function held here on Tuesday. Dr A P Mitra, former director-general of Council of Scientific and Industrial Research, gave away the Shri Hari Om Ashram Prerit Vikram Sarabhai Research Awards and the PRL Award. Six eminent scientists received the Hari Om awards and two received the PRL Award. Those honoured with the Vikram Sarabhai Research Awards for 2001 are Prof Subhasis Chaudhuri of the Indian Institute of Technology, Mumbai, in recognition of his original contributions on digital image processing including medical imaging, super resolution imaging and recovery of depth from defocus.\nDr Bhabatosh Chanda of the Indian Statistical Institute, Kolkata, in recognition of his original contributions on image processing which is of importance to satellite and medical imaging as well as for industrial processing and documentation.\nDr B S Gohil of the Space Applications Centre, Ahmedabad in recognition of his original work on development of algorithms for retrieving atmospheric and oceanic parameters from satellite observations.\nDr Anil K Pandey of the State Observatory, Nainital, in recognition of his extensive studies of star clusters in relation to the structure of the galaxy and star formation processes within these clusters.\nDr Avinash A Deshpande of the Raman Research Institute, in recognition of his significant contributions towards understanding various aspects of pulsars including the first-ever mappings of pulsar radio emission sites.\nMukund K Rao of the Indian Space Research Organisation, Bangalore, in recognition of his original work on design, development and promotion of spatial data bases for remote sensing applications.\nScientists who received PRL awards for 2001 are Dr G S Bhat of the Indian Institute of Science, Bangalore, in recognition of his novel and innovative laboratory experiments on cloud-like flows and pioneering observations to understand atmosphere-ocean coupling over the Bay of Bengal.\nDr A Jayaraman of the Physical Research Laboratory, Ahmedabad, in recognition of his significant and sustained contributions to aerosol research, in particular for assessing radioactive forcing efficiency and its influence on atmospheric dynamics and water cycle.\nThe Hari Om Awards were instituted on August 12, 1974 in honour of Dr Vikram Sarabhai, founder of Physical Research Laboratory by the Hari Om Ashram, Nadiad.\nThe PRL Award has been instituted in 1997 from the Aruna Lal Endowment Fund established by Prof D Lal, honorary fellow and former director of PRL. All the awards consist of a medal and a cash prize of Rs 25,000.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/the-future-of-building-design\/","title":"The future of building design","date":1459382400000,"text":"New technologies are empowering architectural firms to improve quality, capabilities and process. By Martyn Day\nWelcome to the Future of Building Design, an AEC Magazine special edition that takes a holistic view of the technologies and processes which are set to change and enhance the AEC industry in the coming years \u2014 from concept design all the way to construction.\nAll around us technology is revolutionising the way we live, work, play and communicate. It is happening so quickly that it becomes hard to remember what it was like before the Internet, smart phones and wireless data. At the same time, there are significant technologies emerging that are set to enhance our increasingly digital civilization.\nArchitecture, Engineering and Construction is at a tipping point into widespread process change. Along with that comes the adoption of new technologies. Advances in digital design tools, inter-connectedness and computer-controlled fabrication are converging to transform every phase of the design and build process.\nWith the industry finally moving away from relying on 2D symbolic documentation and adopting design within a 3D modelling context, computers can offer additional benefits beyond Computer Aided Drafting (CAD). 3D modelled buildings can be analysed and optimised for performance, be used to control costs, provide immersive Virtual Reality experiences and photorealistc renderings and animations. The software and the project data is also changing, from being desktop-bound to platform agnostic, everpresent, always on and always up to date.\nBeyond the initial design phase, mobile technologies, such as iPads, are providing instant access to design information for fabrication and construction.\nWe can quickly and accurately capture\/ digitise the world around us, with photogrammetry, laser scanners and Unmanned Aerial Vehicles (UAVs or drones). This allows us to put virtual 3D models in context and monitor construction progress.\n3D models also drive digital fabrication. While 3D printing has had more than its fair share of hype, the long-term impacts have yet to be felt. The construction industry is now experimenting with materials and processes for the design of 3D print components or complete buildings.The image above comes courtesy of Gensler, which partnered with WinSun Global, Thornton Tomasetti, and Syska Hennessy to design and build the world\u2019s first 3D printed office in Dubai. A new material will be used to print the structure, which combines reinforced concrete, glass-fibre-reinforced gypsum, and fibre-reinforced plastic. The building will be created in layers and will incorporate 3D printed furniture.\nLooking a little further into the future, digital fabrication methods currently used in engineering, specifically robots, will find their way on site, driven by the 3D model. Robots have been deployed on component creation within the supply chain. Brick laying robots exist but, as you will read, there remain issues to overcome with the technology.\nThe AEC industry faces many changes this year. Governments around the world are in the process of implementing mandates to embed 3D processes (commonly called Building Information Modelling) into construction deliverables. The UK BIM mandate comes into effect in April.\nThe reality is that from this adoption of 3D, the downstream automation benefits will go far beyond anything envisaged by today\u2019s government ministers. The opportunities will range from new business models to additional revenue streams, among many others.\nThe best way to be ready for this change is to be part of it. We hope that in this issue, you find some inspiration.\nThis article is part of an AEC Magazine Special Report into the Future of Building Design, which takes a holistic view of the technologies and processes, which are set to change and enhance the AEC industry in the coming years \u2014 from concept design all the way to construction.\nClick to read the other articles that make up the report.\n1) Introduction New technologies are empowering architectural firms to improve quality, capabilities and process.\n2) Conceptual design There are a whole host of digital tools for early stage design experimentation.\n3) Rapid site design The rapid capture of site topology is being aided by new technologies.\n4) Benefits of 3D design Evolution, not revolution when making the move to 3D CAD.\n5) Moving to model-based design How to get from 2D to 3D, how to roll out training and how to overcome common issues encountered along the way.\n6) Design viz Advanced new rendering technologies are opening the door to design realism in architectural workflow.\n7) Design, analysis and optimisation Once you have a 3D CAD model, optimse your design for daylighting, energy performance and much more.\n8) Collaboration and model checking How to share models with clients, contractors and construction firms and test the quality of your model.\n9) Workstations What to look out for when choosing a workstation for 3D CAD.\n10) Virtual Reality New technologies are now available to support powerful new design workflows.\n11) 3D printing Architects are 3D printing architectural models with impressive results.\n12) Fabrication As building time gets compressed what will revolutionise fabrication and construction time?\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-fujitsu-shrinks-vr-workstations-with-new-desktop-and-mobile-models\/","title":"NEWS: Fujitsu shrinks VR workstations with new desktop and mobile models","date":1490227200000,"text":"New VR certified Celsius models are powered by the latest Nvidia Pascal GPUs\nFujitsu has unveiled two new \u2018VR Ready\u2019 workstations powered by Nvidia Quadro P Series professional graphics cards \u2013 the CELSIUS W570power+, which Fujitsu says is the industry\u2019s smallest VR capable desktop workstation, and the CELSIUS H970, a 17-inch mobile workstation.\nThe CELSIUS W570power+ is a special edition model of a new 21 litre microtower desktop workstation. The machine is 180 x 304 x 375 mm in size, 12kg in weight and has a built-in handle at the front, making it easy to move between design office and board room for VR presentations or design \/ review.\nThe desktop workstation is 30 percent smaller than its predecessor but is still highly expandable, featuring up to three NVMe PCIe M.2 modules and seven drives in total. 2.5-inch and 3.5-inch drives can be swapped out easily using Fujitsu\u2019s hallmark easy rails.\nFujitsu says W570power is an ideal device for 2D\/3D CAD and BIM applications, as well as VR. When configured with the Nvidia Quadro P4000 VR-ready GPU it can be paired with a professional Head-Mounted Display (HMD), such as the HTC Vive or Oculus Rift. It features the latest Intel Xeon E3-1200 series CPUs as well as up to 64GB of DDR4 memory.\nMeanwhile, the CELSIUS H970 offers professionals such as architects, engineers or designers who spend time in the field, enough power for 3D CAD and VR in a mobile workstation form factor.\nThe machine features a 17.3-inch FHD display, plus full connectivity with two Thunderbolt 3, one full-sized DisplayPort and VGA. In terms of graphics, on paper it even outdoes the CELSIUS W570power+, with a choice of GPUs including the high-end Nvidia Quadro P5000 GPU with up to 2,048 CUDA cores. For easy serviceability, components are accessible via the service door conveniently located in its base.\n\u201cThe combination of virtual reality combined with our powerful workstations opens up many exciting possibilities. While there is a trend in the market to sacrifice size versus performance, we still believe in the workstation DNA. This means no compromise in terms of performance, reliability, expandability, space-saving and VR capabilities,\u201d says Ruediger Landto, Head of the Client Computing Devices EMEIA at Fujitsu. \u201cThe revolution has just started. We recognize that VR can promote a holistic development process and enhance many stages of the value chain \u2013 from design, engineering, manufacturing, operations to maintenance.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/digital-fabrication\/adage-project-dfma-for-architecture\/","title":"BIM plug-in targets DfMA for architecture","date":1613433600000,"text":"ADAGE project designed to turn architects and building designers on to Design for Manufacture and Assembly (DfMA)\nA consortium of UK firms and universities is working on a BIM software plug-in that will automatically generate Design for Manufacture and Assembly (DfMA) concept designs based on key parameters from a project brief, such as material choice or building use.\nThe Automatic DfMA Design Generator (ADAGE) plug-in will use a variety of technologies to achieve these goals, including Internet of Things (IoT), Blockchain, cloud computing, artificial intelligence algorithms (AIA) and big data analytics.\nThe DfMA for architecture project is being led by White Frog in partnership with Pollard Thomas Edwards (PTE), Intenttech, Leeds Beckett University and the University of Hertfordshire.\nDfMA is widely acknowledged to offer improvements in cost, quality, construction safety and productivity, compared to traditional construction methods, but according to the ADAGE team currently less than 5% of designers are employing the approach.\nThe consortium has received \u00a30.5m in public funding from Innovate UK, part of the national funding agency UK Research and Innovation. The team will develop the automated DfMA solution over a nine-month period, with the project to be shared with the wider industry in late 2021.\n\u201cWe\u2019re excited to partner with PTE, Intenttech, Leeds Beckett and Hertfordshire University to develop a solution which will benefit the industry as a whole; clients, designers, contractors and product manufacturers,\u201d says Peter Routledge, managing director of White Frog and ADAGE Project Lead. \u201cThis new automated software tool is yet another step forward in driving construction efficiencies through the use of DfMA.\u201d\n\u201cThe wider adoption of DfMA is key to making construction more efficient and more sustainable and addressing the UK\u2019s housing crisis,\u201d Roger Holdsworth, Partner at PTE. \u201cADAGE will tackle the practical challenges faced by architects and designers on DfMA projects and develop new ways of working for the wider industry.\u201d","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/scia-engineer-design-forms\/","title":"Scia Engineer Design Forms","date":1385942400000,"text":"Nemetschek Scia\u2019s new tool is intended to make engineering design more transparent. Greg Corke met with CEO Jean-Pierre Rammant to find out more.\nScia Engineer from Nemetschek Scia is somewhat unique in the world of structural engineering software. It combines modelling, analysis, code design and detailing in a single environment. This is in contrast to traditional structural analysis applications, which focus purely on analysis, or structural BIM tools, like Autodesk Revit, which link to third-party analysis software.\nWith Scia Engineer two models are developed in tandem \u2014 a structural model, which includes the physical representation of the structure, and an analytical model, which contains all the information required for calculations and design. According to Nemetschek Scia this allows the engineer to \u2018master\u2019 the analysis model, while using IFCs or direct links to BIM authoring tools to keep in tune with architects and detailers. Nemetschek Scia refers to this integrated approach as True Analysis.\nThe software is capable of performing static, dynamic, stability, non-linear and other special types of analysis. The results from these studies can then be used directly for design and checks according to the relevant technical building standards.\nDespite the ability to feed these results directly into British, Euro and other design codes, recent research by the Belgian company has shown that many leading engineering and multi-disciplinary firms still rely on spreadsheets and Excel macros to do these types of checks.\nAccording to CEO, Jean-Pierre Rammant, this workflow, which often involves manual input or import\/export, can be inefficient and prone to errors. \u201cIn a BIM workflow, it is asking for trouble because you have revisions and you have changes and you cannot really manage that,\u201d he told AEC Magazine.\nNemetschek Scia\u2019s research showed one of the main reasons firms prefer Excel is because of the control it gives them and the fact that the process is so transparent.\n\u201cEven if we, as a software supplier, deliver those design routines, embedded in the software, they [engineers] don\u2019t use them \u2014 for the simple reason they are black boxes, they don\u2019t trust them,\u201d says Mr Rammant.\n\u201cFor a straight building an experienced engineer will know how much reinforcement there should be \u2014 she or he doesn\u2019t even have to do any calculations \u2014 but if it gets curved and more complex, she or he starts to ask himself questions.\u201d\nScia Engineer 2013: interoperability\nScia Engineer is big on interoperability. It was the first structural BIM tool to be awarded IFC 2\u00d73 \u201cVersion 2.0\u201d certification meaning it can collaborate with a wide range of IFC-enabled BIM tools, including Archicad and Vectorworks.\nIt has roundtrip engineering with Nemetschek Allplan for both geometry and reinforcement and direct links with Revit Structure, Tekla Structures and ETABS.\nThe Nemetschek Scia Revit link is based on the physical model, but more recently UK reseller CADS has used the Revit API to develop a plug in that links Revit\u2019s analytical model to Scia Engineer.\nThe CADS Revit Scia Engineer link (pictured) enables the bi-directional exchange of members, loads and supports.\nAccording to the developer, it supports levels, members, slabs, loads, (point, line, area and wind) and load combinations, support conditions and end releases.\nData within the analytical model, such as unique member names and releases, node numbers etc, are also exchanged. According to CADS, this enables the analysis model to be kept as data rich as possible during the roundtrip process.\nBreeding confidence\nNemetschek Scia is looking to change this perception by making the design process more transparent inside Scia Engineer 2013, which was released in September. A new scripting tool called Scia Design Forms enables users to write their own routines to perform external design checks and deliver clear technical and graphical reports. Calculation sheets can be run standalone or, for maximum benefit to the engineering workflow, linked to the Scia Engineer 2013 model.\n\u201cWe make them [Scia Design Forms] such that all of the formula is visible so you can follow it step by step; everything is explained,\u201d says Mr Rammant. \u201cIt\u2019s kind of a MathCAD, but it\u2019s made by ourselves and dedicated to engineering. You see all the formula, the integrations, the references, everything is there and of course you can make a report.\u201d\nScia Design Forms is intended to be flexible and can be used for the design of steel, concrete, connections, foundations, composites and many others. According to Mr Rammant, it can include anything that is detailed where there are analytical, relatively simple, algorithms, common in most design routines.\nThe software consists of three parts: a \u2018User\u2019 application for calculating the formulas and generating reports; a \u2018Builder\u2019 application for creating new forms and adapting others; and the Design Forms (Calculation sheets) themselves.\nA number of standard Design Forms are included with the software such as for static, loading, concrete, steel, timber, masonry and geotechnics. More will come in 2014 and Mr Rammant explains that part of the work has been already been done, as it has been hard coded inside Scia Engineer. \u201cNow it\u2019s a case of redoing it in an open form,\u201d he says.\nFor the development of more complex Design Forms Nemetschek Scia is working with universities and third party consultants. In the UK, Scia Engineer reseller, CADS (cads.co.uk), is working on composites. Others currently in production include timber connections, steel connections, and advanced statics analysis, all of which will be made available in Nemetschek Scia\u2019s web shop.\nOf course, a key driver of Scia Design Forms is its open approach. Users can create their own design forms or adapt existing ones. According to Mr Rammant, while Design Forms will likely be created by more advanced users, programming skills are not needed. It is sufficient to type formulas and text and the program evaluates which data is input and which is output.\nNemetschek Scia acknowledges that many engineering firms have already invested time and money developing Excel Macros so it allows Excel worksheets to be imported and converted into a basic design form. The company is also working on a community server where users can share their forms with others.\nMr Rammant believes the beauty of Scia Design Forms is the ease with which engineers can use them. \u201cS\/he just needs to type her\/his formula and link \u2018this, this and this\u2019 with her\/his model data and it\u2019s finished,\u201d he says. \u201cS\/he can call out his own design check and it will be automatically incorporated, or s\/he can stop and say \u2018I want to do it my way\u2019 and s\/he can check it step-by-step.\u201d\nThe engineering workflow\nNemetschek Scia has always acknowledged the importance of information exchange in BIM workflows. It was the first structural BIM tool to be awarded IFC 2\u00d73 V2 Certification and has strong direct links to Revit, Tekla Structures and other BIM applications.\nScia Design Forms is all about streamlining the flow of information between design and analysis, as Mr Rammant concludes. \u201cBIM, well it\u2019s modelling, but this is the last part of BIM \u2014 the \u2018I\u2019 is often forgotten. We have to take care of the information that is linked to the model. And that information for engineers is code design, it\u2019s checking, it\u2019s designing the actual details.\n\u201c[By using Design Forms] design iteration is quicker and more transparent, so we\u2019re closing the gap to, let\u2019s say, a more fluent and much better workflow.\u201d\n2013 Nemetschek Structural User Contest: Buildings category winner\nSICA \u2014 Museum of European and Mediterranean Civilizations (Marseille, France)\nThe MuCEM (Museum of European and Mediterranean Civilizations), the vision of architect Rudy Ricciotti, is located on the J4 jetty in Marseille.\nThe jury was fascinated by the innovative use of advanced ultra high performance fibre-reinforced concrete (UHPFRC) for nearly all the main construction elements including a 130m footbridge, which links the structure with the Saint Jean fort.\nScia Engineer was used for the 3D static and dynamic analysis of the MuCEM, plus the design of the tree-like columns, which took into account the real non-linear stress-strain behaviour of UHPFRC.\n2013 Nemetschek Structural User Contest: Prize for Fabrication and Execution\nAECOM \u2014 Serpentine Gallery Pavilion 2013 (London, UK)\nThe 2013 Serpentine Gallery Pavilion was designed by Japanese architect Sou Fujimoto. AECOM carried out the structural design from concept in January 2013 to completion June 2013.\nThe complex nature of the structure meant that a three dimensional analysis model was essential as the structure relies on all 27,000 members for global stability.\nFrom the outset of the project the design concept was conveyed using 3D models, as the structure has very little meaning when expressed as two dimensional sections.\nThe architectural scheme was modelled up using Rhino and bespoke scripts were used to transfer the geometry to Scia Engineer.\nFundamental to the success was the ability to make this a complete round trip process, allowing rapid design development with the architect and iteration of the design to a final solution, which embodied the architect\u2019s dream as well as functioning structurally.\nThe 3D model was also shared with the fabricator allowing integration with its computer aided manufacturing processes, as well as better visualisation of the structure and optimisation of the size of the fabrication modules for delivery to site and erection within the short construction period on site.\nStructural design drawings were produced in Autodesk Revit. The geometry was transferred to Revit using the Revit-Scia Engineer link.\n2013 Nemetschek Structural User Contest: Special \u201cOpen BIM\u201d Jury Prize winner\nGrontmij Nederland BV (NL) \u2014 New Energy Institute (Wuhan, China)\nThe main building in China\u2019s Wuhan New Energy Institute is inspired by nature: the calas lily flower, which symbolises purity,hope and greatness.\nThe organic shape of the building threw up some challenges though. Originally modelled by the designer and architect in SketchUp, the Scia Engineer model was then based on the SketchUp model and sent to Revit to create the final architectural model. This however resulted in some hiccups.\nGrontmij, a leading engineering consultancy, discovered that the hiccups were linked to the advanced way complex shapes can be modelled in Scia Engineer. To solve this, the company turned the process around by first building the model in Revit, based on the SketchUp model, and then sending it to Scia Engineer.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/workstations\/hp-z2-mini-and-sff-g5-workstations-get-graphics-boost\/","title":"HP Z2 Mini and SFF G5 workstations get graphics boost","date":1599436800000,"text":"With the Quadro RTX 3000 GPU, HP\u2019s updated tiny workstations can be used for entry-level VR and GPU\nHP has updated its entry-level desktop workstation line up with G5 editions of the Z2 Mini, Z2 Small Form Factor (SFF) and Z2 Tower.\nAll three models feature 10th Generation Intel Core CPUs with up to 10 cores, although the SFF and Tower models include the more powerful 125W models.\nThere\u2019s been a big boost in graphics in the Z2 Mini and Z2 SFF since the G4 versions: both machines offer up to the Quadro RTX 3000 (6GB), which is powerful enough for entry-level VR, real-time viz and GPU rendering.\nLike all RTX GPUs, it features dedicated RT cores for ray tracing and Tensor cores for AI denoising.\nIt\u2019s a big step up from the Quadro P1000, which was the top end option in the G4 editions and is only really suitable for 3D CAD.\nThe Quadro RTX 3000 in the Z2 Mini G5 and Z2 SFF G5 is not a standard desktop graphics card, rather a custom version of an MXM form factor GPU typically found in mobile workstations.\nWe expect it will offer slightly better performance than the equivalent GPU used in slimline mobile workstations, as they tend to be Max Q Design, so are clocked a touch lower.\nThe Z2 Tower G5 picks up where the G4 left off, continuing to offer up to the Quadro RTX 6000 (24GB), which can be used for very high-end workflows. The PSUs in all three machines have been bumped up to accommodate the higher spec chips.\nThe HP Z2 G5 family might be entry-level by positioning, but the specs are starting to resemble high-end machines of just a few years ago.\nThey\u2019re a whole lot cheaper, of course, but the machines are also significantly smaller. The Z2 SFF G5 and Z2 Tower G5 are 19- and 15 per cent smaller respectively than the previous gen, while the Z2 Mini G5 remains the same.\nThe HP Z2 Mini G5 has received some chassis improvements, with redesigned thermals that use copper rather than aluminium for \u2018much better thermal conductivity\u2019.\nOther features of the HP Z2 Mini G5 include nine different \u2018flex modules\u2019 to add more displays, mic or camera, and integrated Wi-Fi 6 for faster data transfer and extended range.\nThe Z2 SFF G5 has \u2018easy access\u2019 storage, so drives can be swapped out without having to open the chassis.\nHP has also updated its ZBook mobile workstation family with three new additions to complete a re-branding process that started earlier this year which saw HP move further away from numbers.\nThe ZBook Power, ZBook Fury 15 and ZBook Fury 17 now join the ZBook Create, ZBook Studio, ZBook Studio 360. ZBook x2 and ZBook FireFly for what is, without doubt, the largest range of mobile workstations from any single vendor.\nThe ZBook Power G7 is the follow on from the entry-level ZBook 15v G5 but is 19 per cent smaller and 9.4 per cent lighter.\nIn addition to new Intel Core-i9 and Intel Xeon CPUs, the main change is in graphics and the Nvidia Quadro T2000 GPU is a big step from the previous gen\u2019s Quadro P620.\nThe ZBook Power G7 also offers double the memory (64GB) and dual 2TB M.2 SSDs.\nThe ZBook Fury 15 G7 replaces the ZBook 15 G6 and is 12 per cent smaller. The big news is that the new machine supports the Quadro RTX 5000 GPU, which is far more powerful than the previous generation\u2019s Quadro RTX 3000.\nHP is playing a bit of catch up here as both Lenovo and Dell have offered the Quadro RTX 5000 in their 15-inch mobile workstations for some time (read our reviews of the Lenovo ThinkPad P53 and Dell Precision 7540).\nRegardless, this is big news for HP, as it means it can now offer customers a level of GPU performance that was previously only available in the 17-inch HP ZBook 17 G6.\nHP also has a new 17-inch mobile workstation, the ZBook Fury 17 G7, which is 29 per cent smaller than the HP ZBook 17 G6 it replaces. It\u2019s not as slender as the new Dell Precision 5750, but it does offer more expandability and faster graphics, with support for a range of GPUs up to the Quadro RTX 5000.\nBoth ZBook Fury laptops feature improved thermal solutions that \u2018enable full CPU and GPU performance\u2019 and draw air in from the bottom and out at the back through an \u2018unobstructed exhaust\u2019.\nHP also claims that battery life has been extended due to the re-engineering of the graphics controller and is now up to two and a half times longer than the previous generation.\nServiceability has also been improved with the removeable back panel providing access to all memory slots, all storage slots, as well as wireless LAN and wireless WAN. Finally, users now have the option of a 100 per cent DCI-P3 HDR 400, Pantone validated Dreamcolor display.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-graphisoft-goes-live-with-grasshopper-archicad-link\/","title":"NEWS: Graphisoft goes live with Grasshopper \u2013 ArchiCAD link","date":1461801600000,"text":"\u2018Live Connection\u2019 will link graphical generative algorithmic design to Building Information Modelling\nFollowing a 6-month public beta phase involving over a thousand architects and designers worldwide, Graphisoft has released its \u2018intelligent\u2019 Grasshopper\u2013ArchiCAD Live Connection. The bi-directional \u201clive\u201d link allows users of the Rhino-based generative algorithms design tool Grasshopper to connect to ArchiCAD with a view to filling a gap in the design process between early stage design and BIM.\nGraphisoft says \u2018Live Connection\u2019 offers tools beyond bi-directional geometry transfer, as it allows basic geometrical shapes to be translated into full BIM elements while maintaining algorithmic editing functionality.\n\u201cAlgorithmic design represents a global trend in architecture,\u201d says Robert McNeel, CEO, McNeel & Associates. \u201cUsing this new, real-time connection, architects can unleash their creativity and create forms and shapes that otherwise would be nearly impossible. Rhino and Grasshopper are the best tools on the market for these tasks.\u201d\n\u201cThe Grasshopper-ArchiCAD Live Connection offers a unique design workflow, helping to explore a large number of design variations, and create and fine-tune building details and structures using algorithms without exchanging files,\u201d said Peter Temesvari, director of product management, Graphisoft. \u201cThis connection offers users the best of both worlds, by pairing a leading BIM tool with the industry\u2019s leading algorithmic design tool. The result is a smooth, professional workflow.\u201d\nTo learn more about Grasshopper-ArchiCAD Live Connection read our preview.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/3d-laser-mapping-develops-laser-scanning-robot\/","title":"3D Laser Mapping develops Laser Scanning Robot","date":1178064000000,"text":"UK, 01 May 2007: 3D Laser Mapping has developed a laser-scanning robot to provide an electronic eye in dangerous and in-hospitable environments. Using laser scanning units combined with wireless communications and advanced robotic technology the 3D-R1 allows a 3D map of the surrounding area to be created. 3D-RI can be deployed in any environment where a risk to personnel may be present including earthquake zones, fire damaged buildings or hostage situations.\nThe 3D-RI remote operated survey vehicle was developed in partnership with Jobling Purser RSV LLP and was originally designed for use in underground mining operations to reduce the risk to the survey operator and improve operating efficiencies. It was developed from a prototype vehicle first designed by James Jobling-Purser as part of an undergraduate project at the Camborne School of Mines. By combining laser-scanning technology with the remotely operated survey vehicle many of the limitations of traditional surveying have been overcome.\n\u201cThe 3D-R1 is less than a metre square, is light portable and easy to use lending itself to a wide range of applications and above and beyond the mining environment,\u201d said Dr Graham Hunter, Managing Director of 3D Laser Mapping. \u201cBy building on the original design concept we have created a solution that will dramatically improve the safety of survey personnel, increase the range of the survey and improve the speed at which survey measurements are captured.\u201d\nThe 3D-R1 is designed to be compatible with the Riegl LMS-Z series of laser scanners. These units comprise of a high performance long-range 3D laser scanner, software and an integrated high-resolution digital camera. The laser transmits a light pulse which is reflected off a surface or feature and bounced back to a receiver. Using the time taken for each individual pulse to be returned and the known speed the system can automatically calculate the distance of the feature from the unit. From this data highly detailed and accurate 3D models can be produced.\n3D Laser Mapping is a laser scanning solutions provider. 3D Laser Mapping is integrating laser scanner hardware systems with their own software and peripherals to create better solutions for sectors such as mapping, mining and manufacturing.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/technology\/hypar-io\/","title":"Hypar.io - design by algorithms","date":1550707200000,"text":"With the advent of machine learning and Artificial Intelligence, computers are on the cusp of actually playing a more active role in assisting the building industry, especially helping the early conceptual development process. Martyn Day looks at an ingenious cloud-based service\nOne of the comedy moments one can always look forward to in films set in the Star Wars galaxy is the droid who feels predisposed to always calculate and verbalise the probabilities of success, just at the time the main characters are about to do something extremely risky, like navigate an asteroid field. While these doom-laden forecasts of failure are always dutifully ignored on film, I can\u2019t help but imagine how useful it would be to have computational assistance in complex decision making within a work environment. In building design, there might not be a princess to rescue, but when planning site development, building layout and complex ducting, it might be helpful to know that the placement of your exhaust ports has introduced a womp rat sized vulnerability.\nWhile it\u2019s still formative days for C3PO style real-time interjections, tools are starting to become available where multiple design iterations can be created, with each having met the original codified design criteria. Commonly called \u2018optioneering\u2019 it\u2019s a branch of computer generative design which has much to offer early phase \u201cwhat if\u2019 scenarios by testing to see what is possible given a set of variables.\nHypar drive\nStarted in 2018, Hypar.io is headed up by two industry veterans, Anthony Hauck, who was in charge of Revit product development and more recently headed the development of generative design at Autodesk, and Ian Keough who is probably better known as \u2018the father of Dynamo\u2019, the visual programming interface \u2013 also an ex-Autodesk employee. Both have played key roles in developing programmatic design tools which architects can use to help solve problems.\nAt Autodesk, Hauck and Keough were involved with the Labs development, Project Fractal, a tool which worked alongside Dynamo to generate multiple design options for customers\u2019 own design logic (now migrated to Autodesk Project Refinery).\nTypical uses for this were in space design and building layout, optimising room organisation and placement, creating multiple options, all meeting the core specification.\nThe seeds of Hypar seem to start here. As veterans, Hauck and Keough could see that many firms were developing their own software solutions to solve complex computational problems and there were few standard platforms, leading to a lot of reinvention. While the visual programming tools of Dynamo and Grasshopper have grown in usage, they do require base applications to run; Hypar is self-contained.\nHypar.io is a web-based cloud platform and API which executes users\u2019 code, in Python and C#, to quickly create tens, hundreds or thousands of designs based on design logic. These models can be previewed in 3D together with analytical data created with those designs on a desktop or mobile. The system is optimised to produce results quickly, within the snap of your fingers.\nWith Hypar they have produced a flexible platform, accessed from anywhere through the web. It offers a common user interface, which is free from the heavy data sets of today\u2019s BIM tools and operates with a data model that supports IFC definition, so is BIM platform independent. It\u2019s also a place where algorithms could be shared or sold by a community of tool developers and they are looking to harness shared libraries and the growing number of open source resources. So, together with Hypar\u2019s own suite of evolving tools, over time it will become a central resource for access to generative tools.\nWhile there is a lot of focus on programming and algorithm development, as Hypar fleshes out and more tools become available for it, or indeed a firm decides to create its own applications, it also provides an environment where non-programming designers can explore design variations, simply by clicking \u2018more\u2019. Hypar is aimed at both the tool designer and the tool user and the results can flow on to drive other processes through commonly used formats and scripts.\nThe team is currently exploring the Autodesk Forge Revit Design Automation API to connect Hypar to Revit for detail workflows. In the site design example given to AEC Magazine, Hypar can be used to select a building parcel from a map, a tower design is selected from a design iteration and the design for that parcel is generated in that area, to the given height, floors etc. This is then seamlessly imported into Revit, with all the floors and structural framing. Currently in prototype, this will be an exceptionally popular capability.\nDemos of Hypar.io in action can be seen on YouTube. Examples include exploring mixed use tower development, tower placement on a site, creation of various tower designs, floor plans, establishing shortest duct routing and retail store layouts.\nThe geometric results are fairly basic, but at a concept stage it\u2019s all about finding the best option and then developing that in more detail, while safe in the knowledge that the design meets the brief it has been tested against. As we were going to press, Hypar has a new office design layout in the works, together with starting new work on generative formwork from arbitrary walls.\nConclusion\nThe pricing model appears to be evolving, with the potential for tool developers to charge for the use of their algorithms, or a subscription for more complex users, with access to suites of tools.\nSo, it may be reassuring to know that we don\u2019t yet have droid architects who can replace you completely. It\u2019s also good that your BIM workstation isn\u2019t going to tell you the chances of your building meeting the client\u2019s expectations is approximately 3,720 to 1, as you are about to go into a meeting. But there are tools that are willing to help improve the quality of the design and take some of the grunt work out of exploring the multitude of options that fulfil the brief.\nTo participate in this brave new world, it does require knowledge of programming to create an algorithm and a mindset which can define the variables within the potential design. However, over time, the Hypar.io community will grow and there will be more algorithms contributed to the pool, so it could be a useful resource or firms could develop \u2018tool makers\u2019 to create programs to assist in the creation of project- specific optioneering Hypar tools.\nWhat may seem like quite a geeky and abstract development at the moment, it\u2019s really is an exciting environment for the current generation of programming-savvy designers to rapidly ideate with computer assistance. In Hypar space, these are the droids that you are looking for.\n\u25a0 hypar.io\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/structural-engineering\/improving-structural-workflows\/","title":"Improving structural workflows","date":1606089600000,"text":"Nemetschek Group, perhaps best known for its architectural BIM brands, is looking to leverage SCIA and Allplan to move away from manual processes and to drive integrated digital workflows for structural design, writes Martyn Day\nNemetschek occupies a unique position in the BIM market. It is an AEC-focussed holding company which owns no less than three global BIM brands: Vectorworks, Graphisoft and Allplan, as well as thirteen other industry-related companies which are driving the digitisation of many different aspects of the construction industry. With the launch of ArchiCAD 24 earlier this year, Graphisoft introduced a file-less and instant workflow between architectural BIM tool ArchiCAD and multi-material structural analysis software SCIA Engineer, as well as a new open format (structural analysis format \u2014 SAF). It was clear that Nemetschek was looking to extend its focus beyond architecture to improve digital workflows for other industry disciplines, as well as connecting up its own technologies.\nIn truth, Nemetschek through its Allplan brand, has always been in the construction side of BIM, but the company\u2019s past lack of integration between its brands and technology, has tended to keep Allplan to its core geographies, namely Germany, Austria and Switzerland (the DACH region). Now a broader vision is emerging from Nemetschek\u2019s Bavarian HQ which will enable technology from its brands to play better together and it\u2019s mainly thanks to the smart use of the cloud.\nWhile Nemetschek has not gone down the route of creating suites of products (like Autodesk Collections), it has opted to work on filling in the digital divide that exists between the various disciplines in federated projects and different applications. Much of this is driven through the company\u2019s adherence to IFC and OpenBIM and core benefits of cloudbased workflows. This approach helps it connect applications within its own ecosystem and ultimately plays well with the fact that, in the real world, applications used in any one project don\u2019t come from one single vendor.\nAEC Magazine caught up with Nigel Rees, business strategy manager at Allplan and Viktor Varkonyi chief division officer at Nemetschek Group to discuss Allplan\u2019s BIMplus platform, along with SCIA AutoConverter which can automatically convert any structural model from any CAD system into an analysis model in an OpenBIM workflow.\nNigel Rees: Before I started with Nemetschek, the way structural engineers work is a very manual processes, which potentially means data is lost along the way \u2014 or, because it\u2019s a manual process, you get people recreating what someone else has already modelled. Over the last 20 years, people have been trying to do this, but never really convinced the engineering profession that they needed to change, because there was always an element of manual rework. We recognised that there was an opportunity to do things better for engineering, in particular, the civil and structural engineering profession.\nSo how do you do that? Technically we\u2019re in a fairly fortunate time at the moment with cloud technology, with algorithms and cloud being more accepted in the industry. We thought, a very nice and very effective workflow for engineers would be if we could have all the data in one place. We can take it from anywhere, and we don\u2019t care where it comes from.\nBIMplus is a data environment which can deal with the federated model. We can pull data in from all the CAD and 3D modelling systems within the group (Nemetschek) or outside group \u2014 it doesn\u2019t matter where it comes from.\nOur job within BIMplus is to actually neutralise that \u2018file\u2019 problem and democratise that data. We can bring in a model, or part of a model from Allplan, Revit, or from ArchiCAD. From an IFC perspective, we join it all together in BIMplus, and it is representative of the 3D model in any one of those BIM programs. We then send it to AutoConverter.\nAutoConverter\nThe SCIA AutoConverter is a plug-in for BIMplus that can convert any imported structural model from any CAD system, into an analysis model without having to start the model from scratch, using a controlled, automated process that applies recognition algorithms to the data.\nThe combination offers integrated change management and revisioning. Adjustments made to the structural model are then automatically updated to the related analysis model without the loss of data, such as loads. This drastically reduces the time spent on rework.\nNigel Rees: In its simplest form, if you think of a column and a beam intersection, a [simple CAD] system doesn\u2019t know that it\u2019s a column or a beam, it just happens to be \u2018something vertical\u2019, \u2018something horizontal\u2019. [Using SCIA AutoConverter] we walk around the perimeter of that section \u2014 that could be an I-section, could be a square or whatever it may be \u2014 and the algorithm recognises it. The system then stores that information and associates that with a database of sections.\nAfter it has analysed the whole model, it goes from a state of dumb geometry to an engineering model and because it knows it\u2019s an I-section or a rectangular section, it can do the alignment. It then does another pass over the model and it looks at offsets to beams onto the base of the column, for example, or it may need to shift it up to the surface\/ floor plate.\nAt that point, the engineer can enter into a process that they\u2019re familiar with. They can either take control of that analytical model themselves and really drill down to where they\u2019re concerned about the master lines. Or they can click the button and have AutoConverter do it all for them. For simple to medium complexity structures, that\u2019s often good enough, but for more complex structures, it\u2019s important that the engineer can take control.\nThe important thing is that we don\u2019t just keep that in as data for our own tools, we allow that to go to any structural analysis tool. So yes, you can use SCIA and AutoConverter but it can also go to any other structural analysis system.\nAEC Magazine: How accurate is SCIA AutoConverter?\nViktor Varkonyi: I think it\u2019s a balance of the quality of data that is flowing in.\nNigel Rees: The quality of the model coming into BIMplus is critical. If it\u2019s a real jumbled-up mess coming in, then you would get the same issues using AutoConverter that you\u2019d have anywhere else. AutoConverter would do a lot of the cleanup for you in the first pass over the model, but every engineer needs to do a manual check on the data that they are sending through, such is the critical nature of that data. Downstream you always have to do an initial run on your analysis to check whether this is working. That\u2019s just due diligence on the engineering side. Actually, if you click a button on AutoConverter it will join everything together with hundred percent right, but the question always is \u2013 is that what the engineer wants? We introduced many controls into AutoConverter, because the engineers are not comfortable with just clicking a button because then it becomes a black box and we wanted to steer away from \u2018black box\u2019 solutions. It needs to be open, it needs to be configurable, and it needs to be editable for engineers.\nThe thing about passing information through the analysis tools, is that there are so many different types of analysis and use cases where a linear static model is, by definition, different to a dynamic model. You need different components acting in different ways, if you\u2019re going to do a dynamic model analysis. Effectively, you end up with two completely different models. But in terms of \u2018does a column connect up to a beam?\u2019 Yes, AutoConverter does that 100%.\nI think this is new to the engineering process. What people are realising at the moment is that they can actually create models a lot quicker now, than they were [previously] able to. Look at model creation, what would have typically taken two to three days in the past, now it can be done in minutes \u2013 literally ten to fifteen minutes to get that geometry pass through.\nWhat this enables the engineer to do is a lot more of the optioneering \u2014 what if we changed part of the model to steel, what if we did it in timber? And then presenting those options back to the client. Maybe it\u2019s still a linear workflow from that perspective, but the fact that they can bring different options and cost benefits to the client that they were too time constrained to do before [is a big benefit].\nFor this all to work, it\u2019s very important that you can actually do version control. Every iteration that comes into BIMplus has a version against it. One of the things early adopters liked about the version control is that we stored every iteration of that structural analysis in BIMplus, so clients can see what work has been done. Not only does it provide that speed, it actually allows engineers to get the validation process for the client.\nBIMplus and AutoConverter speeds up the process while minimising the errors. A traditional workflow may have someone building a model, and then you have the engineer reading that model or visually inspecting it and then recreating it in another tool. It may also then be the case that you need to go from one structural analysis tool to another to perform different types of analysis, e.g. linear static or dynamic analysis. With BIMplus and AutoConverter you can now push that from one centralised managed environment to different analysis tools with no data loss.\nSAF Files\nNigel Rees: The other part of the project was to develop a neutral file format which anyone can read. We came up with a concept we called SAF (structural analysis format), which actually writes data into an Excel format. The reason why we did that is because virtually every engineer is going to have a copy of Excel on their desk It can then be hooked up to products like Robot, Staad and any other analysis tool like Frilo from the Nemetschek Group as well.\nWhen we send the data out of this managed BIM environment, it exists in multiple forms, in the various structural analysis tools, but now we actually have a system that can bring back that data into the managed environment. So that\u2019s where we differ a little bit as we are sending data out, but actually bringing it back in and managing it again. It\u2019s managed, to unmanaged, and back into managed again. That gives us an edge, certainly in the structural engineering world because once it comes back into BIMplus, then you don\u2019t just have the geometry model anymore, you have multiple versions of the analysis model, sitting within that BIM environment.\nCloud strategy\nAEC Magazine: With BIMcloud for ArchiCAD, BIMPlus for construction, and BlueBeam Studio, is there a single cloud strategy for Nemetschek?\nViktor Varkonyi: Obviously, the Graphisoft ecosystem and the Allplan ecosystem are serving slightly different customer profiles and customer segments. Graphisoft has strengths early in the design process, in very iterative workfows, while the design is still very agile in the architectural processes. Allplan is a little bit later in the process. BIMplus is software agnostic and based on IFC open standards. We have a lot of projects where AutoCAD customers are sharing IFC files to BIMplus and then Allplan is using AutoConverter.\nThe Nemetschek strategy is that we are building these solutions together, but keep the innovation very close to the market with that brands having focus on certain markets.\nFor the Nemetschek Group, cloud is not \u2018the goal\u2019. I think companies that make going to the cloud a goal are too financially driven. For Nemetschek it\u2019s about the deployed cloud when it delivers some customer value. With BIMplus, being in the cloud, it\u2019s to liberate data access, in any form. That\u2019s a real and definite customer value. This is why BIMplus is cloud-based and based on open standards. Actually, I think this is one of the only platforms out there that internally uses open standards as the core data structure. So, we are not converting data. On top of that, we have a powerful API, again, liberating data access but, as Nigel mentioned, the complex functionality that SCIA AutoConverter is doing is also fully built on public API services. So, I think that gives you the impression of strengths that how powerful the system is.\nOn the analysis solution, the Nemetschek strategy for the cloud is that we have put a strong focus to build our data structures into the cloud, this is what BIMplus is doing in the open engineering environment. This is what Graphisoft is doing with BIMcloud, for the building design space, also what BlueBeam is doing with BlueBeam Studio but these are all related to the cloud but whether the SaaS solutions are thick clients, thin clients or a desktop product, it\u2019s a decision as to what best fits each customer.\nConclusion\nNemetschek is still dedicated to having its individual brands. However, as it seeks to expand and connect to generate workflow-based solutions, we can expect to see more data flows between them. Typically software firms that do this are looking for expansion into the federated construction market and don\u2019t necessarily play well with the competition. On this matter, it\u2019s good to see the Group\u2019s commitment to OpenBIM and IFC remains, recognising that in AEC ecosystems, the product mix is highly varied. For all BIM workflows, BIMplus and AutoConverter can help limit the amount of unnecessary rework and remodelling that happens between architects and structural engineers.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/cityengine-20103\/","title":"CityEngine 2010.3","date":1305504000000,"text":"A fast way to build a virtual city, CityEngine is an easy-to-use tool for concept design or modelling cities for in-context visualisation. Stephen Holmes takes a look.\nThe name gives the game away as CityEngine by Procedural is a very quick rules-based city creator with parametric building capabilities and the ability to import GIS and other real world data.\nFor those in search of a quick-fire method of creating a backdrop to architectural renderings this program is what they have been looking for, but be warned: what can be sufficiently and realistically set up in a handful of clicks might have you addicted to fine tweaking every little detail.\nCityEngine is a standalone product that has found its way from the entertainment and gaming effects industries to professional users in architecture, urban planning, GIS and other areas of 3D content production providing a modelling solution for the creation of 3D cities and buildings. It has already been taken up by Foster + Partners, Grimshaw, and RNL.\nCityEngine provides a useful visualisation tool throughout the design process. However, there are examples of it being used for creating buildings using rules-based methods.\nFor example, ETH Zurich, the Swiss Federal Institute for Technology, designed the plans for its Swiss Village within the Foster + Partners Masdar City project (a new sustainable city being built in the desert) using CityEngine.\nOnce the main volumetric language was found, the design was encoded as CGA Shape rules (a shape grammar for the procedural modelling of CG architecture that produces building shells with high visual quality and geometric detail) inside CityEngine and the buildings were automatically generated accordingly.\nBuilding your city\nIn its simplest form, most likely to be used by the majority of users, the modelling process starts by placing a road grid on an editable terrain that is then populated by buildings according to a set of parameters designated by the user.\nSweeping changes can be made in real-time to the city\u2019s buildings via a clever system of assigning demographic info to a colour. Parameters such as building heights, types (Victorian terracing, more modern semi-detached) or the land use-mix can all be controlled this way to show realistic \u2018neighbourhoods\u2019.\nFrom this, individual buildings can be picked out and manually adjusted for height, appearance and purpose; roads can be moved and dragged with real-time effects on the surroundings.\nThe results are quick and easy to achieve and the building design can be slotted into the new city quite happily and in a very short period of time. Having designed the building in a dedicated modelling program the CAD model is exported as an OBJ, Collada or FBX file and CityEngine takes it in as a 3D asset.\nWithin CityEngine this asset can then be used in a number of ways, for instance scale, rotate, split, or re-texture, although it will not automatically re-scale in real time like the \u2018active\u2019 streets and buildings built in CityEngine.\nCityEngine is a not a rendering tool, merely a modelling tool. The viewport provides a rendered scene, but it is only a preview. To have photorealistic renderings or animations of the city model, they need to be exported to a dedicated design viz tool. This can be done via the Autodesk friendly FBX file, or a Mental Ray MI file; imported into 3ds Max and merged with a 3D building model. Render and animate in 3ds Max.\nMore detail\nAt users\u2019 fingertips, however, is the power to do a great deal more, adding much more realism and authenticity to cities.\nInstead of beginning with a random territory and road map, real life geospatial data can be imported with accurate land mapping to provide the basis of the model.\nPre-modelled assets can be dropped in, such as key buildings\/landmarks or photographic flyovers.\nThe software supports industry-standard formats such as ESRI Shapefile or DXF that allow the import of any geospatial or vector data \u2014 whether it\u2019s building footprints with arbitrary attributes, or simple line data.\nBuilding rules can be dealt with through some tricky code or through a series of more simple visual \u2018chain links\u2019. Assets, such as building height and detail levels, can be altered by a simple slide-rule.\nBuilding rules can be defined using textures\/models in the node- or text-based rule editor. More repetitive or pipeline-specific tasks (instancing information for each building, for example) can be streamlined with the integrated Python scripting interface, although this is where the node-based scripting takes over and things get substantially more complex.\nThis is not to say that users are left unsupported; the software comes with a mammoth amount of tutorials, example cities and rules, as well as online support that would suit anyone with time to learn.\nIf you are setting out to design a bespoke city down to the minutest detail then, as you would expect, it is going to take an age in CityEngine. It is easy to get drawn into adjusting the individual building faades and creeping around every little detail.\nAt little over \u00a3300, the \u2018Indie\u2019 version offers a great tool at an extremely affordable price, although it excludes the Python coding implementation, which limits how much automation can be put into modelling, and support for exporting to RenderMan (.rib) or Mental Images\u2019 Mental Ray (.mi) format. Furthermore, simulation data such as flow fields, colour ramps, lanes, or sound agents cannot be exported to programs such as Massive Prime to add in autonomous crowds.\nAt ten times the price, the \u2018Pro\u2019 license has all the bells and whistles and, according to Procedural provides a very specific alternative to Autodesk\u2019s Project Galileo, which the company believes is not focused enough for building virtual cities.\nThe latest version is to be launched with some slight upgrades in the coming months, which will help remove some of the minor stability issues and coding nuances, while adding some more simplicity to the user interface.\nCityEngine is brilliant for creating a realistic city with a few clicks of a mouse, and can save a huge amount of time when creating a backdrop or cityscape.","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/sensoria-corporation-webraska-combine-technologies-to-offer-in-vehicle-wireless-navigation\/","title":"Sensoria Corporation & Webraska combine technologies to offer in-vehicle wireless navigation","date":1015372800000,"text":"Sensoria Corporation, an embedded server solutions provider for the automotive, defense and other industries; and Webraska, the provider of LBS and Telematics applications and enabling platforms, have announced that Webraska\u2019s award-winning Internet-based wireless navigation services have been integrated with the Sensoria Telematics Environment, an open platform for delivering next-generation telematics services to vehicles.\nSensoria will demonstrate the Webraska navigation services in a vehicle outfitted with the Sensoria Telematics Environment during the Society of Automotive Engineers Digital Car show March 4 to 7, 2002 at the Cobo Center in Detroit. Also being demonstrated are advanced diagnostic, multimedia and mobile content services including news, weather, sports and traffic reports.\nWebraska\u2019s IbDN\u00ae (Internet-based Distributed Navigation\u00ae) technology combines high performance server-based algorithms and architecture with the most recent wireless Internet device and cellular location technologies to offer the most comprehensive navigation solutions. Webraska\u2019s application software and enabling platforms allow telematics service providers, car manufacturers and their partners to provide the car driver with an always up-to-date, real-time navigation service optimized for the car environment.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/bim-show-live-reveals-keynote-speakers\/","title":"BIM Show Live reveals keynote speakers for Newcastle event","date":1578960000000,"text":"26-27 February event will feature architect Oliver Heath; Radio 4 presenter Timandra Harkness; and American BIM expert Chris Tisdel\nBIM Show Live has announced three of its four keynote speakers for its February event, which carries the theme of \u2018BIM for Good\u2019, touching on the topics of sustainability, ethics and putting people first.\nOliver Heath is an industry recognised expert and thought leader in biophilic design, sustainable architecture and interior design, striving to create human-centred spaces that are proven to benefit the health and wellbeing of building occupants. With a background in architecture, Oliver has spent 20 years working for the likes of BBC, ITV and Channel 4 promoting sustainability and wellbeing.\nTimandra Harkness is a writer, speaker and comedian who presents BBC Radio 4\u2019s series, FutureProofing. Her book, Big Data: Does Size Matter? was published by Bloomsbury Sigma in 2016 and in 2017 Timandra gave a TEDX talk titled What Is Knowledge In The Age Of Big Data? Timandra speaks with authority, intellectual rigour and humour to audiences about the future, Big Data, AI & robots, and other topics relating science and technology to the wider questions of human life.\nChris Tisdel is the Founder of Ruckus Innovation Consulting in America and has 25+ years of global experience as a designer, catalyst for innovation, strategic consultant, founder, and angel investor in design, engineering, and technology industries.\nOn top of the keynote speeches, BIM Show Live 2020 delegates will benefit from over 30 seminar sessions delivered by industry peers including Mikael Santrolli, BIM and Design Systems Coordinator at Foster and Partners; John Ford, BIM and Digital Delivery Lead at Galliford Try; and Hadeel Safaa Saadoon, BIM Manager at Coventry University. David Philp, Global BIM\/MIC Consultancy Director at AECOM and Simon Rawlinson, Partner and Head of Strategic Research and Insight at Arcadis, will chair the two-day conference.\nBIM Show Live will take place in Newcastle upon Tyne, 26-27 February 2020.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-university-2017-report-2\/","title":"Autodesk University 2017 report #2","date":1516752000000,"text":"At Autodesk University in December, the company had the opportunity to show off a range of rapidly evolving cloud services and expand on its vision for the future, as Martyn Day describes in the second part of his report from the event\nIn part #1 if our report on Autodesk University 2017 we focused on new CEO Andrew Anagnost\u2019s vision for the company, as well as the emerging Forge development platform, which is intended to transform the way that developers create add-ons for Autodesk products and that customers access services and data.\nWe also looked at some of the work done to flesh out Autodesk\u2019s cloud architecture and online services. While this multi-year process has sometimes seemed protracted, confusing and even incoherent, at this year\u2019s Autodesk University, it finally felt as though all the pieces were coming together, with the Autodesk BIM 360 platform looking impressive in demonstration.\nFor the AEC keynote at the event, Nicolas Mangon, vice president of AEC strategy and marketing, was first up on stage. He began by highlighting some of the technologies \u2013 among them geolocation, infinite computing, big data and remote sensors \u2013 that are allowing us to live our lives differently, always connected and more in control.\nAs he pointed out, these technologies have huge implications for the AEC market. They will increase automation, assist design and planning and enable digital fabrication and construction.\nBut while we may be in the midst of a technological revolution, automation is not a silver bullet. Today\u2019s federated AEC process is still fragmented and complex and software that deals with one issue at a time can\u2019t adequately solve all the multiple problems it is bound to encounter, due to the fragmented way that the industry works.\nA more holistic solution must be found and this is where BIM comes in. At the centre of the BIM process is data and, if you combine BIM data with the capabilities of cloud, everything gets pulled together. Mangon\u2019s mantra, then, is \u2018connected BIM\u2019 \u2013 and not just for digital aspects of projects, but also for the physical ones, too.\nMoving on to customer examples, Mangon highlighted the Grand Paris Express, encompassing 200Km of new railway and 68 train stations, with 90% of it underground, due to be completed by 2030. Mangon commented that this \u00a320 billion project is the first of its kind to use BIM from start to finish and has involved 36 architectural and civil firms developing horizontal and vertical models in Civil3D and Revit. The team has used the models to optimise designs, taking into account escape routes, the results of fire\/smoke analysis and the best locations for safety equipment.\nMeanwhile, Dubai\u2019s stunning Museum of the Future by BuroHappold Engineering, constructed by BAM, has been developed using end-to-end digital workflows. In particular, using Autodesk BIM 360 Field has helped to achieve a 65% reduction in rework. The building explores the future of science, technology and innovation and is intended to serve as a platform where the latest inventions and prototypes from up-and-coming start-ups and the world\u2019s technology giants can be demonstrated and tested.\nModern-day designs\nWhile the AEC process may be complex in itself, modern-day designs are increasing in complexity, too. Risk can be better managed through digital tools. Autodesk has been developing some powerful generative design tools, which combine machine learning (a branch of Artificial Intelligence or AI) with the power of the cloud, to optimise complex designs based on real-world design criteria.\nAugmenting human capabilities, computers can explore tens of thousands of design options and come up with a wide range solutions. As Mangon puts it, these kinds of capabilities mean that, \u201cWe can do more and get better results with less.\u201d A case in point is Dutch design build firm, Van Wijen, which has not only embraced modular, factory-based design and construction, but is also using generative design for land development work in urban neighbourhoods.\nBy defining backyard size, sunlight, views and optimum use of land parcels, the company has been able to increase the quality of its work and reduce time to market. All this is based on software running iterations to find the best designs for customers, while maximising the amount of housing on each plot.\nAnd in a case of \u2018eating its own dogfood\u2019, Autodesk used generative design to optimise the layout of its new offices in Toronto. First, the company got feedback from office workers as to how they wanted to work. Then, it fed a range of competing criteria into the system \u2013 variables such as daylight, buzz, productivity, views, workstyle, acoustics and proximity to bathrooms\/kitchens. The system then generated a range of floorplan layouts that met the criteria.\nGenerative design is also finding uses in civil engineering. Complex grading can be rapidly assessed by applying geometric constraints, giving immediate feedback regarding volume of material to be moved and cost, for example. This enables \u2018optioneering\u2019 and potentially means that projects have a less negative impact on the environment, while offering the best return on investment. And all this can be achieved before construction even starts\nOn-site safety, too, is becoming increasingly important and with Internet of Things (IOT) advances, construction sites generate more useful data than ever before. One of Autodesk\u2019s hot developers, Smartvid.io, uses AI to scan video and images of construction sites in order to locate workers, materials and equipment. It\u2019s now possible to connect Smartvid.io\u2019s intelligent solution to BIM 360 Field and BIM 360 Docs, where photographs and videos taken on site can be analysed to locate materials, flag missing safety equipment, identify people and point out potential hazards in less than 90 seconds. Autodesk has made a strategic investment in this developer and so we can expect to see more AI capabilities online soon.\nIoT solutions for construction are growing, in fact \u2013 especially in wearable sensors for workforce safety. These sensors provide real-time information on workers involved in slips, trips or falls, alerting site managers as to the \u2018who, when and where\u2019 of a specific incident. If a site needs to be evacuated, all workers can be alerted simultaneously and tracked in real time. Autodesk believes that the Revit model in the cloud can act as a model connected to IoT sensors. Mangon also suggested that Navisworks might be used for visualising IoT data in real time or for looking at historical reports.\nFor Mangon, all these capabilities naturally feed off Autodesk\u2019s cloud solution and common data platform, BIM 360, which provides connected ways of working, upstream and downstream in AEC, from design to fabrication and construction and on to operations and maintenance.\nBIM 360 developments\nSarah Hodges, Autodesk director for BIM 360 presented on the developments around this product, clearly the subject of a big product push at this year\u2019s University. Using an example of the KPF-designed China Zun Tower in Beijing, Hodges recounted how designers and construction teams used BIM 360 to create the 524-metre monster skyscraper in China\u2019s capital. The building has one of the largest concrete- filled cores ever, which required 256 trucks and 96 hours to pour. The building was so complex, in fact, that many parts were fabricated and assembled offsite. Using BIM 360 to coordinate the construction, the firms involved racked up 80% fewer changes on the project, identified 5,000 issues prior to construction, and avoided 6,000 issues on site.\nOne of the key inhibitors to collaboration is that, due to the way teams work and software has been developed, teams stay disconnected and data lives in silos. Software developers are now trying to break down these barriers by producing common data environments (CDEs), which are basically databases for 2D and 3D models and documents that provide a single source of truth. Strategy firm Boston Consulting Group has identified that, if the AEC industry were to adopt full-scale digitisation in non-residential construction, it could reduce annual global costs of engineering and construction by between 13% to 21%, amounting to a $1.2 trillion saving in a $10 trillion industry.\nLast year, Autodesk introduced BIM 360 Docs to add to the other BIM 360 services Glue, Field and Ops. All these applications, while using the cloud, were not initially connected and in many ways didn\u2019t really deliver on the vision of a holistic solution. This year, Autodesk has made a considerable number of improvements to the individual components, but more importantly, has connected everything up to create a seamless experience, a true \u2018one-stop shop\u2019.\nHodges described the advances made as so significant that they represent \u201csecond- generation BIM 360\u201d. Data no longer needs to be uploaded into different applications and BIM 360 shares information across the whole project.\nThe new platform of BIM 360 builds on the base of project data, with Forge and Analytic layers, and then offers tools for Design, Pre-construction, Execution, Handover and Operation. Recognising that there are many types of applications used in a project, Autodesk has launched a new Connect and Construct Exchange, a one-stop shop where customers can find integrations with apps that work with hosted data within BIM 360 from partners and authorised Autodesk developers.\nAssemble Systems was highlighted as one of these developers: its tools now work with data hosted within the BIM 360 environment. These enable construction professionals to condition, query and connect their data to key workflows. including bid management, estimating, project management, scheduling, site management and finance.\nIn the next few months, Autodesk plans to bring out a beta version of a cost management system from BIM 360 and is prepping its machine learning development, Project IQ, to move into the mainstream. Project IQ has analysed 225 projects and over 30 million issues that have gone wrong in real building projects. This analysis has helped it to learn from mistakes made and better able to predict outcomes. Construction managers get alerted when Project IQ identifies potential problems, enabling workflow changes to be made that mitigate risks and preempt issues before they arise.\nCollaboration for Revit is extremely popular, being used in 144 countries housing 100,000 models and sharing 1.6 million Revit updates a month. Hodges highlighted how this is set to get a refresh with an ability to handle all project data with no upload or download and no more PDFs. It will enable isolated views in real time, Mechanical, Electrical, Architectural and in a centralised model in the cloud. There will be developments in package transmittals from individuals and teams, all to the common data environment.\nTo make this happen, Civil3D, Infraworks, Plant 3D and AutoCAD will all get collaboration plug-ins to be included in the project data. Hodges described this as true multi-discipline BIM.\nAutodesk\u2019s initial development of BIM 360 felt very \u2018bitty\u2019, but every project has to start somewhere. And today, BIM 360 looks and feels much more like the backbone for collaboration originally promised. Autodesk finally recognises that Revit isn\u2019t the only data generator in town. Civil engineering has been welcomed to the party. It\u2019s now possible to view all model data geolocated with intelligence in one environment which, since it\u2019s on the cloud, means it\u2019s everywhere. The ability to connect to third-party cloud-based services will become increasingly significant.\nConclusion\nIt has been a long time coming, but Autodesk\u2019s promise of what the cloud could do is being delivered in BIM 360. The AEC industry faces extreme challenges, not only from its federated composition and complex processes, but also from delivering high-risk, complicated assets in a world of low margins.\nWe don\u2019t have to look too far right now to see what can happen when things go wrong onsite, and how quickly project overruns can kill huge beasts. Last year\u2019s average profit on construction projects in the UK was -0.5%. If that were not bad enough, issues on site can lead to penalties in their millions.\nWhen cloud first became a topic, we were all more focused on desktop applications going online. The real benefits, it turns out, lie in connecting everyone, maintaining one version of the truth, reducing duplication and errors and delivering on-demand experiences. The next phase will come as third-party cloud-based applications and machine learning tools integrate, analyse and drive through the benefits of being able to process big data.\nIn the next article we will look at Autodesk\u2019s predictions for the industry of the future, as BIM drives digital fabrication, mass customisation and modular design.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/mapmart-introduces-on-demand-geospatial-imagery-service-solution\/","title":"MapMart introduces On-Demand Geospatial Imagery Service solution","date":1190332800000,"text":"Denver, USA, September 20, 2007: The MapMart division of IntraSearch, Inc. introduced MapMart On-Demand, an enterprise solution to facilitate the storage, retrieval, and automatic ordering of geospatial raster data. As an ESRI Business Partner, MapMart has been selected to be the provider of bundled solutions using ESRI\u2019s ArcGIS Image Server. MapMart On-Demand provides a single online interface where users across the enterprise have instant access to their organization\u2019s own geospatial raster data as well as the entire library of worldwide MapMart data.\nPowered by the new ArcGIS Image Server from ESRI, Inc., MapMart On-Demand is ideal for organizations that don\u2019t want to dedicate expensive internal IT resources to manage geospatial imagery. MapMart On-Demand hosts, organizes, and maintains the organization\u2019s data on an internet-accessible, fully protected server. ArcGIS Image Server provides standards-based search and retrieval functions so that end users can find and download the data they need from within their GIS and CAD applications via the Web.\nTerabytes of digital mapping data from these MapMart libraries are available through MapMart On-Demand, including:\nMapMart On-Demand Ordering System offers users a robust online system designed to boost the efficiency of purchases and reduce redundant acquisitions. From the map interface, the user can search for available datasets covering the area of interest and have the imagery loaded into their Image Server. If the desired area of interest is not covered by current aerial photography, MapMart On-Demand allows the user to place a custom order for new imagery right online. The system enables anyone in the organization to track the collection and fulfillment process online.\n\u201cWe created MapMart On-Demand to allow organizations to get the most value from their new and existing geospatial raster data,\u201d said MapMart President, Mike Platt. \u201cMapMart On-Demand will become the cornerstone of enterprise GIS strategies for many organizations.\u201d\n\u201cThe value of MapMart On-Demand is that users can search their own internal archives for just the right dataset. If they don\u2019t find it there, they can see if the map data is offered by a commercial source through MapMart. And if they still can\u2019t find it, they can place an order for a custom acquisition \u2013 all from a single online interface on their desktop,\u201d said Platt.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-martyn-day-aec-magazine-nxt-bld\/","title":"Video: NXT BLD 2019 \u2013 Martyn Day, AEC Magazine & NXT BLD","date":1565049600000,"text":"The future of digital fabrication \u2013 NXT BLD London, June 2019\nIntroduction to NXT BLD and AEC Magazine\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/indoor-reality-introduces-geo-tagging-for-indoor-assets\/","title":"Indoor Reality introduces geo-tagging for indoor assets","date":1551657600000,"text":"New capability allows users to add latitude longitude tags to assets, in GPS denied indoor environments\nIndoor Reality\u2019s 3D mobile mapping solution can now automatically assign a latitude longitude tag to an indoor asset, a capability that traditionally has been available only to outdoor assets where GPS is readily available.\n\u201cThe new feature takes asset tagging one step further by providing a unified location identifier for safety, security, energy, and IoT assets, not only inside one building, but also across multiple buildings across the globe,\u201d said founder and CEO, Dr. Avideh Zakhor. \u201cThis in turn can be used by asset and facilities management solutions, and building automation systems to visualize, control, and manage assets seamlessly across multiple buildings.\n\u201cThe location meta data is also critical for configuring building automation control and integrating sensor networks. It allows analytics applications to exploit spatial relationship between sensors and devices inside a building, to provide more comfort to the occupants, and to improve building energy efficiency. Furthermore, the geo-tagging information can be integrated with popular GIS software, unleashing the power of GIS software to building interiors.\u201d\nThis feature will become available for current users of Indoor Reality products as of Q2 2019.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/cad\/ramboll-develops-generative-design-tool-for-site-appraisal\/","title":"Ramboll develops generative design tool for site appraisal","date":1592870400000,"text":"SiteSolve can help developers understand the potential of sites or help architects carry out early phase design work\nSiteSolve is a new generative design platform from engineering and architecture consultancy Ramboll for early stage residential massing \u2018optioneering\u2019. The software can help developers understand the potential of sites or help architects carry out early phase design work.\nSiteSolve can be used to explore new ideas through the automatic generation of hundreds or thousands of random massing arrangements, or to create and tweak designs with the key area data instantly updated. The software also has engineering insight embedded, so teams can progress the design with more confidence that fire, vertical transport and M&E requirements have been considered.\nOther features include: quick capacity studies, the ability to define apartment mixes, data to inform cost, optimised apartment packing, and core analysis.\nGrant Briggs, an architect in Ramboll Norway and early adopter of SiteSolve, has been using the software to quickly evaluate development potential and to visually explore how the massing of varied heights and volumes would look on the site.\n\u201cSiteSolve adds value for clients as we can generate and evaluate many more solutions in a given time than previously possible,\u201d he stated in a recent interview. \u201cIn theory, the greater the number of options we consider, the closer we should come to an optimal solution. It also lets us operate with key information about areas, heights and volumes at an earlier stage and [to] have this information available for the client earlier.\u201d\nSiteSolve was also used to help the Housing Strategy Team at Transport for London (TfL) understand the potential number of homes that could be released for development in key locations in London through the investment of infrastructure routes.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/wooden-it-be-nice\/","title":"Wooden it be nice","date":1193011200000,"text":"The use of Robobat FE software at Structural Engineers evolve has enabled optimised design of laminated wood structures, writes Nick Lerner.\nThe aesthetic beauty of laminated wood is only part of its appeal. As the sole renewable construction material wood reduces building owners\u00dd carbon footprints with the added bonus of enhanced design options. Because of its almost endless design possibilities an FE based 3D frame analysis programme is a must.\nOne Structural engineering consultancy that is pioneering the use of laminated wood in mainstream and alternative construction is Evolve of London and Glasgow.\nThe company has been using Robot Millennium FE software from Robobat for six years and has recently been working on new-build retail stores and innovative structures in timber around the UK. Mark Sinclair, Associate at evolve, described how Robot Millennium is used in this application. \u00fdOur work in this sector has been in conjunction with Tesco and other major retailers that are introducing measures to reduce their carbon footprint in new buildings by specifying laminated timber rather than steel in the building frame. Robot allows design and analysis of 3D frames to be created that incorporate different materials i.e. steel & timber and accommodates their properties. This optimises the use of materials and allows the substitution of steel members within the standard 18.0m x 12.0m grid for timber with the minimum of effort. Aside from the structural engineering data that the software produces, one of the significant advantages of using Robot has been the output of material quantity and specification report schedules that we pass on to our suppliers and others in the supply chain. These reports which would be time consuming to produce manually are generated automatically by the software as the design develops allowing us and other stakeholders to understand the cost and quantity implications of design variations at an early stage.\u00af\n{mospagebreak}\nUsing Robot Millennium, evolve designed the timber frame for Tesco\u00dds eco-store in Wick, Scotland. This was the UK\u00dds first sustainable supermarket, following this they worked on the second in Shrewsbury, England, and are now designing other timber-framed projects around the country.\nBranching out\nMark Sinclair continued, \u00fdWe use Robot Millennium to calculate stability analysis for vertical and horizontal bracing, applying wind loads and other project specific loadings to the main structure and its component details. Having calculated the deflection of long span elements and designed their fixings, the automated quantity report scheduling is a real time saver that allows us to spend our time very productively. The software allows us to optimise designs speedily and produce solutions that use optimal amounts of material \u2013 no more and no less. Wood gives us many more fixing possibilities than steel and we have developed systems for temporary and removable fixing to make the construction process far simpler. For example, we can fix frame elements temporarily and put them in position before permanent fixings are used. Robot is used to calculate the extent to which this methodology is practicable, and also allows us to look at alternative design options within this technique.\u00af\nThe ability to optimise a design and its construction methodology often leads to more elegant solutions with greater visual appeal to shoppers as well as a positive impact on the owners\u00dd carbon footprint.\n{mospagebreak}\nLeaves the past behind\nIt is likely that as more companies realise the environmental benefits of laminated wood in their buildings the market will adapt to meet demand. evolve plans to capitalise on this trend and has been working to secure its already strong position by exploring the potential of optimising designs with Robot Millennium.\nMark Sinclair explained, \u00fdWhile the design of a supermarket to a pre-existing grid plan presents several technical design and methodology challenges, the creative design element is relatively straightforward. This is not the case with other projects where innovative design is at a premium and new ways must be found to ensure structural integrity.\n\u00fdBecause laminated wood is easy to shape and bespoke cross-sections are easy to fabricate, the achievable design possibilities are endless. One of the advantages of designing for this material using Robot is knowing, in real-time, whether a design will work in practice while having a fully reported understanding of the cost implications of design modifications. \u00fdWe can import 3D models into Robot from other CAD systems, such as such as AutoCAD, Digital Project, or MicroStation, or produce designs using the system\u00dds own modeller. Either way, we are assured that the software verifies our design options and choices for final construction in the knowledge that they cannot be bettered from a technical standpoint.\u00af\nSeeds of change\nA recent project for FinnforestMerk, designed in conjunction with Alsop Architects, involved evolve producing finished designs for an exhibition stand using LVL Timber Wall. Taking an original Alsop design and adapting it for construction without any loss of design intent was crucial for this industry showpiece. It was important to retain the dramatic double waveform while ensuring full structural integrity and material optimisation. Since the project was produced to the tightest of deadlines evolve took a copy of Robot software on site to the exhibition and ran Frame Analysis calculations on a laptop computer as the waves were built. Throughout this process new calculations were made and appropriate modifications developed for the construction.\nevolve has certainly not left steel & concrete behind and uses Robot Millennium every day in its structural design work for a range of projects. The company has recently worked on steel designs for Gatwick Airport and T5, Tottenham Hale Retail Park and concrete structures at Thomas Cubitt\u00dds former works, Grovesnor Dock in Chelsea. \u00fdBut\u00af says Mark Sinclair \u00fd While traditional steel and concrete construction forms the majority of our work, the use of laminated wood is fast growing, not just in retail, but also in other commercial and domestic construction. Laminated wood offers so many design options that it effectively gives a new palette to designers. As seen in our supermarket projects it makes a very interesting alternative to steel and in other applications the design freedom and restraints that it presents are very exciting. There is no stopping a fashion once it gets started and laminated wood is very much in vogue right now.\n\u00fdSince a great deal of the appeal of laminated wood is the reduction of carbon usage that it offers, it makes sense to optimise its design and so generate further savings through efficient use of resources. Our use of Robobat\u00dds Robot Millennium in the design process allows us to reduce material quantities by optimisation, develop new construction techniques that save both labour and energy, and keep track of costs throughout. When these factors are added to the ease of producing verified designs that are right first time, the appeal of using Robot Millennium, as we do, is significant.\u00af","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/digitalglobe-unveils-plans-for-new-spacecraft-constellation\/","title":"DigitalGlobe unveils plans for new spacecraft constellation","date":1080172800000,"text":"DigitalGlobe unveiled details of the company\u2019s next-generation imaging satellite, WorldView. The new satellite, set to launch no later than 2006, will be the world\u2019s highest resolution commercial imaging satellite with better agility, accuracy and collection capacity than any other known commercial system. The WorldView imaging system will allow DigitalGlobe to substantially expand its imagery product offerings to both commercial and government customers worldwide.\nOnce launched, WorldView will be the world\u2019s only half-meter resolution commercial imaging satellite, capable of collecting images with 50-centimeter panchromatic resolution and 2.0-meter multispectral resolution. Added spectral diversity will provide the ability to perform precise change detection and mapping. WorldView will incorporate the industry standard four multispectral bands (red, blue, green, near-infrared) and will also include four new bands (coastal, yellow, red edge, and near-infrared 2).\nWhen combined with DigitalGlobe\u2019s existing QuickBird satellite, the company\u2019s imaging constellation will be capable of collecting more than 4.5 times the imagery of any current commercial imaging system. By late 2006, WorldView alone will be capable of collecting up to 500,000 square kilometers (200,000 sq. mi.) per day of half-meter imagery. The satellite will also be equipped with state-of-the-art geo-location accuracy capability and will exhibit stunning agility with rapid targeting and efficient in-track stereo collection.\nWorldView\u2019s higher orbit of nearly 800 kilometers will allow the satellite to revisit collection areas more frequently, letting customers repeat their image acquisitions about once a day. Other impressive capabilities of the WorldView system include more efficient image processing systems and multi-satellite collection planning, shorter tasking timelines, and an expanded network of remote ground terminals.\nDigitalGlobe\u2019s roster of WorldView engineering and co-production partners includes:\n\u00b7 AERO-METRIC \u2013 Co-Production and Data Provider\n\u00b7 BAE SYSTEMS \u2013 Production Segment Integrator and Co-Production Lead\n\u00b7 Ball Aerospace & Technologies Corp. \u2013 Spacecraft Developer and Integrator\n\u00b7 Boeing Launch Services \u2013 Launch Services Provider\n\u00b7 Boeing Space and Intelligence Systems \u2013 Engineering Service Provider and Co-Production\n\u00b7 EarthData International \u2013 Co-Production and Data Provider\n\u00b7 Environmental Systems Research Institute, Inc. \u2013 Co-Production\n\u00b7 Harris Corporation \u2013 Satellite Command and Control System, Production Segment Developer, and Co-Production\n\u00b7 IBM \u2013 Computing Hardware, Engineering Services, and ERP System Integrator\n\u00b7 InSequence \u2013 Systems Engineering Services\n\u00b7 IONIC \u2013 GIS Software and Engineering Support\n\u00b7 MacDonald, Dettwiler and Associates Ltd. \u2013 Production Segment Developer\n\u00b7 NASA Jet Propulsion Lab \u2013 Orbit Determination Tools and Support\n\u00b7 Observera \u2013 Planning Services for Calibrations\n\u00b7 PRA \u2013 Spectral Algorithms and Engineering Services\n\u00b7 RT Logic \u2013 Satellite Interfaces and Control\n\u00b7 SAP \u2013 Collaborative Business Solutions Provider\n\u00b7 ViaSat \u2013 Ground Antennas","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/maporama-launches-maporama-customer-service-solutions\/","title":"Maporama Launches Maporama Customer Service Solutions","date":1018483200000,"text":"Maporama, a provider of online location-centric solutions, announces today the launch of Maporama Customer Service Solutions, the third module of its product suite Maporama Enterprise Solutions. This new module is specifically designed for Customer Service managers in the retail industry.\nMaporama Customer Solutions provides a means for the customer department of an organisation to utilize location-centric information to improve customer satisfaction. Location-centric information is crucial in the following processes:\n- How to assist customers to find the information they need about a company\u2019s key locations\n- How to improve delivery and after sales service\n- How to improve the efficiency of a company\u00b9s call centre and decrease its cost\nBy exploiting the full potential of Maporama\u2019s location-intelligent engines, Maporama Customer Service Solutions offer extremely powerful and flexible tools that are easy to integrate into an organisation\u2019s infrastructure. These tools can be integrated indifferently within any existing communication system, including Web, Wireless, Intranet and Extranet.\nHelping customers find companies\u2019 after sales locations\nIncorporating a location-finder facility into an organisation\u00b9s Internet sites provide customers with details about company key locations and the delivery process. This significantly simplifies the communication between the customer service team and the customers, thereby allotting more time and resources for non-location and non-delivery related customer queries.\nAiding companies to provide real-time delivery status information\nIn order to dramatically improve the quality of after-sales services, Maporama Customer Service Solutions enable a customer support team to follow delivery processes in real time. They are thus able to provide quick and precise phone answers to customer delivery queries.\nSaving call-centre resources through speech automation\nMaporama Customer Service Solutions also utilize the most commonly used communication interface: speech. Maporama Vocal Solutions enable end-customers to access location information through voice-driven applications. Over 30% of customer inquiries fielded by call-centre staff involve location (Where is the nearest store\/after-sales outlet? How do I get there?) With Maporama Vocal Solutions, a call-centre obtains an efficient means to save precious time in speech-automating all location-based queries from customers.\nTechnology built upon industry standard for perfect compatibility and high performance\nMaporama Customer Service Solutions are provided as ASP (Application Service Provider) products and rely upon the same technologies as all Maporama\u00b9s products Active Server Pages, XML\/XSL and Maporama\u00b9s GIS technology interfaced through MAPIL (Maporama Application Programming Interface Library), a development tool allowing for custom application development using Maporama\u00b9s location-centric functionalities. Maporama Customer Service Solutions are compatible with all communication platforms (Web, Wireless, PDA, set-top boxes) as well as with any mobile or landline telephone device via voice technology.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/printing-a-digital-twin\/","title":"Printing a digital twin","date":1459382400000,"text":"Architects are taking advantage of readily available 3D CAD data and 3D printing architectural models with impressive results. By Randall S. Newton\nWhen 3D architectural design software was in its early adoption phase, vendors would extol its benefits for conceptual innovation and creating a more evolved, well-thought -out final product. As adoption rose, socalled downstream uses became more apparent, in engineering, energy analysis, creating construction documentation, and in marketing. Now architects are taking advantage of the 3D data to create 3D printed architectural models, with impressive results.\nCreating architectural models to convey ideas is as old as architecture itself. Model-makers are artisans who give physical presence to drawings or digital 3D models.\n3D printing changes the process from artisan interpretation to printing a digital twin as detailed as the electronic original. 3D printers can print the smallest features, smaller than a human hair. The digital 3D model can be printed in full colour, translating material data for realism, or with an inexpensive monochrome material for conceptual or massing studies.\nCurved rooflines, staircases, and round walls are no more difficult to print than simple orthogonal designs. Project size is not a problem; 3D models can be divided in the computer to be printed in sections and assembled, making it possible to create physical models of any realistic size and scale.\nFrom Utah to the world\nWhiteclouds of Ogden, Utah, USA has become a global leader in providing 3D printed architectural models. Experienced Internet entrepreneur Jerry Ropelato decided in 2013 to start a 3D printing service bureau because it combined his two passions for technology and \u201cbuilding things.\u201d He thought at first his clients would only be manufacturers, but almost immediately got assignments from builders and architects. Today 1\/3 of Whiteclouds\u2019 business is in architecture. The company has become one of the world\u2019s largest fullcolour 3D printing service bureaus, with 30 full-colour 3D printers in a 60,000 sq. ft. (5,574 m2) facility.\nA dedicated in-house design team has all the leading 3D architectural software products on hand, and can also create 3D printing model data from 2D drawings. More than half the company\u2019s AEC business is from builders, not architectural firms. \u201cBuilders are driving [3D printing] more than the architects,\u201d says Ropelato. \u201cPrices are coming down, and the old models are expensive. We have good discussions about finished buildings and the design process.\u201d\nOne client is a large scale builder of homes and commercial building, with thousands of projects per year. Mr Ropelato says this builder now requires architects to submit their plans \u201c3D print ready\u201d. The models are used for internal meetings; when the project is finished the 3D model becomes a gift to the new owner.\nEven though 3D printing is an automated process, artisanal skill is still required to create the finished model. Sometimes detailed elements do not scale well, such as church spires, banisters and deck railings. These elements will be printed separately out of scale and fitted into the final model. Very few of the 3D models sent to Whiteclouds are 100% ready for 3D printing. \u201cWe might work for an hour, or a week, to get a model ready for printing,\u201d says Mr Ropelato. \u201cIt is more about the user than the software, but no product is to the point of simple click-and-print.\u201d\nIt is getting easier for the detail work in D models to be printed. Architects are becoming interested in printing brickwork, roofing, or other elements in realistic detail. \u201cIf there is a granite boulder in the front lawn, we make the boulder look like granite,\u201d says Mr Ropelato.\nSome architects are now requesting models at multiple stages in the design process, often monochrome early and then with colour and full detail toward the end. Interior designers are also ordering 3D printed models. \u201cWe show rooms with paintings on the wall, designs in the rug, or wood flooring. Unbelievable detail is now possible.\u201d\nThe value of a digital twin printed in 3D sometimes comes up in unexpected ways. Whiteclouds once sent a printed 3D model of a $5 million home to the architect. They got a call complaining about the back of the house lacking some windows and walls.\nThe Whiteclouds design team then went back to the digital model and realised it lacked these details as well. \u201cThe cost of the model more than paid for the discovery of the mistake that would have come if the plans went to the builder,\u201d says Mr Ropelato. \u201cSometimes seeing that physical model makes all the difference.\u201d\nThe studio approach\nLondon-based Hobs Studio is part of Hobs Reprographics, in service to the UK architectural industry since 1969. The Studio evolved as the company realised other services besides 2D printing were necessary. Today Hobs Studio offers 3D printing, 3D laser scanning, high-resolution visualisations and animations, BIM support services, and even creation of augmented reality and virtual reality visualisations with offices in London, Manchester, Glasgow and Bristol.\nHobs has found it can create highly detailed, accurate, and cost-effective physical architectural models in 1\/4 of the time it would take using traditional methods. Depending on the job, it uses 3D colour jet for high-resolution, full colour projects; 3D multi-jet for small plastic parts requiring toughness; and 3D stereolithography, the original 3D printing technology, for larger models and early concept architectural design.\nIn 2012 Hobs had one 3D printer, tucked away in a back corner of the studio. \u201cI would fix the files, then print them, take them out, deliver them, then run out and get more work,\u201d says Michelle Greeff, studio director at Hobs. Today the Studio has multiple 3D printers, for a wide variety of work. Greeff says construction and civil engineering clients often ask for colour prints of pipe systems, electrical lines, and even underground train lines. The use of colour in such complex models provides clarity unmatchable by monochrome.\n\u201cArchitectural models are the most difficult,\u201d says Ms Greeff. Large projects must sometimes be shrunk down 1000:1; it becomes a challenge to keep all the details. Some projects require more digital set-up work than others. \u201cThe remodelling of [London] Victoria Underground Station took an entire week to fix.\u201d\nHobs Studio is only too glad to repair 3D files for 3D printing, but also supports clients by teaching them to do the work. \u201cWe teach people how to make good files,\u201d says Ms Greeff. \u201cWe\u2019ve learnt so much through the years, we know what the problems are, so we can throw in a bit extra TLC and help with the file side.\u201d\nThis article is part of an AEC Magazine Special Report into the Future of Building Design, which takes a holistic view of the technologies and processes, which are set to change and enhance the AEC industry in the coming years \u2014 from concept design all the way to construction.\nClick to read the other articles that make up the report.\n1) Introduction New technologies are empowering architectural firms to improve quality, capabilities and process.\n2) Conceptual design There are a whole host of digital tools for early stage design experimentation.\n3) Rapid site design The rapid capture of site topology is being aided by new technologies.\n4) Benefits of 3D design Evolution, not revolution when making the move to 3D CAD.\n5) Moving to model-based design How to get from 2D to 3D, how to roll out training and how to overcome common issues encountered along the way.\n6) Design viz Advanced new rendering technologies are opening the door to design realism in architectural workflow.\n7) Design, analysis and optimisation Once you have a 3D CAD model, optimse your design for daylighting, energy performance and much more.\n8) Collaboration and model checking How to share models with clients, contractors and construction firms and test the quality of your model.\n9) Workstations What to look out for when choosing a workstation for 3D CAD.\n10) Virtual Reality New technologies are now available to support powerful new design workflows.\n11) 3D printing Architects are 3D printing architectural models with impressive results.\n12) Fabrication As building time gets compressed what will revolutionise fabrication and construction time?\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/double-vision-bim-collaboration\/","title":"Double vision","date":1526428800000,"text":"The collaborative efforts of a UK real estate advisor and a Parisian BIM supplier developed a digital twin for Aurora, a 4,830 sqm office building in Ealing for one London client, the Moorfield Group\nFor many high-value, commercial property redevelopments, the client\u2019s expectations are to have the chance to visualise and \u2018experience\u2019 the asset before work has even begun onsite. These often take the form of beautiful and captivating \u2018artist\u2019s impressions\u2019 or \u2018fly-throughs\u2019 \u2013 which no doubt catch the eye, but are often lacking accuracy or specific details.\nConversely, the client also needs the hard facts \u2013 room & facility specifications & net floor areas to allow for occupancy calculations, fire procedures, rent calculations and an endless array of other requirements. These often take the form of rather less glamorous 2D drawings or spreadsheets.\nBut what if the client could have both, all in one model? That was the desire of London-based commercial property advisor GVA, as Rupert Parker, Head of Futureproofing, explains, \u201cIn the burgeoning London commercial property market, our clients are increasingly demanding technology-driven results. Gone are the days when a 2D floorplan and a few site photos would suffice. On multimillion pound property developments, our clients need to know how the end deliverable will look, feel and operate. Thus, we are increasingly turning to technology such as VR, AR and BIM to deliver these results.\u201d\nA French affair\nA chance encounter in the summer of 2017 with Jo\u00ebl Lucas of Paris-based BIM Strategies at the TechXLR8 event in London introduced Rupert to a brand-new form of building documentation. BIM Strategies has been utilising mobile mapping technology alongside digital BIM since 2014. The company was involved in a competitive tender commissioned by Paris City Council in 2016 to compare data collection methodologies in a rather unusual location \u2013 the Parisian sewer network. The task was to quickly and accurately collect 3D geospatial data along a 1.2km section of the network, and, compare the results to an asset information system from the 1990s.\nAfter three days of intense testing in the complex, cramped environment, the handheld ZEB-REVO scanner from GeoSLAM was considered the best tool for the job. The 3D data from the sewers was used as the source material for BIM Strategies to build a fully-interactive, \u2018digital twin\u2019 of the sewer network \u2013 providing invaluable asset information to the city authorities for future-proofing the ageing infrastructure.\nRupert immediately saw the potential for this technology in the commercial real estate industry and invited Jo\u00ebl and his team to become \u2018Entrepreneurs in Residence\u2019 (EiR) at GVA, as Rupert explains, \u201cThe aim of the EiR programme was for both parties to benefit from each other\u2019s expertise and to seek new business opportunities. Having worked alongside each other for a few weeks, the potential uses of what Jo\u00ebl was creating started to flood in.\u201d\nA capital idea\nThe team didn\u2019t have long to wait to test one of these potential uses on a live site. An existing client of GVA, the Moorfield Group, had recently finished the extensive contemporary refurbishment of an office building in Ealing, west London, and were looking to market the property.\nThe five-storey Aurora building was considered a prime candidate for scanning and creating a digital twin, as refurbishment had just been completed and the property was vacant. The team got to work in the autumn of 2017.\nFirstly, the building was surveyed with the ZEB-REVO scanner. Collecting over 40,000 data points per second and with a simple \u2018walk and scan\u2019 methodology, the almost 5,000m2 property was scanned in less than two hours. It took another two hours to collect the thousands of digital images which were desired to add texture and context to compliment the 3D data. In less than \u00bd a day on site, all the required information had been obtained.\nThe digitisation of real estate\nThe geospatial data was fed into Autodesk Maya to build the 3D model \u2013 and was then \u2018textured\u2019 in Unreal Engine \u2013 an application more familiar to computer games designers than BIM modellers. The result was an interactive, first-person experience of the asset which could be viewed as a computer animation to prospective tenants \u2013 no matter where they were in the world.\nBut it was also so much more. As well as showcasing a modern property in an engaging and memorable way, the digital twin also holds a wealth of data on the built asset, as Jo\u00ebl Lucas explains, \u201cNow we have a first-person walkthrough experience, and the centimetre-grade accurate survey data in one model. Furthermore, we have enabled the future addition of other practical functionalities \u2013 including internet of things (IoT), CCTV, sensors, emergency escape plans and so on. For me, this is the new BIM \u2013 all information, all held in one easy to access place.\u201d\nRupert Parker agrees that the power of this offering lies in the wealth of opportunities that it presents, in order to meet the diverse requirements of his client base, \u201cThe model is a blank canvas which can be seen as the basis for digitisation of real estate. Working together, BIM Strategies and GVA can develop the model for any clients\u2019 desired use. I am most excited by the prospect of visualising the twin in a \u2018live\u2019 capacity through monitoring of human movement and vitals \u2013 creating an element of understanding of human interaction with environments never before seen in real estate.\u201d\nThe success of the pilot scheme has garnered a lot of interest in the London property market \u2013 especially the ability to create glamorous and lifelike walk-throughs in as-yet unrenovated properties. The successful collaboration, a result of a chance encounter, seems likely to flourish \u2013 and the team are not afraid to think big. \u201cThe augmented digital twin is the first step,\u201d said Jo\u00ebl Lucas. \u201cFuture offerings will include services for entertainment, marketing, building management, asset management, safety & security \u2013 there is no limit!\u201d\n\u25a0 geoslam.com \u25a0 bim-strategies.com\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/vancouver-gets-fully-loaded\/","title":"Vancouver gets fully loaded","date":1264377600000,"text":"Hosting multiple events simultaneously the Vancouver Convention Centre has uniquely complex delivery needs. But with the help of AutoTurn the transportation consultants on the flagship project were able to design the basement loading court with confidence.\nThe six-acre living roof and the harbour view with the mountain backdrop are perhaps the most talked about features of the recently opened Vancouver Convention Centre West facility. The new building truly is a masterpiece of design, inspiration and sustainability. The commitment to green technology is everywhere, from the landscaped living roof, seawater heating and cooling, on-site water treatment and even the fish habitat built into the foundation. Deep down this expansion facility is more than stunning structure, it is a building that really works.\nThe iconic sails of the Vancouver Convention Centre have been a striking feature of Vancouver\u2019s Downtown waterfront since Expo 86. As Canada Place, a name it is still known by, it served as Canada\u2019s Pavilion and entertained the world. However, the name Canada Place was not only applied to the part of the complex with the sails, but it describes the entire mixed-use commercial facility which is home to Vancouver Convention Centre East, the Pan Pacific Hotel, Port Metro Vancouver Corporate Offices, the Cruise Ship Terminal, the World Trade Centre Office Complex and an expansive parking facility.\nThe sails are still there, but after five years of planning, building, and rooftop landscaping, the new Convention Centre West building, adjacent to Canada Place, has tripled the size of the convention facilities. It now covers 1.1 million square feet (about four city blocks) for a combined total of 473,523 square feet of pre-function, meeting, exhibition and ballroom space. The Vancouver Convention Centre hosts multiple events simultaneously, each with their own separate access and function spaces. Because of this capability, and that the building is partially built over Burrard Inlet, the facility has to handle uniquely complex delivery needs.\nAccess all areas\nAlthough it is a side that the public seldom sees, it was the basement loading court that presented some of the greatest access design challenges. Typical of high density urban locations, goods loading all happens below street level and under structure. Having loading areas under street level contains the noise and minimises the visual intrusion of large delivery vehicles. However, it also presents unique manoeuvring challenges for those mega trucks within building confines.\nTo accommodate the needs of the new facility, 24 loading bays, 18 of which had to accommodate up to a WB-20 design vehicle \u2014 a 22.7 metre long tractor semitrailer \u2014 was required. To further complicate matters, three ramps were needed to connect the loading court area up to the exhibit hall level, providing large truck access directly into the exhibit hall areas within the building.\nFigure 1 shows the swept path for a WB-20 manoeuvring within the confines of the columns and loading ramps in part of the Convention Centre loading court. This illustrates how the angled bays were aligned within the 13.6 metre column spacing and how trucks still have space to U-turn.\nDesign automation\nBefore the days of computerised swept path design, designing this part of the facility would have been a slow iterative process using turning templates. With Transoft Solutions\u2019 AutoTurn, vehicle swept path analysis and turn software, it is possible to replicate standard or custom vehicle types. This includes the ability to simulate realistic turning manoeuvres with front and rear steered vehicles.\n\u201cEfficient movement of large trucks through the loading court facility is essential to the successful move-in\/move-out of large exhibitions and with the assistance of AutoTurn we were confident that we had a good design,\u201d says Peter Joyce, president of Bunt & Associates Engineering, transportation consultant to the Vancouver Convention Centre Expansion Project.\nEven the most challenging vehicle access projects, like the Convention Centre, can be easily navigated in AutoTurn. The ability to perform intricate vehicle movements from a wide array vehicle types reliably delivers the ideal layout. And not just for loading bay design \u2014 intersections, roundabouts, bus terminals all benefit from this application.\nCool runnings\nThe West Building of the Vancouver Convention Centre was opened in 2009 to critical acclaim. In February it will face once of its biggest challenges as it hosts the international media and broadcast centre for the 2010 Winter Olympics. Even with the complex logistics of such an event the organisers will be confident they will be able to deal any access clearance and manoeuvrability challenges that come their way.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/the-quest-for-continuous-construction-verification\/","title":"The quest for continuous construction verification","date":1574380800000,"text":"Topcon\u2019s new GTL-1000 is a compact scanner and fully featured robotic total station in one, enabling a site engineer to complete a layout and scan on a single set-up. Greg Corke caught up with Topcon VP Ian Stilgoe to learn how the device is helping change construction verification.\nWe all know that when construction errors are picked up late, they can prove extremely costly. Topcon estimates that 5% to 12% of a construction project budget is consumed by mistakes and rework. But the industry continues to be heavily reliant on manual checking.\nThe industry is starting to get some excellent construction verification tools which automatically compare point clouds against design and fabrication models, but getting timely laser scan data from site is still proving a major challenge. This is partly down to technology and partly down to skillsets, as Ian Stilgoe, VP Geopositioning Europe at Topcon Positioning Group explains, \u201cThe site engineer\u2019s on site looking after the whole project and knows everything about what\u2019s going on. The scanning team come in, do what they need to do, then go away again. A week later or two weeks later they get the data set. There was a disjoint between what the decision process is on site and this verification. And it tended to become an as built.\n\u201cYou know, verification is a new word around what we\u2019ve been doing for years, called as-built. But as-built was like, you know, \u2018when we\u2019re finished you can do your as-built\u2019.\u201d\nThe problem with a traditional as-built is it essentially becomes a report at the end of a project and even if mistakes are identified at this late stage they may already have become expensive mistakes.\nStilgoe explains that a site engineer typically uses a robotic total station every hour of every day, but tends to leave laser scanning to specialists. But it shouldn\u2019t be a specialist task \u2013 it should just be part of the process, he says.\nThis was the driving force behind the development of Topcon\u2019s GTL-1000, a combined instrument with a laser scanner that sits on top of a robotic total station. It\u2019s a no compromise instrument, says Stilgoe, \u201ca high-speed scanner that is just as good as any other scanner on the market, without any disadvantage of the robot.\n\u201cIt means that if you\u2019re doing layout, which you do every day, you can scan at any time,\u201d he says. \u201cYou\u2019re not thinking about it differently. You\u2019re not going back to the office to get a different product. You\u2019re not going back to the office to get a different guy to do it. And it means that verification becomes much more continuous.\u201d\nData acquisition is only the first stage. In Topcon\u2019s workflow the point cloud data is then processed using MAGNET Collage, before being taken into construction verification software ClearEdge3D Verity (see AEC Magazine\u2019s in-depth review here). The software analyses and compares what has actually been built against the design\/fab model, automatically flagging any out-of-tolerance or inaccurately constructed work.\nVerity works best with steel work and other vertical type construction. Stilgoe recalls one of the pilot projects for the GTL-1000, a renovation of a stately home near Heathrow, where only the fascia was left intact and the rest was gutted. \u201cThe apartments were so expensive that almost every millimetre mattered,\u201d he explains. \u201cInside the building it was all concrete and steel and blockwork. They were literally scanning every time they put a layer of block down, making sure they didn\u2019t lose any volume.\u201d\nVerity is not as effective on horizontal construction projects, such as asphalt and bridges, says Stilgoe, but this is something that is being explored in the DCW Labs.\nIn civils projects, drones are used increasingly for reality capture, using Bentley Systems ContextCapture to turn a collection of photos into a reality mesh. But drones have their limitations, as Stilgoe explains, \u201cIt\u2019s okay for your earthworks, but when you start to get concrete is it accurate enough? Get into anything with a roof, much more complicated \u2013 even the sides of the building is not so easy with drones. But use the drones for the bigger picture, because you do need it,\u201d he says.\nBut this might change. \u201cThere\u2019ll be a combination of drones and scanning in the future when these drones are, say small enough, safe enough and autonomous enough, which they\u2019re not yet indoors.\u201d\nAutomation and the use of Artificial Intelligence (AI) is also coming to reality capture. Stilgoe admits that Topcon is working with Boston Dynamics to use its Spot Mini robot. This is under NDA, so he can\u2019t share details, but later on in our interview he does raise the important point that you don\u2019t necessarily have to scan the entire site as the processing can become too technical.\n\u201cWe\u2019re more interested in recognising what\u2019s there and intelligently measuring what you want to measure rather than measure everything and try and extract,\u201d he says.\n\u201cYou can use visual recognition and then decide what\u2019s got to be measured. Your AI becomes a camera and camera sensors are very cheap, scanners are very expensive, and it just means your deployment can be easier on verification processes.\u201d\nThis is one for the future but, for now Topcon, is laser focused on replacing robotic total stations on site. \u201cSo, most construction sites will have five to ten robots, you know, depends on the size of project,\u201d says Stilgoe. \u201cOur goal is to have a minimum one of these GTLs replacing a robot, and eventually, why wouldn\u2019t you just replace all the robots?\n\u201cBecause if you don\u2019t need specialist surveyors to do the scanning part, you should do the scanning whenever you need it, not whenever the subcontractor or the surveyor or the even in-house team is available.\n\u201cSo that\u2019s certainly the goal, to keep the [GTL-1000] factory busy,\u201d he smiles.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/accurate-dynamic-3d-city-models-streamed-live\/","title":"Accurate, dynamic 3D city models streamed live","date":1559088000000,"text":"Aerial mapping company Bluesky is working with SkylineGlobe UK on a project that will make high accuracy, photorealistic 3D City Models available as off the shelf datasets and via an internet streaming service. The dynamic 3D models will provide a visualisation and analytical platform for smart city projects, transport \/ infrastructure planning, development planning and more.\nTo create these \u2018unique\u2019 3D digital twins, Bluesky is using its hybrid oblique imaging and LiDAR airborne sensor to capture the city imagery. Skyline is then processing and publishing the data into 3D digital twins using its 3D earth visualisation software and services.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/welcome-to-the-future\/","title":"Welcome to the future","date":1550620800000,"text":"To celebrate the 100th edition of AEC Magazine Randall S. Newton explores what Architecture, Engineering and Construction technology might look like in 2036 when the 200th edition rolls off the virtual press\nThe year 2002 was an interesting one in AEC. In 2002 Autodesk surprised many in the architecture technology community by acquiring startup Revit Technology Corporation for $133 million. In 2002 a patent was issued for building a database of infrastructure assets using lidar. In 2002 vendors of wide-format printers generally rented the largest booths at AEC technology trade shows. In 2002 Bentley Systems was in the initial stages of introducing MicroStation V8. In 2002 the mention of \u201cRobot\u201d in AEC circles was about a brand of structural analysis software. In 2002 this magazine published its first issue.\nSo much new technology has been introduced in the 17 years since AEC Magazine Issue No. 1 rolled off the press. There were no smartphones, no tablets \u2026 mobile workstations in 2002 were bulky, and nothing like the lightweight laptops we have today. PDF was one of several competing standards for publishing AEC documents. Such technologies as 3D scanning, virtual reality, and decent photorealistic visualisation were years away. Many new technologies have been adopted by AEC, yet in many ways AEC has not changed as much as we might have predicted in 2002. Draughting is still more common than 3D modelling; printed documentation is still the norm; construction waste is still a major cost; production and construction workflows have not drastically changed.\nTo celebrate AEC Magazine Issue No. 100, we have asked industry technologists to cast their vision forward to 2036 \u2014 well beyond any vendor\u2019s official roadmap \u2014 to when AEC Magazine Issue No. 200 will come out.\nUnderstanding the drivers of change\n\u201cFrom a construction standpoint, 18 years is a relatively short time,\u201d says Nathan King, Research Strategist at the Autodesk Build Space in Boston. \u201cIn the past 18 years there has been progress in construction, but not a lot.\u201d\nKing believes it is important to examine the \u201cdrivers and demands\u201d in order to understand what technology innovations will impact AEC. Demand for built infrastructure will increase in places which are the least developed today. It will be challenging to build in these areas, King says. \u201cHow do we realise the demand?\nThere will be shortages of materials and workers. Such a confluence of challenges presents an opportunity to accelerate adoption of technology in construction.\u201d A \u201ccompression of adoption\u201d mentality is required if AEC is to meet the challenges, King says. There will be a sharp increase in the use of off-site manufactured building components. \u201cThe trend will be toward more complexity in the factory to reduce time on site,\u201d King says. Combining factory production with automation on site \u201csolves issues like staging and sequential construction.\u201d\nSome of the required changes are not technological but social, King says. \u201cOne challenge is the segregation of the construction industry around trades and the ownership of responsibility. New technology allows us to break down these barriers.\u201d There are \u201chundreds\u201d of separate industrial processes in AEC, King notes; \u201cwe need to combine them around a common goal and a common platform to desegregate a segregated industry.\u201d\nRobotics will play a role in 2036, but more likely as exoskeletal devices to increase worker abilities, King says. \u201cWe will see a massive reduction of injuries onsite, due to better control, better tracking, and safer working methods. The trend is clear toward more robotics on site.\u201d\nAEC in the gig economy\nThe biggest technological changes in AEC by 2036 won\u2019t be about specific applications like 3D printing or drone scanning, says Thomas Wendling of Jacobs Engineering. Instead, the most radical changes will be in business models. \u201cLook at the platform revolution already taking shape around us,\u201d Wendling says, citing a book of the same name. Companies like eBay. Amazon, Uber, and Airbnb \u201chave learned to meet supply with demand resources external to them.\u201d Companies will create software platforms offering two-sided markets for AEC services \u201cfor exchanging engineering knowledge in technical engineering memorandum format.\u201d Such a service would allow his employer, for example, to reach out to external \u201cand generally underutilised\u201d talent.\nWendling says an AEC productivity platform could transform the creation, peer review, and approval of calculations, standard operating procedures, tender requests, and other elements of AEC design \/ build practice. Such micro contributions are too tedious to manage using existing practices, but could be automated using elements from social media and blockchain. \u201cIf you harness the distributed sources of knowledge, you gain. Today a 60 percent talent utilisation is considered standard. We think we can do better.\u201d\nWendling leads the Blockchain Community of Practice at Jacobs, one of many internal think tanks inside the company. As part of an engineering services platform, blockchain would have several roles, which he describes as layers of management. \u201cOne layer would be generating tokens for credentials, reputation scores, quality scores, and curation,\u201d says Wendling. \u201cWe believe blockchain can be the organisational structure for how knowledge is articulated.\u201d\nWendling sees a second layer of BIM on a blockchain as the autonomous manager of the virtual job site, linked as a digital twin to the physical reality. \u201cEveryone enters and leaves the virtual job site, and it is all recorded on the blockchain\u201d during the design and engineering phases. Wendling says a wider circle of participants will interact with the developing digital model through virtual reality as the physical project takes shape. A third layer would issue and manage smart contracts for construction management. \u201cThe BIM layer would connect to this third layer through augmented reality at the actual job site,\u201d says Wendling.\nDigital Twins and smart goggles\nResearchers at Bentley Systems are taking a look at how two of the technologies Wendling described \u2013 digital twins and augmented reality \u2013 will impact AEC. Adam Klatzkin imagines the day when \u201cthe combination of artificial intelligence and machine learning with digital twins will play a significant role in how physical assets are designed, built, and operated.\u201d\nAs Bentley\u2019s senior director for Infrastructure Digital Twins, Klatzkin says these technologies will provide the basis for profound advances in AEC practice. \u201cThese approaches are only as good as the data available for analysis,\u201d he says. \u201cWith the increasing maturation of infrastructure digital twins and the aggregation of their data sets, future AEC professionals will derive real-time feedback and predictive insights such as cost, safety, and performance based on entire portfolios of assets \u2013 or even the digital twin of an entire city, state, or nation.\u201d\n\u201cBy accurately displaying CAD models and metadata in context, both indoors or outdoors, smart glasses will augment the built environment right before your eyes,\u201d says St\u00e9phane C\u00f4t\u00e9, a Bentley research scientist. By 2036 such augmentation will include \u201cIoT live data such as pressure and temperature, AI-powered analytics, hidden asset locations, maintenance history and schedules, assembly procedures, and hazardous areas perimeters.\u201d Users of this smart technology will be able to make changes to designs on site using hand gestures, and get help from intelligent virtual assistants. \u201cColleagues will virtually teleport near you to help solve issues related to all aspects of the project.\u201d\nReal-time data for safety & efficiency\n\u201cWe see a continued march toward safety and efficiency on site,\u201d says Andy Evans, the product manager for mass data capture at Topcon Positioning. \u201cWe expect automated construction sites will be mainstream by 2036.\u201d\nRobots will play a major role in construction site automation, Evans believes. \u201cRobots will be reducing the risk to life on site by removing the need for bodies near machinery. Constant positioning and verification will mean projects are completed within the swiftest of timescales.\u201d\nData will be gathered from every direction and used immediately, Evans says. \u201cThe Internet of Things will provide us with a truly connected construction site \u2013 from planning to long term management. Embedded sensors and software to monitor use of infrastructure assets will feed into a digital twin of a project. Ultimately, smart cities will become a reality as we embrace this type of technology. Data across an entire urban environment can be used to inform projects.\u201d\nAI at the job site and in the building\nArtificial intelligence (AI), 5G cellular data transmission, and edge computing are buzzword technologies today that will have a huge impact on AEC by 2036, according to Hexagon Chief Technology Officer Claudio Simao. \u201cProcess optimisation is the holy grail,\u201d notes Simao. With its roots in metrology for manufacturing and site data for construction, Hexagon is focusing its research on using AI to simplify the complex processes required to use sensor data in real time. The goal is \u201chyper-connectivity\u201d that starts with data gathered on-site. Edge computing \u2013 a distributed computing vision that decentralises data processing to smart devices in the field \u2013 will be where AI routines start to make sense of the local sensor data. High bandwidth in the field will be ubiquitous thanks to 5G cellular technology (or its replacement by 2036) allowing distributed use of the data for many purposes in real time. \u201cEdge computing goes straight to distribution of applications,\u201d says Simao.\n\u201cAI-driven analytics and positioning modelling\u201d will allow multiple innovations to flourish on the job site, from self-driving equipment to site-aware robotic welders and fastening specialists who can adjust their work without direct human input using constantly updated site data. These devices will have embedded AI technology so the computation driving the devices can be done internally, not at a centralised computer. AI-driven analytics are the goal, but Simao says the bridge from here to there is not clear due to a lack of AI and Deep Learning experts today. \u201cWe suffer a lot from lack of data talent,\u201d Simao says, speaking of the industry in general. \u201cThere is a lack of knowledge to apply the correct questions.\u201d Today most algorithms and frameworks for Deep Learning \u2013 the ability for an AI program to study a data set and make operational decisions \u2013 are open source and widely shared because there are so few developers capable of writing custom deep learning routines. Simao says open algorithms are good for getting started, but vendors will need custom AI algorithms they can fine-tune for the application. \u201cAn AI model tuned to a specific application must have specific domain knowledge. When we productise, we need full control of the code.\u201d\nCedric Desbordes of CAD developer Graebert GmbH says the use of AI means design and construction documentation software will be much smarter in 2036. Any future CAD software will not replace the need for documenting projects in precise, readable detail. \u201cIt is about workflows that will allow users to collaborate with BIM users in a productivity-driven workflow.\u201d By 2036 such workflows will be assisted to a great degree by AI processing. Desbordes envisions a CAD tool smart enough to analyse a picture of a bridge and provide searchable attributes or offer design suggestions based both on aesthetics and structural analysis.\nToday there is much discussion about making the building information model \u2013 \u201cThe BIM\u201d \u2013 the central organising technology for the AEC process. Richard Harpham of Katerra is a BIM technology pioneer who thinks the future won\u2019t be nearly so model-centric. \u201cToday the tools make the individual more productive, but we don\u2019t have increased coordination for collaboration.\u201d Harpham was an early employee at Revit, before and after the Autodesk acquisition. \u201cI think the industry will move beyond model centricity. The idea that the model is the centre of all design and construction processes is not sustainable.\u201d\nHarpham foresees a \u201cunified data platform\u201d which all participants access in a manner similar to using a browser to access the Internet. \u201cAs you browse you gain information and you add information.\u201d AEC data becomes an operational platform where nothing is lost or set aside over time. \u201cInformation coordination needs to be more like a snowball.\u201d Harpham says. \u201cBIM only works at the early design stage. The inevitability of drones and robots to deliver a building won\u2019t happen unless the data foundation becomes more reliable.\u201d\nNew value propositions\nBy 2036 economic value measurement systems will be based on \u201cformerly intangible values such as collaboration, social, creative, and intellectual capital of people,\u201d says Daniel Robles, CEO and founder of the Integrated Engineering Blockchain Consortium. For AEC, this means a new way to describe the value of infrastructure by measuring the social value and on-going utility of an asset.\nRobles framed his initial thought as if looking back from the future to 2036. \u201cIt turns out these measurements provided a superior representation of human productivity. Public infrastructure became the intrinsic value upon which public productivity was underwritten.\u201d\nReturning to speaking in the present moment, Robles says this vision starts today with the creation of a \u201cdecentralised knowledge inventory\u201d for engineering professions. Such an inventory will allow engineers \u201cto shift from a collection of centralised silos into a self-aware global network to adjudicate physical risk in real world systems. \u2026 The design IS the smart contract.\u201d This is accomplished through an engineering-specific blockchain which includes \u201cmulti-agent game mechanisms to configure human ingenuity\u201d into an intrinsic asset. The result will be a \u201cprioritisation of infrastructure projects for sustainability rather than overt consumption of the Earth\u2019s natural resources.\u201d\nJournalist becomes the prognosticator\nIn 2018 I found myself on the speaking circuit explaining the potential of blockchain for industrial processes including construction. I needed new phrases and concepts to describe where I thought the technology was headed. One was \u201cagora assets,\u201d which I define as \u201cintelligent autonomous buildings, infrastructure, or machines which are situationally aware, interactive, and trustworthy.\u201d\nBy 2036 such agora assets will routinely interact with humans and each other in ways not possible today. Bandwidth speed and latency issues will be eliminated, making asset-to-asset and asset-to-office communications into real-time transactions.\nLight poles will double as giant umbrellas, opening and closing on instructions from a nearby tall building in charge of local weather response services. Self-driving taxis (or dump trucks, bulldozers, etc.) will be owned by \u201cshareholders\u201d who bought a sliver of ownership using blockchain tech but who may never see or use the vehicle, the way individuals and institutions own stocks today. Maintenance, repair, and towing will be contracted directly by the vehicle acting as an autonomous agent for its ownership. Buildings will be able to see, hear, and certify a record of local events. If a self-driving car has an accident in front of an agora building, both the auto and the building can be a witness of record for legal purposes.\nBy 2036 we will be in the Autonomy Economy era. Perhaps not by 2036, but soon after there will be autonomous tendering. Instead of searching first for real estate, then an architect, etc., the owner\/operator will post a Request for Building using a decentralised app connected to a few or a few thousand autonomous construction consortia. Bids will be generated automatically, offering a complete package of land and building. No humans will be directly involved until the potential owner and the winning consortium agree on terms.\nThe Autonomy Economy will rely on a wide cross section of maturing technologies in five areas: Intelligence, Real-time, Operational, Autonomous, and Distributed (iROAD). The seed technologies for all five elements are in play today, but are years removed from working as a synergistic unit.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/reality-capture-modelling\/atkins-launches-cloud-based-digital-twin-survey-platform\/","title":"Atkins launches cloud-based digital twin survey platform","date":1592870400000,"text":"Cirrus Insite handles data collected from drones and laser scanners. Future AI capabilities could include defect detection and automated mapping\nAtkins has launched a new cloud-based digital twin survey platform, Cirrus Insite, which enables clients to access, analyse and further develop ultra-high-resolution 2D and 3D digital models of their assets.\nThe \u2018digital twin\u2019 can be measured, marked and annotated by multiple users, with a view to facilitating improved collaboration between clients and stakeholders. The platform enables users to connect data from external sources, to ensure the digital twin remains a \u2018true and connected replica\u2019 of their asset.\nAccording to Atkins, the platform tackles the sheer size of data collected from drones and laser scans; a common barrier to the adoption of modern surveying techniques.\nBrian Gibbs, product director at Atkins, said: \u201cCirrus Insite is an invaluable tool in democratising access to high-quality data of the built and natural environment. One area we\u2019re already exploring is the addition of AI for tasks such as defect detection on assets and automated mapping of sites; helping ensure that the data arising from modern mass data capture techniques is efficiently and economically analysed and that insights are brought to bear at pace.\u201d\nThe platform was built using the Gemini Principles, an approach to information management created by the Centre for Digital Built Britain to facilitate the ease of data sharing across government, industry and academia.\nCIRRUS Insite joins Atkins\u2019 large portfolio of digital capabilities, which includes aerial and terrestrial high-definition surveying and geospatial data management platforms include AtkinsGo, sCDE and WebGIS.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/excitech-to-deliver-common-data-environment-to-heathrow-airport\/","title":"Excitech to deliver Common Data Environment to Heathrow Airport","date":1550534400000,"text":"New solution, based on Autodesk\u2019s BIM 360 and GroupBC\u2019s Enterprise CDE, will provide one single version of the truth for critical assets\nExcitech has been awarded a 7-year framework agreement to lead a consortium comprising GroupBC and Autodesk technologies to deliver Heathrow Airport Limited\u2019s chosen Common Data Environment (CDE) solution.\nThe CDE is at the heart of Heathrow\u2019s Information Management strategy. It will be its primary common information repository and allow for specific controls to be put in place to share information in a controlled manner with suppliers and across Heathrow Departments. It will provide information and asset management in line with emerging industry-standards, to help ensure the efficient whole-life management of Heathrow\u2019s critical assets.\n\u201cHaving worked on the Terminal 5 construction project we are excited to be involved in this latest strategic project at Heathrow,\u201d said David Hughes, Excitech\u2019s managing director. \u201cThe Autodesk and GroupBC technologies being deployed are market-leading and will enable Heathrow to trust the data they hold and have the confidence to make decisions based on \u2018one version of the truth\u2019.\u201d\nThe project is already underway and is expected to be complete by December 2020. It will see the deployment of Autodesk\u2019s BIM 360 visualisation and design tools integrated and underpinned with management and governance from GroupBC\u2019s Enterprise CDE.\nJo Ellman Brown, PMO Director at Heathrow said, \u201cWe are aiming to be the first airport operator in the world who can leverage value from our digital assets, allowing our people to work in a safe environment, design and plan in a collaborative way, and operate a fully integrated asset system. With a long-established relationship with Excitech, we\u2019re delighted they were the successful bidder on this project. We anticipate that the solution they have proposed will significantly improve the management of our critical assets.\u201d\nAs well as enabling Heathrow to have accurate and up to date information readily available in the CDE, Excitech says the new solution will result in fewer surveys and reduced costs as a result of earlier and greater collaboration across all parties. In addition, maintenance costs will reduce through better, earlier clash detection and more accurate maintenance information being readily available.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/bricsys-partner-hok-helps-drive-development-of-bricscad-bim-v21\/","title":"Bricsys partner HOK helps drive development of BricsCAD BIM V21","date":1604620800000,"text":"\u2018End-to-end\u2019 BIM tool features several enhancements to reduce the time spent on \u2018boring and repetitive tasks\u2019\nBricsys enlisted the help of its BIM Alliance partner HOK for the development of BricsCAD BIM V21, the latest release of the DWG-based BIM authoring tool. The global design, architecture, engineering and planning firm helped analyse workflows in other BIM products in order to optimise the BricsCAD BIM user experience.\nBricsCAD BIM V21 includes a number of enhancements designed to reduce the time spent on \u2018boring and repetitive tasks\u2019. The new release also reinforces the software\u2019s \u2018end-to-end\u2019 workflow, so instead of creating and recreating a model for different stages of the design process, it allows users to model \u2018once and continuously\u2019, adding Levels of Development (LODs) as the design progresses.\nThe new release also extends the software\u2019s Artificial Intelligence (AI) capabilities. A new \u2018QuickBuilding\u2019 tool, for example, automatically converts geometry into a BIM model, shelling \u2018almost any solid\u2019 and creating walls, storeys and slabs automatically. These elements can then be edited and reassigned.\nOther AI tools from previous releases include BIMIFY, that uses machine learning to analyse the whole drawing and automatically add BIM data, and PROPAGATE, which uses AI to replicate component details throughout a model.\nBricsCAD BIM V21 also includes improved construction documentation. New drawing customisation features give users \u2018complete control\u2019 over how their 2D documentation will look. Users can add custom symbolic representations and graphic line overrides on property-based queries, such as, space area plans, fire safety plans and structural floorplans. Those customisations can then be re-used across views and projects.\nBricsys has also improved the workflow for quantity take-off and schedules, including the extraction of quantity data for individual composition plies. Users can create their own quantities for any BIM type using expressions, such as price per linear metre, etc.\nVisualisation has also been improved and a new integration with TwinMotion, the real-time immersive 3D architectural visualisation tool based on Unreal Engine, is designed to make it easy to create photorealistic stills and animations, directly from the BIM model.\nThere have also been some interoperability improvements and users can now import geometry from Autodesk Revit (.RVT) projects, as a reference or as native solids. Improvements to IFC import and export mean users not only get better performance and stability but can now map internal BIM types with IFC types.\nGeneral application performance has also been improved with enhanced multi-core support, while Xrefs now load in the background, meaning users can open a large model and begin working, without waiting for everything to load first.\nWe\u2019ll have more on BricsCAD BIM V21 in the coming weeks.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/generativecomponents-bridging-the-gap\/","title":"GenerativeComponents: bridging the gap","date":1207612800000,"text":"The Marina Bayfront Pedestrian Bridge in Singapore features a double-helix to replicate the designers\u00dd vision of walking through a DNA molecule. The creation of this complex form was made possible with the help of Bentley\u00dds unique generative design software, GenerativeComponents (GC).\nArup, a leading global design and business consulting firm, knew from the beginning that completing the Marina Bayfront Pedestrian Bridge design project in Singapore would require innovative technology. The 280-metre-long bridge features a double-helix exoskeleton constructed from stainless steel to support a 6-metre-wide deck spanning 65 metres between piers. When it opens in 2009, the bridge will replicate for pedestrians crossing it the designers\u00dd vision of walking through a DNA molecule.\nWhat enabled the designers to pursue this inspired design was GenerativeComponents (GC) \u00b1 Bentley\u00dds unique generative design software that captures and exploits the critical relationships between design intent and geometry. GC allows architects and engineers to quickly explore a broad range of \u00fdwhat-if\u00af alternatives for even the most complex structures.\nThe Marina Bayfront Pedestrian Bridge project presented the type of design challenge for which GC\u00dds capabilities are well suited. The bridge\u00dds DNA-inspired double-helix structural support mechanism is fundamentally unique. Unlike the Golden Gate Bridge in San Francisco and Millennium Bridge in London, which though different in appearance utilize conventional structural systems, the Marina Bayfront Bridge doesn\u00ddt belong to any tried-and-tested bridge type.\nMoreover, as an aerial view shows, the bridge\u00dds centreline adapts to the site geometry, with the bridge curving in plan while simultaneously rising to a high point in the centre to create a shipping lane. Incorporating the plan curve and vertical geometry within the constraints of the helix rhythm required complex geometry. To understand that geometry and create the necessary drawings to accurately estimate construction costs and coordinate the design between engineering analysis and documentation, Arup needed an innovative 3D modeling solution.\n\u00fdThe main challenge that impacted our ability to coordinate the geometry was associated with finalizing the bridge centreline,\u00af said Greg Killen, senior associate in Arup\u00dds Brisbane office and design manager of the Marina Bayfront Pedestrian Bridge. \u00fdUltimately, it became a delicate balancing act to provide sufficient clearance for the shipping lane, as well as a gentle slope to move pedestrians comfortably along the bridge. We also needed to create a link to a nearby vehicle bridge, allow pedestrians to pass under the bridge at each abutment, and facilitate supports close to existing bridge supports to reduce the impact on ships navigating the bay.\u00af\nThe design team also understood that any project constructed around complex, nonrepetitive geometry often attracts premium pricing during the tender process. To avoid this, the Arup team deployed sophisticated geometric analysis to rationalise portions of the structure. By using Bentley\u00dds integrated software applications, Arup was able to take advantage of streamlined collaborative workflows that increased the efficiency and productivity of its project teams. For example, by deploying GenerativeComponents, the team could begin modelling the complex helix exoskeleton, even while finalizing the critical bridge centreline geometry, fully confident that the design could accommodate any late changes.\n{mospagebreak}\nArup\u00dds ability to work simultaneously on linked design tasks allowed compression of the project\u00dds critical path. Using InRoads, Bentley\u00dds powerful civil engineering design solution, Arup finalized the bridge design centreline for use in GenerativeComponents. GC\u00dds flexible associative modelling technology empowered the design team to prepare a model of the double-helix geometry around this critical final design centreline.\nBy associating this flexible model with the new design centreline, Arup was able to regenerate a complete complex geometry model of the bridge in minutes. The team then imported the wireframe into the main structural analysis and design software applications. Once the structured analysis was complete, Arup used the model in Bentley Structural, Bentley\u00dds advanced Building Information Modeling (BIM) application.\nThis 3D model was then used to prepare fully coordinated drawings. To ensure the structural analysis and design stayed in sync, Arup then returned a post-processed version of the model to the analysis program. MicroStation\u00dds support for industry file and data standards, as well as connecting the geometry with the analysis engine, greatly enhanced this process. By using MicroStation\u00dds Visual Basic macro capabilities to generate Excel spreadsheets, Arup statistically analyzed the component geometry to develop new curve geometry that used one bend radius for each of the two spirals that formed the double helix. This increased bending speed and reduced the waste associated with the cold-bending process, which saved on stainless steel costs.\nArup then combined the structural analysis results with the Bentley Structural model by using the \u00d9Place Member along Path\u00dd tool to quickly regenerate a 3D solid model. Combining this with the BIM drawing production technology allowed Arup to quickly and accurately create as many drawings as necessary to describe the complex design. With the structural design in hand, Arup could turn its attention to designing the glazing panels that form the bridge canopy. Arup again modified the GenerativeComponents model to include the design team\u00dds intent for the complex panel arrangement.\nInitially, the Arup team was concerned that once it had completed the helix rationalization, the complex arrangement of panels and the bridge geometry would result in the panels being of unique size. However, using the Visual Basic capabilities Arup was able to populate a spreadsheet of the panel geometries for analysis. This proved that for a given tolerance \u00b1 of about 2 to 3 millimetrets \u00b1 the panels could be grouped into sizes that, for fabrication purposes, could be considered the same, which significantly reduced glazing costs. The design team knew that it could reduce fabrication costs by representing the double-curved helix geometry with a series of single-curved (planar) segments.\nSince GenerativeComponents allowed the Arup team to recognize, and have more control over, geometric parameters, it was able to investigate design ideas more quickly. For example, through GenerativeComponents Arup discovered it could reduce the weight of the bridge by using a fifth less steel than is generally required to construct a conventional box girder structure.\nIn addition to facilitating the geometry coordination, Bentley BIM in conjunction with GC allowed Arup to identify sets of repetitive components with the complex spiraling geometry. Moreover, GC enabled the Arup design team to model the complex detail geometry before finalizing the over site geometry. The time-saving flexible modelling technology within GC enabled Arup to update the geometry in minutes rather than hours. This ability allowed the team to make changes and explore options they could not have imagined.\nWithout deploying Bentley products on the Marina Bayfront Pedestrian Bridge, achieving this design would have required using expensive software, a dedicated 3D graphics programmer, and considerably more time, which would have added costs for the client, Singapore\u00dds Urban Redevelopment Authority.\nFor Arup, Bentley products formed the back bone of its design, analysis, engineering, and documentation processes. The products provided the power and flexibility required to successfully complete complex projects, while maintaining construction schedules and holding the line on costs in the process.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-nassim-saoud-trimble-consulting\/","title":"Video: NXT BLD 2019 \u2013 Nassim Saoud, Trimble Consulting","date":1565049600000,"text":"Applications of Mixed Reality in design and construction \u2013 NXT BLD London, June 2019\nAs the technologies for mixed reality applications mature and headsets reduce in price design teams and construction workers will gave access to powerful on demand overlays. Saoud akes us through Trimble\u2019s latest developments and application cases of mixed reality in the AEC market\nView the other NXT BLD 2019 presentations.\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/nvidia-takes-giant-leap-with-real-time-ray-tracing\/","title":"Nvidia takes giant leap with real time ray tracing","date":1535587200000,"text":"3D graphics specialist launches Quadro RTX GPUs, which promise to transform the role photorealistic visualisation plays in product development, writes Greg Corke\nJaws dropped earlier this year at the unveiling of the real time ray tracing Demo: Epic Games\u2019 Star Wars \u201cReflections\u201c.\nIt was the first time Hollywood quality visual effects, albeit with a few cheats, had been done in real time. Lighting moved around a scene interactively as ray-traced effects, including shadows and photorealistic reflections, were rendered at 24 frames per second.\nThe dampener was the hardware needed to achieve this level of realism \u2013 a personal AI supercomputer called the Nvidia DGX Station that comes with four Tesla V100 GPUs and a bill for \u00a360k.\nMost people expected it to take 5-10 years for this technology to trickle down to a single GPU in a desktop workstation. But, amazingly, with Nvidia\u2019s new \u2018Turing\u2019 Quadro RTX family of GPUs, announced last week at professional graphics conference Siggraph, it now looks set to become a reality by the end of the year when the \u201cworld\u2019s first ray-tracing GPUs\u201d are set to ship.\nNvidia CEO Jensen Huang described Turing and RTX as the greatest leap since the introduction of the CUDA GPU in 2006. \u201cThis fundamentally changes how computer graphics will be done, it\u2019s a step change in realism,\u201d he said. Not surprisingly, Nvidia has its sights set on the $250 billion visual effects industry, getting a slice of the colossal budgets currently spent on CPU render farms. But the new technology looks set to impact all areas of 3D graphics \u2013 of course, including design visualisation.\nPreviously, designers and architects would either have made compromises in quality using rasterising techniques for real time rendering or waited minutes or hours to get photorealistic results back from a CPU render farm. With Quadro RTX, Nvidia says designers can now iterate their product model or building and see accurate lighting, shadows and reflections in real time.\nOn stage at Siggraph, Huang illustrated this with a couple of impressive design viz focused real time ray tracing demos \u2013 one from automotive manufacturer Porsche and another of an architectural interior.\nFor its 70th birthday, Porsche created a trailer for its new 911 Speedster Concept car. But rather than ray trace rendering it frame by frame on a CPU render farm, the \u2018movie\u2019 was generated in real time inside Unreal Engine using a pair of Quadro RTX GPUs. The virtual car is fully interactive \u2014 lighting can be adjusted in real time and the car viewed from any angle, with reflections on the physically-based materials updating instantly. Epic Games, the developer of Unreal Engine, refers to this as offline-quality ray-traced rendering in a game engine.\n\u201cPorsche\u2019s collaboration with Epic and Nvidia has exceeded all expectations from both a creative and technological perspective,\u201d said Christian Braun, manager of virtual design at Porsche. \u201cThe achieved results are proof that real-time technology is revolutionising how we design and market our vehicles.\u201d\nHuang then sought the attention of architects through a real time, \u2018photoreal\u2019 ray-traced global illumination demo set in the lobby of the Rosewood Bangkok Hotel.\nThe interesting thing about this demo was that Nvidia\u2019s RTX ray traced viewer had a live connection to both Autodesk Revit and SolidWorks. Any edits or material changes made in the popular BIM and CAD tools instantly showed up in the viewer. Different lighting options could be explored at different times of the day. The demo illustrated how photorealistic visualisation could be used to influence design in real time and not just for polished renders for client communication or marketing.\nA new type of GPU\nNvidia achieves this huge leap in real time ray trace rendering performance thanks to the new \u2018Turing\u2019 GPU architecture, which combines three different engines in a single GPU.\nUntil recently, all Nvidia GPUs including the current \u2018Pascal\u2019 Quadro P Series, only featured CUDA cores.\nThis changed with \u2018Volta\u2019 with the arrival of Tensor cores in the Tesla V100 and Quadro GV100 that are designed specifically for deep learning, a subset of Artificial Intelligence (AI).\n\u2018Turing\u2019 adds a third type of core, the RT core, that is designed specifically to accelerate ray tracing operations. And it is these three processors working together (ray tracing on RT cores, shading on CUDA cores and deep learning on Tensor cores) that allows Turing to deliver significant performance improvements for ray trace rendering \u2013 six times faster than Pascal, says Nvidia.\nNvidia is using deep learning in different parts of the graphics pipeline to cut down on the level of work required to generate photorealistic output. Earlier this year the company introduced the use of Tensor cores for AI-based de-noising, which Huang describes as filling in all the spots that the rays haven\u2019t reached yet. Now Nvidia has introduced deep learning anti-aliasing (DLAA), a new technique which trains a neural network to take a lower resolution image and turn it into a higher quality image.\nThere\u2019s a lot going on here, some of which should become clearer in the coming months, but in short Nvidia is using a combination of AI techniques and different processing engines to accelerate ray trace rendering, rather than just relying on faster CUDA cores to do everything.\nThe cards\nNvidia has launched three Turing-based Quadro RTX GPUs. The top end Quadro RTX 8000 ($10,000) and RTX 6000 ($6,300) are virtually identical, both boasting 4,608 CUDA cores, 576 Tensor cores and a ray tracing speed of 10 GigaRays\/sec. The only difference is the memory, with the RTX 8000 featuring 48GB GDDR6 compared to the RTX6000\u2019s 24GB.\nThe Quadro RTX 5000 costs significantly less ($2,300) but features fewer CUDA cores (3,702) and fewer Tensor (384) cores, and the ray tracing speed drops to 6 GigaRays\/sec. It also only has 16GB of GDDR6 memory.\nThe amount of memory available on these cards is significant as this has been a barrier to adoption for GPU rendering, particularly in visual effects, where datasets and textures can be huge.\nAt 48GB, the RTX 8000 offers significantly more than its predecessors, the Quadro GV100 (32GB) and Quadro P6000 (24GB). All Quadro RTX cards support NVLink, a proprietary Nvidia technology that allows cards to be installed in a workstation in pairs and share their memory. It means those who have the budget for two Quadro RTX 8000s can effectively have a GPU with a colossal 96GB.\nIn addition to DisplayPort, all three GPUs will support USB Type-C and VirtualLink, a new open industry standard being developed to meet the power, display and bandwidth demands of next-generation VR headsets through a single USB-C connector.\nApplications, applications\nAt Siggraph, much of the focus was on Unreal Engine, which uses the Microsoft DXR API to access Nvidia\u2019s RTX development platform that helps ISVs develop RTX-enabled applications.\nHowever, Nvidia also has buy in from several design and engineering focused software developers and even more in visual effects.\nFor example, Chaos Group, which has a huge presence in the architectural visualisation sector, previewed Project Lavina using Microsoft\u2019s DXR to deliver 3-5x real-time ray-tracing performance over Volta generation for scenes exported from Autodesk 3ds Max and Maya.\nDassault Syst\u00e8mes announced that SolidWorks Visualize will use Nvidia\u2019s OptiX denoiser for \u2018instant life-like rendering\u2019 while it plans to use RTX in Catia for rendering with life-like quality materials and to accelerate VR rendering for immersive experiences and design validation.\nSiemens will extend its support for GPU-based rendering in Siemens NX Ray Traced Studio to include AI denoising and MDL support.\nVirtual prototyping and VR specialist ESI Group will potentially support real-time ray tracing on Turing GPUs in a future version of IC.IDO.\nMeanwhile, users of SketchUp, Cinema4D and Rhino will get access to Nvidia OptiX denoiser technology through the Altair Thea render plug-in.\nSummary\nWith the launch of Turing and its Quadro RTX ray-tracing GPUs, Nvidia has sent ripples through the computer graphics industry. Out of the blue, real time ray tracing looks to have arrived and with it the potential to transform the way architects and designers use photorealistic visualisation in their workflows. We look forward to test driving the new cards later this year when they launch in Q4.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-thinkbim-counts-down-to-construction-and-assembly-2017\/","title":"NEWS: ThinkBIM counts down to Construction and Assembly 2017","date":1491177600000,"text":"Half day conference in Leeds on April 5 will focus on Augmented Reality, scanning and robotic setting out\nThinkBIM is gearing up for its spring season Construction and Assembly conference, which will be held in Leeds on Wednesday 5th April. The afternoon event will give delegates the opportunity to try out augmented reality technology, scanning equipment and robotic setting out devices.\nScott Grant, CEO of Soluis Group, recently featured in this AEC article on the new Spurs Stadium, will deliver the keynote about the use of visualisation, immersive environments and specialist apps to bring data to life for project teams.\nIn addition, there will be roundtable sessions from SES Engineering Services discussing levels of detail and Mark Taylor from BAM Construction talking about digital delivery of the P22 framework.\nFinally, there will be laser scanning demonstrations from Central Alliance of Wakefield and robotic setting out from Trimble MEP.\nthinkBIM: Construction & Assembly takes place on 5 April from 12:00 to 17:30 at Squire Patton Boggs, Leeds. Tickets cost \u00a370.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/smartgeometry-09\/","title":"SmartGeometry 09","date":1243900800000,"text":"Generative design is an application of parametric modelling technology that has quickly swept through the design schools and high-end architectural firms, and is now on the cusp of going mainstream. Martyn Day attended the key annual conference, SmartGeometry 09.\nSignature architectural practices, such as Frank Gehry and Foster + Partners, have been using 3D modelling and innovative programming to create beautifully sculpted forms for a number of years to advance the vocabulary of architectural design. However, while momentum for advanced modelling is gaining pace, the skill-set of some graduates continues to lag behind in geometric knowledge.\nTo rectify this, and to skill the next generation of digital designers, a handful of industry luminaries set up the SmartGeometry group in 2001. The group represented a partnership between practice, research and academia and included High Whitehead of Foster + Partners, Lars Hesselgren of KPF, Jay Parish of Arup Sports, and Dr Robert Aish the then chief scientist at Bentley Systems. Affiliated education institutions included the Architectural Association School of Architecture (AA), MIT, Delft Technical University and the University of Bath. The SmartGeomtery group organised workshops and seminars to educate the industry on the benefits of parametric design, scripting and digital manufacturing.\nEight years on and to say things are going well would be a huge understatement. Parametric modelling and an explosion in interesting architectural sculpted forms is literally revolutionising our city landscapes. Many of the architectural lecturers I know have difficulty finding words to describe the complex designs that their students produce \u2014 some aren\u2019t even sure if it is \u2018Architecture\u2019. The days where 2D CAD was used to merely document a design for construction are gone. With an increasing number of options, from SketchUp to Bentley Generative Components, architects are finding ways of exploring 3D in highly complex ways.\nThis year\u2019s SmartGeometry event was held in downtown San Francisco and followed the now familiar format of workshop, alumni day and a conference day. Bentley Systems is the key sponsor of the event and so the workshops teach Bentley Generative Components technology, based on MicroStation, to a mix of both university students and designers from practicing firms. While students may let their imaginations run riot, those with jobs frequently try to use the training on offer to apply GC to an existing project or problem. This year there was the addition of a rapid fabrication room, where a number of Z Corporation rapid prototyping machines were available to produce physical copies of the generated designs.\nNew kids on the block\nGenerative Components is without any doubt the leading parametric system in architectural practices today. This is in part due to its MicroStation base and the length of time it was in extended beta, with Dr Robert Aish engaging with practices and architectural schools that wanted to participate. However it is clear that GC is no longer the only game in town, with McNeel Associates\u2019 Rhino-based Grasshopper technology gaining popularity, together with architects using any number of other modelling tools to explore paramatrics, such as SolidWorks, Catia and SolidThinking \u2014 to name but a few. A year ago Dr Aish left Bentley Systems and joined its arch rival Autodesk to develop parametric modelling tools for architects and designers in AutoCAD. So the competition is heating up for expressive modelling tools and this is a great indicator as to the level of interest in this market.\nJust prior to the first Alumi day, I met with a San Franciscan friend who introduced me to a local architect, Andre Caradec, who also lectures at the California College of the Arts. Mr Caradec invited me to the FLUX workshop and exhibition that was running at the same time as SmartGeometry. There were some overtones that FLUX was a response to competitive vendors which couldn\u2019t take part in SmartGeometry due to Bentley\u2019s long-term sponsorship.\nIndeed, the companies highlighted at FLUX were McNeel, Autodesk and SolidThinking. Both events were about the same thing: educating students to use the 3D tools to create complex designs, and exploring modular assemblies, meshes, material systems and environments. At the exhibition many of the SmartGeometry hierarchy were present to wonder at the models on display.\nThe main display was a massive, internally lit, twisted sculpted form designed in 3ds Max, Rhino and Grasshopper. Mr Caradec had fabricated much of it in his garage with CNC machines and it is the largest Grasshopper project to date to be manufactured.\nThis was the second SmartGeometry since Dr Aish left to join Autodesk. While Bentley sponsors the event the content of the key presentations is independent. Attendees hear about projects that have been completed in all manner of CAD applications. It is not about what software was used, but the application of technology to solve design problems.\nThat said, GC was always to the fore and the new man in charge of the product, Makai Smith, was on-hand to update and explain the development work that Bentley had been putting into the latest versions. There was even a team of developers at the event to add to GC\u2019s capabilities should it be required in the workshop sessions.\nDuring the two-day event attendees heard from the visiting lecturers and viewed the students\u2019 work produced during the training class and four-day hands-on workshops. Every year the designs just get better and more complex, with everything from adaptive shading designs to the generation of internal spaces in tower blocks, derived from the movement of termites (termite mounds are, apparently, inherently good examples of self-regulating systems).\nConclusion\nThe overriding feeling I was left with from SmartGeometry 09 was that of convergence. Architects now can help and learn from medicine, aeroplane designers and mechanical engineers, to name just a few. The move to modelling presents us all with the same challenges and benefits.\nWith the concentration of presentations being about green issues and limiting carbon emissions, it is obvious that all things that humans design have a price to the environment \u2014 should that be a car, a plane or a building. With buildings estimated to produce 50% of the world\u2019s carbon emissions it is down to each industry to innovate and find solutions and to share these findings. GC used to be the only event that showcased exciting applications of computer technology, but this is increasingly no longer the case. With \u2018Rules Based Design Symposium\u2019 in Berlin, FLUX and a number of other events springing up the appetite for parametric design is no longer only for the signature architects.\nWhile there is much to learn and the systems are still complex we are witnessing a change in the way we use computers and collaborate to solve design problems. Using a quad core, 64-bit workstation to draw flat lines is like using a sledgehammer to crack a wallnut. With products like Generative Components and an emerging new class of applications, the sledgehammer can build green cities.\n{mospagebreak}\nParametric and script-based design\nSmartGeometry 09 showcased no less than 40 presentations. There was a lot to take in and much of what is shown is either research or explanations of completed projects. Here, Martyn Day highlights some of the more exciting projects.\n\u2022 Jenny Sabin of Sabin+Jones Lab Studio gave for me the most fascinating presentation. While being an architect and teaching architecture, Sabin and her students had been working with the University of Pennsylvania medical research laboratory. The process is one of mutual benefit, with Sabin helping to explain and model medical structures at a cellular level for the medical team, while she gets insight into nature\u2019s solutions for building structures and behaviour. One of her projects involved the modelling of \u2018forces between vascular cells\u2019 using 75,000 cable zip ties. The utterly stunning model is so large you can walk around and inside it.\n\u2022 James McBride of Mankani Power gave an astounding analysis of an individual\u2019s power consumption. Mankani Power is developing a kite-based wind turbine, which is currently being tested on the cliffs of Hawaii and is trying to play a part in the developing green energy industry. CEO Saul Griffith chronicled all the energy he has consumed in a year \u2014 and it wasn\u2019t a pretty picture. The basic problem is that people in the West have high power consumption, while in the East there are massive populations on relatively low power consumption. As the East\u2019s need for power grows the exponential requirement for energy will outstrip our ability to produce and we can\u2019t burn any more coal.\n\u2022 Marty Doscher of the innovative and multi-award winning practice, Morphosis, gave an insight into how computer tools have liberated the creativity of its designers, enabling highly complex and original designs. The projects really speak for themselves. The interesting post note was the culture of the firm where some architects that went on site to work with the contracts sometimes \u201cdidn\u2019t make it back\u201d. This wasn\u2019t because they were hugely error prone, but because it was deemed a drawback to enhance their knowledge of how things were made. By thinking about limitations of construction in the design process, they felt that the design was compromised. Certainly a presentation for purists!\n\u2022 Steve Downing of Arup and Dominik Holzer of AEC Connect introduced the concept of \u2018Optioneering\u2019, which is a programmatic way to look at all the options and trade-offs between multiple criteria in complex projects. With projects becoming more collaborative, if used early on, performance criteria can be weighted, driving analysis of all the designs done by various disciplines on the project, giving rise to the best design. This is being tested by Arup having developed a collaborative framework that connects all the digital project information from modelling products to analysis tools.\n\u2022 David Kasik of Boeing was an unusual presenter, introducing himself to the architectural audience as from a company that makes \u2018flying buildings\u2019. He took us through the many permutations that Boeing has made to visualise the massive amounts of data that it created in the forthcoming 757 aeroplane. With literally millions of polygons, shortcuts have to be made and every trick in the book used to fly through the model in real time.\n\u2022 Hugh Whitehead from Foster + Partners always wins the day for the quality of projects and beautifully rendered images. His topic this year was collaborative visions and their interpretation. Using the stunning Beijing Terminal 3 building as an example, he gave a walk through of the design, highlighting engineering features and architectural details. While Heathrow Terminal 5 took 20 years to plan and build, this gargantuan Terminal (the largest in the world) was completed in under five years. This was made possible by using the software to calculate and generate the parametric elements.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/myvr-p-visualisation-over-the-web\/","title":"MyVR","date":1119657600000,"text":"Distributing CAD models to potential customers or manufacturers has become a highly competitive market. A new solution from Norway, offers a way to distribute photorealistic \u2018VR\u2019 sessions of models over the web.\nProduct: MyVR\nSupplier: 3D to Go\nPrice: see text\nThere are a number of real time visualisation systems on the market today which enable the viewing of Architectural or Mechanical models. These could be a simple as sending an executable, like SolidWorks eDrawings or as powerful as building a complete 3D environment in a product like NavisWorks. With the advent of the Internet there have been many other solutions which utilise light-weight formats, Java or other devices to distribute design information but none have really cornered the market yet.\nAn exciting new venture in this space comes from Norway and is called simply, MyVR, and in the UK the product is sold through a company called 3DtoGo. In short, MyVR provides a series of tools to distribute interactive, real-time, VR sessions of large models over the web. The quality of the rendering is photorealistic and even though the models are streamed over the web, the interactivity is as if you are looking at the model on your hard drive. The concept behind the product is that firms that create 3D models will be able to store especially thinned out versions of these models on a server, which will stream sessions over the web for potential customers to walkthrough, interact and visualise buildings, landscapes or products \u2013 before they are built or manufactured. The developers claim that MyVR could even be used for training or as a virtual front end to shopping. While the streaming over the web is played heavily on, the VR environments can also be run from local hard drives, or across a network.\nTo use the product, one obviously has to have 3D content to start with. 3DtoGo realises that many firms may not have made the switch to developing full 3D models and so offer a service to generate 3D models from 2D plans or drawings. Those companies that are using either Autodesk\u2019s 3D Studio or Viz product will have a head start, as at the moment MyVR works with data created exclusively in those systems, although other key industry applications are currently being considered. Environments created in Max or Viz are the key start point, as MyVR takes the polygon models, textures and lighting from the software.\nMyVR is actually made up from a number of modules, each of which have a specific in the process of making, distributing and viewing the models. MyVR Export is the Max\/Viz component which takes 3DS files and runs special algorithms to \u2018thin out\u2019 the file size. Typically 3DtoGo claims that model size reduces by 50% to 75%. In one of the demo files, an 800MB model shrank to just 131MB. In the general scale of the web, 131MB is still a very large file to stream but this is where 3DtoGo feel its technology has an edge.\nOnce the MyVR file has been created, it\u2019s loaded onto the MyVR realtime server, ready for distribution. The MyVR Publisher component then allows administrators to allocate rights and settings for access to the models, adding additional control like passwords. Once in the MyVR format, the data is encrypted and cannot be modified by a viewing client, if streamed, it also cannot be stored (stolen) on a PC as it only occupies Video memory on the graphics card.\nThe final part of the product is a viewing application, which comes in three flavours; a freely downloadable viewer, an Active X component for embedding into company websites and a \u2018Pro\u2019 Viewer, which provides the ability to generate walkthrough paths, AVIs and set up cameras (which can also be extracted from Max\/Viz).\nViewing\nAs you will be able to tell from the images, the quality of the output is pretty good, as good as you\u2019d get within 3D Studio. Loading files from a local hard drive takes only a few seconds, while downloading the initial set-up over the Internet, the initial download is around 4MB, which may take a few minutes over standard broadband. To move about the model you can chose to walk (3 speeds), fly (helicopter) or Fly around at Jet speed! All of which provide smooth graphics, as if it were a game (for those of you that have played Half Life, it\u2019s pretty similar). Clash detection can be switched on, so you can\u2019t pass through walls, or more importantly, when the gravity button is hit, you don\u2019t go through the floor! Fog can be applied to add a little atmosphere, the clouds move and with a good Max model, animations like water movement will be displayed.\nMoving about is simple, left mouse button forward, right back and moving the mouse allows you to look around. Scenes can be saved with a number of set camera views, which you can flick through, or go back to should you get lost in your model, which is possible. The product supports paths, so it\u2019s also possible to fly along pre-determined paths, like a road, or a route around a product or house. If models are animated in Max, it\u2019s also possible to have moving mechanisms within an environment.\nThe only pre-requisite to run a MyVR session is that your PC has an OpenGL capable graphics card, which to be honest has been standard in most machines for a number of years.\nConclusion\nWith visualisation tools, the proof of the pudding is in the quality of the images. I think you can tell from these screen grabs that the quality is certainly there. Obviously the magazine medium isn\u2019t great at showing moving images, but I was impressed in the frame rate and smoothness of MyVR, as well as its simplicity.\nThe sales model for the product is a tad more complex. To own the technology outright, costs \u00fa20,000, and this includes the export, server and a \u2018Pro\u2019 viewer. Here you can make as many models as you want and host the service on your own server and embed MyVR sessions into your website This obviously limits the product to larger firms. However, MyVR is available as a service from 3DtoGo, with the company hosting the model on its servers and charging a monthly fee for a set number of \u2018streams\u2019 (client web sessions). Also, for those that don\u2019t have the in-house 3D capability, the company offers a Max\/Viz modelling service. Alternatively, the web portion could be avoided and MyVR environments could be sent to clients on a CD with the viewer.\nThe end results are very impressive and guaranteed to wow clients. Obviously with the fixed cost or ASP payment model, the value of distributing the model to clients or potential clients has to be weighed up. By paying per stream, you\u2019d really want only qualified customers to view your designs. While if you opted to buy MyVR, the cost of opening up sessions to everyone will be based on what you pay your provider for bytes transmitted. To launch MyVR in the UK, 3DtoGo is offering a month\u2019s free hosting for the first 25 AEC customers that want to evaluate the technology. A demonstration of the streaming technology is available as a download from the company website.\nwww.myVR-software.com\nwww.3dtogo.com","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/executive-guide-to-bim-part-2\/","title":"Executive guide to BIM: part 2","date":1374019200000,"text":"The second installment of this two-part guide (Read part one here)goes into greater detail on Building Information Modelling (BIM) workflows, when a model progresses from concept to construction and facilities management. We cover the benefits and implications of adopting BIM, including changes to teams and workflows.\nHaving looked at levels of Building Information Modelling (BIM) and dimensions of information in the first part of this guide, the third factor to consider is the level of detail or level of development (LOD) contained within the model. Both terms mean the same thing and aim to clarify the expectation in terms of information granularity. These LODs are often linked to time and stage of work with a model progressing from concept through to construction and facilities management (FM).\nThere are two strains that have developed independently, either side of the Atlantic but have matured into near alignment. Table 1 (top page 23) compares and contrasts the two.\nIn table 2 the two standards diverge slightly as the purpose of the LOD is interpreted differently with the AEC terminology looking to differentiate between the geometric simplicity that is acceptable for general draughting purposes versus the complexity required to produce quality rendered images.\nThe majority of design work is carried out in the first three LODs where the terminology is interchangeable.\nThis terminology is important because it may have a major impact on the quoted price of a job and a detailed scope should go much deeper in specifying what scale and at what stage certain elements will be defined and in what dimension.\nThe following practical example may help to clarify: For the next delivery deadline, all pipes above ## will be modelled in 3D. Pipes between ## and ## will be shown schematically in 2D. Connections below ## will not be shown.\nA succession of these types of statement will clarify, without ambiguity, the extent to which the model is expected to deliver on clash detection and to what extent the final fixtures are connected by the skilled fitter on site.\nOnly the first part of this scope is BIM with the second part being a CAD workflow and the third being blind luck.\nThis is not an uncommon scenario where BIM methodology is delivered to a defined point but no further. Where traditional, manual construction methods are ultimately to be used, the advantages gained from the BIM, versus the cost of creating the model do tail off as we approach higher levels of detail.\nThe counter argument is that this does lead us towards a process which is open to human error.\nImplications of adopting BIM\nThe buildingSMART MacLeamy Curve (pictured page 22) promotes a workflow whereby the decision making process is moved further forward and hence lowers the cost of refining design.\nBIM methodology can assist in this effort by improving collaborative communication and by drawing closer parallels between the virtual concept and the built reality, thereby highlighting clashes and construction issues.\nThis desire to improve can also lead into one of the most common BIM adoption pitfalls, potentially causing undue financial expense and leading to BIM software and protocols becoming unpopular among staff.\nIt is an incorrect assumption that in developing a design using BIM methods, all design decisions have to be made at a far earlier stage of the job than is preferable or would previously have occurred.\nTable 1\n| AEC(UK)BIM Protocols | AIA BIM Standard (US) | Description |\n|---|---|---|\n| LOD 0 | LOD 100 | A conceptual massing study where shapes and forms are explored against the client brief and design intent. At this stage, floor areas and volumes can be extracted from the 3D model and departmental flow can be rationalised |\n| LOD 1 | LOD 200 | The basic form is developed using categorised components such as walls, floors, columns and equipment. These elements are generic place-holders in terms of their associated meta-data but also appearance, often with all objects made from one common material, hence the application of the term \u2018White Model\u2019 |\n| LOD 2 | LOD 300 | As design decisions are made, the generic place-holders are replaced with precise, manufacturer-specific objects, rich in associated information but still simple in their 3D geometry |\nTable 2\n| AEC(UK)BIM Protocols | AIA BIM Standard (US) | Description |\n|---|---|---|\n| LOD 3 | N\/A | Simplified geometric form is adequate for most BIM tasks but when aesthetic images and rendered scenes are required, it is necessary to replace elements with more accurate objects |\n| LOD 4 | LOD 400 | Objects are either swapped or additional information is appended to include fabrication and assembly information. This does not always mean that the objects get more geometrically accurate, but that information is added which is relevant to the construction process |\n| LOD 5 | LOD 500 | The model is updated to reflect the As-Built nature of the finished building |\nWhile the buildingSMART MacLeamy Curve diagram shows us that this can be beneficial to the overall project, it is inappropriate during the tender stage for instance, when work is carried out speculatively and with minimal effort.\nIt is often a symptom of the self-taught user that undue consideration of the components is applied prematurely whereas BIM software is just as capable of being vague and conceptual as CAD or the pencil are.\nVery closely linked to this topic is the problem of over-modelling, which is not uniquely a BIM problem but is just as prevalent among CAD users. Just because software allows us to draw or model a component with accurate anatomical geometry, does not mean that it is the right thing to do.\nThe worst protagonists of this have and remain to be product manufacturers who provide their electronic libraries, complete with seals, washers, grommets, bolt-threads and fan-blades.\nThe user often does not have the time to clean and rationalise these elements and the result is too much graphical information going into a drawing or model.\nBIM should be the technology that removes this problem but it is also the technology that accentuates it. It has the potential to eliminate it because associated metadata can provide specifics on a chosen element without the need for graphical recognition \u2014 rather than model the hinges of a door, the properties of a simple generic component can advise that Furniture set A is applied as per specification, for example.\nUnfortunately, bad practice and over-enthusiastic users tend to get carried away, enjoying the ability to recreate a component as a virtual work-of-art. This is repeatedly evident by reviewing the copious online repositories for BIM content where proud modellers are willing to share their creations in return for plaudits and followers.\nThe problem is that such extremes are not scalable, nor do they necessarily add to the delivery or the value of the finished product.\nStandards and protocols\nVarious international standards will be bandied around and may be referred to on different projects. A brief explanation is provided on the most relevant of those (box out, left).\nAll of the standards mentioned, left, are industry-wide protocols with many more coming up for consideration in each of the different disciplines, such as transfer protocols specific to environmental assessment of buildings and structural analysis of structures.\nThere are too many to list in this exploratory article but further information is available in this series of training modules.\nTeam and workflow changes\nThere are many areas of working practice that are touched by the BIM ethos if properly embraced.\nThat is not to say that complete turmoil will ensue on the first day of use, and BIM methodology can be eased into a practice and allowed to prove its merits before spreading its wings, but in order to take full advantage, old processes and workflows may need to be re-assessed.\nA few such examples are highlighted here.\nIn the traditional drawing office, a pyramidal structure would see a lead designer at the pinnacle and a tier of tracers at the base with various grades in between. Often the lead designers would have a hands-off approach to drawing production work.\nBIM encourages, and in many cases insists that operators understand what is being modelled because the elements are not arbitrary lines to which meaning is assigned by a skilled overseer; they are intelligent objects with properties and inherent characteristics specific to the category of component.\nAs such the more building-savvy the user \u2014 the more efficient they are at modelling and manipulating information in a BIM environment.\nParticular skills will naturally emerge and talents for specific tasks will surface among the users. Some will take to content creation while others will gravitate towards schedules, detailing or model management.\nOver time the team will settle into a new, more horizontal structure as these skills develop.\nAnother area that can lead to frustration in the office is the timeline for producing deliverables. Again looking at a traditional environment, a twelve week programme with a requirement for 200 drawings should see somewhere in the region of 150 drawings at or near completion by the nine week stage. Management and the client can see the progress and gauge whether deadlines will be met.\nIn a BIM scenario however an inexperienced team may not have a single drawing prepared by the same milestone, but they have a fantastic model filled with metadata and resplendent with fabulous views which are compiled at the eleventh hour to deliver the job. This can only lead to stress that can easily be avoided.\nMost BIM tools allow the drawing sheets to be created, named and views allocated, even if those views are largely empty or under development.\nAs the model progresses, so do the drawing sheets, so the boss can see what is going on.\nAEC(UK) BIM standard\nA working set of protocols and best practice for SMEs. Free of charge and targeted at users of specific software applications in order to use recognised terminology rather than generic vocabulary. A Revit version and Bentley version are available, with other versions in production alongside generic documents such as BIM Execution Plan Pro Forma.\nBS1192:2007\nA UK standard that establishes the methodology for managing production, distribution and quality of construction information, including CAD data, using a disciplined process for collaboration and a specified naming policy. Some of the techniques and protocols are dated and a review is underway.\nCOBie\nConstruction Operations Building Information Exchange is a subset of IFC (see below), designed as an exchange format for the handover of a construction project upon completion and is a large spreadsheet. It does not need a 3D model to create it but tools can automatically transfer BIM data into the COBie format.\nCI\/SfB\nThe favoured system for many architects, this element classification schema is no longer maintained and is superseded by Uniclass.\nUniclass\nDeveloped by the Construction Industry Project Information Committee (CPIC) representing RIBA, RICS, CIBSE and others, this system replaces the CI\/SfB classification and is due for a new release.\nIndustry Foundation Class (IFC)\nA means of passing information from one BIM software platform to another. The difficulty is in the way that compliance is applied: imagine a list of one hundred bullet points and in order to comply, you must hit forty of those points. It is possible for two applications to be IFC compliant and yet completely miss each other. Future releases should tighten up the gaps.\nOmniClass\nA classification system for the US construction industry with a growing number of global users. The tables are made up of various other naming and numbering systems and the table for designed elements comes from UniFormat.\nUniFormat\nThe dominant system in North America and those parts of the world where the US has influence, such as the Middle East.\nAIA BIM Protocols E202\nFocuses on the processes surrounding BIM use and collaboration between the various stakeholders rather than the internal standardisation of BIM software use. It is very similar to the BIM Execution Plan mentioned in the AEC (UK) documentation set and the Penn State version as well.\nFees and deliverables\nIt is imperative that the implications of BIM are considered when preparing fee proposals and agreeing terms with clients and contractors. By this we do not imply that it is more expensive to deliver a project when BIM methodology is utilised, nor is it as simple as saying that the reverse is true.\nOften the increase in efficiency and hence the reduction in time and costs of preparing deliverables is offset by the increase in expectation and potential additional services, which may be naively promised for no additional fee. Those seeking work and agreeing terms at the top of a company need a good understanding of the various costs and efforts involved in preparing a particular set of data by those at the coal-face.\nThis is nothing new, but the upheaval of new technology and the clamour for available work has seen a few companies over-promise and strip out all profits from a job.\nThere are new roles emerging within the industry that will need to be filled, and hence opportunities to offer new services and grow additional revenue streams. Someone within a project team will need to take on BIM leadership and co-ordination. Do you want that role and the associated fee and responsibility?\nWho owns the model? This is always an interesting question because unless it is stated in the contract documentation, the BIM data is simply a means to an end in preparing the drawings that you are obliged to deliver. If the client or contractor then asks the architect for a copy of the model for clash detection, review or even FM, should he expect to get it for free?\nMany would argue that if I paid for your time in preparing the information, then I can expect to have it in the original format that it was prepared in. Whichever side of the fence you sit on, it is much better all round if this is clarified at a contractual level and not fought over at the back end.\nProtecting embedded intellectual property is a topic that logically leads on from the model ownership.\nIf a lot of time is spent developing a data-rich and efficient library of components, and a model containing such elements can easily be mined for elements, this will inevitably lead to these elements falling into the hands of competitors and there is very little you can do about that. This should not be used as a reason not to build libraries but should be considered when defining the specification of such efforts.\nData format is also an aspect of the market which will come under intense scrutiny in the future.\nBIM is not about a single piece of software, nor even a specific type of software, but a means of interacting with data from many perspectives and objectives, and hence the exchange formats are ultimately more important than the software used in any one part of the building life-cycle.\nFormats such as IFC are hopefully going to step in and fill the gaps that currently exist and allow stakeholders to pass information freely around the design, construction and maintenance teams without large amounts being lost along the way.\nContract and insurance\nMuch has to change in order to allow BIM methodology to reach full potential and for the entire industry to properly embrace the new way of working and collaborating.\nVarious initiatives aimed at proving the effectiveness of IPD contracts (Integrated Project Delivery) or similar are both available and underway and these new contract documents should ease the adoption of BIM by eliminating some of the risks surrounding litigation, but this mitigation of risk is not the only contract-related change that we will probably see in the near future.\nOne example of an area that will have to change radically if we are ever to realise the collaborative advantages of working in a fully co-ordinated 3D model is the terms of contract for building services consultants and designers.\nRecent years have seen a steady decline in their involvement, often to nothing more than schematic design and plant-room layout.\nEven this small amount has been done at a relatively advanced stage of the design process, leaving little room for influencing proposals without significant cost implications.\nAs far as the insurance industry is concerned, BIM adoption has made them nervous to say the least. The current consensus seems to be that BIM is a free-for-all and that the only way to protect themselves is to ring-fence BIM activity and effectively treat it as a means to an end in the production of drawings, with the sharing of models prohibited.\nThis is due in no small part to the lack of common standards and mandated protocols by which they can gauge the capability of a company or the requirements of a project, and hence assess the inherent risks. It is something that the whole industry has to work to overcome and as standards are defined and gain acceptance in the future, we will see the emergence of professional accreditation and qualifications which can then be mandated by underwriters and sought as a mark of quality.\nThe final word\nBIM is a mind-set, not a software application, and adopting BIM methods can require an overhaul of the design process, team structure and even business practices in order to achieve the full potential of the technology.\nThis is not going to happen overnight, as much because the industry will need to adapt as it is because of the internal upheaval and cost, but the potentials are an exciting area of development with new revenue streams, additional services, increased efficiency and a wider involvement in the whole building lifecycle.\nIt is not so new as to be an untested technology, but it is new enough not to be able to see the ultimate potential yet.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-mx3d-gives-update-on-robot-printed-canal-bridge-2\/","title":"NEWS: MX3D gives update on robot printed canal bridge","date":1505692800000,"text":"Startup releases video that shows design updates and 3D printing progress\nAmsterdam-based startup MX3D has given an update on its 3D printed steel bridge project, releasing a new video that shows the latest developments.\nThe pedestrian bridge is scheduled to be finalized early 2018 and to be installed on the Oudezijds Achterburgwal canal in the red light district of Amsterdam in late 2018.\nThe initial design for the10 metre bridge has changed significantly. As knowledge of the (safety) requirements, material properties and technical potential grew, a final model emerged in early 2017. This led to the final bridge design by Joris Laarman Lab.\nIn March 2017, the printing and assembly of large 1 metre bridge parts began. In parallel MX3D engineers have continued working on realising their vision of autonomous robots 3D-printing infrastructure. This summer, a robot has been placed on the bridge to test the company\u2019s vision of robots printing live, onsite, without human intervention.\nMX3D also announced to reopen the doors to its Expert Centre, which gives visitors the opportunity to see the printing process. A small entree fee will help support MX3D in finishing the bridge, which is a non-profit project.\nMeanwhile, check out MX3D\u2019s Tim Geurtjens\u2019 talk from NXT BLD 2017\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/dmti-spatial-to-enhance-restaurant-order-management-system\/","title":"DMTI Spatial to enhance restaurant order management system","date":1185840000000,"text":"Markham, Canada, July 30, 2007: DMTI Spatial, a leading provider of Enterprise location intelligence, has been selected by St-Hubert Bar-B-Q Ltd., a major restaurant chain based in Quebec, to coordinate and optimize its order management system located in Montreal for home delivery from the company\u2019s 100 restaurants located in Quebec, Ontario and New Brunswick.\nEach year the company receives close to 2 million calls. Using DMTI\u2019s CanMap Street Files and postal code products utilizing ArcGIS Server and ArcGIS Desktop software from ESRI Canada, customer service agents at St-Hubert\u2019s call center can pinpoint the exact location of a customer and immediately send their order to the restaurant closest to them. Additionally, the same application is used to located customers using the web site to order from the restaurant online.\nFirst time St-Hubert customers calling into a central number at the company\u2019s state-of-the-art call center are asked for their coordinates including name, address and telephone number. After agents enter the information into the system, the address is geocoded to determine the closest restaurant to send the order for fulfillment. Once in the data base, repeat customers need only give their telephone number and the system automatically forwards the order to the appropriate restaurant for delivery. Should the customer have a non-conventional address, agents can use points of interest such as hospitals, malls, parks and buildings to pinpoint the location. This enables St-Hubert to decrease the amount of time it takes to process an order, optimize the flow of orders to restaurants and enhance the overall customer experience.","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/mike-doyle-joins-des-lauriers-to-strengthen-gis-team\/","title":"Mike Doyle joins Des Lauriers to strengthen GIS team","date":1015286400000,"text":"Des Lauriers Municipal Solutions, the leader in Government Automation, today announced that Mike Doyle has joined the Des Lauriers team to lead GIS development and implementations for municipalities nationwide.\n\u201cMike adds a new dimension to our offerings which allows us to provide additional solutions to exiting as well as prospective clients\u201d, said Dorian Des Lauriers Founder and President. \u201cMike will not only be able to enhance our GIS offerings but also expand our offerings to allow Des Lauriers to become a one-stop-shop for municipal solutions\u201d.\nPreviously Mr. Doyle was GIS manager for the Town of Wellesley MA. It was at Wellesley that he created one of the premier GIS sites in the North East. Using a blend of technologies including MapObjects, and desktop suites he created tools to: Locate Parcels, Create Wetland Buffers, Create Thematic maps, and Inventory conditions on everything from street signs to road maintenance. With this site, Wellesley is now able to monitor conditions saving both time and money on maintenance and economic development.\nBefore Wellesley, Mr. Doyle was a principal at Northern Geomantics Inc. (NGI) a GIS consulting firm. NGI worked extensively with both Federal and Local government on a variety of GIS projects.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-developing-next-generation-cloud-based-bim-tool\/","title":"Autodesk developing next generation cloud-based BIM tool","date":1448841600000,"text":"Having heard from several reliable sources that development work for a next generation cloud-based BIM tool is underway Martyn Day looks to Autodesk\u2019s manufacturing division for some clues as to how it might work.\nAutodesk has a vision. In the future it believes that applications will be available on any device, anywhere, irrespective of operating system. People will be able to collaborate with many others in real time through a managed environment and utilise the power of large servers to optimise, enhance, compute and suggest design solutions that we could never envisage. Our tools, capable of deriving complex geometry, will be able to directly drive fabrication \u2018machines\u2019, which will recreate these forms quickly with high precision. It is a big and compelling vision for anyone with an imagination.\nBut what we have now is quite far from that. The rapid proliferation of Autodesk point solutions and services is going through a messy patch, with the mitosis of desktop products with cloud equivalents. For instance AutoCAD has a cloud-based version called AutoCAD 360. The cloud version of AutoCAD is a ground-up redevelopment and does not really compare in feature set to the desktop version, which is what Autodesk eventually intends for all its products.\nThere is also a Mac version of AutoCAD that has a slightly different feature set again, mainly due to the OS differences. Convergence of even one product line is not so simple and will take many more years of development.\nRevit in the cloud?\nBefore we dive into the latest news and scuttlebutt on Revit and the cloud, I need to first highlight what is happening in the manufacturing division of Autodesk \u2014 as developments there will likely eventually impact the word of Revit and Building Information Modelling (BIM).\nWhile AEC magazine concentrates on covering technologies impacting Architecture, Engineering and Construction, we also have a sister publication DEVELOP3D, which monitors the manufacturing world. The cloud is having a more immediate impact on the next generation tools in manufacturing, with a number of brand new cloud-based applications looking to unseat the status quo of popular desktop modelling tools.\nAs the AEC industry moves at a much slower pace, developers of manufacturing CAD software are testing out their new modelling tools that are delivered via cloud technology. Here Autodesk has Fusion360, which is its next generation 3D modelling tool for engineers. Ultimately Fusion360 will replace Autodesk\u2019s previous generation manufacturing application, Inventor, at some point in the future.\nInventor is a Windows-only 3D modelling application that only works on a desktop PC. Fusion360 is a cloud-based application that resides on a server and is accessed from a 200MB desktop thin-client on Mac OS or Windows. It will also be available via a web browser from any machine or operating system soon.\nA Fusion360 subscription is just $25 a month. As Inventor is a mature product (released 1999) and Fusion360 is new (released 2013), written from the ground up, Inventor still does a whole lot more than Fusion does. But what Fusion does, it does better and faster.\nGiven Revit\u2019s age (launched 2000), huge models and well known performance \u2018issues\u2019 and Autodesk\u2019s future cloud vision, we have been eagerly waiting for something similar to happen in Autodesk\u2019s AEC development.\nWatching Fusion360 make leaps and bounds these last two years has seemed like an eternity. AEC Magazine has now heard from several trusted sources that development work for a next generation cloud-based BIM tool is underway and will be with us \u2018sooner, rather than later\u2019.\nThere are very few, if any details, available at present but Fusion could be a template to look at should you be interested.\nWhen it came to creating a new 3D cloud-based modelling tool for manufacturing Autodesk chose to start from scratch and Fusion360 had a thin-client \/ server architecture, and now has an in-browser option (see Project Leopard). But with Revit, I would be very surprised if Autodesk did a complete ground up re-write to make it cloud-deliverable \u2014 even though that is what I believe Revit needs.\nAutodesk realises that the AEC market moves at quite a slow pace and people are quite conservative. To come out with a radical new product that could confuse all their customers would be risky.\nFor the first few years, Fusion did not really offer an alternative to what was available on the desktop, just a few key capabilities. With Revit, Autodesk may look at a way to get what is available today but host it online, offloading some of the hardware problems by splitting off the Revit Database (as seen in Project Skyscraper) and having a thin-client style interface as Fusion has.\nSaying that, it may surprise us completely and create a new Revit style product. A cloud-based BIM tool would also liberate Revit from Windows and finally bring it to the Mac. The benefits of having a BIM tool hosted in the cloud would be huge for collaboration, with datasets being available to all project participants via the BIM360 service, as well as hosted management, rendering, analysis and benefits on-site. And instead of a beast of a machine with 32GB of RAM, you might be able to get away with an iPad Pro! It is at this point I must seem like a child feeling his Christmas presents a little bit too hard.\nConclusion\nFor now, most of my thoughts are on what the Revit team will deliver given the strong rumour of a Fusion360 cloud-delivery template. We have already seen cloud-delivered Revit sessions in the way Autodesk demonstrates forthcoming versions of Revit such as \u2018Sunrise\u2019, which became the R2 release.\nWhile I am sure many customers would prefer to have the current system completely replicated and delivered, I would be more impressed if it was a ground up rethinking of what a modern BIM tool should be.\nThe more advanced BIM practitioners are crying out for better tools and are fed up of work-arounds. The whole concept of breaking down models in projects is because of the limitations of Revit to handle large files. The need for expensive networking and compressors to shunt huge models data around is also because of the software architecture. A move to the cloud should eradicate all this. Instead of hardware bottlenecks, large firms will need robust and thick internet connectivity.\nTo just take the existing code and host it online would be a travesty, not to take the opportunity to re-architect and modernise the code to perfrom better and make use of the cloud infrastructure.\nIt\u2019s an important port as it\u2019s the most important BIM platform out there and could impact hundred of thousands of users.\nThis week Autodesk is hosting its annual Autodesk University in Las Vegas, where more details of these developments may be announced. AEC magazine will be there to bring you all the latest news.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nWhat else is going on at Autodesk?\nComputation design\nAutodesk is keen to rapidly develop and link its products to the Dynamo computational design engine. Integrating FormIt with Dynamo is the first stage of enhancing conceptual design with computational-driven forms. Again this echoes Autodesk\u2019s manufacturing division\u2019s vision of providing optimisation and generative solutions for structural designs in products.\nIn the time it takes a designer to model one solution to a problem, with generative and evolutionary design tools, the computer could suggest 50 or 100 geometrical variants, which would solve the design criteria. We are not that far off from when the computer actually aids the design process, as opposed to just helps document it. Expect a deeper integration of Dynamo in all products as well as more cloud-based computational tools.\nFabrication\nWith modular construction becoming increasingly popular, Autodesk is aware that its AEC products need to be able to link to and drive fabrication machines. These could be laser cutters, robots, CNC milling machines, or concrete 3D printers.\nAutodesk has a team dedicated to developing solutions and interfaces for core products such as Revit to \u2018play friendly\u2019 with this emerging, connected, modular, fabrication market. It seems convergence is not just for software development but also for industries, as \u2018digital manufacturing\u2019 becomes the end goal for all professions.\nBIM360\nAutodesk BIM360 is the cloud backbone to connect Autodesk\u2019s AEC products and customers. It currently consists of four products, Glue, Field, Plan and Building Ops. Glue is like an online Navisworks for model viewing, analysis and markup. It works in the browser and on an iPad and drives the BIM360 Layout App, which links BIM to a robotic total station for on-site layout.\nField links on-site workers to the latest design information for collaboration and reporting. Plan is a web service for creating and tracking project work plans, while Building Ops is a mobile facilities management asset management service.\nComing soon to this mix is Project Alexandria, Autodesk\u2019s latest attempt at delivering a document management system.\nAutodesk has not had much success at developing a enterprise-wide document management systems, with a few historic failures under its belt. Recently it has fared better with the likes of Buzzsaw and Vault but still does not compete with the likes of Bentley ProjectWise.\nProject Alexandria looks to be the company\u2019s most serious attempt to date to provide a project-based management layer. It will be based within BIM360 as a paid-for service. Project Alexandria will manage both 2D and 3D data, from multiple applications. Document viewing, markup, approval and sign-off workflows can be created together with some degree of automation. As it is in the cloud access can be from anywhere \u2014 on-site, on a laptop or phone \u2014 and it appears to have a very clean user interface design.\nWhile the codename Alexandria is more than a hint at the historic library in Egypt and one of the seven wonders of the world, one hopes that the product does not share a similar fate.\nMemento and Recap\nThrough acquisition and development Autodesk has created a number of laser scanning and photogrammetry applications and capabilities for all vertical markets, product design, architecture, surveying, manufacture, construction, films, and games. The two main products it is championing are Recap and Memento.\nRecap supports photogrammetry and laser scans but is probably more targeted at the surveying and wide area data capture field, which is dominated by very expensive, difficult-to-use applications, mainly from laser scanning vendors.\nRecap is a relatively low cost solution that can work on a tablet and show in real time the results of collective scans. Unfortunately, despite a soft release, the product suffered some negative feedback with a number of early technical issues but with subsequent releases, the team is really starting to get the product in shape.\nMemento is a generic application that again works with point clouds or photogrammetry data, offers a very minimal interface but can pull off magical results interpreting gigabytes of data.\nAutodesk has used Memento on a number of prestigious projects for data capture for international museums, archeology, environmental, gaming and reverse engineering. It is a \u2018Swiss Army knife\u2019 for creating rich project models from masses of dumb data. This video will blow your mind on the possibilities of photogrammetry.\nForthcoming capabilities will enhance the resolution of taking these huge \u2018life-size\u2019 models to 3D printers enabling prints with details down to 10 micron. As Autodesk has its own 3D printer, the Ember, the company has been refining its scan-to-print, to preserve the detail captured at high resolution. The market of reality computing is still embryonic but all it takes is for designers to realise the potential of this new capability.\nSkycatch\nAutodesk has just announced a strategic investment in US Unmanned Aerial Vehicle (UAV) company Skycatch, which it hopes to support with Autodesk Recap.\nSkycatch is an autonomous drone that can fly around building sites at regular intervals on set flight paths, capturing structures and landscapes with video. The drone can return to base, drop the battery and stored video, reload with a new battery and memory and refly the route. Autodesk can take this image data and create accurate 3D survey models of building sites. It could also be used to find materials, check building progress and create 4D timeline models.\nSkycatch transforms the way sites can be surveyed and monitored. Autodesk ReCap can be used to process the collected data.\nAutoCAD\nAs a general rule we tend not to write much about 2D and AutoCAD, however it is still a significant contributor to Autodesk\u2019s income and makes up for a considerable percentage of usage within Autodesk Suites.\nAutodesk wants to improve the way its software upgrades on customer\u2019s desktops. As opposed to having to delete the previous installation and reinstall, or perform a partial update to the code, which can damage customisations and tool layouts, Autodesk is investigating ways to stream updates to the software without negatively impacting the system. Here, AutoCAD is the guinea pig and advances made will be shared throughout Autodesk\u2019s other deskbound applications.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/workstations\/nvidia-launches-ampere-based-quadro-rtx-a6000-gpu\/","title":"Nvidia launches Ampere-based Nvidia RTX A6000 GPU","date":1601856000000,"text":"Professional GPU promises 2x performance increase over RTX 6000 for GPU rendering and other graphics workflows\nThe new Nvidia RTX A6000 workstation GPU promises to deliver more than double the GPU rendering performance of its predecessor, the Quadro RTX 6000. In the product design-focused rendering software Luxion KeyShot the performance increase could be 255% or even more, says Nvidia.\nUpdate 22\/2\/21 \u2013 Read our in-depth review of the Nvidia RTX A6000\nProfessional users can also expect a significant boost in graphics, VR, and AI workflows in sectors including product design, automotive design, and architectural visualisation.\nThe Nvidia RTX A6000 is the first professional GPU to be built on the Nvidia Ampere architecture. It is also the first to support PCIe Gen 4, in workstations such as the Lenovo ThinkStation P620, which features the new AMD Threadripper Pro CPU.\nThe Nvidia RTX A6000 is expected to be the top-end model in the range and will be available in December. Replacements for the mainstream Quadro RTX 4000 and RTX 5000 will likely come next year but Nvidia would not be drawn on any details.\nThe Nvidia RTX A6000 features 48 GB of GDDR6 GPU memory, double that of the Quadro RTX 6000, plus significantly improved processing cores, delivering a \u20182x performance per watt increase\u2019.\nAccording to Nvidia, a total of 84 second-generation RT cores deliver up to 2x the throughput of the pervious generation, plus concurrent ray tracing, shading and compute. Meanwhile, 10,752 new CUDA cores deliver up to 2x the FP32 throughput for significant increases in graphics and compute, while a total of 336 third-generation Tensor Cores provide up to 5x the throughput in AI workflows.\nOther specifications include four DisplayPort 1.4 connectors, support for Framelock and NVlink, and a max power consumption of 300W.\nEarly adopters of the Nvidia RTX A6000 have reported significant benefits in a range of workflows. Car manufacturer Renault, who uses Quadro to review design concepts in ray-traced photorealism, has reported performance gains of over 2x with ray-traced exterior scenes.\nPredator Cycling, which specialises in custom carbon bicycles, has seen performance gains of 2-6x across a number of key applications, including Luxion KeyShot and Ansys Discovery CFD. \u201cThe real power is that our team can run CFD analysis, be modelling and screen-sharing on a video call, all simultaneously,\u201d said co-founder Aram Goganian.\nKohn Pedersen Fox, one of the world\u2019s largest architecture firms, has noted the ability to triple the resolution and accelerate real-time visualisation of complex building models in cityscapes.\nThe Nvidia RTX A6000 is focussed very much on graphics. It does not have accelerated double precision performance, which is important for applications including engineering simulation. Nvidia told DEVELOP3D that for customers that need double precision there\u2019s the Quadro GV100 or Nvidia A100 for the data centre.\nThe Nvidia RTX A6000 is primarily a GPU for desktop workstations, but support for vGPU Software including Nvidia GRID, Nvidia Quadro Virtual Data Center Workstation, and Nvidia Virtual Compute Server, will come next year. vGPU profiles will be available for 1 GB, 2 GB, 3 GB, 4 GB, 6 GB, 8 GB, 12 GB, 16 GB, 24 GB and 48 GB.\nNvidia has also released a new datacentre GPU, the Nvidia A40, which is almost identical to the Nvidia RTX A6000, but is passively cooled.\nNvidia RTX A6000-based workstations are expected from BOXX, Dell, HP and Lenovo and others, while Nvidia A40-based servers are expected from Cisco, Dell, Fujitsu, Hewlett Packard Enterprise, Lenovo and more. Pricing was not disclosed.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-university-2017-report-part-1\/","title":"Autodesk University 2017 report #1","date":1512345600000,"text":"At the first Autodesk University in Las Vegas under new CEO Andrew Anagnost, delegates got a taste of the company\u2019s vision and focus for its current and next generation of products. Martyn Day of AEC Magazine attended the event\nAutodesk University in Las Vegas is probably the biggest CAD event of the year, attended by over 10,000 customers, developers and resellers and hosted in one of Las Vegas\u2019s vast casino hotels. The four-day event offers classes from key Autodesk staff, as well as high-level product experts, along with selected keynotes from the Autodesk top brass and its most innovative customers.\nFormer CEO Carl Bass generated an explosion of development at the company, firmly switching it from a reliance on 2D products to a range of 3D tools, 3D printers, cloud-based apps, high-end CNC machining, mobile applications, educational software, biotech and even social media.\nUnder Bass, the company also started its transition from charging customers traditional perpetual software licenses with added annual maintenance charges, to suites of subscription-based products.\nRoutes to market also started to change, with Autodesk selling more direct and relying less on its army of VARs (value added resellers). The company actively engaged with the resurgent \u2018maker\u2019 movement and established BUILD spaces for their activities at major offices (including San Francisco, Toronto, Boston and Portland).\nAfter a run-in with investors who joined the board of the company, an agreement was reached in which Bass stepped down as CEO. A prolonged search for an external or internal replacement ensued, and in June 2017, Autodesk\u2019s board gave the job to the company\u2019s chief marketing officer and senior vice president of business strategy and marketing, Andrew Anagnost.\nAnagnost is a 20-year Autodesk veteran who previously worked at NASA Ames Research Center. Under Bass, he served as architect and leader of Autodesk\u2019s business model transition, moving the company to today\u2019s softwareas- a-service (SaaS) model.\nUnfortunately, in this reshuffle, Anagnost\u2019s main competitor for the top job, Amar Hanspal, another long-term Autodesk employee and senior vice president of products (that is to say, the executive pretty much in charge of all product development), chose to leave the company.\nWith Autodesk currently in an income trough, a side-effect of moving from perpetual licence sales to subscriptions, Anagnost has not wasted much time in reorganising the company, adopting a \u2018three horizons for innovation\u2019 methodology. The first horizon (H1) is for incremental, successful products (AutoCAD, Inventor, Revit); H2 is for next-generation core businesses (Fusion, Forge); and H3 is for future growth engines (Machine Learning, Artificial Intelligence, Bio and so on).\nThe consequences of this new strategy have been profound. Many software developments that had failed to gain traction or that were costing more to deliver than they were bringing to the company in terms of revenue have been designated \u2018end of life\u2019. These include many iOS applications that are no longer developed, with the exception of Sketchbook Pro.\nAnd, at the time of writing, the company has announced plans to cut some 13 percent of its workforce, or 1,150 jobs. Anagnost says that every penny saved from these lay-offs will be reinvested in the company\u2019s digital infrastructure and construction focus.\nFor customers, one of the key benefits that they might see from the organisational changes underway is the removal of vertical market silos. For many years, Autodesk has tried to get its software suite to \u2018speak the same language\u2019, so that information can be shared seamlessly between each application. Industry silos inhibit these efforts, but a flat structure, combined with the cloud-based Forge back end could help Autodesk to finally achieve its unified data nirvana.\nAnagnost\u2019s keynote\nThis year\u2019s Autodesk University was the first time Anagnost could publically deliver his vision of the future of design technology and the company\u2019s key areas of focus for the next few years.\nPrevious keynotes from Autodesk CEOs have tended to look much further out into the realms of science fiction, rather than on near-term deliverables. Anagnost, in contrast, gave an insightful address on the benefits that automation will bring to society and on how it will change jobs and the nature of work. While many people are concerned about robots in the workplace and the prospect of Artificial Intelligence replacing knowledge jobs, Anagnost pointed to data from past revolutions, which without exception, led to the creation of new jobs. In the case of the digital revolution, for example, the introduction of the PC was seen as a threat to secretaries, but the reality was that personal processing power made many new jobs possible.\nAnagnost argued that robotics and Machine Learning (a branch of Artificial Intelligence) should not be feared, but instead embraced. At the same time, however, the existing workforce should look to re-educate and upskill, in order to make the most of this opportunity.\nIn the field of design, Autodesk is leading this AI charge, with projects like Dreamcatcher and generative tools like Dynamo, which help engineers and designers create high-quality designs, guaranteed to meet customer needs every time. In other words, AI-powered design tools hold out the promise of enabling design teams to create more designs in less time.\nIn the past, Autodesk has tended to focus on the manufacturing space, with the development of products like Fusion. But Anagnost stressed that he has an expanded view of where Autodesk should be focusing next. In some ways, linking the past vision of manufacturing with a future vision of digital construction, where the company\u2019s AEC tools are heading, could drive the next generation of building through digital fabrication. This is a much more balanced view for a company that has its eyes on so many markets. As a result, attendees from the construction space were given a lot more food for thought than in previous years.\nNever one to mince his words, Anagnost told AutoCAD LT users that they should not bet the future of their company on 2D. The message was clear: if they do, they are betting on the wrong thing. Instead, he urged customers to explore the value that comes with products and services delivered on a subscription basis, rather than stick on perpetual licenses. That makes sense: Autodesk is now developing new machine learning functionality that can only be accessed via the cloud. Older products, now taken off maintenance, will not benefit from new functions like this.\n#\nProject Quantum & Forge\nFor those who have followed our articles on Autodesk\u2019s Project Quantum closely, this year\u2019s Autodesk University may have come as something of a disappointment. Quantum was only mentioned once and only then to say that it would be included in BIM 360. It turns out that by mentioning Quantum and \u2018productifying\u2019 the term, Autodesk became sensitive that it was creating demand for an application that was never intended. Quantum is, in fact, a range of services that will be added to Forge and BIM 360, with some of the underlying collaboration capabilities being made available to other Autodesk products. AEC Magazine hopes to bring you more information on what has become of Quantum and the technologies we have written about in the not-too-distant future.\nWhen it comes to Forge, much time and effort at Autodesk has been spent, with little to show. This year\u2019s Autodesk University, by contrast, was dramatic in terms of the number of application developers who had embraced the Forge environment. In the past, Autodesk developed products such as AutoCAD, with APIs (application programming interfaces) that developers would use to access data created in these programs to perform tasks such as rendering, document management, symbol libraries, raster-tovector conversion and so on. With each application, we had a different API that required the presence of the software to operate. Additionally, if you really wanted to create a complete, tailored application, you would have to license the underlying application (the original versions), which was very expensive.\nForge is a total rethink of this. Key components that are common to Autodesk\u2019s applications have been written as services, which sit on the cloud backbone. These would include DWG, rendering, document management, analysis tools, import export, data translation, reality capture, 3D print optimisation, generative design and so on.\nThrough APIs, these underlying services become accessible to developers, who can mix and match them to quickly create bespoke applications with complete file fidelity and easy expansion not only to Autodesk services, but also through webhooks to other services, such as Dropbox. These capabilities are also open to Autodesk customers, who can develop applications for their own bespoke solutions.\nAutodesk continues to flesh out Forge\u2019s capabilities. In March 2018, the company will add a range of \u2018IO\u2019 capabilities \u2013 AutoCAD IO, Revit IO, 3ds max IO. These are headless (interfaceless) cloud versions of desktop applications and will enable developers to access, edit and create entities in models and drawings that are stored on Autodesk\u2019s cloud services. The promise is that this will fundamentally change the way that thirdparty applications work, relying less on desktop applications and more on data and where it resides. It is an additional incentive for customers to move to cloud products, such as BIM 360, where storing data in the cloud can open up new possibilities to leverage model analysis and machine learning.\nThat doesn\u2019t mean that desktop applications are going away. Authoring tools will still be required but, over time, there will be more benefits to storing project data on the cloud for collaborative workflows.\nForge Fund invests in construction\nTo help developers get on board and start working in the Forge environment, Autodesk previously announced a multimillion dollar fund. The lion\u2019s share of this, however, went towards the manufacturing space. With the new focus on construction, Autodesk has systematically invested in a number of start-ups this year, including SmartVid.io, ManufactOn and Project Frog.\nAt Autodesk University, the company announced the fourth investment in five months, by leading the funding round for Assemble Systems. This company provides a SaaS platform that consumes BIM models, drawings and point clouds, enabling construction professionals to condition, query and connect the data to key workflows.\nTools cover bid management, estimates, project management, scheduling and finance, in view of the way that adoption of BIM and cloud technologies continues to accelerate across the construction industry. Assemble Systems will integrate its technology with Forge and will strengthen BIM 360\u2019s pre-construction data management, quantification, estimation and other associated workflows.\nAssemble extracts and federates models from Revit and AutoCAD and other design systems into projects. This enables the customer to group, sort and filter data into usable construction packages, to be used for quantification, estimating, shared subcontractor views and other workflows. As design models change, Assemble checks for those changes and reports on the impact to the project quantity, cost and schedule.\nESRI partnership\nOn stage at Autodesk University, Autodesk and ESRI announced a new strategic alliance to advance infrastructure planning and design. This partnership will build the bridge between BIM and GIS mapping technologies. Combining their technologies, Autodesk and ESRI are looking to enable a broad range of industries to gain better context by visualizing data of the man-made world, the environment, its residents and the networks that weave these all together.\nEsri is a major supplier of geographic information system (GIS) software, with an estimated 43% share of the global market. It is headquartered in Redlands, California, with annual sales of some $1.1 billion.\nThis partnership will result in the integration of Autodesk and ESRI technologies, allowing industry professionals to synthesise information from both BIM and GIS, to enable a more connected infrastructure.\nBenefits will include reductions in permissions, through improved stakeholder engagement; more sustainable and resilient design, through enhanced project insight; and reduced risk, through improved end-to-end flow of materials, resource availability and scheduling during construction.\nConclusion\nAutodesk may appear to be a company in flux. Anagnost is making radical changes, while it simultaneously goes through a subscription-related trough, as well as lay-offs, and aims for a 50\/50 direct sales\/reseller model.\nAt the same time, the company needs to keep an eye on delivering value for the subscription money on which it now relies. This year\u2019s decision to add even more products to its subscription collections, while welcome, does not necessarily mean that customers will derive any extra value from their participation. In our interview with Anagnost back in June, he acknowledged that there were ongoing issues and asked for a year to really deliver on subscription value for existing customers.\nBy getting rid of its silos, the com pany now better resembles the flat structure of its new, cloud-based applications. Hopefully, it will finally get interoperability nailed and its collections will s tart to work more like products in the Adobe Creative suite. Meanwhile, Forge is now starting to make more sense and deliver on some of the promises that the move to the cloud could bring. On the exhibition show floor at the event, a number of developers were showcasing Forgebased applications, giving an idea of the type of capabilities we can expect in future. We suspect this will bring a renaissance to Autodesk\u2019s developer network and, with some very visual dragand- drop tools in development, could open up customisation to a new generation of users.\nIn the second part of this report, we look at core developments within Autodesk BIM360 and analyse the AEC keynote at Autodesk University, which provided insight into the company\u2019s future vision for constr uction with digital fabrication.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/soaring-through-the-cloud\/","title":"Soaring through the cloud","date":1279584000000,"text":"Point clouds and 3D laser scanners are in the news at the moment, with both Autodesk and Bentley now offering native access and manipulation of scanned data. Yet it is a small UK developer that is acknowledged as having the fastest point cloud technology.\nThis year\u2019s visit to the SPAR 2010 conference on 3D imaging and positioning for the engineering construction and manufacturing markets proved one thing, that this is a technology in its ascendancy. While 3D laser scanning is not new and is not cheap, the productivity benefits of using it in architectural, plant and civil engineering are becoming more widely understood.\nThe wave that the scanning industry is currently riding does not really come from the hardware segment but from the increasingly broad access to load and manipulate 3D cloud data. Recently Autodesk announced that it had introduced point cloud capabilities into its latest release, AutoCAD 2011. At the same time, Bentley signed a licensing deal to include the fastest point cloud engine in town to MicroStation. Literally millions of designers will soon be able to load scanned data in their native CAD systems and mix and match vector geometry with data collected from the real world.\nBentley\u2019s partner of choice is a UK developer called PoinTools. If you ask around, even most of PoinTools\u2019 competition will admit that it wins on speed; with the ability to load millions of points and fly through a model in real time \u2013 and with today\u2019s high resolution scanning technology, it looks just like flying through an incredibly accurate CAD model.\nHowever, it is not just the end result that is breathtaking, for architects and engineers the survey and data capture can lead to massive productivity benefits. A survey that would have taken four weeks to complete can be done in one day with a 3D laser scanning crew. Although to make the most of this, the designer needs tools to convert or reference these point cloud models. This is why both Bentley and Autodesk have almost simultaneously anticipated the growing demand.\nProduct development\nPoinTools has a number of products and integrations. PoinTools Edit and PoinTools View Pro are stand-alone products, while for AutoCAD there is PoinTools Model, and McNeel & Associates\u2019 PoinTools 4 Rhino works with the popular Rhinocerous modelling engine.\nWhile Autodesk now has its own point cloud engine inside AutoCAD, joint founder of PoinTools Faraz Ravi explained that is limited in that it only displays 1.5 million points maximum, which restricts the amount of detail that can be displayed.\nPoinTools has no such limit. There is a new version of PoinTools Model for AutoCAD which is just starting a beta cycle. This new version does not make use of the new Autodesk engine.\nIn addition to these finished products, there is also PoinTools Vortex, which is an Application Programming Interface (API) and is the technology that Bentley has included in its latest MicroStation solution. For those that purchase a scanner, it is also very possible that the PoinTools products will be included in the hardware bundle, with partners such as Faro.\nPoinTools has traditionally been about visualising, measuring and animating the point cloud data as fast as possible. Mr Ravi prides his company\u2019s work on the ability to load huge datasets and quickly examine and display these points. PoinTools View Pro is the latest version of this core product, featuring real time clipping, accurate measurement, rendering, animation, stereoscopic viewing and annotation.\nPoinTools Edit is the platform for manipulating, cleaning and re-purposing these huge scanned models. Edit is the place where noise and unwanted data can be easily removed, colours corrected and data sorted onto layers for faster downstream processing. PoinTools also has some great utilities to break down huge models into workable cells points, allowing big jobs to be segmented and distributed among teams.\nTo date, PoinTools has not offered solutions to covert the point clouds into surfaces and solids, as there are many other solutions that have the intelligence to do this (such as Geomagic \u2013 www.geomagic.com), however there is increasing demand for this kind of technology.\nIn the driving seat\nWhile PoinTools can name multiple big companies that use its products (Arup, Ordnance survey, Halcrow and English Heritage), the most interesting current user is Ford Motor Company. Ford is currently scanning all of its manufacturing installations to ensure it has full 3D representations of its factory floor and assembly lines. The company has detailed 2D plans of its facilities, however it recognises that what was designed is very different to what was installed. It would be a waste of time to model each plant in 3D CAD, but by using a scanner and PoinTools, it is able to capture a plant in very little time and then have an accurate 3D representation of what is actually in-place.\nThis approach comes into its own when a new car is under production and changes may need to be made to the assembly line or certain work cells in an existing process. Using apparently \u2018dumb\u2019 point clouds, Ford can import CAD geometry and perform clashes against the point cloud, or see if any new robot or equipment can reach necessary objects from proposed positioning. Design teams can test all this without visiting the site and can get sign off with this virtual modelling approach before any metal is cut, or parts ordered. This saves a lot of money and has already paid for itself multiple times.\nFuture technology\nMr Ravi sees this ability to bring data from multiple sources and different formats to be a key benefit of point cloud applications. When deciding to generate design information, the original sources may vary and capturing the existing design is probably quickest using a 3D laser scanner, while new components may well be modelled in a CAD system or just be dumb tessellated surfaces.\nPoinTools is increasingly opening up this approach and with customers like Ford, is refining new workflows and best practices to virtualise refit and renovation work. To me, this makes a lot of sense; I could never work out why things such as clash detection cost so much in the design world. Surely a product like PoinTools, with some enhancements, would provide a sturdy solution and most importantly, from multiple sources?\nMr Ravi told me that there were many new niches that the company was exploring in both the architectural and manufacturing markets.\nA looming problem of enhanced scanning technology is the size of scanned files. With each new generation of scanning the accuracy and number of points that can be scanned increase dramatically. What were huge files quickly grow to massive files and customers really do have a headache in manipulating and storing these. The \u2018cell\u2019 sectioning technology being developed is a result of this pressure, with new and intelligent ways to sort through the data always on the development priority list. Document management will be a harder nut to crack \u2013 imagine if all your company\u2019s word docs were Gigbytes each.\nConclusion\nThe PoinTools session that Mr Ravi held at the SPAR conference was pretty much standing room only and gave a great indication to just how important these tools are to the 3D imaging community, where data sets are huge and speed is essential. PoinTools\u2019 business outlook and pricing reminds me of McNeel & Associates\u2019 Rhino, in that you get a lot of bang for your buck, and competitive products appear to cost tens of thousands more. The community feel of the company and its ethos is strong and very passionate.\nThe other take away from the SPAR conference was that every company has resolved to use multiple software applications to do different parts of the process, causing all sorts of complications and unnecessary problems. Mr Ravi is keen to develop PoinTools into solving these data issues, while fundamentally maintaining the speed of data manipulation. There is also the hint that the next area of expansion will be in manufacturing and factory simulation and it seems that Ford\u2019s experimentation with scanning may ultimately provide a bigger benefit for us all.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bluebeam-software-launches-revu-2015\/","title":"NEWS: Bluebeam Software launches Revu 2015","date":1422921600000,"text":"Latest release of PDF creation, annotation, editing and collaboration tool aims to simplify document management and speed up project communication\nBluebeam Revu 2015, the latest release of the PDF-based annotation, measurement and collaboration tool, introduces new features that enable project teams to keep complex document sets organised and up to date. The new Batch Slip Sheet (eXtreme edition only) automates the process of keeping complex file sets up to date by allowing users to automatically match new file revisions with previous corresponding file versions. All annotations, hyperlinks and bookmarks are carried over from revision to revision.\nAn existing Revu feature, Sets, which allows teams to navigate a large collection of various PDF files as if they were a single multi-page document in a single tab, also receives automation and categorisation upgrades in Revu 2015.\n\u201cTo meet the demands of complex projects and condensed timelines, project teams need technology to simplify communication between project stakeholders and remove workflow redundancies,\u201d says Richard Lee, Bluebeam Software President and CEO. \u201cRevu 2015 presents and polishes features that make marking up and managing document sets easier with greater annotation tool functionality and batch automation, as well as more flexibility with how project information is shared.\u201d\nSketch Tools is a new feature that allows users to quickly create new shapes to exacting measurements based on a PDF\u2019s calibrated scale. Users are given real-time length and angle feedback as they sketch details with their mouse or enter dimensions using their keyboard.\nA Dynamic Tool Set Scaler enables annotations to automatically resize precisely and proportionately when used on documents of different scales. Grouped annotations in a drawing can also resize proportionately, regardless of scale setting.\nVideos can also be embedded into annotations using Revu\u2019s Camera tool, Capture 2.0. Users can scroll through images and play back videos directly from each annotation\u2019s Capture viewer. Annotations containing Capture media can be tracked and viewed in Revu\u2019s Markups list and exported as a PDF Summary.\nBluebeam has also updated its mobile apps. Studio GO in Revu iPad 3.0 is said to enable large format files in Studio Projects and Studio Sessions to render instantly for quicker annotations and navigation, without modifying the original documents. With Background Sync, Revu iPad users can download files while they continue to work in Revu.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/reality-capture-modelling\/open-all-hours-trimble-beck-group\/","title":"Open all hours","date":1591660800000,"text":"Design-build firm overcomes the complex challenges of keeping an existing mall active while redeveloping it into an upscale mixed-use development\nFor over a century, The Beck Group\u2019s unique approach to architecture and construction has led to groundbreaking and award-winning work. Today, it is one of the largest integrated design-build firms in North America. More than 90% of the firm\u2019s projects serve repeat clients.\nOne of those clients is Simon Property Group, the owner of the South\u2019s premier luxury shopping destination, Phipps Plaza. Since acquiring the mall in 1998, Simon has relied on The Beck Group for multiple projects ranging from parking structures to interior spaces and fa\u00e7ade updates at Phipps Plaza. With years of successful projects between them, Simon approached The Beck Group in 2017 with its biggest project yet, to redevelop Phipps Plaza from a mall into a mixed-use, upscale property with a 150-room Nobu Hotel and restaurant, 13-storey office building, outdoor event venue and 90,000 square foot athletic facility.\nTo redevelop Phipps Plaza, The Beck Group would need to demolish an existing department store and parking deck to create a new buildable area in roughly the same footprint, all while keeping the existing mall active. In addition to a tight schedule that would minimise impact to retail revenue, the project presented its fair share of challenges. Existing utilities, pile caps and other structures created a below-grade obstacle course that would be difficult to navigate during planning and construction. At the same time, attaching new construction to the existing mall would require precision.\n\u201cNot only is this project extremely complex, it\u2019s also the first of its kind to repurpose an existing mall this large in North America,\u201d said Tim Riefenberg, BIM manager for The Beck Group. \u201cWe\u2019re a tech-driven firm, so it was clear early on that we would need the most advanced technology to help us overcome the challenges and deliver the project on time.\u201d\nA new dawn\nThe Beck Group wanted to use new technology on the project, so Riefenberg turned to BuildingPoint Southeast, a Trimble distribution partner who had provided hardware in the past.\nThe Beck Group adopted a constructible process that combined design, project management, and engineering models into a collaborative and data-rich platform centered around Trimble Connect for data sharing. Field-oriented technologies, including Trimble Field Link and two Trimble RTS773 Robotic Total Stations, provided the contractor with the flexibility to handle a broad range of layout and data collection.\nCollaboration and data sharing between the office and the field played an important role in the project from the start. In the field, Trimble Connect allowed The Beck Group\u2019s field crew to download and view data-rich 3D models in real-time on Kenai rugged tablets. \u201cThe ability to work from one source of data and extend the value of the model with Trimble Connect has been amazing,\u201d said Riefenberg. \u201cWe can control the information that goes out to the field from one point of contact. The project\u2019s electricians, drillers and mechanical and plumbing crews are all working from the same data-rich constructible model. Hands down, it has helped us eliminate errors and work faster with incredible accuracy.\u201d\nConnecting site and office\nTrimble Field Link with the RTS773 allowed the crew to easily pinpoint locations from the 3D model and also document existing conditions for real-time data sharing between the field and the office. \u201cAs our field crews were laying out the new structure, they were also capturing existing conditions that would provide critical insight for demolition of the existing structures,\u201d said Riefenberg.\nTrimble Field Link allowed The Beck Group to collaborate with the project\u2019s pile installer, Berkel & Company. Using the software, both crews could make changes that were automatically updated and shared seamlessly across project teams.\n\u201cWhen we made a change in the field, Berkel\u2019s crew would already have the updated file when they arrived on site the next day,\u201d said Riefenberg. \u201cWithout the technology, making a change would require us to estimate an RFI, wait for the architect and structural engineer to review the change and then communicate with the pile installer, which would take weeks.\u201d\nIn addition, each pile had a 3-inch tolerance, making accuracy extremely important during drilling. \u201cWorking from the same model across teams gave everyone the confidence to drill with precision in the field,\u201d said Riefenberg.\nThe project also presented significant challenges related to grading and elevation. There were no drawings to indicate grading throughout the site, so the team went to work to capture existing topography using Trimble Field Link and RTS773. \u201cWithout the robotic total station, our architects and engineers would have needed weeks for redesign, which could have delayed this phase of the project,\u201d said Riefenberg.\nA new reality\nTo help its crew visualise existing utilities and routing of new utilities, The Beck Group used Trimble Connect for HoloLens, a mixed reality solution for project coordination that provides precise alignment of holographic data on the job site, enabling workers to review models overlaid in the context of the physical environment. \u201cIt was incredible to walk through the site and visualize the civil drawings overlaid in the physical space. Our crews could see existing utilities and where new utilities would be routed at the same time. It helped our crew better plan for excavations and avoid costly mistakes.\u201d\nIn order to ensure that The Beck Group maximized its use of Trimble\u2019s tools and technology, Building Point Southeast worked closely with the project engineers, \u201cBuildingPoint Southeast has become a trusted provider for us, making sure that we have the best Trimble products for the job and helping to keep the project on schedule,\u201d said Riefenberg. \u201cThey came to the jobsite and did an amazing job training our crew and making sure they were comfortable using the robotic total stations, tablets and other Trimble solutions.\n\u201cThey also provided insight on the best way to set controls and where to place our prisms. In just two days, our engineers were using the RTS773 in the field and training others on our team with all they learned from BuildingPoint Southeast.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/geolocator-version-1-0-launched-by-digital-reasoning-systems\/","title":"GeoLocator version 1.0 launched by Digital Reasoning Systems","date":1165968000000,"text":"Brentwood, USA, 12 December 2006 \u2013 Digital Reasoning Systems Inc., the intelligence-software innovator, has announced the release of GeoLocator version 1.0.\nGeoLocator is a precision-based tool that extracts populated places from unstructured text, while providing their respective geo-coordinates. GeoLocator finds locations in a body of unstructured documents. Once GeoLocator determines a given term is a location, it aligns that location with GIS coordinates and outputs the aligned location using industry-standard Extensible Markup Language (XML).\nThis use of XML allows for easy integration into existing systems and workflow. GeoLocator works out-of-the-box with NASA World Wind mapping software, allowing for visual analysis and map-based linkages with other data.\nMap-based visualization systems require GIS coordinates in order to accurately tag a map with precise location information. This, however, leaves a critical hole in those solutions when it comes to unstructured data.\nCertain analysts estimate that on average only 5% of unstructured data in the IC has geospatial coordinates embedded in documents. The remaining 95% of documents which mention locations do not contain proper GIS coordinates. Intelligence analysts must manually mark-up each of these documents to extract the locations and align these locations to proper coordinates. This is a time- consuming and painstaking process.\n\u201cWe know from our customers and partners that extracting and generating usable geospatial intelligence from unstructured data is a key problem for many operational concerns in the IC and other related agencies. Our approach marries the unique and powerful technologies behind our unstructured data analytics engine with effective geospatial reasoning logic to produce precise results on noisy data without substantial investments in customized training and modules that only tend to function well on a subset of a customer\u2019s data,\u201d said Tim Estes, Chief Executive Office for Digital Reasoning Systems.\n\u2013 About Digital Reasoning\nFounded in 2000, Digital Reasoning is a privately held company based in Brentwood, Tennessee. Digital Reasoning provides solutions that enable organizations to reduce costs as well as increase revenue and customer satisfaction by mining and analyzing the over 80% of data that is unstructured and currently untapped. Digital Reasoning is headquartered in Brentwood, Tennessee, USA. For more info visit: https:\/\/www.digitalreasoning.com\/","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-gallery-paris\/","title":"Autodesk Gallery Paris","date":1416355200000,"text":"Autodesk gallery showcases a snapshot of innovations currently shaping design from small-scale jewelry to giant infrastructure projects.\nAutodesk\u2019s One Market headquarters in downtown San Francisco is charged with inspiring innovation. It is used for monthly, themed \u2018Design Night\u2019 events and engagement with start-ups and the local community. It also acts as a design gallery for the display of customers\u2019 work.\nOutside the Bay City area, and in Europe in particular, however, \u2018Autodeskers\u2019 have long complained of a lack of similar outreach facilities. But in October Autodesk capitulated in style, opening a pop-up gallery in the heart of Paris. The free three-week event was held at the Galerie Nikki Diana Marquardt on the edge of Marais.\nAutodesk branding was kept to a minimum, alongside a combination of interactive and static displays. The space had a 3D print zone and Fablab, an artist in residence and evening events. It showcased customers\u2019 work in architecture, product design, fashion, automotive, aerospace, 3D printing and laser scanning. Autodesk\u2019s top brass flew in for customer meetings and evening events with talks on a wide variety of design topics.\nWith over 1,500 visitors in the first four days, Autodesk is considering other pop-up galleries in Germany, Italy and the UK. Local projects were prominently displayed alongside attractions from Pier 9, which is likely to be replicated at future events.\nWelcome, Spark\nVisitors were given a chance to try out Autodesk\u2019s new 3D printer, Spark, which was designed in the UK and made at Pier 9. Autodesk is using Spark as a testbed for its own 3D printer software development and to promote its brand. It is expected that at some point, fabrication will be moved to a provider with great capacity.\nPop into 3D printing\nAutodesk gallery showcased a snapshot of innovations currently shaping design from small-scale jewelry to giant infrastructure projects. It was inspiring and educational and, above all, there was no hard sell, which made it a relaxing entry point for industry folk and the public alike. It is a shame the pop-up element means that it will never be in town for long. One wanders what ever next for Autodesk? From this successful gallery outing, I would expect another to pop-up sometime soon.\nHyperGreen Tower\nThe first thing attendees saw on entry to the gallery was a highly detailed 3D printed model of the HyperGreen Tower, by Jacques Ferrier Architectures. Standing 246 metres tall, the tower design has a curved, concrete, exterior lattice facade that acts as the building\u2019s primary structural system. The design featured solar cells, wind turbines, a geothermal heating system and a rain water harvesting system.\nSan Francisco Museum of Modern Art\nThe highly impressive San Francisco Museum of Modern Art expansion building took up a whole wall with a fa\u00e7ade relief specially created to show the design\u2019s styling and innovative fabrication techniques.\nSmoke Dress \u2014 Anouk Wipprecht\nMs Wipprecht\u2019s 3D printed dress has built-in LED lighting. Should you stand too close to the wearer, the built in sensors trigger the lights to flash, warning the interloper to move away. Should this fail to get a reaction, the dress emits a cloud of dry ice smoke from a necklace\/tube. This is either for the wearer to escape into the fog or to irritate the \u2018close talker\u2019.\nAirbus concept plane\nAirbus set out to revolutionise the structural design of aircraft.\nNike analysis and 3D printed trainers\nHacked 3D printer and tattoo machine\nRenault Twizy\nMakers\nRobot programming\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/digital-twin\/data-ownership-in-a-collaborative-construction-industry\/","title":"Data ownership in a collaborative construction industry","date":1600128000000,"text":"If a digital twin \u2013 or in the future, National Digital Twin and smart cities \u2013 is all about connected data ecosystems, what happens when data ownership and management are brought into the equation? Who owns the data, and how do we ensure the information can still be accessed in 10-30 years\u2019 time. Stuart Bell makes a case for greater clarity around data and accountability, stressing the need for companies to be clear on whose role it is to maintain it\nThe construction industry is highly complex and slow to change. With the industry facing skills shortages and anaging workforce just as the country needs critical infrastructure investment to boost the economy, the ways in which we design, procure and construct are under pressure to evolve.\nFor the industry to confront these challenges there is a general agreement that the way in which it works needs to change. A culture of genuine collaboration and the sharing of information must happen, and digital technology could be the answer. Digitisation is helping project teams and asset owners drive efficiencies through more effective modelling, design, procurement, mobilisation, construction and operation, yet amongst this there is a great deal of confusion as to who owns data.\nThese issues should be addressed at the earliest stage of the project, within the contract along with any interoperability expectations. Generally, as it is the client that is procuring the information, they must ensure they specify responsibilities or have the resources and knowledge to manage their model to maintain it. If we look into the not too distant future, it may will be that autonomous systems and robots will be using it for operations and maintenance. Document naming conventions and standards create a futureproof information management framework and are fundamental to the creation of scalable, interoperable ecosystems. The information needs to be updatable and maintainable to feed the digital twin, otherwise it will be a stagnant model.\nSecurity is another significant consideration. If we have a connected data ecosystem that shares data, the client again has to specify what data is required to be shared, and ensure data is managed correctly to prevent risk from supplier insolvency as well as cyber threats.\nWhat about asset owners insourcing data?\nGenerally speaking, the fragmentation of the construction supply chain complicates data ownership, as at an information management level there can be a great deal of disconnection. When it comes to maintaining a full, clear audit trail of communications, it is incumbent upon the contracting parties to perform this duty, subject to their contractual responsibilities. Yet where there is disconnection, piecing together disparate datasets in order to identify them is all the more complicated, making for a complex tapestry that takes months to complete.\nPushing the responsibility of information management onto the supply chain is not a new phenomenon. This behaviour started back in the 90s with the advent of the Private Finance Initiative. This specific public procurement route shifted responsibility out onto the construction supply chains; meaning they were in charge of designing, building and operating privately financed capital projects. The government consequently cut their members of staff (construction information management professionals) who specialised in this field and outsourced responsibility to the fragmented construction industry.\nMoreover, ahead of the 2016 BIM Level 2 mandate, government advisors noticed the significant inefficiencies in the industry, and suggested digitalised processes and standardisation around common naming conventions would allow 40% of construction project costs to be saved. Whilst ideal in theory, in reality it was problematic to force the industry to take ownership of a 40% cost saving, especially when the industry is perpetually working off tight margins. To alleviate the burden of this mandate the industry cut corners, which had all sorts of negative implications on building delivery and quality.\nWhere outsourcing becomes a problem\nIf project and asset data is fragmented and then for some reason compromised e.g. the principal contractor falls into administration, all of the project information which is hosted and managed is locked-in, and clients and other project parties are locked out from accessing it. The onus is then on the asset owner (client) and not the contractor to retrieve and re-procure the thousands of datasets from different parties involved on the project. As an alternative to re-purchasing the data from the design team or supply chain, the built asset may need to be re-surveyed to establish the required data, and this can incur further costs.\nWhat is lacking here is client ownership of a clear, accessible audit trail to ensure all transactions throughout a project and asset\u2019s lifecycle are maintained. Rather than outsource the information management service, wouldn\u2019t it be better for the asset owner to insource the data so they have access to the information and audit trail? Can pressures on the supply chain be lightened by providing a clear destination for data drops and information handover?\nWhat are the alternatives?\nMost organisations procure information differently and have multiple systems with different users managing data, and these are some of the main reasons why data can be so fragmented. Often it is better to have a single version of the data in one place to be accessed by many, as it avoids duplication and ensures each piece of data is in its truest and trusted form.\nProviding a single version of the truth for all project and asset data, a project and asset information management system or common data environment (CDE) enables information to be accessed and shared by all parties throughout an asset\u2019s lifecycle. A CDE enables all parties to securely communicate and collaborate on a structured workflow basis whilst simultaneously continuing a robust audit trail. It might be worthwhile to mention that on most major projects the Tier 1 organisation is often the prime and information management lead, provisioning the CDE on behalf of the client, ensuring project information is delivered, assured and approved prior to formal handover.\nWith all parties in the chain managing and exchanging information in a CDE, there will never be a missing link. With a clear audit trail of all asset data, the access to and provenance of all records is maintained. This means asset owners can question and query a trusted data store to answer everyday queries about assets in their estate at any stage.\nMake it contractual\nIf you use an office analogy, a CDE is essentially an electronic filing cabinet housing copies of audited information. Yet, asset owners have to be explicit in which information the CDE must contain, and must outline their individual requirements in the contracts to maximise the CDE\u2019s performance.\nAs an asset owner, at the start of the contract you will procure both the asset and a level of detail of the digital information. A well-written contract must be produced as this will act as a reference point for what the contracting teams have to deliver. It will also be a vital archive if and when maintenance upgrades are required.\nWith a contract which clearly outlines requirements, there will be a greater understanding of the purpose of the information and who owns it. When asset owners come to operate their buildings, this record will enable them to make proactive decisions, instead of turning to their supply chain to acquire information which should belong to them.\nAn asset-owned CDE is part of the change towards better data ownership; creating an unbreakable chain where every piece of data regarding a built asset can be retrieved at any time. With data fragmentation and unreliability key concerns for the construction sector, surely the adoption of a system which provides one version of the truth across a building\u2019s lifecycle is a valid solution to ending any disconnection?\nStuart Bell is director of sales & marketing at Business Collaborator from Bentley Systems UK\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/software-licensing-taking-control\/","title":"Software licensing: taking control","date":1586390400000,"text":"Fabio Roberti and Alexandros Bertzouanis of WilkinsonEyre explain how custom technology is enabling the global architectural practice to get valuable insight into its licence usage to help drive down costs\nSoftware licences represent a sig nificant investment for companies in the AEC industry, but most don\u2019t have a process to analysethe licence usage. Major software companies need to provide a robust solution for this issue which will help companies to control the licensing budget and plan future licence use. The solution needs to be viable for both single sign-on licences (single-users) and multi-user licences alike as many companies base their IT strategy on the multi-user scenario.\nIn February 2020 Autodesk announced a price increase for multiple products which includes the 33% increase for new licences in the multi-user AEC collection.\nWith the licence costs ever increasing, an insight into licence usage is critical for a business. This will help them understand and predict when it becomes necessary to purchase more licences, reduce licences or change the licence subscription. An example of this might be a migration from multi-user to single-users licences, driven on the difference in pricing criteria.\nThe technology in the AEC industry has improved signifcantly in the past years with multiple cloud services, virtual reality, augmented reality, generative design and optimisation tools being routinely employed, but it remains diffcult to prove the licence usage without the use of third-party plugins.\nWhile there are third-party plugins to analyse licence usage, an increasing number of companies are developing their own, bespoke applications. This option not only gives them greater control of licensing analysis but also allows for customisation and reduces the need to pay for a third-party plugin. The benefts of a plugin analysis tool are many, and include the following points:\n\u2022 Better management of software resources\n\u2022 Maximising licence utilisation\n\u2022 Software licence optimisation\n\u2022 Improve the project budgets\n\u2022 Identify when extra licences are required\n\u2022 Identify users with high licence consumption\nWilkinsonEyre presented a class at Autodesk University 2019 regarding licence management and unveiled a plugin to manage and analyse Autodesk licences. The source code for the plugin has been made freely available online and, with some coding experience, you will be able to utilise the plug-in. See: Autodesk University Class and Github.\nThe licence plugin was developed in collaboration with Alexandros Bertzouanis and with the support of IT Director, Christian Poulton.\nThe following two good practices help to optimise licence consumption.\n1) Licence Time-Out\nWilkinsonEyre advises introducing the licence \u201cTimeout\u201d of 30 minutes. If the people are not using the software, the licence goes back to the pool of licences and allows other users to access the licence.\nYou will need to open the \u201cadsk\ufb02ex.opt\u201d fle in Notepad to edit it. This fle could have a different name in your company. The text to edit is the following line:\nTIMEOUT 66800REVIT_F 1800\nTIMEOUT= Text code\n66800REVIT_F= Product Name\n1800= Number of seconds of inactivity. You can adjust this to whatever timeout you require.\nOnce initiated, the licence goes back into the pool when a user is inactive for the prescribed amount of time. When the person returns to use the Autodesk software, it will take a licence back.\n2) Multiple software packages\nCompanies purchase licences over time from multiple software packages such as single licences for AutoCAD, Revit Architecture, Building Design Suite or the AEC collection. The cascade licensing with multiple software packages may utilise more licences compared to having a single package such as the AEC Collection.\nData workflow\nUnderstanding the data work\ufb02ow will help you to replicate this process in your company. Data work\ufb02ow uses two data sources and three processes to manage data.\nData sources\n1) The primary data source is the FlexLM License Manager that will export the data log (see figure 1 below).\n2) The Revit Synchronisation is the data source to identify users in the projects. If you don\u2019t use Revit to collect this data, you can use Excel to create a list of projects and assign users to the respective projects.\nManaging data\n1) WilkinsonEyre has created two plugins to organise and push data to MySQL. These are the Licence Service plugin that organises the data from the LOG file and the Revit Plugin to identify the users in the project.\n2) MySQL is the open-source database that can be used to structure data.\n3) WilkinsonEyre is using Power BI to create the dashboards, but there are other options such as the Metabase.\nData workflow diagram\nThe FlexLM License Manager and the Revit Synchronisation provides data that will be structured in the open-source MySQL and Power BI will be used to create the dashboards. See figure 2 (below) for the complete data work\ufb02ow.\nYou can access more information about the process of decoding the Flex License data, organise the information and how to prepare the MySQL database in the class material.\nData analytics dashboard\nThe work\ufb02ow presented in the Autodesk University class will enable you to create the following dashboards to analyse the licence consumption in your company.\n\u2022 Current licence usage\n\u2022 Historical licence usage\n\u2022 Historical licences versus Unique User\n\u2022 Software usage\nThe Power BI fle is available in the class material along with content from other dashboards.\nCurrent licence usage\nThis dashboard shows the licences currently acquired, those available and the total number of licences. This information is useful to identify how your company is using the licences and it helps on the decision to purchase or reduce the number of licences.\nIn figure 3 the bottom left table shows the users that are using more than one licence. In another dashboard, you can identify which applications are consuming two or more licences.\nHistorical licence usage\nThe historical data provides weekly information about the licence\u2019s consumption across multiple packages.\nIn Power BI, you can change the date and visualise the previous weeks to analyse how many times your company has reached the maximum consumption. In figure 4 the Autodesk AEC Collection is the blue graph, and each vertical bar is one day of the week.\nHistorical licences versus unique user\nThe graph in figure 5 (below) shows the relation between the maximum number of licences used (the grey column) and the total number of unique users (the brown column) in the specifc week. You can fnd the ratio between the used licences and the overall number of users in the company, which helps to predict a future increase in the number of licences if a new large project starts.\nSoftware usage\nThe graph in figure 6 (below) shows the percentage of each application used in the specifc day but, since you are in control of the data, you could edit in Powers BI to include the projects or other felds.\nConclusion\nLicence insight helps a company to manage software resources, identify the licence consumption, and support the decisions on the licence renewal or future purchases.\nIt is important to proactively monitor licences to optimise the utilisation and provide licences when necessary.\nThe difference in price between the single sign-on and multi-users is substantial so companies may decide to change the users that utilise the software over 75% of the time to a single sign-on instead of using the multi-user licence as a means to reducing costs.\nOverall, it is hoped that the major software companies will provide better tools to understand the licence consumption and facilitate the process to manage licences with transparency.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/supporting-the-smart-city-agenda\/","title":"Supporting the smart city agenda","date":1537747200000,"text":"In the first of a series of articles examining how smart cities will impact our urban environment, Rebecca De Cicco, Director at Digital Node shines a light on what the UK is achieving in this burgeoning sector and how BIM will need to play its part\nAccording to the UN\u2019s latest 2018 Revision of World Urbanisation Prospects, the world\u2019s urban population currently stands at 4.2 billion, with around 68% of the whole world\u2019s population projected to live in urban areas by 2050.\nThe report even suggests that by 2030, we will see 43 megacities with more than 10 million inhabitants, mostly in developing regions. It\u2019s no surprise then that this shift toward urbanisation presents a complicated set of challenges in meeting the needs of the population regarding sustainable development, housing, transport, energy, employment, education, health care, and infrastructure.\nHow we approach and solve these challenges is an urgent issue with smart city systems being hailed as the answer to manage resources and the economies associated with such a population shift.\nOur future cities must be \u2018smart\u2019, but what does that mean? The British Standards Institute (BSI) defines the smart city term as \u201cthe effective integration of physical, digital and human systems in the built environment to deliver sustainable, prosperous and inclusive future for its citizens\u201d. The definition sounds simple, but the challenges of delivery are not.\nStandards to support smart cities\nIn 2014 BSI produced the Smart Cities Vocabulary within PAS180, recognising that terminology and analytical language surrounding smart cities would require some standardisation regarding this agenda. Aimed at city leaders, this PAS defined terms which included intelligent city concepts across different infrastructure and systems elements. They also developed the PAS 181 Smart City Framework to enable city leaders to develop, agree and deliver smart city strategies, and the PAS 182 Smart City Concept Model; a guide to establishing a model for data, tackling the barriers to implementing intelligent city concepts, including the interoperability of systems and datasharing between agencies.\nThese PAS documents outlined the standards to provide the necessary conditions for innovation and collaboration, recognising that each city would likely have a different vision to meet its individual vision for its future. When one considers BIM in the world of smart cities, intelligent clients and a BIM-enabled supply chain would certainly be desired, ensuring the information translated to the asset owner or city would be \u2018smart\u2019 enough to use and reuse downstream. Keeping systems connected to other smart city systems such as roadways, lighting systems, underground services etc., is the key, and a building built with BIM at its heart can enable the integration with other systems and supply the data required for future infrastructure, planning and maintenance.\nThe London approach\nSo, how well is the UK adopting smart city principles, concepts, and programmes? A recent study from Philips Lighting cited London, along with Singapore and Barcelona as being the world\u2019s best, with London being commended for its focus on communities when implementing technology. However, the report indicated that local authorities are being hindered through budget limitations, a lack of leadership in the implementation, limited capabilities in infrastructure and challenges around short-term planning.\nThe Mayor of London, Sadiq Khan, recently unveiled the new Smarter London Together smart cities roadmap aiming to tackle the main inhibitors to digitised cities, such as those mentioned above. Khan\u2019s plan to make London the smartest city in the world relies on innovative data capture and how best that can serve citizens.\nTo capture that data, the roadmap requires the city\u2019s 33 local authorities and public services to work and collaborate more effectively with data and digital technologies. The collation of this data will be supported by a new London Office for Data Analytics (LODA) which was developed during a 2016-2017 pilot. LODA will use data science techniques to deliver significant benefits for citizens that can be designed around their needs, prove efficiency gains for public services, and how new technologies and smart city devices can be used to solve urban challenges like air quality.\nRebecca De Cicco is the director and founder of Digital Node, a BIM-based consultancy working with clients all over the world to educate, manage and support the implementation of a clearly defined process, underpinned by technology.\nThe technology market\nAs smart cities rely on everything being connected, the technology to enable not only intelligent services for citizens but to connect them with authorities is the bedrock of how a smart city can be achieved.\nFrom sensors to The Internet of Things, geospatial technology to AI, the marketplace is estimated to reach $400 billion by 2020 of which 10 per cent can be reaped by the UK. Fortunately, the UK is recognising this potential market, and programmes such as Innovate UK and the Future Cities Catapult are supporting innovative companies to create the solutions to our urban challenges.\nInnovate UK\u2019s work to support businesses to realise the potential of innovative new ideas has already seen a commitment of over \u00a31.8 billion, creating nearly 70,000 jobs and \u00a316 billion for the UK economy.\nTheir Future Cities Missions intend to offer businesses improved access to knowledge, markets, skills and partners based outside of the UK to help remove the barriers to global growth. These missions have seen companies visit Malaysia, Singapore, and most recently, Australia.\nDigital Node was chosen on the Future Cities Mission to Australia this year out of hundreds of organisations in the UK to travel to Melbourne to support how BIM and future cities are interconnected. Melbourne was chosen as the Australian city to support this mission as it was noted that future predictions on the city confirmed its rapid population increase in the next ten years, as well as the opportunities for UK businesses to offer their services in this city. An example of other companies who joined us on this mission included Just Park, a consultancy using a smartphone application to support intelligent parking systems as well as Grid Smarter Cities (gridsmartercities. com), a consultancy offering smart city eco-systems using data to connect communities, people, transport and parking. We were honoured to be part of such an intelligent forward thinking group.\nSmart cities form a crucial component to the future of our cities, and although BIM is vital to this incentive, it is only one part of the broader picture to enable technology, data and innovation to thrive support this agenda.\nAs more businesses realise the potential of engaging with, and delivering smart processes, we will see the expansion of skills, knowledge, and the use of such processes become part of the norm rather than their current status of something to be admired from afar.\nClcik here to read \u2018Smart City Innovators\u2018, the second article in this series, in which Rebecca discusses the importance of supporting innovative companies in developing technologies for smart cities.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bim-show-live-2017-announces-line-up\/","title":"NEWS: BIM Show Live 2017 announces line up","date":1481760000000,"text":"BIM Show Live 2017 will take place in Newcastle Upon Tyne at the \u2018Boiler Shop\u2019 venue\nHaving been run in London and Manchester the original UK BIM show, BIM Show Live now moves to Newcastle Upon Tyne, a city that is synonymous with BIM development, standards and content.\nThe team behind the event has just announced the speakers and topics for the 32 sessions which will run in four streams on the 1st and 2nd of February.\nFollowing tradition, BIM Show Live will be opened by David Philp, Global BIM\/IM Consultancy Director at AECOM and Chair of the Scottish BIM Delivery Group who will set the scene for digital construction, forward thinking and what will be the key take-aways from the show.\nThe four streams cover the following topics: Data, Stories, Next Generation and Strategy. These seminar sessions will feature case studies from Sainsbury\u2019s, Capita, BDP and AECOM, all leading international firms with success stories to share on their BIM implementation.\nDigital master builders and architectural practices, David Miller Architects, Grimshaw, Metz Architects and Swedish based White Arkitekter also present on projects, new design technologies and the benefits both they and their clients enjoy from working in the BIM environment.\nTechnology experts from BIM Technologies, coBuilder, NBS and Autodesk share insights on how the industry is transitioning into augmented construction methods and how artificial intelligence has the potential to redefine every aspect of the design and build process.\nFull details of the BIM Show Live 2017 content programme can be found here.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-london-conference-johan-hanegraaf-mecanoo-architecten\/","title":"Video: NXT BLD London conference - Johan Hanegraaf, Mecanoo Architecten","date":1502668800000,"text":"Communicating the certainty of conceptual ideas through immersive means \u2013 NXT BLD London, June 2017\nUntil recently, digital tools could portray how an idea would look, but how an environment feels were left to the imagination, explains Philippe and Akshay. Colour, materials, scale and daylighting in renderings were no more than an illustrative approximation.\nHowever, by using the latest visualisation, simulation and VR technology, architects have increasingly more tools in their arsenal to communicate the certainty of their conceptual ideas through immersive means.\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Paul Nichols, Skanska\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/investing-in-people-at-david-miller-bim-training\/","title":"Investing in people at David Miller","date":1416441600000,"text":"A strong focus on staff training and skills development keeps David Miller architects ahead of the curve in BIM readiness.\nIn 2006 architect David Miller committed to grow his practice to service larger clients and to take on more challenging projects. With a long-term vision for growth and a well-developed \u2018emotional intelligence\u2019 (EQ),\nMr Miller embarked upon his search to recruit like-minded architects. \u201cRight from the start I looked for people with a certain temperament,\u201d he explained. \u201cThe best practices hire great thinkers, great communicators, and team-players, but I wanted more.\u201d Mr Miller searched specifically for individuals with minds open for learning and an abundance of creative energy for problem solving in response to project requirements. \u201cThese people are the real transformers of projects because they build enduring and trusted client relationships,\u201d he said.\nAccording to author Daniel Goleman, this rare combination of abilities is common in architects with high EQ. Mr Miller believes it is the primary reason for the unusually high volume of repeat business enjoyed by his practice. Today the DMA team is 19-strong, having grown steadily to service its expanding portfolio of commercial, education and residential projects.\nThroughout this growth period staff turnover has remained low. \u201cI consider it my job to make everyone on our team feel supported by investing in technology and processes to enable them,\u201d Mr Miller said.\nDMA applied for ISO 9001 accreditation when there were only four people on the team. It quickly became apparent to Mr Miller that when supported by good processes his team would be free to make decisions without deferring to him for approval. This approach nurtured and insulated new recruits in a way that quickly enabled their self-confidence to grow; giving them a greater sense of autonomy and achievement.\n\u201cThe whole practice found ISO 9001 to be very liberating and for the first time we were able to pitch for public sector projects that require ISO 9001 compliance from all consultants,\u201d practice director Fiona Clark said. There was also a human benefit. \u201cYounger people found it easier to increase their contributions to the team leading to increased efficiency and more predictable operations across all projects,\u201d Ms Clark said.\nSuch positive outcomes quickly led to DMA applying for and securing \u2018Investors in People\u2019 recognition; placing it among the top 0.5% of architectural practices in the UK.\nPerhaps unsurprisingly then, with an office filled with emotionally intelligent architects and designers with minds open to learning, the adoption of Building Information Modelling (BIM) processes, technologies and collaborative behaviour has also been successful.\nThe result; DMA is now years ahead of the curve for complying with the UK government\u2019s BIM initiative for all centrally procured projects to achieve Level 2 BIM status by 2016.\nAs with other parts of the business, DMA approached the transition to BIM with an appetite for learning and a hunger to understand all aspects of the new collaborative workflows.\nDavid Miller Architects has pre-empted the threat of email overload by installing Outlook plug-in Oasys Mail Manager. The software tool complements the collaborative work processes that have underpinned the practice\u2019s creative and commercial success.\nEmail has always been seen as an integral part of the BIM collaborative process, and was shared within the practice using drag and drop Outlook project folders. However, as the number of concurrent live projects grew, the time taken to file and retrieve emails came onto the radar as a potential future problem. At that time, a new member of the team who had used Oasys Mail Manager before and missed it sorely introduced the concept.\nPractice director Fiona Clark said: \u201cIt was a perfect fit, but seemed almost too good to be true. However, as it had been developed for Arup, a firm we know and trust, we had complete confidence in it.\u201d\nMail Manager automates email filing into system file folders and, because it learns user behaviour, filing emails and their attachments quickly becomes a prompted one-click operation. Emails are filed instantly in a place where they can be securely shared. \u201cThere is no time lag while people might get distracted by a phone call, meeting, or even wait until the end of the day,\u201d said Ms Clark.\nFiling errors are virtually eliminated, and will only occur if the user makes an error. However, even then, the faceted search capabilities will enable team members to find messages in seconds.\n\u201cGood storage and retrieval is the platform on which automation and collaboration is based,\u201d said Ms Clark. \u201cWe have complete confidence in our mail storage and our ability to retrieve it.\u201d\n\u201cOrdinarily across the practice we measure everything so we can continue to improve,\u201d said Clark. \u201cWe therefore committed to measure the cost of our BIM investment and compare it to the value we realised at the practice and project level.\u201d\nAs with ISO 9001 and Investors in People, DMA engaged external consultants to help measure its BIM performance. First it looked at DMA\u2019s BIM adoption on a macro level using the American \u2018National Institute of Building Sciences \u2014 Facilities Information Council National BIM Standards\u2019 (at the time, UK standards were still under development).\nNext the consultants looked at DMA\u2018s capabilities on a micro-level by assessing staff skills when using Revit, Autodesk\u2019s BIM software.\nTo assess staff skills, DMA engaged the KnowledgeSmart team to identify individual Revit skills gaps and to plug those gaps using customised training programs.\nModular training was provided by White Frog, and prescribed in response to KnowledgeSmart skills-gap assessments.\n\u201cWe hope to measure significant productivity and efficiency improvements\u2026\u201d explained Mr Miller, \u201c\u2026but that isn\u2019t our primary goal. We first want to ensure that our BIM adoption is strong and the best way to raise the bar is to make sure our architects can achieve everything they need to achieve inside the BIM software environment.\u201d\nIndeed when each software license costs around \u00a34,000 it makes little sense to use only a tiny fraction of the software. Not all practice leaders believe in the value of training staff. Some even reject the notion of training on the grounds that the newly skilled employee will just up-sticks and leave for more money elsewhere.\nOf course, people do not always change jobs for more money, many move in search of greater job satisfaction. But as American author, salesman, and motivational speaker Zig Ziglar once said \u201cWhat\u2019s worse than training your workers and losing them? Not training them and keeping them.\u201d\nIt is common for employees to feel under-appreciated and unsupported by employers when they are passed over for training or promotions. As new talent joins a firm, with new skills in hand, the current staff can start to feel a little overshadowed, causing them to seek out pastures new; this is highly disruptive to active projects as project-specific knowledge leaves the project along with the staff.\nThe longer the construction project the greater the chance of team members leaving. As many construction projects take a long time to complete, and as starting and finishing with the same team in place is a good way to provide the best client service levels, retaining staff that are well trained and highly knowledgeable is likely to contribute more than anything else towards improved client service levels and project quality.\nDMA constantly reinvests for growth. For example, R&D Tax Credits realised from their investment in BIM have been reinvested in its staff to assess and improve their individual knowledge and productivity. And this investment in practice performance is amplified at the project level where DMA clients are also able to measure improved performance.\nMr Miller explained why this matters to DMA: \u201cOur clients make no secret of their project and consultant performance measurement processes and we know that we score very well because they continue to give us larger projects.\u201d\nAs UK projects move towards more process-driven workflows DMA is ideally suited to further expand its portfolio of projects.\nMr Miller concluded: \u201cOur continued investment in the practice, our people, and our processes is paying off. Clients have come to know that they will always enjoy a consistent level of output from DMA, which greatly reduces their project risk. And I know that our achievement in that regard is a team achievement of which I am very proud.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/www.wired.com\/story\/karen-hao-empire-of-ai-water-use-statistics\/","title":"You\u2019re Thinking About AI and Water All Wrong","date":1765411200000,"text":"Last month, journalist Karen Hao posted a Twitter thread in which she acknowledged that there was a substantial error in her blockbuster book Empire of AI. Hao had written that a proposed Google data center in a town near Santiago, Chile, could require \u201cmore than one thousand times the amount of water consumed by the entire population\u201d\u2014a figure which, thanks to a unit misunderstanding, appears to have been off by a magnitude of 1,000.\nIn the thread, Hao thanked Andy Masley, the head of an effective altruism organization in Washington, DC, for bringing the correction to her attention. Masley has spent the past several months questioning some of the numbers and rhetoric common in popular media about water use and AI on his Substack. Masley\u2019s main post, titled \u201cThe AI Water Issue Is Fake,\u201d has been linked in recent months by other writers with large followings, including Matt Yglesias and Noah Smith. (Hao said in her Twitter thread that she would be working with her publisher to fix the errors; her publicist told me she was taking time off and was unavailable to chat with me for this story.)\nWhen I called him to talk more about AI and water, Masley emphasized that he\u2019s not an expert, but \u201cjust some guy\u201d interested in how the media was handling this topic\u2014and how it was shaping the opinions of people around him.\n\u201cI would sometimes bring up that I used ChatGPT at parties, and people would be, like, \u2018Oh, that uses so much energy and water. How can you use that?\u2019\u201d he says. \u201cI was a little bit surprised when people would be talking so grimly about just a little bit of water.\u201d\nAs local and national opposition to data centers has grown, so, too, have concerns about their environmental impacts. Earlier this week, more than 230 green groups sent a letter to Congress, warning that AI and data centers are \u201cthreatening Americans\u2019 economic, environmental, climate and water security.\u201d\nThe AI industry has started fighting back. In November, the cochairs of the AI Infrastructure Coalition, a new industry group, authored an op-ed for Fox News that touched on environmental worries. \u201cWater usage? Minimal and often recycled\u2014less than America's golf courses,\u201d they wrote. One of the authors of the op-ed, former Arizona senator Kyrsten Sinema, is currently advocating in favor of a data center project in the state that has prompted local pushback, including because of concerns about water use. The coalition also approvingly retweeted a post from Masley on the impact of AI on energy prices. (Masley maintains an exhaustive disclaimer on his Substack refuting allegations that he\u2019s being paid by industry to share his opinions.)\nIt\u2019s true that much of the discussion around water use and data centers lacks nuance. While carbon emissions are a zero-sum game\u2014we need to cut greenhouse gases as much as possible, period, and climate change\u2019s impacts will touch us all, regardless of where emissions come from\u2014water use is much more complex, and geographically varied. A project that can wreak havoc on one region\u2019s water supply may be a great match for an area with healthier reservoirs or fewer thirsty industries.\nExperts I spoke to agreed that people often have a muddled understanding of how data centers use water, and that their overall consumption, in many places, is less of a risk than the public may think. But as the number of data centers continues to grow across the country\u2014and as the Trump administration rolls back environmental protections to encourage more development\u2014it\u2019s worth understanding what, exactly, data centers are using water for, and how popular estimates are produced. And it\u2019s worth having a bigger conversation about how and why we\u2019re choosing to use water to cool data centers in the first place.\nHow AI Uses Water\nYou may have seen estimates of how much water a ChatGPT prompt uses, including the statistic that writing an email with AI consumes an entire bottle of water. But calculating such a figure is more complex than simply slapping a metric on an \u201caverage\u201d query, experts say.\nOnsite at a data center, water is mostly used for cooling. Processors in data centers run hot, and circulating water through them is one way to keep them at the right temperature; the water that absorbs the heat is then transferred to a cooling tower, where some of it evaporates. Salty and brackish water can corrode machinery, so many companies use potable water, drawing directly from municipal supplies. (Some big companies, like Amazon, Meta, and Apple, are increasingly using municipal wastewater that has been treated.)\nThe amount used depends heavily on the individual data center. Using more water means that data centers can avoid running electric cooling systems. Using more electricity, by contrast, lessens the water footprint, but ups the power bill\u2014and causes more greenhouse gas emissions. (There are technologies that use special kinds of cooling liquids to cut down on both electricity and water use, but they have the potential to introduce forever chemicals into the mix. That\u2019s made some Big Tech companies skittish of investing too heavily in them.) Cooling needs intensify in the summer when the weather is hotter, so data centers may use more water\u2014or power\u2014then.\n\u201cEvery location and every state is different,\u201d says Fengqi You, a professor in energy systems engineering at Cornell and an author of a recent analysis on the most sustainable places to put data centers. \u201cHow much water you will need for the same amount of AI depends on the climate, depends on the technology used, depends on the [energy] mix.\u201d\nComplicating things even further, some calculations around AI and water also include indirect water use\u2014mainly from the massive power generation needed for data centers\u2014to estimate their total water footprint. These numbers are generally much bigger than onsite use, but the calculations themselves are region-dependent.\nThis type of calculation is standard when talking about indirect greenhouse gas emissions\u2014it\u2019s known as Scope 2 emissions accounting\u2014but it\u2019s relatively rare to use the same calculations for water, says computing researcher Jonathan Koomey, coauthor of a recent paper from the Lawrence Berkeley National Laboratory that crunched numbers around AI and water. Koomey says he is increasingly convinced that offsite water use from energy shouldn\u2019t factor into data-center water footprints, simply because we don\u2019t tend to count this use when we talk about other industries.\nFinding out details about water use in a specific data center isn\u2019t always easy: A lot of companies use nondisclosure agreements to hide even basic information about projects from the public. A city in Oregon dragged state newspaper The Oregonian through a monthslong legal battle in 2022 to avoid disclosing how much water a Google data center used, arguing that it was a \u201ctrade secret.\u201d (Following the lawsuit, Google started disclosing how much water its data centers use in its annual sustainability report.)\nIf the complexity makes measuring the water use of a given data center difficult and contingent, isolating the effects at the level of a single user or prompt is nearly impossible. Understanding the environmental impacts of specific LLMs is almost entirely dependent on sustainability disclosures from Big Tech, and while some have gotten more transparent, a lot of questions still remain. When OpenAI CEO Sam Altman mentioned in a personal blog post this summer that an \u201caverage\u201d ChatGPT query used \u201croughly one fifteenth of a teaspoon\u201d of water, he gave some parameters for understanding the company\u2019s water and energy use\u2014but also didn\u2019t clarify key details, like the definition of an \u201caverage\u201d query and whether or not the figure includes the energy and water cost of training an AI model.\nWe Use a Lot of Water Without Thinking\nOne of Masley\u2019s main arguments in his popular Substack post is that there are industries that currently use much more water than AI, and that context needs to be part of the conversation. This is undoubtedly true. A single burger takes more than 400 gallons of water to produce; a humble cotton T-shirt takes more than 700. The United States\u2019 16,000 golf courses, meanwhile, each have the potential to use on average between 100,000 to 2 million gallons of water per day. (For comparison, Google says its thirstiest data center in Iowa consumed about 2.7 million gallons per day in 2024; most of the company\u2019s data centers used substantially less.)\nArizona, one of the areas in the US where data center growth is exploding, has more than 370 golf courses. I can understand some of Masley\u2019s points when I think about all the water that has, for decades, been going to help people play golf in the middle of the desert, with seemingly no one making a fuss.\nBut experts caution against dismissing concerns about water outright. \u201cIn the near term, it's not a concern and it's not a nationwide crisis,\u201d says Cornell professor You. \u201cBut it depends on location. In locations that have existing water stress, building these AI data centers is gonna be a big problem.\u201d\nKoomey made a similar point. While he notes that people have a tendency to exaggerate the environmental impacts of computers, data centers\u2019 water use is \u201c not something where you can just hand-wave it away,\u201d he says. \u201cEach situation has to be evaluated in the context of the specific design of the facility that's being proposed. You just can't say a priori that it's always not an issue.\u201d\nIt\u2019s obviously crucial for journalists to get their numbers right. But it\u2019s also clear that data centers do not have a negligible impact on an area\u2019s water supply when water is scarce. While Hao\u2019s numbers on the data center in Chile were likely substantially off, a single facility requesting more than 100 percent of the water consumed by a city\u2019s residents is still nothing to sneeze at. (Google paused and then halted the project Hao cited last year after a court ordered the company to reconsider the potential impacts of climate change on the aquifer. More than a dozen other data centers have been constructed or are planned in the Santiago region.) Chile is nearing its 15th straight year of an unprecedented drought, and water supplies near Santiago have reportedly been endangered by other industries, including lithium mining.\nWhy Water Use and Data Centers Is a Big Deal to Some People\nThese considerations are running up against an inconvenient truth: The American public needs to seriously reconsider how it thinks about water as a resource. Droughts across the American West, juiced up by climate change, are showing in real time that the way the US economy has oriented itself around a seemingly unending water supply is pretty quickly becoming unsustainable. A 2023 New York Times investigation found that groundwater reservoirs across the country\u2014not just in areas experiencing drought\u2014are being overpumped, threatening both drinking water supplies and economic activity.\nKoomey says that the concern about AI and water reflects an age-old tension over how we appropriately price public resources for private use, especially when the scarcity of that resource has changed over time. \u201cPart of what we're seeing with water is that the rules and the norms and the prices are set based on a previous reality,\u201d he says. \u201cIt does all come back to this question, of what is the value of the service being delivered?\u201d\nThis seems like a pretty accurate analysis of what\u2019s driving a lot of the reaction to water and AI. People who don\u2019t think twice about eating a burger or buying a new T-shirt are angry about LLMs and water because they are rejecting the entire premise that AI is worth the price of its water use. A societal mass value judgement on AI is, fairly or unfairly, playing out in real time in the uproar around data centers. Part of the visceral environmental shame that folks like Masley\u2014and maybe some readers of this newsletter\u2014feel from others about their AI use is probably less about the specific water footprint of a ChatGPT search than about the acceptance of a culture where AI is woven into everyday life, regardless of its environmental impact.\nAnd I don\u2019t think it\u2019s entirely unjustified to differentiate the conversation around AI and water from other thirsty industries, merely because of how much importance is being placed on the breakneck growth of AI right now\u2014and how big the promises being made about this technology already are. After all, people who run golf courses aren\u2019t dining with the Trump administration, gaining massive policy concessions and economic gifts in order to reshape society solely for their putting greens, which they say will simultaneously solve all of our problems while also putting most of us out of work. Big Golf isn\u2019t making headlines about how it\u2019s upending entire industries, or causing mass psychosis, or keeping coal plants open\u2014or how we could all be facing an economic catastrophe if the market for golf bottoms out.\nIt is correct to question the environmental trade-offs of a technology that is being presented as inevitable. And it\u2019s essential to demand more environmental transparency from companies that are reshaping the economy to achieve their goals. It\u2019s also just as important to double-check the statistics along the way.\nThis is an edition of Steven Levy\u2019s Backchannel newsletter. Read previous newsletters here.","source":"wired.com"}
{"url":"https:\/\/aecmag.com\/news\/news-graphisoft-offers-free-link-from-rhino-to-archicad\/","title":"NEWS: Graphisoft offers free link from Rhino to ArchiCAD","date":1430870400000,"text":"Rhino \u2013 ArchiCAD Connection converts Rhino NURBS models into ArchiCAD GDL objects\nRelated articles:\nAdvertisement\nA new link between Rhino and ArchiCAD allows ArchiCAD users on both the Mac and Windows to import Rhino models into ArchiCAD as GDL objects (ArchiCAD\u2019s generic object format). There are likely to be many uses of the technology, but AEC Magazine wonder if users of the Rhino-native generative design tool, Grasshopper, may find it particularly beneficial for taking complex models into a production BIM environment.\nThe free software, Rhino connection for ArchiCAD, comprises both a plug\u2010in for Rhino 5 and an add-on for ArchiCAD 18. It allows complex shapes created in Rhino\u2019s NURBS-based engine (or acquired through 3D scanners) to be brought into ArchiCAD\u2019s BIM environment.\nFor larger and complex models there is the option to convert a Rhino model into a set of (smaller) GDL objects. According to Graphisoft, this process enables the user to retain separate logical parts within the same model while providing functionality to monitor and easily update the imported Rhino models in ArchiCAD.\nThis technology has been developed as part of an ongoing collaboration between Graphisoft and Nikken Sekkei that aims to develop future technologies for architectural design.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nYou must be logged in to post a comment.\nNewsletter \u2022 FREE digital magazine \u2022 print magazine","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/mpsi-systems-inc-introduces-retail-explorer-plus-at-a-special-introductory-price\/","title":"MPSI Systems Inc. introduces Retail Explorer Plus at a special introductory price","date":1024704000000,"text":"MPSI Systems Inc., a global provider of spatial decision support systems and information services, today announced the release of a new, advanced retail analysis and modeling system called Retail Explorer Plus(TM). Designed as a strategic and tactical retail decision support tool, this product, essential for network, capital, and operational planning, offers users a new level of sophistication and flexibility. A dynamic, yet straightforward user interface offers easy access to a suite of powerful, retail analysis applications including GIS and Predictive Modeling. Users can readily create, analyze and display standard or customized data, maps, reports, forms and photos and evaluate and predict consumer and retail store behavior.\nRetail Explorer Plus is positioned as an enhanced version of the current Retail Explorer(TM) product from MPSI\u00ae, providing new capabilities and additional functionality to the features users employ most often. By combining superior mapping with cutting edge modeling capabilities and timely, comprehensive retail data, Retail Explorer Plus gives users a penetrating, revealing look at their market, their competitors, their brand and their customers. Users can access and analyze data graphically, geographically or through comprehensive reports and tables. Customized capabilities include the ability to import proprietary or third party data, bind data to pre-existing retail model data and display information in any number of ways including thematic maps, dynamic graphics or HTML reports. Also hailed as exciting new features of Retail Explorer Plus are the ability users have to create a separate database that reflects user-generated market changes and the ability to generate volume projections beyond the boundaries of the market study area.","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/intermap-technologies-announces-agreement-with-visteon-corporation\/","title":"Intermap Technologies announces agreement with Visteon Corporation","date":1184630400000,"text":"Denver, USA, July 16, 2007: Intermap Technologies Corp. announced the signing of a joint development agreement with Visteon Corporation, to provide highly accurate 3D road geometries for the entire country of Germany. Derived from Intermap\u2019s proactive NEXTMap Europe mapping program, the data will be incorporated into the development of advanced applications of a 3D road vector map database for use in future automotive systems. The initial focus will be on predictive adaptive front lighting systems, which offers enhanced visibility for drivers at night by directing the headlamp light, before the driver directs the vehicle into the bend.\nUnder the terms of the agreement, Intermap will supply Visteon with 3D elevation data and geometries that the Company has already collected for the entire country of Germany. The data is currently being processed and will be delivered incrementally over the next several months. The agreement also calls for ongoing market analysis to ensure that the development of specific applications meet the required data content, accuracy, interfaces, and formats necessary to enhance future automotive systems.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/opinion\/revit-structure-3\/","title":"Revit Structure 3","date":1151452800000,"text":"After much anticipation, Autodesk has finally launched its new Structural Design tool, Revit Structure, into the UK market. But what is the target market, why has it been introduced at Version 3 and what happened to 1 & 2? CADline\u2019s Paul Woddy explains.\nWhile we\u2019ve been hearing a lot about Revit Structure over the past year, it was only officially launched in the UK last month. Versions 1 and 2 were tried out on the US market first in order to develop the functionality of the product without having to worry about localising everything. So Revit Structure 3 is effectively the first worldwide release and will be sold here in the UK together with AutoCAD 2007 as AutoCAD Revit Series \u2013 Structure.\nRevit \u2013 or Autodesk Revit Building as it is now known \u2013 has contained a menu dedicated to structural modelling since the early days. So is Revit Structure any different?\nWell yes and no. No, in that Revit Structure and Revit Building are developed as a common platform. Yes because it is different in terms of the interface and some of its capabilities which are tailored for structural engineers and technicians. Revit Structure has all the benefits of information-rich co-ordination and rule-based intelligence that we have started to take for granted with Revit, but with an engineering focus.\nMany of the immediate visual differences can be explained as a change in the default template, view settings, and component libraries, but we have tools here that do not exist in the Building package \u2013 some of which would be very useful but let\u2019s leave that for another article! The main target market here is Structural Engineers and Technicians and many of the interface choices are based around that demographic.\nAutodesk define the theoretical process of using Revit Structure in two ways;\nAutonomy: The engineer will develop the structural elements of a building in Revit Structure as an anatomical model, add loads and tweak the analytical model before round-tripping through an analysis tool to clarify the section sizes etc. Back in Revit, building sections and fabrication details are compiled and prepared for publication.\nTeamwork: The architect supplies the engineer with a model of the building, either substantially complete or as a work in progress. The engineer extracts the relevant information from the model, taking for example walls and associated openings, while effectively ignoring the content of the opening. Grids and levels are transposed and if required, Top of Steel levels replace Finished Floor Levels. The engineer then works his magic and adds extra support where needed. The above analysis and preparation of drawings is the same, and then the model is issued to the architect for information and review. They can then show the structural frame in relation to the architectural fa?ade. If either party makes a change, then the copy monitor will highlight the alterations to the other team. Once we get the HVAC boys using the same system, we have a powerful building coordination tool.\n\" As the product and its associated links to analysis are still in their infancy, we will have to wait and see how they work effectively, but each analysis software house promises full two-way communication with the Revit model \"\nWhilst the benefits of the latter option are obvious, in the short term, whilst the market share of Revit Building is relatively small, we must assume that Revit Structure can operate effectively on its own. Several producers of analysis software have developed links with Revit, including RoboBAT\u2019s Robot Millennium and Fastrak from CSC, which are both used extensively here in the UK. The decision by CSC is on the surface an odd one as they are effectively supporting a product in direct competition with their 3D+ software. When you dig deeper, however, it is a similar argument to the time-worn Architectural Desktop versus Revit discussion, which boils down to whether customers are in the market for SBIM (Single Building Information Modelling) or not. Either way CSC wants to maintain the position of Fastrak which is the company\u2019s main focus.\nAs the product and its associated links to analysis are still in their infancy, we will have to wait and see how they work effectively, but each analysis software house promises full two-way communication with the Revit model, replacing elements and maintaining a co-ordinated model. The model can be started in Revit as suggested by Autodesk, or it can originate in the likes of Fastrak before being moved across to Revit for model presentation and drawing production.\nOne thing that we can say from experience is that where engineers have opted for Revit, the reasons have not always been for the links to analysis but more fundamentally, co-ordination of information and communication of this information in various formats. \u2018A change made anywhere is a change made everywhere\u2019 is a concept that interests people just as well in engineering as it does in architecture.\nWhen an engineer\/architect partnership adopts Revit as its platform, the results are astounding, with unparalleled levels of communication between the disciplines and then on to the customer. This has major implications on the industry as a whole with closer ties and better understanding of the finished building. Something I think worth striving for.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/the-future-of-smart-cities\/","title":"The future of smart cities","date":1511308800000,"text":"How can architects, engineers and construction professionals help to build smart cities that put the needs of citizens front and centre, ask Rohit Talwar, Steve Wells and Alexandra Whittington of Fast Future?\nAcross the disciplines of architecture, engineering and construction, three forces are coming together to drive the next waves of opportunity: people, intelligent systems and smart city concepts.\nAt the core of the opportunity is the notion of creating truly livable environments for humanity, designed using intelligent tools and d elivered and managed through a range of technologies that will help us bring smart city visions to fruition.\nBy \u2018livable\u2019, we mean cities that are human, vibrant, forward-looking, functional, smart and sustainable. The core tools underpinning their design will be those that can amplify human intelligence on a massive scale to interpret, predict and create solutions based on immense volumes of information about life in the city, gathered daily.\nHolding it all together will be highly interconnected smart environments where people, government and business can live and work together effectively using emerging and exponentially improving technologies. These include big data, the Internet of Things (IoT), cloud computing, hyperconnectivity, artificial intelligence (AI), robots, drones, autonomous green vehicles, 3D\/4D printing, smart materials and renewable energy.\nTheory and practice\nWhile in theory, the potential of smart cities is exciting, in practice it can be very hard to develop a clear, inclusive and universally supported vision and strategy that delivers on everyone\u2019s needs and leaves nobody behind.\nPart of the challenge is that stakeholders\u2019 goals are not always aligned. At the same time, every sector is being disrupted and all our assumptions are being challenged.\nHence, few of us can see precisely what the needs of businesses, localities or families might be in the next 12 to 24 mon ths, let alone in the five to 15 years over which a true smart city infrastructure might be rolled out. At the same time, that\u2019s exactly what we must try to do. City governments must create inclusive processes that inform citizens about the forces shaping the future and the possibilities and challenges on the horizon; then engage the population in dialogue about the future we want to create.\nThis is where architects, engineers and construction specialists have an important role to play. They can help us explore and model what a livable city might mean to its people, and contribute to the articulation of a clear vision. Along the way, they must also offer insights into the ways in which the physical, digital and human elements of a smart city infrastructure might be delivered and managed.\nNew approaches, new tools\nIncreasingly, the tools available to architects, engineers and construction specialists are becoming more sophisticated and intelligent. From visioning to construction planning, increasing use is being made of the analytic and predictive capabilities of AI.\nAt the same time, the digital drawing board is coming to life through virtual and augmented reality (VR\/AR), to create immersive experiences throughout design, planning and construction processes. Hence, the impacts of a development on its surrounding ecosystem can be modeled in more precise detail than ever before.\nFor example, the implications of a range of events \u2013 from emergencies to natural disasters to security incidents \u2013 can be simulated, to help ensure the robustness and workability of designs and to provide greater confidence in the rigour of risk assessments.\nFurthermore, the capability of the technologies available will continue to amaze. For example, the combination of 3D printed structures and rapid building construction may lead to more agile forms of urban planning than exist today. By embedding sensors and detectors into assets and machines, we could also get vital insights into city life such as waste collection to traffic control, and better understand emerging needs in different areas of a city.\nThis idea of treating physical infrastructure more like software with builtto- suit homes, offices and public spaces might create cities that respond in almost real time to a range of behavioral fluctuations. Hence, smart cities might evolve to respond to demand fluctuations in much the same way that software applications do: sometimes requiring new functions or the withdrawal of older ones, sometimes relying on extra storage capacity or processing power or communications bandwidth.\nThis might mean that big events like the Olympic Games could be accommodated rapidly with largely temporary, \u2018pop-up\u2019 infrastructure and then disappear in a few weeks, rather than leave a permanent footprint and the costly challenge of ongoing usage and maintenance.\nAnother example of technology tools \u2018on steroids\u2019 can be found in the range of IoTbased home automation and protection products. For example, US start-up Vayyar is experimenting with the use of 3D imaging to see through walls, meaning that no structure would be impenetrable. This omniscient type of surveillance could put building designers and architects in the curious position of having to decide on the aesthetics and purpose of walls that are technically invisible.\nSmart cities, smart decisions\nThis emerging class of intelligent cities is typically being designed to enable smart management decisions \u2013 capturing and interpreting massive amounts of data about the population and its patterns. This information gathering via different forms of surveillance results in what is called big data.\nWithin five years, the deployment of ever-smarter AI and advanced analytics will mean that analysis could be completely automated. The data might be collated from a constantly evolving and expanding IoT \u2013 from traffic lights, surveillance cameras, pollution sensors, building control systems, and personal devices \u2013 feeding giant data stores held in the cloud.\nA leading example of a smart city in operation is Singapore, with its rapidly evolving \u2018city brain\u2019. With a backbone of technologies, this brain helps control pollution, monitor traffic, allocate parking, communicate with citizens, and even issue traffic fines. Singapore\u2019s brain is also attempting to modify human behavior; for example, one system rewards drivers for using recommended mapped routes, and punishes those who do not.\nUltimately, Singapore\u2019s planners hope to discourage driving, and steer commuters towards greater use of public transportation. In total, the city is planning for 100 million \u2018smart objects\u2019, including smart traffic lights, lamp posts, sensors, and cameras on its roadways, which will be used to monitor and enforce laws.\nVendors and planners are already beginning to explore and model the possibilities presented by this trend towards total data capture. For example, a case study from India suggests that streetlights along the highways can offer both smart city and connectivity solutions. In addition to helping monitor road conditions, they could be fitted as high-speed data connections. Data is a critical element of the smart city\/smart road future.\nHowever, because this option will further expand the relationship between internet service providers, surveillance, and private businesses including advertisers, there are issues around privacy to be considered. Naturally, most would want the information from smart cities and roads to be used to keep citizens moving, healthy and protected. But should companies then be allowed to use this information to target users with adverts, when it was collected for other purposes?\nSmart road systems\nThe smart roads that link up the smart cities of the future, we believe, are where planners can put into effect many of the ultra-efficient mechanisms that best characterise their vision. In general, the concepts around smart cities, smart roads and smart infrastructure are becoming less visionary and more strategic and sustainable by the day.\nAs cities grow in size and importance to the global economy, it will be increasingly important that they adopt the most innovative and forward-thinking design and sustainability ideas, particularly around road infrastructure. As a smart future unfolds, three important new technologies \u2013 big data, the IoT and renewable energy \u2013 are being used in parallel to transform the day-to-day.\nSouth Korea, for example, is planning an entire network of smart roads by 2020. This will include battery-charging stations for electric vehicles (EVs), as well as infrastructure to handle autonomous vehicles. The introduction of driverless vehicles requires roads to be transformed into information superhighways, as vehicles will need to communicate with each other and the city infrastructure. Mapping, traffic signals and safety regulations, for instance, are all parts of the physical and digital infrastructure that will need to become highly coordinated for autonomous vehicles to function safely and effectively.\nAll this data will enable decisions that make efficient use of space, fuel, water, electricity and waste products, with an emphasis on sustainability. For example, anticipating major traffic jams to provide alternate routes could reduce journey times, cut fuel consumption and ease congestion.\nUrban development evolves\nThe smart city movement has the potential to transform the organisation of people and physical objects, transcending urban development as we know it. The shift to smart infrastructure is not simply fashionable or aspirational; in many ways, it appears to be a critical enabler of the future sustainability of cities.\nIt can be argued that the future of human life on the planet relies on a smooth transition to cities that are more efficient, less wasteful and more conscious of the impacts of the individual on the greater good. This may involve a range of new negotiations along the boundaries of individual freedom and privacy; for example, replacing human drivers with self-driving cars in the hope of preventing death and injury in auto accidents, increasing traffic efficiency and removing environmental impacts.\nSimilarly, to reach municipal conservation goals, we might have to agree to invasive monitoring of waste generation, energy and water use in the home. These are the kinds of tensions with which future planners will need to wrestle.\nThe challenge and opportunity for architects, engineers and construction specialists is clear. The smart city shouldn\u2019t be an apocalyptic future where citizens are stripped of their free will, and we cannot be seduced by the technoprogressive view that the pursuit of smart roads will lead to utopia.\nHowever, it is now possible to create and deliver a city vision with citizens at its heart \u2013 one that is enabled by forwardthinking infrastructure planning, coupled with judicious use of enabling technologies. A well thought-out vision, enabled by a robust and well-executed smart city model, could provide a foundation stone for the next stage of our development, where science and technology are genuinely harnessed in service of creating a very human future.\nRohit Talwar, Steve Wells and Alexandra Whittington are from Fast Future, publisher of books from future thinkers around the world exploring how developments such as AI, robotics and disruptive thinking could impact individuals, society and businesses and create new, trillion-dollar sectors. Fast Future has a particular focus on ensuring these advances are harnessed to unleash individual human potential.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/radeon-prorender-makes-bim-software-debut\/","title":"Radeon ProRender makes BIM software debut","date":1564963200000,"text":"Physically-based, AI accelerated ray tracing technology embedded in ACCA Edificius\nACCA Edificius has become the first architectural BIM software to be integrated with AMD\u2019s Radeon ProRender ray tracing technology. The photorealistic PBR (Physically Based Rendering) engine uses Artificial Intelligence (Machine Learning) denoising to \u2018drastically reduce\u2019 processing time.\nAccording to the developers, the Edificius AIrBIM (Artificial Intelligence render BIM) technology allows users to create renderings without having to be an expert and without having to spend hours trying different settings to get the right result. It helps architects, designers and their clients use photo-realism to better understand lighting solutions and to see how different materials will influence design concepts.\nRadeon ProRender is an open technology and works with \u2018any GPU or CPU\u2019. For the Machine Learning Denoiser, AMD has \u2018trained\u2019 it to compare noisy images with their fully processed counterparts, and to quickly and automatically reconstruct the fully rendered image. It means there is no need to render the entire multitude of light rays involved in a given scene (rays) and process hundreds or thousands of render passes: the render is now completed much faster because only fewer passes are necessary. The Machine Learning algorithms \u201cremove\u201d the residual noise normally present in a partially rendered image.\nRadeon ProRender is also integrated in Cinema4D and is available as a plug-in for Autodesk 3ds Max and Unreal Engine, among others.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-countdown-to-digital-construction-week\/","title":"NEWS: Countdown to Digital Construction Week","date":1505865600000,"text":"DCW will offer visitors a chance to meet the teams behind the tech and try out the latest developments for the AECO industry\nFrom drone races to VR\/AR demos, 3D printing to robotics, this year\u2019s Digital Construction Week (at London\u2019s ExCeL from 18-19 October), offers a chance to get to grips with the technologies and processes disrupting the AECO industry.\nFor registrants before October 1, the exhibition is free to attend and runs alongside the DCW thought leadership conference which has been co-curated by the UK Government\u2019s Digital Built Britain Strategy, and boasts 5 themed seminar streams covering: Visualisation, Geospatial, Industry 4.0, Tech and BIM. In addition to the free seminar content, visitors will be able to meet the \u2018teams behind the tech\u2019 in each themed area \u2013 with experiential exhibition stands offering demonstrations, hands-on demos, advice & real-life project insights.\nExhibitor highlights include the newly opened Construction Scotland\u2019s Innovation Centre, which will be showcasing the expertise and equipment available at its 35,000sq ft prototyping and training facility in Glasgow. The team from the \u2018Innovation Factory\u2019, which is available for industry wide collaboration projects, will be demonstrating tech including their AR\/VR kit, Sawyer Collaborative Robot and 3D scanner, as well as showcasing a WikiHouse structure.\nMore developments in robotics and automation will also be on show via The Design Computation Lab \u2013 a new research laboratory at The Bartlett School of Architecture, University College London. The most current work of the postgraduate students involved in the lab will be presented, alongside demos of an industrial robot fabricating the building elements of an assembled structure live at the show.\nFor those interested in visualisation, stepping into the VR\/AR Lab will offer a chance to focus on the business case and practical application of AR & VR in the built environment, using HTC Vives and several mobile VR systems to try out advanced applications used on real projects. While HP will be revealing its pioneering VR project \u2018HP Mars Home Planet\u2019 at the show and offering visitors the chance to try out their fully immersive and untethered VR solution \u2013 the Z VR Backpack.\nIn the Geospatial area, a chance to meet the teams behind key infrastructure projects including Thames Tideway and HS2 will offer insight into real-life applications of the latest innovations in this sector, and visitors will get the ultimate hands on experience in the DCW Drone Racing competition where they can take control of their own UAV and pit their drone operating skills against DCW\u2019s \u2018stig\u2019 racer.\nFor a full list of features and to view the free seminar programmes, visit www.digitalconstructionweek.com.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/revit-structure-2008\/","title":"Revit Structure 2008","date":1181174400000,"text":"Now on its fifth release, Revit Structure is finally catering for the design of more complex structures with a whole range of new advanced modelling tools. Greg Corke reports\nRevit Structure has been gaining more traction in the structural engineering sector since its UK launch last year. The product is based on Revit\u00dds parametric change management system, which means a change anywhere will be reflected across the whole project. This means elevations, plans, sections, details and schedules, and not just changes to the 3D view.\nWith links to Revit Building, AutoCAD Architecture (previously known as Architectural Desktop) and AutoCAD MEP (Mechanical, Electrical, and Plumbing), Revit Structural can also be used to co-ordinate project information between structural engineers, architects, and building services engineers. Interference checking can be carried out between the various models to flag up potential problems before the construction phase. And when Revit Structure is used in conjunction with Revit Architecture a co-ordination monitor can be used to automatically notify engineers and architects of any changes made during the design process.\nIn addition to providing better co-ordination between architects and engineers, Revit Structure can be used as a central repository for a wide range of structural analysis data. Dynamic links can be established with a number of analysis applications and Revit Structure can co-ordinate any design changes.\nThis is made possible by the fact that Revit Structure builds two models concurrently. One, a physical model which represents the exact structure as it would appear on site, which is used for layout, drawing production and documentation; and two, an analytical model, which is a simplified nodal model which can be used inside their dedicated structural analysis package. This analytical model can be edited and simplified by the engineer so it can work more effectively with their chosen analysis application.\nThe new release\nThe first two releases of Revit Structure were for the US market only and the product only came to the UK last year at Release 3. It now appears to be on a six monthly release cycle and the fifth release (2008) was rolled out to customers this Spring.\nThe main focus for the new release has been in the area of advanced modelling, with the system now able to model curved beams, more complex openings in structural beams and columns, parametric trusses and warped concrete slabs. We\u00ddll look at each of these in turn.\nCurved beams: These are becoming increasingly popular in Northern Europe and Revit Structure now has the ability to create horizontally or vertically curved sections. Beams can be based on arcs, ellipses and splines, so you can pretty much generate any shape you want. While many analysis applications won\u00ddt understand or be able to analyse such complex geometry, users can overcome this by simplifying the analytical model \u00b1 by splitting curved sections into multiple straight lengths, for example.\n{mospagebreak}\nComplex openings: Castellated and cellular beams were introduced with Revit Structure 4 but 2008 sees the introduction of \u00d9openings by face\u00dd. This enables the user to sketch openings directly onto the face of a beam or column, so they can tie in to satisfy specific structural or building services requirements. Stiffeners can also be placed directly on faces.\nParametric trusses: Revit Structure Release 4 saw the introduction of a truss wizard, which was invoked through the API. This has now been embedded directly inside the application and is accessed through the modelling bar. Revit Structure provides a whole range of truss types, which can be adapted by the engineer. These include Fink Truss, Pratt Flat Truss, Gabled Truss, Warren Truss plus many more.\nWarped concrete slabs: The 2008 release has seen a lot of work done on the creation of concrete slabs. These can now have variable or constant thickness, which can be defined individually for all the layers or materials. Users can modify slabs with straight edges to include multiple slopes for draining and slabs can be morphed to vertices or points.\nMany \u201cExtensions\u201d are provided free of charge, though a small fee is charged for others. In addition to the Analysis Integration Enabler, which facilitates the bi-directional integration of Revit Structure and Robot Millennium, users can download Structural Analysis for Revit Structure. This is a totally free variant of Robot Millennium which uses a slightly different file format. The catch is that the software is limited to 200 members in 2D\/3D multi material frame structures, and 1,500 nodes for FE plates and shells.\nRobobat also provides a free tool to import\/export models to the CIS\/2 format; another set to define reinforcement patterns for RC beams, columns and footings; and an Excel based model generation tool. This enables user defined MS Excel data to automatically generate the geometry of a structure in Revit Structure. The base (free) version produces beams, columns and levels, while the enhanced version (\u00fa149) also generates walls and footings. Finally, a \u2018Freeze Drawings\u2019 extension enables the user to unlink the drawing from the object model, so the drawing will remain unchanged despite any changes in the Revit Structure model.\nConclusion\nThe advanced modelling enhancements made in Revit Structure 2008 are sure to be welcomed by structural engineers who have found previous versions restrictive. The software has always been good at modelling core structural elements, but as soon as you moved away from standard components, it struggled a little. Elsewhere, the third party analysis links continue to improve, but it\u00dds important to note that not all deliver the same level of functionality. Some might just inform of a change in beam size or a deletion of a member, while others will bring back forces into Revit Structure.\nRobobat\u00dds Robot Millennium appears to be at the forefront of this integration, and despite its proposed acquisition by Autodesk falling through, it\u00dds clear Robobat sees a strong future in Revit Structure, which is evident by its development of the Revit Extensions.\nHowever, whichever analysis partner you work with you\u00ddll still need to tailor the modelling process inside Revit Structure to make sure you get the most out of your analysis package. This will become more complex if you use multiple applications (though the ability to co-ordinate analysis data from multiple sources is naturally a key attribute of the product). Unfortunately Autodesk provides little guidance as to which analysis applications work best with Revit Structure, but you should be able to get good advice from your reseller.\nOf course, while Revit Structure excels in its ability to link to third party analysis applications, some engineers\/drafters simply want to use the product to co-ordinate with the architectural model, and for this reason alone Revit Structure is still an excellent tool which can be used to generate all design documentation. After all, this is the very foundation of Building Information Modelling (BIM).","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/sponsored-content\/beyond-form-and-structure-generative-design-and-ai-in-aec\/","title":"Beyond Form & Structure: Generative Design and AI in AEC","date":1596672000000,"text":"Generative design and AI are starting to have a huge impact in the AEC industry, improving quality, lowering costs and reducing risk\nWhen Computer Aided Design (CAD) went mainstream in the 1980s, it simply replaced the act of manual drafting; it did not actually \u2018aid\u2019 the design of buildings. Now with advances in local computing power, 3D graphics and an acceptance of a 3D modeling paradigm, we are finally at a point where CAD can live up to its name.\nGenerative Design and Artificial Intelligence (AI) are two key technologies now taking CAD way beyond the capabilities of its 1980s predecessors. Rather than using the computer to document designs, it is the computer itself that is doing the heavy lifting, generating hundreds or thousands of design options, which all meet pre-defined goals. The designer then chooses the ones that best meet the project\u2019s objectives, then refines them to create the best functional solutions. In contrast, with a traditional design workflow, time and cost constraints often lead to playing safe and following what\u2019s been done before. But this means the optimal design may never even be explored.\nThe benefits to the industry can be huge. Generative design and AI can improve quality, reduce waste and lower costs across the industry. These technologies can also de-risk the design and fabrication of complex buildings, by giving Architecture, Engineering and Construction (AEC) firms much more certainty that project and client demands will be met and delivered on time.\nThe generative challenge\nWithout generative design, many of the iconic, free-flowing buildings of the 21st Century would not have been cost effective to design and build. Some would have even been impossible to envision or communicate to fabricators. It was Robert McNeel & Associates\u2019 Rhino\u00ae Grasshopper\u00ae that helped global architecture firm Gensler resolve the split-parabolic curve of the Shanghai tower. Buro Happold\u2019s engineering team used the same algorithmic modeling software to rapidly generate design alternatives for the complex gridshell roof of Singapore\u2019s Jewel Changi Airport.\nIt is only through custom scripts that efficiently generate many alternative building forms that incredibly complex buildings like these have been able to be realized. Such is the complexity of the geometry, that modeling each scenario manually simply wouldn\u2019t be possible or the project team would be limited to developing one or two options. For each iteration, powerful PCs are needed to handle the regeneration of surfaces and elements in a model that results from every change to the underlying control points.\nWhile early applications of generative design focused largely on form and structure, the focus is now shifting to other design goals and project-specific metrics. The technology is being used to optimize for multiple, often competing criteria, such as cost, performance, sociological and environmental metrics, or simply how to make the best use of space within a building.\nAutodesk\u00ae, for example, used generative design to plan its new office space in Toronto, taking into account several differing goals including work styles, location preferences, daylight and outside views.\nGenerative design software is extremely powerful as it can be fully customized to help solve many different design challenges. But this has also been one of the barriers to wider adoption, particularly for smaller AEC firms. \u201cMany generative design tools out there can be really hard to leverage because they typically require you to know how to code or understand special terminology, or how a genetic algorithm works,\u201d said Autodesk AI development manager Racel Williams, speaking at Autodesk University 2019.\nTo this end, Autodesk is looking to democratize the technology. Building on the foundations of Project Refinery, Autodesk recently introduced Generative Design for Revit\u00ae 2021 that allows users to select a templated design study, define goals, and then use the computational power of a high-performance PC to return a range of options.\nIt is important to note that generative design does not do all of the work for the designer. While the software can generate hundreds or thousands of design alternatives, and structure the data to help organize and prioritize each one, only the designer can decide which ones are important and worth refining further. The intention is not to replace the human with a computer, but to free up a designer\u2019s time for more strategic work.\nThe AI future\nComputers are now starting to take a more proactive role. AI will offer the AEC industry a huge number of new possibilities at nearly every stage in the lifecycle of a building or infrastructure.\nAt the moment, the use of AI is at an embryonic stage, but there are a handful of software developers exploring its potential. AI is having a big impact at the feasibility or early stages of design, where it can be used to maximize site usage, residential capacity, daylighting or access to premium views.\nArchitects are also using AI for the more mundane tasks, such as checking if a building meets local codes. It can also be used to optimize construction scheduling and management or give construction planners advanced warnings of project delays.\nDigital Blue Foam is developing a tool to help address the risks of COVID-19, optimizing interior building layouts, while meeting social distancing rules. The aim is to identify high-risk zones by simulating the flow of people and air.\nTestFit is an AI-enhanced site feasibility study and building configuration software. The designer creates a site boundary and the software uses a proprietary algorithm to generate a building given the user\u2019s specifications.\nAI is also being applied to visual computing to bring game changing technology to construction sites for health and safety, materials tracing and inspection using just dumb images or video. This will bring the digital model and the analog construction process together in closer harmony, enabling further processes such as digital twins and all the benefits that big data brings.\nWhile it took decades for the industry to move from drawing board and pencil to digital 2D drawings on personal computers, the move to 3D modeling has been rapid within the leading design practices. By creating a digital prototype, the industry is now open to the benefits of next generation design technology.\nOn one hand, there are generative design tools that enable designers to advance the architectural vocabulary of modern buildings, through the creation of complex forms. On the other, there will be AI assistants and expert agents analyzing and advising everywhere. This could be from the early design brief, all the way through to the lifecycle of a building, where data from smart sensors is continually analyzed so environments can be optimized, and failures predicted. We are finally in the era of true Computer Aided Design.\nZ by HP recommends\nFor generative design tools like Rhino Grasshopper, Project Refinery or Generative Design for Revit, Z by HP recommends its Z4 Desktop and ZBook Studio with high-frequency Intel\u00ae Core\u2122 or Xeon\u00ae processors, high-end NVIDIA\u00ae Quadro\u00ae RTX graphics, 64 GB memory and 1 TB NVMe storage.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/bae-systems-introduces-automation-technology-for-map-creation\/","title":"BAE SYSTEMS introduces Automation Technology for map creation","date":1178582400000,"text":"California, USA, 07 May 2007: BAE Systems has released its Automatic Terrain Extraction (NGATE) software for creating 3D models from satellite or aerial imagery. The 3D terrain and surface models generated by NGATE can be used for all the GIS applications wherever elevation data is needed.\nNGATE uses a hybrid matching process to create precise elevation data for 3D terrain and surface models. The software is specifically designed for mapping urban areas, large geographic landscapes, mountainous or rough terrain, and areas with little contrast, such as road surfaces on large-scale imagery and vast deserts.\n\u201cNGATE represents a significant advance in image processing automation,\u201d said Dan London, vice president of sales and marketing for BAE Systems\u2019 in San Diego. He added, \u201cThe software can dramatically reduce labor hours and is highly accurate when compared to other automated algorithms.\u201d\nBAE Systems is a defense and aerospace company, delivering a range of products and services for air, land, and naval forces, as well as electronics, information technology solutions, and customer support services.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/autodesk-university-2015-jedi-academy\/","title":"Autodesk University 2015 Jedi academy","date":1454457600000,"text":"Complete with a dedicated army of Stormtroopers, Autodesk\u2019s 23rd annual learning and networking event, held in Las Vegas last December, was certainly one to remember. By Martyn Day\nAutodesk University is always big on showmanship but it will be hard to top this December\u2019s event, which was infiltrated by an army of Imperial Stormtroopers, fresh from their exploits in Star Wars Force Awakens. In theory, they were hired to keep the rabble in line, although some were spotted taking classes, perhaps trying to perfect their major weapon, the Death Star, and finally design out the flaw that the Rebels always manage to exploit. Maybe with Autodesk\u2019s software they will have more luck with revision 4.\nAny lessons learnt constructing intergalactic space stations could soon be applied to the real world. Autodesk is on a course to connect its software with digital fabrication in manufacturing and, now, AEC. The design process, as it is today, with dedicated phases of design, documentation and fabrication are hangovers from CAD replicating a tried and tested process but not recognising that design can bypass the middle stages and integrate with the actual production of parts. This is very obvious in manufacturing but perhaps less so in AEC. However, with the growing adoption of 3D and Building Information Modelling (BIM) processes in building, projects are now starting to lend themselves to being able to benefit from off-site digital fabrication.\nIn his keynote address Autodesk Chief Executive Office (CEO) Carl Bass recalled a visit the new Apple HQ, One Infinite Loop, a Foster+Partners building that is well on it way to completion.\nMr Bass witnessed the laying of paving slabs in the car parks and noted that each slab was manufactured off site and had a unique identifier and RFID tag. Suddenly the building was an assembly of parts, not unlike any other manufactured product. Steelwork is already being precision cut and rapidly delivered, using their ability to cut costs but offer quicker delivery as a major competitive advantage.\nChief Technology Officer (CTO) Jeff Kowalski had two impressive demonstrations of the company\u2019s Project Dreamcatcher technology. Dreamcatcher is Autodesk\u2019s big differentiator to the other CAD companies. It is working towards delivering design tools that can assist in the design process, to the point that the solutions it delivers exceed the best designs possible from an \u2018unaided\u2019 designer.\nUsing Dreamcatcher, engineers would benefit from the algorithms having deep knowledge of structural forms from the world of biology, understanding the crystalline structures of materials, 3D print methods and being able to only provide solutions that fulfil the design constraints. This is truly using a computer as an amplifier of the mind and will be eventually available for manufacturing and building design.\nThe first example Kowalski gave was of a car chassis design. Having built a high-power racing car, it was fitted with telemetry sensors and driven multiple times around tracks it would be used on. Taking the forces and shocks that the car actually experienced into Dreamcatcher, the system iteratively analysed and modelled an organic looking chassis that would offer the best weight for strength for the constraints supplied. The end result was a unique chassis \u2013 that could probably only be manufactured through 3D printing.\nThe second examples was of an Airbus partition cabin wall. Applying Dreamcatcher to the design essentially solved the internal structural layout, given the forces and constraints. The system used generative design to run through thousands of possible solutions and compared these to the base criteria. The end \u2018bionic\u2019 result was a pretty unusual looking lattice, which was 3D printed in metal.\nThe algorithms mimicked cellular structures found in bone, enabling a lightweight and strong design that would not have ever been possible by any traditional process. Airbus has since fitted the part to a plane and proven it fits the purpose. The lightweight structure could enable huge jet fuel cost savings. Mr Kowalski didn\u2019t say how much it cost to develop or manufacture, one would assume that it was a pretty expensive solution to the problem as a one off.\nMay the Forge be with you\nAutodesk made a big umbrella announcement that featured only a few details. Forge is a new Autodesk development platform (PaaS \u2013 Platform as a Service) with a $100 million fund to assist the next generation of third party developers work with Autodesk\u2019s solutions.\nForge runs on Amazon Web Services and on Autodesk\u2019s own server centres, which are currently in development. This cloud platform is a single place where Autodesk\u2019s products and services are available for access by third party developers through their APIs and SDKs. Over the past five years Autodesk has been rewriting or creating new web-based platforms, such as 3D rendering, model collaboration (A360) and DWG read\/write as componentised \u2018web services\u2019, which all of its cloud-enabled applications can call on when functionality is needed. Now developers will be able to hook into Autodesk\u2019s PaaS and integrate or build new applications to focus on specific markets. This will enable third parties to develop cloud-based add-ons to Autodesk\u2019s next generation of tools, which could also be hosted on Autodesk\u2019s servers.\nAutodesk is looking to attract start-ups to Forge and pay for development work in key markets. The focus, we are told, will mainly be in the manufacturing segment.\nRevit and the cloud\nThere was not a lot of news on Revit development with the focus primarily on collaboration and document management, which we will come to next. However we did pick up some more chatter on our revelations in the last issue that Autodesk is working on a new generation replacement for Revit, in the same way its Fusion360 application is to replace Inventor.\nRevit\u2019s software architecture is not really designed to make use of today\u2019s computing power, let alone the future\u2019s, and its data structure is less than ideal for large models or streaming over the web. If you look at Fusion360, it runs in a browser, on a PC or Mac, has relatively low local system requirements, is highly multi-threaded and can be accessed from any device from anywhere.\nRewriting Revit for the cloud would be a major undertaking. However, Autodesk\u2019s development teams have been moving chunks of common functionality up into the cloud for years, so the actual process will be completed \u201cquicker than you think\u201d as many of the required functionality building blocks are already there for the Revit team to hook into (BIM 360, Glue, Rendering, DWG, A360, analysis etc).\nIt will be interesting to see how Autodesk deals with potential legacy models and how the existing intelligence and object classifications are ported to any new system.\nWhile it is great that Revit is now gaining traction, many expert users want much more capability and power. Autodesk has long been aiming to move to Subscription and Cloud and Revit appears to have been bumbling around in a bit of a cul-de-sac.\nFor now it\u2019s well worth downloading a trial of Fusion360 just to see how its cloud architecture benefits from modern code and is literally \u2018everywhere\u2019.\nNew World Order\nAs we pointed out in the last issue, Autodesk\u2019s big launch, or pre-launch, was the beta of BIM 360 Docs, formerly known as Project Alexandria. This cloud-based document and model management service is built on top of the BIM 360 technology stack, which includes Glue and Field. While essentially focused on the document management part of the process, you can see Autodesk setting its sights on fleshing this out to manage projects and compete against Bentley ProjectWise, but here Autodesk has to learn to walk before it can run.\nBIM 360 Docs is a permissions-based online 2D\/3D repository, providing mobile connected access with version control, redline\/markup, collaboration with audit trails and some elementary workflows. In many ways it replaces Buzzsaw, which I was told \u2018will exist as long as people want to use it\u2019 but the writing is \u2018on the cloud\u2019 for that service.\nAutodesk states the product will feature:\nLinked 3D and 2D experience, allowing users to interact with models in 2D views and visualise them in 3D on the same page, and vice-versa\nPermission-based access control and approval processes to manage the updating and release of documents, preventing project teams from working from out-of-date information\nBlazing-fast viewing experience for large-format PDF design documents, optimised for Apple iOS devices; and Automated organisation of original and updated construction docs into sets, including highly accurate and customised optical character recognition (OCR) of title blocks.\nBIM360 Docs is currently in limited free preview and is expected to launch properly in early 2016.\nStingray\nBy far the most fun interactive display was one called \u2018Live Design\u2019 which demonstrated a new technology combination that Autodesk will be bringing to market very soon and will enable designers to bring their models into VR worlds for interactive exploration.\nStingray is a real-time 3D games engine that Autodesk acquired and is integrated with 3ds Max, Maya, and Maya LT software. Revit models can be imported to Max for prepping for use within a Stingray environment. Once loaded, it is possible to walk through the model in a fully rendered environment in desktop, web browser or using virtual reality glasses (Oculus Rift, or as was demonstrated the forthcoming HTC headset).\nIn one booth Autodesk had modelled a VR model of a San Francisco penthouse apartment. Using HTC goggles and a wand you could strip back walls, reveal measurements, open windows and draw blinds. The wand movement had zero lag. The HTC headset was great but has yet to be launched. Oculus is $599, which for professional use is cheap as chips.\nVR firms are aiming for the consumer market, which is to the AEC industry\u2019s benefit, as consumer graphics cards and low cost headsets will mean VR will rapidly become something that every architecture firm will be able to offer clients.\nMy only concern is the process, which requires the data to be exported through 3ds Max to get to Stingray, which seems unnecessary. Why can\u2019t it be done straight out of Revit? I understand Max may add tools for scripting interactions it should not be mandatory.\nBusiness change\nNext year Autodesk moves from individual licence sales to subscription only sales, by which time the whole model will be turned on its head.\nOver time Autodesk is expecting to see a rapid increase in the amount of income from this change as money will no longer paid to resellers and distribution (read this article from Barron\u2019s), instead the reseller gets a small fee for managing its on-going subscription base. But the subscription fee will be the same from the reseller as it is from Autodesk.\nAutodesk has been weaning its resellers off the rewards of selling box products and many have moved to selling services\/training and other business activities such as software development.\nFrom talking with resellers at AU, it seems that there will be considerable consolidation in the number of resellers, which has already started and will mean that there will be fewer resellers, servicing a lot of subscribers. With lower rewards, many are de-skilling and cutting down on sales staff to rejig their business models to map their lower projected incomes \u2013 some will give up and sell their installed base to another reseller.\nWhile there are varying views on the benefits provided by resellers within the installed base, support, installation and training will all be chargeable, one reseller told me they would even have to charge to demonstrate new software to clients. It is going to be a different world for those that rely on their local dealer for support \u2014 as it may no longer be that \u2018local\u2019. While Autodesk has been expanding its direct sales capabilities, this is focussed at big customers and many services get handed on to local resellers, of which there will be fewer.\nThis may be less of an issue in the US and UK Where Autodesk dominates, with highly knowledgeable customers, such as AEC and M&E. But in the markets where Autodesk is in a gunfight, specifically MCAD and AEC in Europe, a dis-incentivised reseller channel selling $25 a month subscription Fusion seat vs SolidWorks with its VAR channel chasing, as they still retaining decent margin, would be an interesting match. It\u2019s going to be interesting, and Autodesk is betting on the lower subscription fee beating the competitor\u2019s offerings.\nAndrew Anagnost, senior vice-president, Industry Strategy & Marketing, has been overseeing the move to subscription. He refused to come up with a number when asked how many resellers would be lost through this business model change but agreed there would be \u201cconsolidation\u201d.\nOn the fate of resellers, he replied, \u201cThere are already partners out there that have moved over 60% of their new run rate to our new business model. What they are asking from us is for more stuff like Forge, they want development platforms and they want a way to connect processes and make things work and deploy these new types of solutions in their customers. These are the partners we are going to invest in and build our ecosystem.\n\u201cYou are right, in the new world where we are moving towards subscription for everything, some partners who made their business from just turning the crank, answering phone calls\u2026 I think it\u2019s going to be tough for them, that\u2019s the reality. We told them over and over. Our best partners are already figuring out what they need, they are asking us for it and we are delivering it. The more capable our partners get with the Forge platform, the more business they are going to have.\n\u201d For end-users that rely on the local reseller channel this could impact any level of \u2018free\u2019 help on offer, access to on-site demonstrations, training and potentially a change in reseller. Mr Anagnost seems to be blurring the lines on what Autodesk wants from its future channel, more developer than product sales.\nConclusion\nAs usual there were plenty of great talks, events, demonstrations and opportunities to socialise in the semi-twilight craziness that is Las Vegas. The Stormtroopers tried to control the situation but I suspect they were slightly undermined at how easy it was for us to send the plans for the Death Star 4.0 to everyone in the galaxy, at once, using BIM 360 Docs.\nAutodesk is still developing a lot of capability in the background, which means the subscription features being added may not exactly excite the hardened users of products, but Dreamcatcher in particular really promises a completely different type of design process in the not too distant future. Having the computer model an idea, and help decide which aesthetic solution of the thousands instantly tested is best before sending it off for fabrication, is going to be the norm for future generations. There is no doubt that Virtual Reality is coming and will explode into consumer and professional markets this year. If you are making 3D models now, this is great news. If you are producing 2D drawings, the gap between the capabilities of design firms will be even greater.\nAutodesk continues to move to cloud and subscription and this year is D-day for its business model to flip from perpetual licenses. I can\u2019t explain just quite how a big move this could be and will impact customers in terms of long term cost of ownership in exchange for benefits in short-term flexibility. There is no real choice in this, the best thing to do is to make sure that your business extracts the maximum value of the software and services contained in the subscription. As the suites contain multiple products, do not skimp on training. Skills in using multiple software packages is becoming increasingly essential for improving any firm\u2019s capabilities and ultimately competitive edge.\nQ&A with the CEO and CTO\nAs has become a tradition at Autodesk University, CEO Carl Bass and CTO Jeff Kowalski take questions from the global press. The questions were varied and ranged from the aims of the new Dreamcatcher technology to the cloud.\nOn the subject of sustainability, the pair were asked about the usage of Autodesk\u2019s sustainability analysis tools and if it is an important part of Autodesk\u2019s sales. Bass replied, \u201cI wouldn\u2019t look at our investment in sustainability as one which is based on a return, financially speaking. It\u2019s one of the areas where we think there are big problems in the world and there are a number of things that we can contribute. This can range from energy analysis of building to great examples such as \u2018lightweighting\u2019 parts with Dreamcatcher.\n\u201cThere is an enormous positive long-term effect in deploying this technology. This helps our customers, which is good business.\u201d\nMr Kowalski added, \u201cWe have given customers tools so they can understand the impact of designs, assess performance, operational cost and optimise all of that before they commit to a final design. We are now looking to take back the actual \u2018in use\u2019 operation data back into the model, so they can understand the discrepancies and \u2018debug\u2019 the design.\u201d\nOn the development of Autodesk\u2019s 3D print platform and hardware, Spark and Ember, Mr Kowalski commented, \u201cThere are three components to Spark \u2013 the API, the reference printer, the Ember, and an investment fund to boot up the ecosystem. We were successful in transferring the conversation around 3D printing from \u2018trinket\u2019 making to actual additive industrial manufacturing. The printer we developed is actually capable of producing real production parts, it\u2019s very high resolution and open sourced. Through the fund we got to see nearly everything that was about to happen in the market and have meaningful impact.\nMr Bass added, \u201cAs a side note here, the difference between Autodesk the software giant and Autodesk the hardware start-up, the other day we had one of those hardware start-up ecstatic nightmares, that take place simultaneously, when someone came to use and asked for 1,000 Ember printers. We had an incredible desire to say yes but had no capacity to produce them!\n\u201cIt\u2019s giving us a different lens into how some of our customers live. Just to add to Jeff\u2019s point about trinkets, we have moved from talking trinkets to Airbus flying a 3D printed part.\u201d\nThe duo were asked about Autodesk\u2019s acquisition strategy, Mr Bass explained, \u201c We tend to do 10-15 acquisitions a year and I put them in three buckets, small, medium and large. The smallest category is a team and technology, people who are passionate, educated, love the thing they are doing and know a lot about it. The second one is folks that have a product, or almost have a product; and the third is a company, that has multiple products, established and mature. Many of our acquisitions fall into the first two buckets. When have a desire to get into a market and the best way is to acquiring or \u2018acquihire\u2019 a team or buy a product in development to shorten our time to market. It\u2019s fairly rare for us to actually buy a company.\u201d\nBass was asked what the Autodesk will look like in 2020, he replied, \u201cOur products will become indistinguishable, we will have a range of cloud-based products and services that people will put together in ways that makes sense for them, they will access and reconstruct that platform in their own way. The desktop \u2018thing\u2019 will now take place in the cloud\u2019. Look at Fusion 360 and PLM 360, they have front ends for input and almost all the computation happens on the other side of the wire. That\u2019s the cloud-based world of products and services that all software companies will be in.\u201d\nOn the move to the cloud, Mr Bass thinks \u201cMobility is becoming mainstream and people are increasingly not wanting to travel with their laptops. With the advent of products like the MS Surface Pro and Apple iPad Pro, it\u2019s becoming increasingly more obvious that tablets are taking over, even for people that use design software.\u201d Mr Bass added that the super computer we have in our pocket is going to play an increasing role in cloud services.\nThe exhibition show floor\nBack in the 1990s Autodesk University\u2019s exhibition floor was packed full with developers selling plotting tools and dull, but useful, utilities for AutoCAD. Those days are long gone and the exhibition is now the real star of the show.\nThe developers are now showing some seriously excellent add-ons for Revit Inventor, 3ds Max and AutoCAD, together with VR headsets, laser scanners, robots, UAVs, huge CNC machines, photogrammetry solutions, 3D printers, live demonstrations and many areas displaying customers work. Given that the hours of access are restricted, it is really a shame that attendees can spend only a limited amount of time enjoying it.\nThis year there was an excellent array of software exhibitors, including the subjects of many of our reviews \u2014 Newforma, Oasys, Panzura, Cl3ver, Revitzo, Sefaira, Solibri, Synchro, ClearEdge 3D and COINS to name but a few. Each Autodesk division also had huge areas for demonstration of their software.\nHardware is always a big component at the event. 3D printers were everywhere, naturally including Autodesk\u2019s own Ember 3D printer.\nWorkstation technology was in full force with HP, AMD, Dell, Intel, Nvidia, Lenovo, BOXX and others showing off their latest and greatestdesktop and mobile machines, CPUs and graphics cards.\nThere was also a strong presence from the up and coming \u2018virtual\u2019 players such as Frame, Citrix and VMware looking to get current desktop software in the cloud. Frame showed Revit running in a browser on an iPad Pro, MackBook Air and Chromebook and announced that Autodesk had certified Revit and AutoCAD to run on its cloud platform. Laser scanners and data capture devices were everywhere, with UAVs and measuring devices from Faro, Topcon, Leica, Z+F, Skycatch (which Autodesk recently invested in), Riegl and a cool sub-$5,000 handheld 3D capture solution from dot product. Autodesk is now also generating 3D models from underwater SONAR data.\nThere were plenty of examples of designs created using generative design. There were lots of sensors, showing possibilities of the Internet of Things, the Airbus component designed with Dreamcatcher and 3D printed in Titanium was also on display, together with 3D dresses and generatively designed bicycle components and a 3D printed car parts.\nWe discovered a huge \u2018robot\u2019 that could make cocktails, robots weaving structures, robots drilling metal. There were Universal Robots that were being taught how to move without programming but by human positioning of arm positions through waypoints.\nSix random facts we learnt at Autodesk University\n1. Autodesk hires \u2018young people\u2019 to look at Autodesk solutions and interfaces with \u2018fresh eyes\u2019 and to compare them with competitors products.\n2. CEO Carl Bass gets 98% excited by new technology but there\u2019s always 2% that \u201ccreeps him out\u201d. \u201cBad people do bad things\u201d with any technology, he said.\n3. CTO Jeff Kowalski admits that a few years ago they had taken a 3D printed gun to a conference in the UK and waved it around on stage to show technology can be used or abused.\n4. While Autodesk had been developing software in China for ten years and has many customers there, it is \u201cnot the easiest place to do business\u201d.\n5. In response to Russia banning state-owned firms from buying foreign-made software, CEO Carl Bass says that all government procurement is \u201cincredibly short sighted and about the most stupid thing that anyone can do in the IT realm, should that be for security concerns or to support the domestic software industry.\u201d\n6. Carl Bass owns a drone and thinks the big benefits are going to come from industrial uses.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/autodesk-university-09\/","title":"Autodesk University 09","date":1264377600000,"text":"In the CAD calendar, December is always Autodesk University (AU) and it is always in Las Vegas. The 2009 event was the 17th consecutive AU and, as in previous years, consisted of non-stop presentation, classes, meetings and socialising. Martyn Day reports.\nAutodesk University runs on a scale that no other CAD vendor can pull off. Autodesk is the biggest design technology developer in the world and so can regularly pull in thousands of customers to its annual show. It is so big in fact that Las Vegas has become its home due to the city\u2019s ability to cope with the sheer number of delegates, having the required hotel and conference facilities. In previous years attendance numbers have approached 10,000.\nWith the global downturn, this year\u2019s AU was shortened by a day and the attendance dropped. However, there were still an impressive 6,000 delegates representing 2,705 companies from 89 countries. In total there were 391 speakers, 449 classes, 68 labs sessions, and a fair amount of evening social events. Autodesk further expanded the event by broadcasting 116 of the classes, with the keynotes online at Virtual AU, which added 16,000 online customers. So while it was the smallest AU for a considerable period of time, it had the widest audience. The content is also still available on Autodesk University Online (au.autodesk.com).\nOn the day prior to the main keynote presentations, Dr Robert Aish, Autodesk\u2019s director of software development, arranged a Design Computation Symposium, which investigated how computers can be used to aid complex design, as opposed to merely documenting it.\nDesign Computation Symposium\nWhile at his former employer, Bentley Systems, Dr Aish created Generative Components, which offers designers a parametric tool to build complex, adaptive models to explore designs in MicroStation. Now at Autodesk, Dr Aish is developing an extra layer of technology for Autodesk\u2019s products, starting with AutoCAD, to capture design intent and optimise the results. The Design Computation Symposium is the one chance each year that we get to see how far his work has progressed and Dr Aish always invites an interesting array of researchers and designers to challenge current design philosophy.\nDr Aish\u2019s 09 symposium was quite heavily biased towards fabrication and comparing the manufacturing usage of analysis, which is more mature than the AEC industry. It was clear that Aish\u2019s new code was greatly advanced on last year, with a really impressive demonstration of AutoCAD modelling the Centre Pompidou in Metz \u2014 a very complex form indeed. I will have a full report on this session in the next edition.\nKeynote address\nThe masses were gathered in the huge auditorium of the Mandalay Bay hotel on the second day to hear Carl Bass, president and CEO of Autodesk, together with Jeff Kowalski, chief technology officer, and invited speakers Amory Lovins of the Rocky Mountain Institute and Jon Landau, producer of the Avatar film.\nMr Bass acknowledged the tough times and talked about becoming competitive by getting advantage out of Autodesk tools; getting more done with less and tighter deadlines. The focus was on managing complexity and being able to do \u2018what if\u2019 using analysis tools for design optimisation.\nAmory Lovins gave a great talk on integrated design and sustainable energy, using his passively heated house as an example. Despite being at 7,000 ft in the Rocky Mountains, the house, which incorporates a greenhouse with bananas, saves 99 percent of its heating energy, reuses half its water and produces 90 percent of its own electricity, with a 10 month payback \u2014 and it was designed in 1983.\nMany elements in the building have multiple functions that save cost. Mr Lovins compared that to transportation design, where 87 percent of fuel energy does not reach the wheels. There were some great facts, like cars used 100 times their own weight in ancient plants every day in the form of petrol. Really thought provoking stuff.\nMr Landau gave us a sneak peak of Avatar, which was rendered using Autodesk media and entertainment wizardry. Later in the week we were to get 33 minutes of 3D footage at a very special preview, before it went on major release (there were security guards walking up and down the isles with night vision goggles to ensure no footage got out). Visually it was absolutely amazing.\nJeff Kowalski gave an update on where last year\u2019s technology previews had got to. The real world and the digital world are colliding. For the keynote, Autodesk had laser scanned the casino into 15 million points and manipulated it in real time, even snapping to it, all in something that could well be AutoCAD.\nMr Kowalski described augmented reality using Personal Digital Assistants (PDAs) and design data. There are new applications that make your phone camera and GPS combine to act like a bar code scanner for real world objects, pulling up relevant additional information from the web. Underground pipes and service manuals will be served up automatically. What is in the computer and what is out of the computer will blur.\nThere were other demonstations of real time analysis of lighting and Finite Element Analysis (FEA) in mechanical design, as well as online content. All connected via the web, either to share, to process or to analyse\/advise. We are moving from \u2018design then analyse\u2019, to \u2018analyse then design\u2019, with the potential for design technology to offer multiple possible solutions.\nThe AU keynote with Carl Bass, Amory Lovins and Jeff Kowalski\u2019s presentations are available on the Autodesk University Online website and are well worth an hour of your time.\nAEC keynotes\nThe week before, Autodesk had run an Autodesk University in China. Jay Bhatt, senior vice-president, Architecture, Engineering and Construction Solutions, opened up the AEC sessions with some reflections on the trip, highlighting the Anji Bridge, the world\u2019s oldest stone arched bridge built 595 AD.\nNext came a quick succession of AEC VPs: Mark Strassman, vice-president of Autodesk\u2019s Plant Design Solutions group; Geoff Zeiss, director of technology; Doug Eberhard, Autodesk industry evangelist for Digital Cities; Phil Bernstein, vice-president, industry strategy and relations and Armundo Darling, MEP marketing manager.\nThe \u2018new\u2019 technologies were the concentration on the cool point cloud engine that had been demonstrated in the main keynote sessions and a demonstration of the Solar Analysis engine that has been on Autodesk Labs. Autodesk is also making waves in the Process Plant industry with its new 3D piping solution.\nThe main concentration of the session was on Revit and benefits on moving to Building Informaiton Modelling (BIM) and 3D products. Sustainability and the emerging renovation \/ retrofit market were also big topics together with the promotion of Integrated Project Delivery (IPD), which is the American lingo for multi-discipline design\/build. The point cloud technology also got an airing.\nKey points\nThere is just so much information and scuttlebutt available at these events that it is really hard to pull it all together in one fluid piece. So here are my key takeaways from conversations at AU.\nPoint-Cloud: Wherever you looked there seemed to be point cloud models. Autodesk has licensed some technology and developed its own engine, which I am expecting in the next release. While products like AutoCAD Civil 3D already have some point cloud technology this is the old license code. There will undoubtedly be a revolution in capturing the real world in 3D for use in CAD systems. All the vendors will have point cloud capabilities in their native products \u2013 but the cost of the scanners need to come down.\nApparently, there is an internal debate as to whether the future of scanned data will be laser-based or optical. There have been great advances recently in 3D cameras, which can be used to capture real world scenes.\n\u201cCarl Bass acknowledged the tough times and talked about becoming competitive by getting advantage out of Autodesk tools; getting more done with less and tighter deadlines. The focus was on managing complexity and being able to do \u2018what if\u2019 using analysis tools for design optimisation.\u201d\nAnalysis: Autodesk has made a good start with Green Building Studio and Ecotect but some of this will be built into Revit and other products as the years progress. While Autodesk is concentrating on the renovation and refit markets (as the downturn has left fewer new building projects), analysis and simulation will have to be incorporated into the conceptual phase of every project. The growing demand for green and energy certification and taxation, will drive sales of analysis products as every building will need to eventually be profiled.\nAutodesk is also working on ways to have all manner of analysis run automatically in the background as a design is being created, giving feedback in parallel to the design creation.\nContent: The biggest productivity gain will come when architects do not have to draw or model at all. Autodesk Seek is building momentum to offer all sorts of content for drag and drop usage across a number of Autodesk products. While this is mainly happening in America at the moment, it is certainly one to watch, and if Autodesk gets it right, will make working practices much easier.\nWeb: Autodesk is serious about delivering its applications over the web on-demand. While it sounds far off and impossible, there is considerable effort being put in to make sure all its technology can work in this way. On Labs, look at Project Dragonfly and Project Twitch. The slew of tablet PCs will also push this over the next two years. There is a growing competition between CAD developers to offer these kinds of products first to get any advantage of dominating a new platform.\nMac: Autodesk supports the Apple Mac under virtual Windows and has a number of Mac native products. Autodesk has had amazing success with its iPhone SketchBook Mobile, with over one million downloads so far. I expect this interest to expand with more Mac design software in the future.\nNew platforms like the proposed forthcoming \u2018iSlate\u2019 from Apple will offer new opportunities for mobile design. The other main reason for the resurgence of Apple in design is the sheer numbers of students that have opted for Macs in college. While Apple share is growing nicely, the long tail expects an explosion of Macs in business a few years down the line.\nAutoCAD: The biggest selling and longest running, most popular (select your favourite accolade) CAD tool is having a serious \u2018crossing the Rubicon\u2019 moment. It is hard to say what it is these days. With the recession, LT sales are in serious trouble, as its price has inflated, no longer being the cheap retail option, and now there is plenty of competition. I expect a shake up and reposition of these tools in the coming two years.\nIf you look at Project Cooper on Autodesk, it is shaping up to be the resurrection of the original concept of \u2018AutoCAD LT\u2019. Meanwhile, AutoCAD LT is the old dependable highly proficient 2D AutoCAD for documentation needs and \u2018full\u2019 AutoCAD is everything but with a concentration on 3D with maybe specific industry flavours or capabilities. There has certainly been a change of heart at Autodesk about AutoCAD now that much more powerful 3D functionality is being included. I have my fingers crossed for 3D parametrics in the next release.\nServices: One of the greatest limitations for analysis or rendering is the speed at which these run, as well as the disconnect that these produce in the process. Autodesk is evaluating hosted applications on massive server farms. While you may not realise it, your model may reside on a server somewhere, providing instant renderings or analysis results, possibly even multiple instant versions for a design solution that is being worked on. This could be a massive productivity gain never having to wait for time consuming process-centric results (see Project Showroom on Autodesk Labs).\nConclusion\nAs we cover both Mechanical CAD (in our sister publication, Develop3D, www.develop3d.com) and AEC, it was obvious that the AEC track had been the most mauled in year-on-year attendance. While BIM was still centre stage, the analysis tools, energy certification and renovation were obviously where Autodesk was seeing the majority of work being done. That said, given the economic backdrop, this year\u2019s AU was upbeat with an array of cool \u2018forthcoming\u2019 technology and ideas on design process innovation.\nAutodesk is a reformed company, especially when it comes to technology development. One only has to look at the Autodesk Labs website to see the huge number of applications and cool technologies that are in development at the moment \u2014 and that is only the stuff they are willing to share with interested customers, let alone the top secret \u2018black ops\u2019 software that is also in the works.\nIt is a shame that the economy tanked when it did, as these new tools are just waiting for some big projects. In a year or two, hopefully, there will be a recovery and a new generation of mature products will be online to help us.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/life-on-mars\/","title":"Life on Mars","date":1511740800000,"text":"Your brief is Mars: what would you design, construct or engineer for a new smart city in the Mars Valley, home to one million humans? Tanya Weaver reports on a new global competition that is asking participants to take their imaginations to infinity and beyond\nIt is 2117 and you\u2019re standing on the side of a hill, looking down onto your city and its one million residents. The sun is setting and Earth is just a distant star. You\u2019re on Mars, colonised by humans just a few decades ago.\nAs you stand there, are you breathing normally or inside a mask? Are you wearing a spacesuit or is your skin exposed? Is your home above or below ground? How do you move from place to place? What food do you eat and how do you grow it? What do you do for fun?\nThese are all tantalising questions \u2013 and they\u2019re questions that the Washington-based National Space Society (NSS) is keen to see answered. The organisation\u2019s vision, after all, is to see people and working in thriving communities beyond Earth. \u201cI grew up under the stars in the Canadian prairies and developed a passion for space,\u201d says director Chantelle Baier.\n\u201cThe prospect of human exploration really excites me. We get to start thinking about what our houses will look like in space, what we\u2019ll be wearing and what technologies will make living in space more comfortable.\u201d\nRace to the Red Planet\nIt is no longer a question of whether humans will get to Mars, but how soon. The race to the Red Planet has begun and several organisations are competing to win.\nThere is, of course, NASA, with its plan for humans to set foot on Martian soil by the end of 2030s.\nThen there is California-based SpaceX, which also plans to boldly go where no human has gone before. Its founder, enigmatic entrepreneur Elon Musk, has revealed that SpaceX will begin manned flights to Mars by 2026 in a mammoth spacecraft that can take 100 people on each trip.\nDutch company Mars One, meanwhile, proposes to send 24 people to Mars by 2035. The number is precise: potential participants are being carefully vetted, since they will be required to dedicate their lives to the mission. It\u2019s a one-way trip and the intention is for them to become Mars\u2019 first permanent residents.\nLong-term prospects Getting to Mars is one thing. Surviving on the planet is another thing entirely. From data so far received from Mars rovers, it\u2019s not exactly a hospitable or desirable environment in which to live, especially if you\u2019re one of the Mars One crew who will live out the rest of your days there.\nThe planet\u2019s atmosphere contains almost no oxygen. It is subject to intense radiation. Its seasons are extreme, with maximum temperatures in winter reaching -126C and in summer reaching a balmy 20C (the average temperature is -63C unlike earth\u2019s 14C).\nMars is prone to massive dust storms and has 62.5% less gravity than Earth. On the positive side, it\u2019s good for weight loss \u2013 if you weigh 100 pounds on Earth, you\u2019d only weigh 38 pound on Mars.\nBearing all this in mind, why design for Mars? Why dedicate time and resources to creating technology for an inhospitable planet, when we have enough going on in our own planet, which could soon become pretty inhospitable itself, if we aren\u2019t careful?\nThe reasoning offered by NASA and others is that technologies created for space can often be used here on Earth. In fact, NASA has a whole section of its website dedicated to technology transfer projects and spin-off companies based on that proposition.\nFor instance, Houston-based architect Garrett Finney was tasked by NASA to create space-saving designs for a \u2018habitation module\u2019. Inspired by this work, he then went on to set up Cricket (now known as TAXA Outdoors), a camper trailer company renowned for its compact, yet comfortable, ergonomic products.\nThen there are water filtration systems, structural analysis software in the form of Nastran, and even a CO2 recovery technology system created for Mars that is being used by small-scale brewers to put the bubbles into craft beer.\nFuelling the imagination\nThe prospect of designing for another planet, no matter how inhospitable, helps fuel the imagination. It\u2019s inspiring \u2013 not just for adults, but for children too.\nAs Mars One CEO Bas Lansdorp recently said at the Institution of Engineering and Technology\u2019s (IET) Festival of Engineering event in London,\n\u201cI think landing on Mars will change our planet. It will inspire children to want to be engineers and scientists and astronauts again, instead of pop stars.\u201d\nBut from Chantelle Baier\u2019s point of view, the human-centric aspect of designing for Mars and space is paramount. Just because we\u2019re leaving home behind, doesn\u2019t mean that our destination shouldn\u2019t be comfortable and aesthetically pleasing.\n\u201cThe emphasis on designing for space is predominantly focused on engineering and functionality. Right now, the International Space Station looks like a tin can \u2013 it doesn\u2019t look warm and homely,\u201d says Baier, whose background, interestingly, lies in the diverse fields of geology and fashion design.\n\u201cIt\u2019s about the importance of beauty and design that surrounds us on Earth and how we can implement this when planning our Mars colonisation,\u201d she adds.\nBaier is bringing these views to a new competition \u2013 HP Mars Home Planet \u2013 for which she sits on the advisory panel. Organised by HP and Nvidia, this yearlong competition, launched in August 2017, is a challenge to the creative community to design solutions for a time when there are one million people living on Mars. More specifically, the challenge envisages they\u2019ll be living in Mawrth Vallis \u2013 Mars Valley in Welsh \u2013 already identified by NASA as a potential landing site.\n\u201cWe\u2019re not trying to figure out why a million people are on Mars, we are just accepting that they are there and living a happy life,\u201d says Sean Young, HP\u2019s worldwide segment manager for product development and AEC, who is managing the competition. \u201cThis implies a number of innovations in technology and we wanted to tap into the talent and capability among architects, engineers, civil engineers and designers to really reinvent life on another planet,\u201d he adds.\nBlank sheet design brief\nIn a sense, this is the ultimate \u2018blank sheet design brief\u2019, because Mars is literally empty. Of course, there are considerations and limitations in terms of climate, soil and atmosphere, but you could literally let your imagination run riot in creating solutions for a Martian smart city.\nFrustrations on our current planet could be solved for another. Participants have the opportunity to completely rethink how humans can survive and thrive. Their mission: to think outside of the box or, more to the point, to think outside the planet.\nAs Baier says: \u201cWith Mars having onethird gravity of Earth and very, very low pressure, things that aren\u2019t safe in one planet might be safe in another. Perhaps you can design higher buildings that won\u2019t topple over.\u201d\nThe Mars Home Planet competition, unlike its predecessor, Project Soane, which was targeted predominantly at architects, will be relevant to all HP\u2019s customers, including students. \u201cThe key thing is that they are using the tools they use daily, such as Revit, Maya, Fusion 360 and 3d Max, and so for that reason, we partnered with Autodesk,\u201d says Young.\nThe competition consists of three phases: concept (now complete); 3D modelling; and rendering. Each phase doesn\u2019t lead on from the previous one \u2013 it\u2019s not a question of asking entrants to follow a design through from concept to rendering. Instead, each phase is independent, with its own deadline and prizes.\n\u201cThe idea with the concept phase was to create a low barrier for entry: anyone can get involved as they don\u2019t need to use a 3D application. The aim was to get the word out there and to inspire the next phase,\u201d Young explains.\n\u201cWe received over 470 entries and we were truly blown away by them. They ranged from basic schematics to detailed architectural concepts and even some that looked like full on thesis papers,\u201d he laughs.\nWith help from the advisory panel, which includes individuals representing space, design, architecture, science and virtual reality, among others fields, the entries were whittled down into a shortlist.\nThese were then presented to a preeminent panel of judges, including Dr Robert Zubrin, president of the Mars Society; Daniel Libeskind, architect and founder of Studio Libeskind; Chris deFaria, president of DreamWorks Animation Group; and Andrew Anagnost, CEO of Autodesk.\nThe winners were announced at the recent Autodesk University Las Vegas 2017. Among them were Jesus Velazco from Venezuela, who received the award in the architecture category for \u2018Solar Powered Colony\u2019, which features a main structure underground with only a solar farm above ground.\nKenny Levick from the US received the Infrastructure award for \u2018Mars-Genesis & Mawrth-Integra: Interplanetary Design\u2019, which focuses on how semi-autonomous architecture and the construction of an early colonisation infrastructure network can aid in the growth of humans on Mars.\nXavier Albizu of Spain received the transportation award for his \u2018MARS Multi Utility Vehicle\u2019 and Jorge Moreno Fierro of Columbia received the design award for his adaptive parametric construction machine called \u2018Bio System\u2019.\nMars Home Planet Urbanisation Concept Challenge\nSecond phase launched\nAt Autodesk University this month, the second, 3D modelling phase of the competition was launched. The deadline for entries is 25 February 2018.\nFor this phase, participants need to create 3D models of their designs in categories including civil engineering, architecture, industrial design, vehicle design, mechanical engineering, 3D art and interior design. As Young says, \u201cThe rules are: respect the physics of Mars, use your imagination and have fun!\u201d\nAsking Baier of NCC what her advice to entrants of this phase would be, she says, \u201cDesign your base for people to have things to do. I suggest science and entertainment. Don\u2019t design a Mars base like an Apollo spaceship that ignores human factors. Take into account that people need space. You want people to be happy and healthy.\u201d\nLaunch Forth\nAlthough the competition consists of these three phases, there are a whole range of other activities to get involved in at the Mars Home Planet competition website. This is being managed by Launch Forth, a software-asa- service (SaaS) platform for product design.\nParticipants are encouraged not just to log in to submit their entry and log out again, but also to join the site\u2019s community to brainstorm, comment, give feedback and collaborate around other non-challenge categories such as lifestyle, farming and robotics. There is even a leaderboard to track progress and to see how you stack up against others in the community.\n\u201cLaunch Forth is a tried-and-tested cocreation platform from Local Motors that uses open innovation to accelerate the product development process. And for Mars Home Planet, they have created a whole Mars community based on this platform and already 35,000 have signed up,\u201d explains Young.\nThe competition accepts entries from both individuals and teams. A virtual team could even be set up amongst community members on the platform. \u201cFor example, if you\u2019re an architect and you need some chemical engineering help, or you need somebody that really knows about the science of Mars, you could invite them to partner up and join your team,\u201d he adds.\nBaier considers this a real advantage, as it means entrants aren\u2019t working in silos. \u201cYou get conversations started and that helps stimulate your imagination. I think that scientists like that, too, because they are so used to thinking about things like jet propulsion that they are often not allowing themselves to think creatively,\u201d she says.\nThere is also a virtual reality (VR) element to the competition, running in parallel with these three phases. Using Mars Valley terrain from Fusion\u2019s \u2018Mars 2030\u2019 VR game, based on real NASA-supplied terrain data and imagery, special effects company Technicolor will help co-creators on the Launch Forth platform to bring the winning entries into the Unreal Engine.\nThe end result will be a VR simulation of a smart city in the Mars Valley, created through the combined imagination of Mars Home Planet contributors.\nThis crowdsourced VR experience will be revealed when the year-long competition draws to a close at Siggraph in August 2018. Here, event visitors will be able to strap on an HTC Vive headset and imagine themselves as one of the one million people experiencing life on Mars.\nYoung is looking forward to the upcoming phases of the competition. \u201cThe competition is basically a petri dish for imagining the best possible utopian society and how this one million martians will be living in this built environment. And who knows where it could lead? We could even see these designs being used in Mars in the future.\u201d\n\u25a0 launchforth.io\/hpmars\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/reality-capture-modelling\/faro-launches-webshare-enterprise-for-reality-data\/","title":"Faro launches WebShare Enterprise for reality data","date":1606780800000,"text":"Enables as-built 3D reality data to be stored on company\u2019s private server or cloud infrastructure\nFaro has launched an enterprise version of its WebShare platform, that offers real-time access to as-built 3D reality data for project management and scan-to-BIM workflows.\nThe enterprise version allows data to be stored on a company\u2019s private server or cloud infrastructure, to give firms full control over the security of their 3D reality data, and single sign-on support to simplify the log-on process.\nFaro WebShare allows reality capture data of \u2018unlimited size\u2019 to be viewed, evaluated, and shared using a web browser. The platform supports multiple reality data sources and provides users with a \u2018conversion-free\u2019 way to export point clouds, including to other popular industry standard formats like Autodesk ReCap and Bentley Pointools.\nFor the AEC industry, Faro WebShare also enables the integration of 3D reality data as part of digital twin for the operation and maintenance of buildings and facilities, and the continuous monitoring of the construction process.\n\u201cWebShare Enterprise marks another important step in driving industry applications that can leverage digital twin technology,\u201d said Vito Marone, senior director of software product marketing at Faro. \u201cOn the heels of our recent acquisition of ATS, we will continue to introduce solutions that enable customers to save time and money and improve project efficiency and reliability.\u201d\nIn August, Faro acquired Advanced Technical Solutions in Scandinavia AB (\u201cATS\u201d), to accelerate the adoption of digital twin solution technology. According to the company, the planned integration of ATS software and proprietary Traceable 3D system will enable highly accurate and repeatable 3D scans into the Faro WebShare platform, with \u201810x faster imaging at up to 1mm accuracy\u2019.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/digital-project-at-som\/","title":"Digital Project at SOM","date":1181174400000,"text":"Recording and retaining creative integrity through to the build, while optimising the design around multiple analysis criteria, has become possible for SOM architects since the introduction of Gehry Technologies\u00dd Digital Project software. By Nick Lerner\nRecently Skidmore Owings Merrill Architects, (SOM) announced their purchase and planned global rollout of 100 seats of Gehry Digital Project (DP) software. This commercially available development of Dassault Systemes\u00dd Catia software, which was developed at Frank Gehry\u00dds famous architectural practice, brings the design and production efficiencies, normally associated with auto and aero manufacture, to architecture. Digital Project (DP) is supplied and supported in the UK by Desktop Engineering, which with 21 years in the industrial design and production software business is the UK agent for DP.\nAlthough DP will be new to most of SOM, one part of the practice has been using it for nearly two years with some remarkable results at all stages of its work.\nAybars Asci, design assoc. partner at SOM, explains how his studio operates within SOM. \u00fdThis practice has many different studios, ours recently relocated from New York to London under the mentorship of SOM NY Partner Gary Haney. Part of the reason for the move was to be among, and work with, the talent and culture that can only be found in London. We work as a group of ten exploring our identity and ideas as architects. We are the opposite of a signature architect there is no single genius here; but together\u00cd\u00af\nExploring concepts\nThe group has international commissions and is currently working for Middle East clients in Dubai, Qatar, Kuwait, Saudi Arabia and Riyadh as well as for others in New York and London. The Kuwait project is a 412 m tower, the same height as SOM\u00dds nascent, Freedom Tower. Their other work is remarkable for its completely fresh optimised forms and novel and surprisingly beautiful aesthetic. On these subjects Aybars has very strong views. \u00fd We do not start out with an aesthetic vision or a preconceived typology. We look at our work in stages, the first being conceptual. At this stage we consider different layers of culture from vernacular dwellings to local customs. We learn from culture by looking in the Middle East for example; at rammed earth walls, the meaning of water, the labyrinth as a metaphor and other social, geological and ethnic symbols. We investigate plans of existing cities and the buildings within them. We write and I encourage our studio to use this and other non-visual techniques to explore the concepts that our work embraces.\u00af\nBecause each of the studio\u00dds commissions is different, Aybars sees its work as similar to that of a bespoke jeweller. The work is clich? free yet contains ideas and forms based on a deep but somehow innocent cultural understanding, which is almost na\u00c8ve at the conceptual stage.\n{mospagebreak}\nAt the initial conceptual stage the group spends some time in academic and cultural research. One of its members recently spent time studying 18th century stone cutting techniques (stereotomy) and delivered the results to the group. Of this Aybars commented.\n\u00fdWe need this type of input to keep the intellect fresh.\u00af\nBy acting as a repository of rules, developed into constraints, DP allows SOM to apply strict scientific rigour to this series of creative activities. And, because DP enforces design rules that have been developed at the creative stage it also contains the group\u00dds cultural intelligence.\nThe stages are set\nRules are important to the group because it is upon their foundation that the next stage of their work is built. Following the Conceptual Stage is the Analytical, and with it comes the change in question from Why? to How? At the Analytical stage, new criteria are applied to the process. Performance-based design using thermal, daylight, wind, acoustic, structural and other analyses inform the design. And, it is on these bases that the conceptual design is tested and optimised.\nThroughout these stages the use of Digital Project has become an increasingly valuable tool. Aybars explains how it is used. \u00fdWe generate rules which are kept in DP. These rules are precise, have the highest levels of geometric rigour and are maintained through to construction (the third stage). We write formulae into DP. These allow us to achieve a performative design approach. They can vary from structural form finding algorithms to intelligent staircases that conform to building regulations. It is through these rules that the form is refined. This means that the final design is completely optimised as we progress.\nThis is not the computer designing buildings; it does however modify them according to inputs of cultural and physics based data. Aybars comments, \u00fdSometimes the rules are in conflict with each other in which case we modify the hierarchy\nGenuinely new forms\nThe parametric and rule based capabilities of DP allows the studio to create genuinely new patterns, forms and shapes. The software also offers choices because sometimes there are several solutions to the same problem. Being based on Dassault Systemes\u00dd Catia V5, DP offers significant productivity benefits up to and beyond the level of documentation. It also allows the studio to easily fine tune shapes based on further analysis \u00b1 or client requirements.\nThe rules for designs that this young dynamic group creates are handled from start to finish within DP, which has been adapted from Catia V5 not only by Gehry Technologies, but also by a host of contributors including SOM itself. Geoff Haines of the newly renamed DesktopEngineering, DP\u00dds UK agent, has been providing Catia as part of Dassault Systemes CMP VAR channel and recently hosted the European DP User Group Meeting at the Institute of Physics in London. He said. \u00fdThis cross fertilisation between two diverse engineering disciplines using what is effectively the same software, was bound to happen. Once architects realised the potential for risk reduction and accelerated rule making and output that Catia offers; they were not slow in taking it up.\u00af\nZaha Hadid Partner, Patrik Schumaker, another user and champion of DP recently elaborated on this view when he said. \u00fd Productivity, creativity and elegance\u00cdare available from this software\u00af\n{mospagebreak}\nChallenging buildings\nHaines continued. \u00fdThe ability to combine disparate design-to-manufacturing systems into one coherent system has been known to produce multiple benefits in a PLM context for several years. Boeing, Toyota and many others, use this software and methodology to control the design-to-manufacture process of rule based aeroplanes and cars retaining design intent throughout, in a very scientific way.\n\u00fdBuildings have always challenged design and manufacturing technologies. Industry has been able to responded to that challenge with DP, which is being adopted throughout many AEC supply chains and driven by each players\u00dd specific needs.\u00af Gary Haney\u00dds SOM group in London is working on some of the world\u00dds most exciting buildings; the levels of creative innovation being applied are very high. For the first time ever the essence of both the creative process and subsequent analysis and production stages can be unified and retained; this produces an often-surprising but ultimately practical outcome with no loss of design intent.","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/new-marine-information-service\/","title":"New marine information service","date":1092182400000,"text":"Digital Media has developed an innovative, new marine information service for use by a wide variety of marine interests including sportsmen and commercial fishermen, as well as sailing and diving enthusiasts. Digital Media is a member of the Enterprise for Innovative Geospatial Solutions\u2019 industry cluster.\nWhat makes Digital Media\u2019s product unique is they utilize remote sensing data in combination with proprietary software and processing system. This allows fishermen to make the best use of their time and money. Digital Media uses a proprietary mathematical algorithm that accurately identifies where conditions consistently result in concentrations of baitfish. Using this information they can direct fishing interests to these locations using GPS coordinates. Originally developed by the National Marine Fisheries Service, the algorithm has been tested, studied over a number of years and has now been refined to achieve amazing accuracy. Factors contributing to the final composite image include sea surface temperatures, temperature gradients, water clarity, and water depth. Remote sensing technicians to accurately deliver the location of baitfish in the Gulf of Mexico as well as the Atlantic and Pacific Oceans analyze this combined information.\nPursuing game fish as well as commercial species can be a time consuming and expensive process. Being able to eliminate a significant portion of a trip spent searching for the right conditions are a tremendous benefit. Today, commercial and recreational anglers have access to GPS systems that can put a fishing vessel within feet of a specific coordinate in the ocean. Knowing where the game fish are most likely to gather is the other link in the saltwater fishing equation. The subscriber receives a full color image of the offshore area of his choice accompanied by a text of information drafted by one of Digital Media\u2019s processing experts. Digital Media\u2019s composite model allows the angler to go directly to the closest location, minimize his running time and expense, and maximize the time spent fishing.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/technology\/pc-workstations-hard-disks\/","title":"PC Workstations: hard disks","date":1104969600000,"text":"In the latest instalment of his series of articles which look at the components of a PC workstation, Robert Jamieson gives us the low down on hard disks, their physical characteristics and which are best for CAD.\nThe hard disk is the slowest component (bar floppy and CDs) that is used in a modern workstation. This means it has more effect on the total performance than most people realise. Even if you work from a network, loading up applications, or using the pagefile, will all be done locally using the hard drive. Most CAD applications put some of the loaded information into the pagefile or at least create temporary files which all sit on the hard drive.\nPhysical characteristics\nI will first go through the physical characteristics of what a hard disk is. Hard disks are mechanical devices. As a result they fail more than any other device in a computer. Each hard disk has a rotating platter (glass for example) coated in magnetic media. These spin at different rates from 4,200 RPM to 15,000 RPM in the latest SCSI (Small Computer System Interface) models. A drive can have multiple platters depending on the amount of storage it has. The data is read and written to this by a head that tracks across the spinning disk. In the old days the disk would spin three times before the data was read off but today it\u2019s all read in one go. Each drive has its own RAM or Cache on the drive and this ranges from 2Mb to 16Mb. Drives are measured in access times and data transfer.\nWorkstation disks fall into two main categories IDE (Integrated Drive Electronics) and SCSI. SCSI was the traditional workstation drive and attracted the latest technology in spin rates and cache. SCSI needs a dedicated intelligent controller card or high-end motherboard to support the drive. The Ultra 320 standard is the latest SCSI generation. The problem with SCSI is the cost of each drive is more and the controllers aren\u2019t cheap either. IDE, on the other hand, has drive controllers embedded on each motherboard and the IDE drives offer cheap reasonable performance. The current sub types of IDE drives are parallel and serial.\nParallel ATA is the primary internal storage interconnect for the computers, connecting the host system to peripherals such as hard drives, optical drives, and removable magnetic media devices. Parallel ATA is an extension of the original parallel ATA interface introduced in the mid 1980s and maintains backward compatibility with all previous versions of this technology. The cable standard is 40-wire cable, which has been replaced by an 80-wire version for the Ultra standard. These are often \u201cround cables\u201d to improve airflow.\nSerial ATA is an update of Parallel with better interface speed and cabling. They also require a different interface and are not interchangeable with Parallel ATA. However, you can get an adapter to fit a Parallel ATA drive to a serial controller. There is no great improvement of performance between PATA and SATA \u2013 it\u2019s more of a future standard that will give improvements once the mechanical performance of drives improve. New SATA drives support NCQ technology, which can manage the internal queue in which commands can be dynamically rescheduled and reordered. This is supported on Intel\u2019s 925 chipset, for example. This looks to be a good technology and shows that the various manufacturers have put a lot of development into SATA.\nRaid\nThere is a lot of talk about whether or not RAID (Redundant Array of Independent (or Inexpensive) Disks) gives increased performance. RAID is where several physical disks are combined into an array for better speed and\/or fault tolerance. A Level 0 Raid array implements data striping where file blocks are written to separate drives. The setup doesn\u2019t provide any fault tolerance, because failure of one drive will result in data loss and this actually increases MTBF (Mean Time Before Failures). However, in practice a Level 0 array does help the performance of intensive applications quite a bit.\nLevel 1 implements data mirroring. Here, data is duplicated on two drives either through software or hardware. It provides faster read performance than a single drive. Level 3 requires at least three drives. Data block is striped at byte level across drives and error correction codes (parity info) is recorded on another drive. Provides fault tolerance but slower writing performance. Level 5 improves performance but also striping parity info across multiple drives and provides redundancy for three or more drives.\nRAID can be implemented in IDE (PATA and SATA) and SCSI. However, the cost of some of the controllers with extra cache is quite a lot and often more of a technology for a server than a humble workstation. A lot of the newer motherboards come with RAID onboard for IDE, which means you need only an extra drive to have one. However, a lot of larger manufacturers don\u2019t give you the option as it\u2019s harder to pre-install Windows (the RAID has to be functioning before the OS is installed).\nWhat\u2019s a hard disk cache?\nThe controller on a drive copies the most recent access information into the RAM or Cache on the hard disk. The algorithm that controls this has an effect on what is copied. The RAM is 10x faster than accessing the physical platters and if the data is accessed again, it\u2019s supplied faster.\nWhat does CAD need\nNow I have talked about the physical hardware \u2013 lets look at what\u2019s best performance for CAD? SCSI still just gives ultimate performance in a RAID array. This is expensive and an IDE RAID setup with high performance IDE drives is good and a lot more affordable. The thing about IDE drives is there is a great range from entry-level to performance, whereas practically all SCSI drives are performance drives so you always get a good one. The performance of a single top end IDE drive can be good enough for most CAD applications.\nI have seen tests in some computer magazines stating that RAID doesn\u2019t make any difference; this is because they tested loading games which are single large files. An assembly of a 3D model would have a lot of smaller files often repeated. Imagine how many nuts and bolts are in a given assembly. If the drive or RAID has a large cache which is RAM (RAID 0 doubles up the cache) this bolt would come from the cache and therefore load faster and not need to access the platters. This is also true when saving or closing an application where the data has to be written.\nThis is all OK if you are buying a new computer but what can you do to improve your current performance? Defragment your drives! As data is written to drives it\u2019s placed in the first available space. After a while as data gets deleted and replaced with bigger files the hard disk gets messy with a single file spread all over the disk. The standard defragmenter in Windows is a start but 3rd party defragmenters are a lot better and can put the applications and most recent access data together. Some can even defragment the pagefile and put it at the front of the disk. Why should this make it faster?\nIf you look at the disk platter the outside edge track covers more distance in one revolution than the inside edge track. The areal density is the same \u2013 this can account for 20% difference in performance from the front of the disk to the back. This is also why disks with increased areal density have better transfer rates. For this same reason it\u2019s better to keep the disk half full. With the sizes available today in IDE drives this should not be a problem. One tip on defragmenting from old \u2013 after you have defragmented a drive restart the computer and don\u2019t run other applications while it\u2019s working. This will stop applications going for data that\u2019s been moved.\nReliability\nAs I said at the beginning drives will always fail. Hard Disks will fail when they are new i.e. manufacturing defect or when the bearings have worn out after extended use etc. It\u2019s a good idea to test a new system before it\u2019s put into a production environment and likewise cycle out two year old drives that have had a hard life. Different manufactures have different track records with reliability. I have a large collection of two to three year old drives from one manufacturer that have all failed with similar faults. I\u2019m not buying them again! Some of the bleeding edge stuff tends to have slightly more problems \u2013 a candle that burns twice as bright lasts half as long.\nIf I\u2019m saying drives fail, why increase data loss chance by using RAID 0? I have been using SCSI RAID and now IDE RAID 0 for ten years and as long as you have good backup and replace drives after two years, I will still take the chance. I want the performance it offers!\nRobert Jamieson works for workstation graphicsspecialist, ATI","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/prized-assets\/","title":"Prized assets","date":1463097600000,"text":"A closer look at the role of engineering software in the design of two winning projects from the 2015 Institution of Structural Engineers Structural Awards\nSSE Hydro, Glasgow\nThe SSE Hydro opened in Glasgow in 2013 and plays host to a galaxy of national and international music megastars, as well as other entertainment and sporting events. In the IStructE Structural Awards, it won the Regional Groups award, for projects that have benefited their community and had a positive impact on lives.\nThe multi-purpose indoor arena has a 12,500-capacity auditorium, with a combination of fixed, retractable and removable seating that offer versatile layouts for a wide range of events. The structure was designed to the goal of providing audiences with the best view from every seat.\nMuch of the design for the new arena was conceived when BIM was still emerging as an important process for the engineering industry, giving BIM collaboration a chance to prove its value. Parameters from structural engineering software Oasys GSA were used to define the complex roof geometry, with the remainder of the structure developed with Bentley solutions. This work was brought together to create one model for analysis and design\nFrom the resulting GSA model, piling schedules and steelwork centreline lengths were derived to calculate the material quantities required.\nThe BIM model\u2019s intelligence was used to communicate information between the design team, the contractor and the sub-contractors, reducing risk on the project and supporting the client\u2019s traditional procurement route. The use of BIM was central to the success of this geometrically complex building, with full 3D co-ordination of the structure, M&E and architecture.\nVegas High Roller\nThe Vegas High Roller, the tallest observation wheel in the world, won the IStructE award for Arts or Entertainment Structures. Its pioneering features, including its double-glazed spherical cabins supported from a single tube rim, all came together with the support of structural engineering software Oasys GSA.\nThe complexity and precision required for such a project were developed, designed and portrayed using the most advanced BIM practices available. The High Roller was a \u2018one of its kind\u2019 project in many ways: materials were custom made, for example, so a successful BIM model and coordination in 3D were vital requirements.\nGSA was used to prove early design concepts using a beam-element model, the geometry for which was imported from a model created with Bentley Systems\u2019 generative components.\nThe integration between GSA and a number of other third-party tools, including Bentley Systems, Rhino and Navisworks, all assisted in the successful delivery of the project.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-bentley-systems-release-staad-pro-v8i-selectseries-6\/","title":"NEWS: Bentley Systems releases STAAD.Pro V8i SELECTseries 6","date":1436400000000,"text":"3D structural analysis and design engineering tool gets a boost in user experience and productivity\nSTAAD.Pro V8i SELECTseries 6, the latest release of the 3D structural analysis and design engineering software from Bentley Systems, has a major focus on improving the user experience\nThere\u2019s a comprehensive new editor, which provides a parallel method for defining and tweaking the software\u2019s data file.\nA new Building Planner allows buildings to be quickly defined in a new modelling mode, using a set of regular plans that are constructed into a building form, analysed and the key concrete components then designed and detailed using the detail reinforced concrete design and detailing program, STAAD RCDC.\nThere has also been a complete redesign and restructuring of the solution algorithm in the Advanced Analysis solution (Advanced Math Solver). According to Bentley, this can result in a noticeable performance boost with some of the more complex models used with STAAD.Pro. Also the implementation of a Ritz Vector Method eigen extraction can provide improved dynamic analysis solutions when dealing with large numbers of modes.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/preview-nxt-bld-2018\/","title":"PREVIEW: NXT BLD 2018","date":1526256000000,"text":"AEC Magazine\u2019s inspirational conference and exhibition returns to London on 13 June. Looking beyond BIM, NXT BLD presents incredible future technologies that will help change the way buildings are architected, engineered and constructed\nNXT BLD (Next Build) is AEC Magazine\u2019s annual industry event, taking place this year on 13 June at Congress Centre in central London. We aim to build an open forum for forward looking designers and construction professionals to gather and hear from key developers, researchers and practices, which are experimenting with revolutionary future technologies.\nAEC Magazine was the first journal to concentrate specifically on BIM and the move from drawings to modelling. With this process underway and with many firms now focussing on digital processes, NXT BLD builds on this to cover technologies which extend model-based design to drive digital fabrication, digital processes and other benefits such as real-time visualisation, immersive VR and onsite AR.\nThe last 10 years of our industry have mainly been spent in developing the skills to model and to derive processes to collaborate in a much more complex, yet explicit, manner. While this is still very hit and miss, advanced firms are already exchanging digital models with fabricators, replicating practices adopted by automotive and aerospace firms. While this is usually found in high-end projects, if we\u2019re ever to solve issues such as the UK\u2019s housing crisis, then digital fabrication and modularisation must play an important part in mass produced housing. For the industry this represents an incredible challenge and change to the way buildings are constructed and designed.\nTopics and speakers\nIf there were to be a main theme for NXT BLD 2018 then it would be Digital Fabrication. This combines a number of key themes: modularisation, factory automation, robotics, CNC, 3D printing and pre-fabrication. We are excited to have an incredible group of expert speakers from practice and research, to examine the many aspects of what \u2018going digital\u2019 in the context of fabrication means. Hedwig Heinsman (DUS architects \/ Aectual), Bruce Bell (Facit Homes) Stefana Parascho (Gramazio Kohler Research), Andrei Jipa (ETH Zurich) and Andrew Watts (Newtecnic).\nVirtual Reality and Augmented Reality were certainly hot topics for last year and as the industry rapidly matures, we are seeing the technologies being evaluated for deployment within the industry. We welcome speakers Marc Petit of Epic Games and Dr Max Mallia-Parfitt from Fulcro to look at what\u2019s coming in terms of software capabilities and headset designs. Whilst VR is certainly more usable, mature and a \u2018known entity\u2019, Augmented Reality still appears to be in a formative stage.\nLondon\u2019s own UCL has an array of construction- related research projects and labs and we are honoured to have two researchers from the University, including Dr. Eleni Papadonikolaki, to talk on aspects around Blockchain, one of the hottest topics in computing. The Construction Blockchain Consortium is building a proof-of-concept transactional ledger system to potentially be applied to replacing building contracts, enhancing supply chains and even linking to modelling.\nChanging the scale from buildings to cities, IoT (Internet of Things) technology is heavily impacting the field of urban design enabling the monitoring of traffic, waste, people, assets, power, water \u2013 just about every economic and environmental condition. We have talks from Dipa Joshi of Assael Architecture and Rebecca De Cicco of Digital Node looking at the planning and impact of Smart City projects.\nAs AEC software development continues at pace, having the right workstation hardware in place to support graphics intensive applications, such as VR, is always a challenge. Mike Leach from Lenovo will examine the hardware trends and identify potential choke points to avoid for optimum performance.\nExhibition\nNXT BLD is not just a conference, but also a high-tech exhibition where delegates can get their hands on the very latest technologies. This year there\u2019s a big focus on real time visualisation and VR with experts from Enscape, Virtalis and Soluis to take your BIM models into another dimension.\nSimulation will also be \u2018real time\u2019 with Ansys showing how advanced analysis tools can now be used by non-experts.\nLenovo will be showcasing a whole host of desktop and mobile workstations that can deliver the power that advanced AEC, design viz and VR software demands.\nThere\u2019ll also be 3D printing from ARRK, real-time issue tracking from Revizto, point cloud to intelligent 3D mesh modelconversion from Pointfuse, collaborative online BIM from 3D Repo and lots more.\nNetworking\nWith regular coffee breaks, lunch and a post-event drinks mixer in the space adjacent to the NXT BLD theatre, there will be plenty of time to socialise, meet with your peers and talk to our illustrious speakers.\nEvent details\nRegistration \/ exhibition \u2013 8:30am \u2013 6:45pm\nConference \u2013 9:45am \u2013 5:15pm\nNetworking drinks \u2013 5:15pm \u2013 6:45pm Congress Centre, London, WC1B 3LN\nExclusive 2-for-1 offer\nFor readers of AEC Magazine, we are offering a strictly limited number of tickets on a special 2-for-1 offer. Simply use the promotional code 241AEC and you can pick up a pair for \u00a350. Tickets include full access to the conference and exhibition, refreshments, lunch and drinks at the networking reception. When they\u2019re gone, they\u2019re gone.\nThe Speakers\nAs director of applied technologies \u2013 Mixed Reality, Augmented Reality and Virtual Reality \u2013 Dr Max Mallia-Parfit is a computer scientist who builds VR simulations for Fulcro\u2019s clients, including CrossRail. In his talk, he will look at VR\/AR in construction and forthcoming immersive technologies. \u2022 VR \/ AR \/ Mixed Reality\nRebecca De Cicco runs a global business assisting clients to upskill to get the most from BIM and associated digital processes in multidisciplinary design and construction environments. De Cicco will bring a global perspective on BIM adoption, specifically with implementation of Smart City projects. \u2022 Smart cities\nMike Leach will present on the key technology developments driving the workstation marketplace today, and how these will impact your ultimate productivity and workflow. Learn what to look out for, where to invest your budgets and how to best configure your next workstation investment. \u2022 Workstation technology\nAs co-founder of DUS architects and 3D print tech firm Aectual. Heinsman\u2019s practice won fame with its 3D printed canal house project, small 3D printed homes. In this talk Heinsman will explore Aectual\u2019s bespoke technology that enables the production of 3D printed building products on an industrial scale. \u2022 Architecture \/ 3D printing \/ robots\nThe journey into real time architectural design viz and Virtual Reality (VR) has only just begun, with visual quality now approaching that of ray trace rendering. Marc Petit from Unreal Enterprise will give a glimpse into the future with game engine turned serious enterprise viz tool Unreal Engine. \u2022 Real time design viz \/ VR\nEngineer and architect Andrew Watts specialises in the engineering design of facades. In this future looking presentation he will cover a wide range of technologies including optimised design for manufacture, mass customisation, fabrication, assembly and installation, as well as machines, robots and drones. \u2022 Architecture, engineering & construction\nAs a PhD researcher at Gramazio Kohler Research and as part of the National Centre of Competence in Research (NCCR) Digital Fabrication at the ETH Zurich, Stefana Parascho is an expert in multi-robotic assembly processes for architectural applications such as robotically fabricated structures.\nPhysical architectural forms are not keeping up with the limitless design freedom of digital environments. But 3D printing is here to help us bridge this gap. In this fascinating presentation Jipa will give his insight into the potential impact of 3D printing on concrete construction. \u2022 Smart concrete \/ computational design\nThe built environment is ripe for disruption and there are plenty of emerging technologies queuing up to get a piece of the action \u2013 from IoT to VR. But as we start to grow and sculpt digital smart cities, we also must be vigilant about the way we use technology and who is in control of it. \u2022 Smart Cities \/ IoT \/ VR\nDr. Abel Maciel is honorary senior research associate at the Bartlett School of Architecture specialising in computational design, Artificial Intelligence (AI) and Distributed Ledger Technology. At NXT BLD he will present on the future of BIM, Common D ata Environments (CDEs), IP, and Blockchain. \u2022 CDEs \/ IP \/ Blockchain\nPre-fabrication has had its day, says Bruce Bell. Digital Construction is the future. In this fascinating presentation he will explain how Facit Homes is reinventing how homes are built through innovation, R&D and digital technology. Think digitally manufactured big wooden \u2018lego\u2019 designed for assembly. \u2022 Digital construction \/ BIM to manufacture\nBlockchain isn\u2019t just a technology for crypto currencies. It has potentially revolutionary applications within the AEC sector. Dr Eleni Papadonikolaki, from UCL Bartlett School will talk on the research of the Construction Blockchain Consortium and its application to digital management. \u2022 Blockchain in AEC\nWatch the presentations from NXT BLD 2017\nTo help get you in the mood for NXT BLD 2018, all of the videos from our 2017 event are available now at nxtbld.com\/videos Highlights include Tim Geurtjens from MX3D on 3D printing structural metal objects and Johan Hanegraaf\u2019s live demo 3D modelling buildings inside VR.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nDr Max Mallia-Parfit \u2013 Fulcro Group\nAs director of applied technologies \u2013 Mixed Reality, Augmented Reality and Virtual Reality \u2013 Dr Max Mallia-Parfit is a computer scientist who builds VR simulations for Fulcro\u2019s clients, including CrossRail. In his talk, he will look at VR\/AR in construction and forthcoming immersive technologies. \u2022 VR \/ AR \/ Mixed Reality","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/raising-the-roof\/","title":"Raising the roof","date":1124841600000,"text":"A London school was recently transformed, with the central courtyard redeveloped and covered it so it could be used throughout the year. The ambitious design required the expertise of structural design consultants, Michael Hadi Associates, using Robot Millennium structural analysis software from ISS.\nExciting architecture and a positive environment are well understood to improve performance in the workplace. Yet within the education system, many schools find themselves stuck with the legacy of less sympathetic and inspiring architecture, without the resources to make significant changes to the structure and design of their surroundings. However, one struggling school in South London secured Government funding to help transform its uninviting structure, identified as a key factor in explaining its poor results.\nThe school now boasts an extraordinary covered quadrangle, flooding the reclaimed space with light and creating a more open and inviting space for pupils and staff alike. The ambitious design of the quadrangle required the expertise of structural design consultants, Michael Hadi Associates, using Robot Millennium structural analysis software from ISS.\nSecondary failure\nIn 1998 Kingsdale School in Dulwich, London, found itself struggling with poor results, and behavioural issues among its pupils. The built environment of the school was not aiding the situation. The school was built in a rectangular formation in plan, with a largely unused, open section in the middle. Pupils had to walk round the quad along narrow, dark corridors to get to their classrooms. The cramped, busy corridors were difficult for staff to monitor and time-consuming for pupils to navigate between lessons.\nAn extensive research programme was initiated, which included interviewing pupils, to devise the best solution to improving the school. The chosen design involved redeveloping the central courtyard and covering it so it could be used throughout the year. Walkways would be created through the courtyard for easy access across the school, with the rest of the space remaining open for flexible use such as group work or mealtimes.\nRoof of concept\nThe design of the roof was clearly significant in realising the regeneration of the School. Structural design consultants, Michael Hadi Associates, were brought on board as structural consultants for this vital part of the school\u2019s transformation process. John Roycroft is an associate at Michael Hadi Associates, \u201cOur challenge was to find a roof design that would satisfy several requirements,\u201d he explains. \u201cFirstly, we had to ensure that the weight of our proposed roof was as low as possible to enable a lightweight structure to span the full 40m courtyard width without intermediate supports. We were also knew that the existing building that the roof would sit on had very little spare capacity to resist extra lateral loads from wind. In addition, since this was a Government-funded project, we needed to keep costs as manageable as possible, yet still respect the aesthetic integrity required by the architect.\u201d\nMichael Hadi explored several possibilities, including a folded plate ply roof, but the final design was a tied arch structure using transparent ETFE cladding. The ETFE panels contain three skins, controlled by sensors. Both the top and middle layers are patterned. To reduce light transmission in the summer the middle layer moves closer to the top layer, this intensifies the pattern and reduces light penetration. De-humidified air is pumped into the cushions to control them. \u201cThe big advantage of ETFE is that it is really light,\u201d says Roycroft. \u201cThe quadrangle is 40m by 80m, which is quite a big space, but the roof design only weighed in at approximately 40kg per square metre. It\u2019s only the steel arches that have any significant weight. Lateral loads were minimized by tying the aches at their feet and through detailed analysis of wind pressures through wind tunnel testing.\u201d\nUnderneath the arches\nThe chosen design of the roof created a significant structural engineering challenge for Roycroft and his team. The roof comprises 22 arches of 11 different shapes and sizes, which needed to be pre-stressed on site to compensate for wind uplift pressures they could encounter once in position. \u201cWe had to define what the curvature of each arch would need to be prior to installation for the steel fabricator, and then what the final position of each arch would be once attached to the school roof,\u201d explains Roycroft. To complicate matters further, the slenderness of the arches was outside the limits set by the now superceeded code BS5950 Part 1:1990, in order to meet the need for weight reduction and aesthetic appeal. The slenderness of the structure, plus the presence of cables dictated that a full detailed geometrical nonlinear analysis be carried out.\nAt Michael Hadi Associates, the structural engineers prefer to use analysis and simulation software to confirm their hand calculations, rather than immediately turning to their computers for a quick answer. \u201cUsing computerised analysis cuts out people\u2019s intuition,\u201d explains Roycroft. \u201cIf you\u2019ve done preliminary calculations yourself beforehand, when you run the problem through a computer you have a much greater understanding of the process \u2013 and a better idea of whether the result that the computer comes up with is accurate.\u201d\nBut on this occasion, Roycroft knew he needed some specialist assistance to ensure the arches were pre-stressed in the right way and to the correct geometry. \u201cWe had to be confident that second order effects were being modelled accurately, particularly as we were using a compression member that was very slender,\u201d he explains. \u201cWe hadn\u2019t got a program here that modelled cables in a non-linear fashion \u2013 crucial for the accurate positioning and tension of the arches. We tried several structural analysis packages, but we quickly determined that Robot Millennium was the best tool for the job.\u201d\nStructural support\nRobot Millennium from ISS is a complete structural analysis solution, with a particular strength in cable analysis, but also featuring frames analysis, FEM plates, shells and solid elements, together with code checks, dynamic analysis, both geometric and non-linear analysis and many other advanced features. The very straightforward GUI allows the user to model complex structures quickly and accurately and, crucially for Michael Hadi Associates, Robot is a \u2018true\u2019 non-linear solution with many different non-linear solvers and parameters available, ensuring model convergence of even the most challenging of structures. Using Robot, Roycroft could confidently model the true behaviour of the structure and not only the applied prestress but also the insitu cable prestress under all loading conditions.\n\u201cRobot is very user-friendly and as it\u2019s Windows based the interface is intuitive and quick to learn,\u201d says Roycroft. \u201cIn addition, if you do come up against difficulties, the Robot support team is excellent. You can email them an analysis problem and they are quick to help and sort it out. It\u2019s also important that they understand the engineering challenges we face, not just the software issues. We can explain situations from a technical point of view and they have a good understanding of how best to find the right solution.\u201d\nFull marks\nThe pre-stressing of the arches needed to be absolutely accurate to ensure the sliding bearings on one side of the arches were specified correctly in terms of range of movements expected from temperature, gravity loading and wind.\n\u201cWe also had to be confident that enough prestress was present to make sure the arch was always in compression under design wind uplift pressures. Having spent a lot of time looking at the problem both qualitatively and numerically ourselves beforehand, we were confident that even though our solution went beyond normal slenderness limits it would still work. Even so, the positive and accurate results we got from Robot gave us the confirmation and confidence we needed,\u201d says Roycroft. The solution has also saved the team a considerable amount of time. Roycroft estimates that to manually calculate the pre-stress requirements of the 11 different types of arch would have taken weeks. \u201cIt was a weekend\u2019s work using Robot Millennium,\u201d he says.\nOn the day of installation, each arch arrived on site in three pieces before being bolted together. Special lifting and pre-stressing cradles had been designed by Michael Hadi Associates for the specialist steel contractor SHStructures to lift each section into position. When the roof was finally ready to install, Roycroft and his team checked the pre-stress levels on site and discovered that Robot\u2019s results had indeed been very accurate. \u201cIt\u2019s fair to say this was a very successful project,\u201d says Roycroft. \u201cFrom our point of view this was a real team effort, and the people at ISS and also the ROBOT Millennium software were a part of that team.\u201d\nEnd of term report\nThe resulting roof has transformed Kingsdale School. The new roof has opened up the space and narrow corridors are a thing of the past. In designing the roof, Michael Hadi Associates were prepared to go beyond pre-defined limits to create the best possible solution for the architects, and ultimately for the benefit of pupils at the school. \u201cThis was a project that we all enjoyed working on, since the results could have such a positive effect on the school,\u201d concludes Roycroft. \u201cWe knew this roof and space could be very special from the beginning, we were entrusted by all involved to make it a reality; Robot Millennium was pivotal in getting us to that end point.\u201d\nThe culture and academic fortunes of Kingsdale School have transformed alongside its architecture. Grades have risen significantly, and the school is now a specialist college for the performing arts as well as a multimedia environment for academic, creative and vocational studies. Attitudes towards the school have completely changed, and these days staff and pupils alike are eager to welcome visitors to what they call \u201cthe DFES\u2019s greatest educational project \u2013 Kingsdale School\u201d.","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/municipal-software-corporation-welcomes-gis-systems\/","title":"Municipal Software Corporation welcomes GIS systems","date":1053043200000,"text":"Municipal Software Corporation, a developer of local government business process automation software and a wholly owned subsidiary of Municipal Solutions Group, Inc. is proud to welcome the Village of Arlington Heights, Illinois (population approx. 76,000) as their 7th client in the state. The village plans to use CityView 8.NET Enterprise for all major daily business including Building and Zoning, Planning, Engineering, Public Works and Health, including service requests, work orders and engineering in order to develop a comprehensive asset management system. Along with this move to an enterprise solution, there will be a high level of integration with their existing GIS system. The village began their search for an enterprise solution last year and found that many solutions were \u2018modular\u2019 in nature. Arlington Heights searched for a solution to satisfy multiple needs in many different departments. \u201cIt is always difficult to find a system that will combine all issues related to building permits, inspections, planning, and licenses and still handle issues related to asset management including water and sewer and work orders. The main goal was to integrate daily business operations with GIS and automate all of these tasks to a maximum,\u201d commented Kwiatkowski. The village found the solution they were searching for in CityView 8.NET Enterprise.\nSince its incorporation in 1887, the Village of Arlington Heights has grown into the largest Cook County suburb and the fifth largest suburb in the metropolitan area.\nCityView 8.NET Enterprise is a true enterprise solution that integrates functions across an organization and enables e-government. CityView 8.NET helps government automate business processes such as Building, Planning, Code Enforcement, Business Licensing, Public Works, Emergency Management and more. Award-winning CityView integrates with existing systems, including financials, other departmental automation software, document management, IVR, cashiering, field data collection and GIS. With clients in 30 states and five provinces, and in business for over 20 years, Municipal Software Corporation is the one-stop provider of everything government needs for an enterprise solution, including the software, the training, the services and the post-purchase support.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-simeon-balabanov-chaos-group\/","title":"Video: NXT BLD 2019 \u2013 Simeon Balabanov, Chaos Group","date":1565049600000,"text":"Getting it real: AEC workflows real-time, real fast and ray traced \u2013 NXT BLD London, June 2019\nInteractive rendering, raytracing, real time are all not uncommon and unheard of in Chaos Group\u2019s rendering world, and Simeon Balabanov himself can look back at years of experience in using all of them throughout their evolution and in numerous workflows. Fresh off the release of V-Ray for Unreal\u2019s first major Update 1, he joins with an overview of the way Chaos Group\u2019s development team is set to answer and lead AEC industry\u2019s growing interest in interactive real-time workflows across platforms. From bringing interactive rendering directly to the Unreal viewport, through providing cross-platform exchange of geometry, materials, lights, to 1-click export from and to host applications \u2013 all is set to provide AEC professionals with a powerful new design tool, seamlessly integrated within their process. Photoreal, reality-based and reality-tested.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nFederico Rossi, DARLAB (Digital Architecture & Robotic Lab)\nAdvanced Robots for Advanced Architecture.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/revit-plug-in-to-accelerate-residential-feasibility-studies\/","title":"Revit plug-in to accelerate residential feasibility studies","date":1586390400000,"text":"Matterlab\u2019s Unitize designed to accelerate the assessment and reassessment of the architect\u2019s masses\nLondon-based Matterlab is gearing up for the launch of Unitize, a new generative design tool designed to help architects assess residential masses in seconds. The Revit plug-in was commissioned by Make Architects.\nAccording to Matterlab, most designs for residential projects follow a similar workflow, requiring the architect to assess a site, taking into consideration a series of building or council requirements. During this process the architect will generate multiple volumes representing the building and will fit units within them, thereby generating floor layouts to obtain the total unit mix. Usually the resultant mix does not meet the brief, so the process repeats again and again with different masses until a good solution is found.\nTo eliminate that cyclical process Matterlab\u2019s Revit tool is designed to accelerate the assessment and reassessment of the architect\u2019s masses. It works by providing an architect with a high-level interface to play around with when evaluating design options. The mass can be quickly pushed and pulled, with the unit mix and floor layouts recalculated instantly.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-autodesk-adds-ifc-export-to-inventor\/","title":"NEWS: Autodesk adds IFC export to Inventor","date":1449187200000,"text":"New export option for Autodesk Inventor 2016 R3 will improve exchange with BIM tools\nIn a move to help manufacturers of building products to more easily exchange data with Building Information Modelling (BIM) systems Autodesk has added Industry Foundation Classes (IFC) export capability to Autodesk Inventor.\nAutodesk says the new addition to Autodesk Inventor 2016 R3 will help facilitate the delivery of 3D mechanical designs for building and construction applications.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nLeica Geosystems boosts construction layout\nNEWS: WorldViz puts products in place for collaborative VR\nNEWS: ArchiCAD 21 centres on optimised stair design\nChaos Group brings real-time to V-Ray for SketchUp\nNEWS: Allplan tunes bim+ for better collaboration on BIM projects\nArskan using compressed reality meshes to drive digital twins\nNEWS: AMD makes aggressive play for professional VR\nCesium to connect AECO with 3D geospatial\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/dreaming-spires-towering-vision\/","title":"Dreaming spires - towering vision","date":1516838400000,"text":"Greg Corke caught up with Newtecnic\u2019s Andrew Watts to learn how the firm uses technology to optimise the design, manufacture and construction of complex facades on iconic projects\nAndrew Watts is not your typical building engineer and architect. The CEO of fa\u00e7ade-led multidisciplinary engineering firm Newtecnic is more likely to draw inspiration from Concorde or the Bentley Blower than from a Le Corbusier building. He is fascinated by components that perform multiple functions and by how they fit together, both functionally and aesthetically.\n\u201cI\u2019m still inspired by the world of Lotus from the 1960s, where they didn\u2019t just put an engine into the back of a Formula One car. The engine was the structure,\u201d he says. \u201cThe wheels were fixed directly to the engine. But it needed to have a double function in a way that adds to the visual quality of the assembly.\u201d\nSuch influences can be seen in many of the high-profile projects undertaken by the global firm, which has offices in London, Cambridge, Los Angeles and Riyadh. Watts has helped develop an advanced system whereby components can be put together in different ways, a bit like Lego. \u201cWe try to create more components that can do more jobs,\u201d he explains. \u201cWe might have, for example, a set of components for a bracket and that covers everything, rather than reducing it to one with vast numbers of bits added to it.\n\u201cEach component for a fa\u00e7ade could be formed by ten standard sub-components that can then be put in different combinations and permutations.\u201d\nThis kind of standardisation is a great way to cuts costs, but Watts is keen to point out that the same custom components are not used across multiple projects.\n\u201cWe don\u2019t go as far as it being a product \u2013 because every project is different. Every building is different,\u201d he explains. \u201cIt\u2019s very tempting to want to shortcut a process \u2013 but shortcutting a process for us, as engineers, means that we can miss something that actually allows us to reduce cost.\u201d\nTechnology and imagination\nNewtecnic is very technology-led and is always looking at new materials and processes to deliver the highly complex buildings conceived by the firms it works with, including Zaha Hadid Architects, Gensler and Aedas.\n\u201cBy using technology, it is possible to reduce risk while creating more imaginatively conceived buildings at lower cost that use less energy, are more durable, look better and are interesting to inhabit,\u201d says Watts. \u201cThey also take less time to make and on completion appear effortless.\u201d\nThe firm specialises in iconic buildings and advanced building envelope systems. The futuristic KCTV Tower in Istanbul, for example, features an innovative facade that allows inhabitable spaces to be attached to the whole of the tower\u2019s core. Likewise, the design of the envelope system on the KAFD Metro Station in Riyadh was driven by the need to minimise installation time through prefabrication while achieving a highly durable facade assembly.\n\u201cIf you\u2019ve got a regular box, there\u2019s no point coming to Newtecnic \u2013 there\u2019s nothing we can really add to that,\u201d explains Watts. \u201cBut if you\u2019re setting yourself out with a complex geometry problem that requires structural engineering, that requires fa\u00e7ade engineering, that requires being able to squeeze in environmental performance into a shrink-wrapped box \u2013 then we\u2019re the guys for that.\u201d\nClose collaborations\nNewtecnic works very closely with both contractors and architects. With a contractor, the firm might work to interpret a design that is already well-defined. With an architectural firm, it typically gets involved much earlier on in the process, even at competition stage. Here, Newtecnic might be presented with an architect\u2019s spatial and aesthetic vision and asked to give it an authenticity by defining what it\u2019s made out of, how it will be made, and who will make it.\nIdeas are developed with the structure at their heart, in recognition that it has to stand up and be cost-effective, but Newtecnic also works very closely with the architect to develop the aesthetic. \u201cWe want to show off the engineering design as reinforcing the architectural design, not something which is somehow drawing out limitations in the architectural design.\n\u201cAt all stages, we need input from the architect to see how does this fit into the bigger picture of the building \u2013 particularly as we are always encouraged to move any decorative elements which are not essential. We try not to cover anything up, but of course, in revealing everything, it\u2019s got to look good.\u201d\nThe design process often starts by evaluating the performance of a building design in the context of its surroundings. An architect\u2019s 3D CAD model might be brought into Rhino, for example, and combined with third-party survey data to carry out virtual wind tunnel testing in Computational Fluid Dynamics (CFD) software tool Comsol.\nVirtual wind tunnel testing can also help drive the design. By combining preliminary airflow results with computational algorithms, completely new designs for the building or its facade can be automatically generated. This could be to reduce material quantities or to maximise airflow to help reduce energy consumption in the building. This is particularly relevant for projects in hot climates, of which Newtecnic works on many.\nThe surrounding site can be modelled precisely using photogrammetry or more loosely by tracing the outline of a photo. Buildings are generally simplified, concentrating first on form, followed by surface texture, to see how that impacts things like turbulence.\nAt this early stage, Newtecnic places a big emphasis on future-proofing the building, by exploring exactly how it might perform in its ever-changing environment. \u201cWe want to understand what is happening now, what might happen in the future and therefore how much can we realistically reduce the quantity of material. We\u2019re thinking always 25 years, 50 years into the future.\u201d\n\u201cSurrounding buildings are always a bit of leap of faith, as you don\u2019t know which ones are going to get knocked down or which ones are about to be built,\u201d says Watts.\nThe design is assessed from different wind directions, generating pressures for the structure and fa\u00e7ade. Wind speeds used for ultimate limit state design are applied to see how the building responds when it is theoretically pushed to its limit. Newtecnic also places a big emphasis on typical wind conditions and how they might affect the serviceability aspects of the design.\n\u201cWe want to know how it behaves on a day-to-day basis,\u201d explains Watts. \u201cIs it comfortable, is the structure moving in a way that makes you feel a bit queasy? Is the door always slamming, or are you fighting to open the front door?\n\u201cYou don\u2019t want to end up having to have sliding doors put in [retrospectively] to solve a problem that could have been addressed in a completely different way in the beginning.\n\u201cIt\u2019s also user comfort. You don\u2019t really want to have a restaurant or a big terrace ten metres up in the air when it\u2019s going to be windswept.\u201d\nVirtual wind tunnel testing is always proven out with physical wind tunnel testing, to get an accurate a picture as possible. This is done in conjunction with the University of Cambridge, where Newtecnic has strong ties with the Engineering Department.\n\u201cYou do get unexpected effects in areas where the [CFD] mesh is not really dense enough or the CFD software is not picking up local behaviours,\u201d explains Watts. \u201cSubtleties, secondary effects, emergent effects \u2013 those are picked up by the physical tests. So what we thought was working really well, we may find we now have to smooth out a whole area.\u201d\nOnce the physical wind tunnel results have been assessed, the geometry model and structural model can be refined and the CFD study set to run again.\nNewtecnic hasn\u2019t gone so far as to feed back physical test results into the CFD engine to better align the virtual and physical worlds in order to improve the accuracy of future simulations. \u201cWe would need bigger computers for that,\u201d admits Watts. \u201cWe already have Aston Martin\u2019s set downstairs. We would need to get more into Airbus-level to do that.\u201d\nCurves, planes and connections\nSimulation is also used extensively in the development of the complex structural systems with curves, sweeping planes and composite connections that are typical of most Newtecnic projects.\n\u201cYou analyse it [the building] globally, you analyse everything, you analyse every connection,\u201d says Watts. \u201cYou build the whole building virtually in the computer. You make sure that the parallel world inside the computer tells you that it\u2019s going to work. We do endless investigation and we think that is a way to provide greater certainty at reasonable cost.\n\u201cWe need to know the forces acting or being acted upon at any given point \u2013 usually a point of intersection of the structure or fa\u00e7ade.\u201d\nFor structures, Newtecnic uses the simulation tool SOFiSTiK, as it offers the flexibility of being able to analyse complex forms, as Watts explains. \u201cIt\u2019s a Finite Element Analysis software that takes into account curvature, whereas other softwares are pretty much limited to rectilinear frames.\u201d\nAnother reason for its popularity is that it responds very well to change. \u201cIt\u2019s very good at being able to demonstrate structural behaviour,\u201d Watts says, adding that with other software, when removing small components in large-scale structures, you don\u2019t know if it has destroyed the structural integrity, stability and stiffness of the structure as it\u2019s creating local stresses or deformations.\n\u201cIf you build these models in SOFiSTiK, you can understand the behaviour by removing pieces and seeing what effect it has. It tells you what you need to do if you want to keep making changes.\u201d\nFor connection design, Newtecnic uses Comsol, which is popular with the team because it is customisable and extremely good at dealing with complex small-scale assemblies. \u201cHow things are connected, how two things next to each other will separate as they are pushed and pulled \u2013 those can be very closely analysed in Comsol, which other software doesn\u2019t allow us to do,\u201d explains Watts.\nThis could be when a piece of steel joins a reinforced concrete rebar assembly, for example, and problems such as leaks in the facade or micro-cracking in the concrete can occur.\nNewtecnic also uses fatigue analysis to help ensure its buildings continue to perform well into the future. The technique is widely used in the aerospace and automotive industries but less so in building design. \u201cWe want to know how the long-term movements are going to affect the structural integrity of specific small-scale components, which are generally in the fa\u00e7ades and exposed to the effects of the weather,\u201d he says.\nLet\u2019s get physical\nWhen connections are made from many separate components, it can be very hard to predict performance with simulation alone, admits Watts. As a result, Newtecnic works with the University of Cambridge to test, verify and rubber-stamp the performance of these parts.\nMock-ups are often made by third-party contractors, on spec before tender, in order to demonstrate their ability to deliver the final components, which are often complex to manufacture.\n\u201cWe get components made \u2013 it might be a piece of a panel and a connection bracket. We might get those from a couple of manufacturers,\u201d says Watts. \u201cWe then put them together and apply loads in a very specific way that allows the assembly to be tested and evaluated through codes.\n\u201cIt means we can go to a competitive tender, not just with a BIM model, but a BIM model where things are costed and where the specific assemblies that have not been done before, which have been innovated, have been tested and we have some kind of sign off from the [University of Cambridge] engineering department.\u201d\nNewtecnic also relies heavily on engineering- grade 3D printing. In its headquarters in Shoreditch, London, it has an Objet 3D printer that it uses to make functional prototypes out of polymers. This not only helps it get a much better understanding of components in the development phase but also allows it to effectively simulate the process of construction.\n\u201cWe print a combination of 1:1 connections and scale assemblies that shows how pieces would go together,\u201d says Watts. \u201cWe can show how things would go together if a crane was in place and why you\u2019d need to assemble things in a certain sequence to allow them to be self-stabilising, for instance, rather than requiring lots of additional props.\u201d\n3D printed models are shared directly with contractors, not only for better communication, but also to keep costs down.\n\u201cWe do it, so a contractor says, \u2018OK, you guys have got this worked out. You\u2019re not setting a problem and asking us to resolve it. You\u2019re providing us with a resolved design\u2019.\n\u201cAnd that allows us to say, in return, \u2018We want the tender price to come back pretty much the same price as a standard building. We don\u2019t want you multiplying by two or three because it\u2019s iconic or it\u2019s bijou or it\u2019s a one off. We want you to take it on and construct it, following a basic set of instructions.\u2019\u201d\nSoftware for problem-solving\nNewtecnic uses a huge range of modelling software throughout its development process. The choice depends not only on the capabilities of the software itself and its applicability to different workflows, but also on its ability to share data effectively with third parties.\n\u201cWe like to be able to do quick sketching and quick build-up of a model in Rhino,\u201d explains Watts. \u201cThat can be ultimately the form that is agreed with the designers, but it has to keep on changing.\n\u201cThen, as we start to quantify everything and work out how it fits together, we either continue working with Rhino or we move to Revit \u2013 we\u2019re big fans of Revit. Or we use Catia, because then we can talk to [CNC] machines. It depends on whether we\u2019re talking to a machine or talking to a committee.\u201d\nThe company also uses generative design as a problem-solving tool to help deliver logical solutions to visual criteria, often as a means to realise an architectural vision. Common tools include Grasshopper, Dynamo and Matlab. Newtecnic is also taking tentative steps into Python and R to go beyond visual scripting.\n\u201cGenerative designs are often the starting point for human designers to adapt these shapes and to be inspired to develop new types of fa\u00e7ade and detailing,\u201d he says.\nWatts stresses that with all software, it\u2019s not just the model or data that is important. It\u2019s also the process. \u201cFor any output, we need to be able to know how we got there and therefore how we can start again,\u201d he says. \u201cWe used to find this a little intimidating in the early days when we were used to creating something and somebody saying \u2018Wow, that\u2019s great, but could you make it a little smaller or a bit this or a bit that\u2019.\u201d\n\u201cAs time has gone on, every part of the process is interrogated as being a part of the design and therefore we\u2018ve needed to adjust to becoming much more accountable for not just the output but the process.\n\u201cWe\u2019ve had this major shift from having an unregulated creative process, where one was always proud of the output, to one where we\u2019re more sceptical of the output, but more proud of the process.\u201d\nIn order to formalise this process, Newtecnic has taken a leaf out of another industry, that of software development. \u201cWe\u2019ve developed agile management in the firm, scrum, as if we\u2019re designing software, in order to provide a work method that is different from the traditional architectural and building engineering methods of being output-focused.\u201d\n\u201c[With traditional methods] if you want something changed, you mark it up and make changes, but when you want to start all over again, you need to be able to know how to do that without it being a disaster.\u201d\nChange is a recurring theme at Newtecnic and making changes and being required to make changes is a huge driver for the firm.\n\u201cAll the governments, developers, contractors, architects that we work with, expect us to respond to change, because they themselves are responding to some change in the brief.\u201d\nWatts recalls a recent project where there was a problem with installation on site, where something was made bigger than it should have been. \u201cThe fabricator changed a few things and there were unexpected consequences, but we were able to come up with a solution on site,\u201d he explains, adding that it involved a complex weld.\nPhotographs of this \u2018proof of concept\u2019 were then sent to the Cambridge office as a series of photographs and subsequently added to the Comsol model for analysis and verification.\n\u201cWe are able to solve difficult structural problems, a changing of a connection within half a day. No other tool we have can do that with the degree of accuracy that we need.\n\u201cIt means we are able to respond to how the world is, rather than how we would like it to be.\u201d\nVisualisation and optimisation\nWith a constant need to balance functional and aesthetic requirements, assess ideas and communicate its designs internally and to third parties, Newtecnic places a big emphasis on visualisation. This includes entire buildings, detailed systems and components.\n\u201cEveryone is a viz specialist at Newtecnic,\u201d explains Watts, adding that the company uses a heavily customised version of 3ds max that has been \u2018scripted to death\u2019 combined with the V-Ray render engine.\n3ds max is a complex product and not the type of tool one would expect your average building engineer to be able to use. So, to keep its team up to date on new technologies Newtecnic places a huge emphasis on training.\n\u201cEvery fortnight we have a training day for new skills \u2013 not just engineering skills but skills that allow you to contribute to the visual artbook,\u201d says Watts.\nWith its holistic approach to fa\u00e7ade engineering, Newtecnic has optimised many different parts of the process from concept design to fabrication and construction. But the company is not finished yet in its drive to cut costs and accelerate project delivery.\nThe next big step, says Watts, is to further optimise fabrication by drilling down as far as factory design. This would be \u2018design for manufacture\u2019 in action, a practice championed by the manufacturing industry where the ease and cost with which components are made has a major influence on the design itself.\n\u201cIt\u2019s a way of reducing cost through a process rather than through the product,\u201d he says.\nCurrently, when contractors make initial mock-ups for Newtecnic, they are often built from a process that is not optimised. \u201cIt\u2019s put together like in a giant model shop,\u201d says Watts. \u201cSo, technically, a contractor will know that they can do it, but they don\u2019t know if it\u2019s the right process for them, and that\u2019s a real limitation.\u201d\nHe adds that when it comes to manufacture, some contractors may even end up sub-contracting jobs to a company that is better set up for a specific process but this can introduce further delays.\nNewtecnic is currently developing links with the Institute of Manufacturing at Cambridge University to help optimise its process. The aim is for Newtecnic to have in-house manufacturing specialists that would be actively involved in the design process, working alongside engineers to advise how components could be made with similar functional characteristics but quicker and cheaper. These specialists would also work closely with regional factories to help optimise their manufacturing process in advance.\nIn this way, Newtecnic will continue to bring some of the rigour and exactitude of the mechanical engineering and manufacturing disciplines to the world of construction \u2013 an industry synonymous in some minds with inefficiency. It could be just the kind of approach to help drive things forwards, and upwards.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/municipal-software-welcomes-freehold-new-jersey-as-a-new-cityview-client\/","title":"Municipal Software welcomes freehold, New Jersey as a new CityView client","date":1027036800000,"text":"Municipal Software Corporation, a developer of local government business process automation software and a wholly owned subsidiary of Municipal Solutions Group, Inc. (TSX: MSM), announced today that the Township of Freehold, NJ (Pop. 31,537) purchased CityView Enterprise in order to automate their construction department business processes.\nThe Township of Freehold, NJ will rely on Municipal Software Corporation to complete the initial install of the software and provide ongoing support over the years. CityView Enterprise is powerful automation software that is designed specifically for local government and easily converts paper systems and business processes into a computerized system without costly and time-consuming programming. Their conversion to CityView Enterprise will save the township time and money and ultimately improve customer service delivery to its citizens.\nCityView Enterprise allows the township to design forms that conform to the Uniform Construction Code Administrative Record System (UCCARS) monthly reporting requirements. Monthly municipal permit and certificate activity reports are easily and accurately produced in CityView. This is vital to all local governments in New Jersey because UCCARS monthly reporting is mandatory in the state. They will also be using CityView\u2019s Enhanced Mapping in conjunction with their GIS system.\nThe Township of Freehold is situated in central New Jersey and spans almost 39 square miles. It is home to the oldest daytime harness track in the country and is known as Western Monmouth County\u2019s \u201cfamily town\u201d.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/nxt-bld-london-preview\/","title":"NXT BLD London preview","date":1496620800000,"text":"The move to BIM is far from complete, but adoption is accelerating on a global scale. We are now at a stage of refinement and of agreeing standards for traditional federated design and construction chains, with Stage 3 looming and a new deadline about to be announced.\nBut as you know, it\u2019s not all about BIM authoring tools, which are fairly mature, \u2018known entities\u2019 with a low development velocity. The AEC technologies that are accelerating the fastest are in parallel areas, such as VR, AR, laser scanning, photogrammetry, drones, reality capture, robots, collaboration, digital fabrication, generative design, smart cities and artificial intelligence.\nIt seems one could go to a BIM event in London almost every week and trawl over the same uninspiring topics like PAS 1192 conformance and how to try to write good COBie. With this in mind, AEC Magazine has decided to buck the trend and launch NXT BLD (NEXT BUILD), a new conference covering areas which are set to drive the digitisation of the industry far and wide, beyond the finer details of BIM modelling. NXT BLD is not a BIM conference \u2013 it explores future technologies that could completely change the way we design and build.\nOur inaugural event will be held in London on 28 June at The British Museum, in association with our partner, Lenovo. The focus will be on a number of technologies that are already causing a buzz.\nVirtual Reality\nVR development for architecture and construction has gone into overdrive with a range of fully immersive technologies to help firms make informed decisions that directly impact project outcomes. From conceptual modelling to collaborative design review, VR is touching all bases. The cost of VR headsets is reaching commodity pricepoints and the AEC industry has been making 3D models for years. It seems the perfect time for adoption as the entry-level kit needed doesn\u2019t cost the earth.\nAugmented Reality\niPads on site are becoming more commonplace, but AR headsets will take on-site digital information to the next level, making it available in context and hands free. There are still some uphill battles as the headsets are relatively expensive and low resolution. However, there are already some major firms exploring how AR in construction can deliver information on site, at the precise point that it\u2019s needed to drive efficiency and enhance safety.\nReality modelling\nLaser scanning has been with us for years but remains expensive and requires surveying skills. Now, it competes head on with photogrammetry, which uses complex algorithms to compile and process hundreds of photographs taken with a simple point-and-click camera to derive accurate 3D models. Both technologies are very close in terms of accuracy, bringing access to reality modelling within the grasp of everyone. It has never been so easy to grab the as-built state of a project, both outside and in, and link this to VR or AR.\nRobots\nThe word robots summons all sorts of thoughts, from automated vacuum cleaners to terminators from the future. Many firms in the AEC sector are already experimenting with robot-assisted construction, from robot arms creating 3D prints and laying bricks, all the way to fully automated onsite assembly.\nWith increased demand for making larger, more complex models and delivering seamless immersive experiences, there are new pressures on workstation portability, CPU and GPU power. An entry-level graphics card which drives a CAD or BIM system is not good enough for a smooth VR experience. It\u2019s time to rethink the hardware dynamic for these next generation design and construction tools.\nOne day, 12 speakers\nWe are bringing together 12 experts from around the world to talk about the realities and future possibility of using these technologies, including speakers from leading firms such as Gensler, MX3D, Skanska, dotproduct, Laing O\u2019Rourke, Bentley Systems, Soluis, Mamou-Mani, Space Group, ArchiSpace and others. See the full conference program here\nNXT BLD takes place on 28 June at the BP lecture theatre in the Great Court at The British Museum. Doors open at 8.30am and the conference starts at 9.30am. There will also be an exhibition for hands-on experiences. The conference ends at 5.30pm, followed by a social drinks reception.\nGeneral Admission tickets are very limited in number and cost just \u00a325. Price includes coffee, lunch and drinks at a social mixer. Visit the NXT BLD website for more information.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/the-smart-revolution\/","title":"The smart revolution","date":1140393600000,"text":"This month\u2019s Cambridge-based workshop and conference, held at the British Museum in London, demonstrated that the momentum behind the Smart Geometry group continues to grow.\nAccording to the CAD software companies, industry analysts and even the press, CAD usage can be pretty much divided into 2D and 3D; you either use one, the other, or a bit of both. Despite what the CAD vendors would like everyone to think, most companies still rely heavily on 2D, because CAD software\u2019s primary deployment has been that of a \u2018drawing board replacement\u2019. 3D usage is quickly growing in the Mechanical CAD space but in the construction industry 3D is still only found in the realms of specialist designer, perhaps within the practice of a \u2018Signature Architect\u2019, a structural analyst or a 3D visualisation expert.\nWhen one thinks about 3D design, the natural assumption is that the 3D model would be a virtual model of the finished building, complete in detail but static. Just as 2D plans are the blueprint of a design\u2019s plan, elevation and section. This view comes from the common understanding of \u2018what CAD does for us\u2019 and how it\u2019s applied merely as a documentation tool.\nThere\u2019s a growing number of AEC companies that use their CAD tools differently, and are researching new applications, processes and group working to use 3D in a different way. The challenge is to use the computer and the software to enhance the design part of the process, to easily produce\/control complex geometry parametrically and provide a framework for design exploration. It\u2019s not good enough to build a static 3D model as these have to be rebuilt if there are any changes. If we are to see the benefit of 3D, the models have to be able to change and adapt. The embodiment of this movement seems to be centred on the SmartGeometry group.\n\" New technology is needed and a fresh approach to using CAD within the design and manufacture process has to be found \"\nSmart thinking\nInitially recruited mainly from within London\u2019s elite architecture and structural firms, the group is dedicated to educating the construction professions in the new skills which will be required to use these new 3D systems effectively. The SmartGeometry Group\u2019s founding members include Lars Hesselgren (KPF), Hugh Whitehead (Foster and Partners), J Parrish (Arup Sport) and Dr. Robert Aish (Bentley). To a certain extent, it would be possible to construe that the group is mainly concerned with promoting Generative Components, a technology that Robert Aish, Bentley\u2019s Director of Research, has built on Bentley\u2019s MicroStation CAD platform, but it\u2019s clear from the presentations given at their events that the platform isn\u2019t the key issue, it\u2019s the approach to using computers in an explorative design context that is the common link.\nHowever, the key reason for the group\u2019s existence is to offer a selection of AEC students access to Generative Components (which is still in development), and develop a programmatic solutions to design issues they are interested in. The \u2018students\u2019 can come from education institutions or from within practices. The SmartGeometry\u2019s conferences offer a mix of thought-leader presentations, interspersed with demonstrations of what the students managed to achieve with Generative Components in the workshop sessions.\nFrom successive SmartGeometry events it\u2019s clear to see that the concept for more powerful design tools is growing in the community. As to what\u2019s driving this interest, I talked with a number of attendees at February\u2019s SmartGeometry event held at London\u2019s British Museum. It seems there is growing interest in designing more complex forms, driven by practices like Foster and Partners with the Swiss Re building. The only way to do this is to use computer technology to assist in the design, test and manufacture. It\u2019s a simple tale of competition driving the technology through the industry. If you want to compete you need to be at the leading edge.\nPractices told me they were establishing Research and Development teams to look at the technologies and their possible impact on their design capability. On this subject, I remember talking with Frank Ghery on his use of Dassault Systemes\u2019 Catia to realise his designs. He quipped, \u201cUsing Catia I can make a wavey wall for the same price as a straight wall. So why make a straight wall?\u201d\nTo produce affordable yet impressive designs in the today\u2019s architectural vernacular (or tomorrow\u2019s!), the traditional application of 2D and a static 3D model simply fail. New technology is needed and a fresh approach to using CAD within the design and manufacture process has to be found.\nWhile many architects may not see the need to move to 3D, those that have, including those in attendance at the SmartGeometry sessions, seem \u2018liberated\u2019 in their attitude towards their use of forms. While the tools do require considerable technical knowledge to drive them, the \u2018art\u2019 and aesthetic element of architecture can come to the fore, with the computer handling the complexity and perhaps making the design viable by automating the pannelisation, or automatically calculating the optimum number of structural elements.\nAish explains, \u201cWith the advent of digitally controlled fabrication, the \u2018geometrically aware\u2019 and \u2018computationally enabled\u2019 designer is now as close to the materialisation as in the original craft process, but with precision and control and the ability to explore variation which was previously unimaginable.\u201d\nGenerative Components\nGenerative Components is being used and evaluated by key industry players like Foster and Partners, Morphosis, KPF, Grimshaw, NBBJ and ONL, as well as a number of Schools of Architecture, including the Architectural Association, MIT, Georgia Tech. and the Technical University of Delft. The technology is about to go into beta and will be commercially available on MicroStation at some point in the future.\nThe system has been created on three themes: Geometry, Composition and Algorithmic thought. In terms of geometry, all the usual primitive elements are used-points, planes, coordinate systems, line arc, curves, surfaces and solids. Then there are geometric operations which are applied to the geometry, such as projection, intersection, union, difference and transformation. Using these operations, Generative Components can create simple and complex relationships between base geometry to allow modification, or configuration. Aish relates this to building a \u2018control rig\u2019, building geometry which may never be seen but may indirectly control the subsequent geometry created in the design process. Essentially the real time manipulation of these control points allows variation within the design.\nIn terms of composition, Aish explains, \u201cIt is pretty rare to find a building which is a realised as a single discrete object. Normally we are considering assemblies of components which, at intermediate levels of aggregation, form identifiable sub-systems. While these components may be pre-defined, or the subsystems may follow established industry conventions, there are increasing opportunities for each design to use mass customisation and digital fabrication to define project specific components. There is a tremendous advantage in using computational design tools which directly support the idea of \u2018composition\u2019 and which allow these strategies to be developed and tested.\u201d\nOn algorithmic thought, this concept is based on the need for the system to be able to calculate, based on the intention of the designer. This could be calculating a fa?ade, automatically placing structural elements or applying a transformation equation to a surface. To do this the designer has to have explicit knowledge of how to drive the system. This is probably the most difficult hurdle to overcome with Generative Components, as the designer really needs to be part programmer and programming requires a logical approach to problem solving.\nIt appears that many of the firms that have used or evaluated Generative Components have opted to have a dedicated team to build these design rigs, based on the limits of the initial concepts. This allows the architects on the project to then interact with the rig, a live 3D model with embedded logic and quickly evaluate design alternatives. However, the team will only be able to operate on the control points that the original rig developing team deemed it necessary to create. Aish puts considerable emphasis on the time spent prior to building the rig, breaking down and anticipating the level of variation required further down the line. \u201cWe have to match our tools to the concepts around which designers want to build their skills,\u201d added Aish. \u201cOur expectation is that geometric skills, compositional skills and algorithmic skills will be the key to future design.\u201d\nTo put that into layman\u2019s terms, Generative Components is a layer on top of MicroStation which provides a programmatic and visual way of building control geometry which can be used to interactively control a model. This isn\u2019t the \u2018walls, doors and windows\u2019 \u2018intelligence\u2019 of products like Architectural Desktop, which is essentially about quickly generating 2D general assemblies. Generative Components works at a much lower level and isn\u2019t bothered about the interaction of pre-defined recognisable building components. It simply concerns itself with geometry, complex relationships, control and applying complex, user-defined computations to a design.\nConclusion\nIn the next instalment I\u2019ll look at the presentations given at the February SmartGeometry event. The keynote was given by Chuck Hoberman, a talented artist and inventor whose designs for morphing and transformable objects has won him world acclaim. Hoberman has created everything from toys, furniture and art installations to military designs for tents and quickly deployable blast walls.\nAlso of note, Spencer de Grey, partner at Foster and Partners wrapped up the event with a presentation on the British Museum Great Court and Smithsonian roof projects, which he oversaw. Fosters use of expressive forms has, in part, helped drive this movement and was a fitting end to the proceedings. At the after conference drinks, we all stood underneath the undulating roof, as a practical reminder to all those who had attended.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-viewpoint-unveils-construction-business-intelligence-solution\/","title":"NEWS: Viewpoint focuses on construction business intelligence","date":1515542400000,"text":"Spectrum Business Intelligence solution provides customisable data analysis through \u2018simple, automated\u2019 tools\nViewpoint, a specialist in ERP, project management and mobile solutions for the construction industry, has introduced a new solution for construction data analytics and business intelligence.\nSpectrum Business Intelligence, part of the Spectrum Construction Software suite, allows users to collect, compile and analyse construction data \u2018virtually any way they want to\u2019. The solution includes in-depth reporting tools, dashboards, creative charts, graphs and geographical mapping of data.\nUsing drag-and-drop features, users can access their Spectrum data, as well as apply measurements and dynamic calculations. The system can be used to create comprehensive reports, dashboards and data visualisations.\nOnce construction data has been compiled and analyzed it can be save and shared with other people, groups, project teams\u2013or throughout the entire organisation. Through folder sharing and an integrated scheduler, reports can be pushed to their targeted audience one at a time or on a periodic basis.\n\u201cOne of the long-standing challenges with construction data reporting software is, if the report or data isn\u2019t presented exactly as the user wants it, the company may need additional report development work to create the required report,\u201d said Viewpoint\u2019s Scott Rosenbloom. \u201cToday\u2019s construction leaders want tools that are going to help them quickly identify and drill down into data in ways relevant to their business. Spectrum Business Intelligence provides that level of flexibility.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/automatic-for-the-people\/","title":"Automatic for the people","date":1526428800000,"text":"Three professionals from global engineering, architecture, environmental and construction services firm GHD explain how they are embracing design automation and applying it to real world projects\nPaul Murphy is GHD\u2019s global technology and technical applications leader, focused on evolving GHD\u2019s skills, tools, workflows and delivery models to adapt to changing trends.\nQ: How is automation changing the design process?\nA: It\u2019s liberating to free our people from painful and repetitive tasks, and enable them to spend more time doing true \u2018design work\u2019, be creative and deliver a real benefit for the client and the community.\nEssentially, any task that\u2019s computable and rule or algorithm based can be automated. But our focus is on developing new workflows that can be applied on many different types of projects regardless of location and maximising their use across our organisation.\nQ: Are new technologies making automation easier?\nA: Our partnerships with technology providers are helping us automate some design tasks. For example, Dynamo is a visual programming platform for computational design and Building Information Modelling that extends the capabilities of Revit and other Autodesk software, without necessarily needing specialist programming skills.\nWhile the technology makes it easier for us to use automation there is still considerable effort required from our teams to adapt these solutions to the tasks and requirements of our projects. This is where the passion of our people comes to the fore as they share experience around the world, learn from each other, develop new workflows and solutions and most importantly consider how these automations create value in our project delivery.\nQ: Dynamo is a relatively new application for design automation. Do you or your team have any experience with other tools like Grasshopper for Rhino or Generative Components? And, if so, what are your experiences with those products?\nA: We do use these as well as other tools. Without going into a product comparison, the reason we have invested so much time and effort with Dynamo in particular is because it offers a more scalable solution for automation thanks to its native integration with many of the core design applications in use across the company. We have also found it is easy for people to learn and this speeds up adoption. We have set up a global Dynamo Development Group to help our people share their experience and upskill as well as pushing commonly used scripts to users to aid wide-scale implementation.\nQ: How do you decide which processes will benefit most from design automation?\nA: We are focusing on processes that have broad applicability across multiple teams, locations and projects. If something is really painful for our people to do over and over for different clients around the world, chances are automating it will maximise the benefits across our entire company.\nQ: What QC processes do you have in place to make sure results are as expected?\nA: Our current use of Dynamo is focused on the automation of conventional design tasks and processes. We are using Dynamo in particular to automate the interaction between various design and analysis tools. This enables us to continue to use the underlying rule or algorithm in its native environment and validate the results before a broader adoption of the script.\nQ: Has there been any cultural resistance within GHD to using design automation and how have you managed that?\nA: Resistance is not something we have experienced. In fact, many of our people are learning Dynamo or Python and are starting to explore the new possibilities, asking, \u201cWhat more could we be doing?\u201d.\nNo one likes doing highly repetitive tasks. Once people see the value and time savings being created by the new way of working, they naturally gravitate to it. There is an immediate benefit in being able to redirect your time into other design activities that are more creative and personally fulfilling.\nQ: What\u2019s next for automation?\nA: There has been a lot of discussion in the media recently about the potential for robots to take over many tasks currently undertaken by designers and engineers. I expect that we will soon be using \u2018expert systems\u2019, leveraging the power of machine learning and artificial intelligence to make better design decisions or to help manage the mass of data being generated and consumed across the project lifecycle. For some people this causes a fair degree of anxiety, but I believe this is going to be a largely positive change for our industry, because it means we can devote our time and attention to more productive or more creative activities.\nRapidly testing ideas for cities of the future\nWarwick Hatfull manages GHD\u2019s Urban Planning & Design team in the United Arab Emirates, which delivers significant large scale urban developments.\nWhen we work on a masterplan, we calculate a significant amount of interconnected data, from the gross floor area, built up area and gross leasable area to name a few. Automation enables us to rapidly test different spatial arrangements and concepts. We can easily change the width of a road or the size of a residential plot, increase a plot\u2019s gross floor area and see what the implications are for the development as a whole.\nEach time we make an adjustment, no matter if it\u2019s large or small, our workflow automatically calculates and tabulates all the different statistics for the development, from how much land area is allocated to each type of land use, to the population and parking requirements for each use. Previously, completing all of these tasks manually was extremely time-consuming and was susceptible to human error.\nThis solution has been made possible thanks to the latest generation of design tools and our engagement with Autodesk under its enterprise business agreement. We have drawn on the support of Autodesk technical specialists to help share and reuse data between different products, such as Infraworks, Civil3D and Dynamo, and this has enabled the new masterplanning workflows.\nBeing able to automate these tasks is making a huge difference to efficiencies and timely delivery of projects. We can spend more time on what\u2019s truly important: the design. This helps us meet client and community expectations, because we are able to create truly robust designed solutions that are environmentally, socially and economically viable.\nMost importantly, optimising the design of new urban areas is improving the quality of the life for residents and employees.\nStreamlining workflows for major infrastructure\nDaniel Moodie, BIM project lead in Western Australia, explains how GHD is using design automation to streamline workflows for major infrastructure projects We have used Dynamo and other Autodesk software to automate the development of an integrated model of the rail environment for a large infrastructure project.\nFor example, our workflow automatically generates rail track profiles, rail sleepers at desired intervals, kinematic envelopes and turnouts from 2D data.\nOnce the geometry is within a 3D environment we can check for clashes with other elements within the project, allowing for better coordination with other disciplines. We can also export 2D information, which is still required for some rail design and construction tasks.\nThe model is like a highly-realistic \u201cdigital twin\u201d of this project. It helps designers, constructors and operators to better understand how different elements come together. We are better able to work together to solve design issues, improve construction sequencing and find operational efficiencies.\nTraditionally, the design of rail projects has a very complex workflow, switching between different software. A lot of the design and documentation is still done in 2D.\nWhen we decided to move our workflows into a BIM model based on Revit, we had to make a number of changes to get the transition right.\nOne of the first challenges we encountered was the lack of rail content for the model, such as tracks, sleepers, slabs, overhead wiring and electrified equipment. All of these had to be created from scratch \u2013 luckily I\u2019ve had a lot of experience in content creation.\nAnother challenge was working out how to bring mostly 2D data from different sources, such as CAD drawings and custom software, into a BIM environment.\nFor example, we created custom Revit families and Dynamo scripts to replicate 2D symbols for chainage, curve positions and transitions between different track profiles in a 3D environment. We can now use them on other infrastructure projects.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/attention-to-detail\/","title":"Attention to detail","date":1048204800000,"text":"In the last issue of AEC Magazine we looked at Nemetschek\u2019s Allplan product portfolio, and its architectural design solution. This month it\u2019s the turn of Allplot, which among other things, makes the RC detailing process a whole lot more efficient.\nNemetschek has been active in the UK AEC market for some two years now, and in our October issue we ran through the German company\u2019s extensive portfolio (you can find the article elsewhere at www.cadserver.co.uk). Within the scope of the feature we saw how the Allplan suite provides a wide range of modules to cater for the variety of disciplines involved in the construction process, from architecture and structural engineering, to HVAC and urban planning. We also took a more in-depth look at the core architectural design tools, and the single building model approach that Nemetschek adopts. Now \u2013 moving from architectural design into hardcore engineering within the space of a month \u2013 it\u2019s the turn of Allplot, Nemetschek\u2019s structural and civil engineering design solution. However, with the wide range of tools available in the Allplot product range including digital terrain modelling and site design, for the purpose of this review we\u2019re going to focus on the structural side of things, with a specific focus on Reinforced Concrete detailing, and those that battle with bar marks and reinforcement schedules on a daily basis will know you can\u2019t get much more hardcore than that.\nRC Detailing\nFrom drawing board to CAD, reinforced concrete detailing involving bar and mesh reinforcement has come along way in its hundred or so years. While the introduction of 2D CAD has provided RC detailers with a range of productivity enhancing tools, the working method still pretty much mimics the drawing board process, albeit with some automated routines. Of course, working in this way requires the RC detailer to be entirely responsible for the consistency of data between views, and as a result can be susceptible to errors. In addition, the process often requires numerous written notations to make the drawings clear.\nThe advantages of working in 3D: since the drawing is generated from a central three-dimensional model, the plan views, elevations and sections are interdependent. This means that any changes made to the central model are automatically reflected in each view, and of course in the reinforcement and bending schedule.\nFor example, a construction group consisting of bar reinforcement, fixtures and void formers \u2014 a dynamic macro commonly found in pre-fabricated units \u2014 can be pulled from a catalogue and placed in or on top of a column. The user then immed-iately sees the steel correctly represented in all views and sections. The creation of this 3D macro alone is a simpler process than fashioning several 2D symbols, and that\u2019s even before you take into account that placement can be made at a single location. Perhaps of more importance though, is the fact that you can be confident that the views and sections derived from the 3D macro are auto-matically correct. What\u2019s more, you can carry out collision control during reinforcement design, with any clashes highlighted in the animation window.\nAs a result, you could think of 3D programs such as Allplot as more than just a \u2018drawing and design\u2019 tool, as it could be used consistently for quality assurance during all phases of the building process.\nTo the untrained eye, a reinforced concrete detailing drawing is just a mess of lines, tags, and schedules, and even to the trained eye it takes a great deal of concentration to fully understand the positioning, type and quantities of bars within the structural concrete element. Therefore, it\u2019s of the utmost importance that any drawings created are clear and perhaps more critically, accurate in terms of their depiction of steel reinforcement in all views, plans, sections, and elevations.\nAs with its parent application, Allplot addresses the design process from a 3D perspective. That is, you build a 3D model, which is then used to produce all your plans, elevations, sections, and indeed bar schedules automatically. If you want, you can also work in a semi-automatic 2D mode, but while applicable in some scenarios, this fails to take advantage of Allplot\u2019s powerful functionality. It\u2019s a bit like buying a Porsche and never getting out of first gear.\nWhether you\u2019re creating pad or continuous foundations, door recesses, stairs or simply a slab, the starting point for RC detailing in Allplot is always the shell. The easiest way to think of a shell is the concrete element in its entirety, and this can be produced in a number of ways. You can either take architectural elements direct from Allplan, convert 3D geometry imported from another CAD program (such as AutoCAD DWG, DXF, or IFC), use imported 2D data as the basis to build up a 3D model, or use Allplan\u2019s 3D modeller to start from scratch.\nOnce you have your structural shell, you can start detailing. Here, Allplot provides a range of tools to help automate the process, at the same time as building up a central database of all the steel used. From Allplot\u2019s bending shape catalogue you need to select shape, diameter\/steel grade, concrete cover, number of bars\/spacing and away you go. Because you are working within the confines of a shell, the product\u2019s in-built intelligence helps ensure all the steel is automatically placed in the correct place, so you don\u2019t have to work out exact lengths etc. Bar marks are also placed automatically, which of course link into the central steel database to enable automatic scheduling, and any reinforcement elements with the same characteristics are automatically assigned the same mark number. In addition, as well as providing the full range of standard bar shapes, you can also design non-standard shapes.\nBamtec is a relatively new economical method of creating concrete slab reinforcement. Instead of conventional steel mesh or cut and bent bar, Bamtec uses pre-fabricated reinforcement carpets produced using an automated engineering process and rolled out on site just like a carpet.\nWith Bamtec you can automatically reinforce plate structures optimally designed by finite element analysis using the minimum amount of steel. The round steel bars are used in carpet production with different diameter and lengths, allowing carpets to fit slab layouts precisely and cope with varying load requirements.\nOf course one of the main principles behind Bamtec is one of saving \u2013 both time-wise (during the construction process) and in terms of cost (as Bamtec is designed to make significant savings on the amount of steel used). The figures quoted claim on-site fixing cost savings of up to 80%, materials savings of 20-40% and shorter and more automated design and detailing time. So how does this revolutionary system work?\nStarting with a slab model or plan, structural calculations are carried out using a finite element analysis system such as Staad.Pro. This determines the reinforcement required in the X and Y directions in both the top and bottom of the section. After dividing up the floor surface into the minimum number of carpets, the Bamtec software automatically determines the make up of the carpets, as well as the position, length and diameter of all its bars. Four carpet positions are required per floor \u2014 two at the bottom (in X and Y), and two at the top. The engineer can also select parameters himself \u2014 for example, the bar diameter and spacing of the main reinforcement can be individually set.\nThe end result of this process is a number of assembly and production drawings, which are automatically created and then used in the Bamtec manufacturing process.\nThe actual carpets are made with a giant welding plant where a robot welds the bars at the calculated spacings onto the carrier strips to create an accurately dimensioned reinforcement carpet. These carpets are then delivered on site and rolled out quickly and easily; two men and a crane driver can place a ton of reinforcement in 15 minutes \u2014 pretty impressive stuff! Bamtec was developed by consulting engineers Haussler, Kempten, and Nemetschek, which is not surprising when you look at the amount of Bamtec functionality built into Allplot. While it is used extensively in Germany, it has also been used on hundreds of UK projects, pioneered by Hy-Ten.\nOn top of typical bar reinforcement, Allplot also provides a full range of tools for Bamtec for slabs (see box out) and area and mesh reinforcement to facilitate the detailing of large areas. Here, recesses can be taken into account, and as the bars are associated with the placing polygon (or shell), any changes you make to the geometric outline of the polygon (or to the settings) are reflected in the corresponding bars.\nWhile huge productivity gains can be made from placing standard bars, or mesh or area reinforcement, Allplot also provides a number of parametrically-defined reinforcement components, which enable standard structural concrete elements to be detailed automatically e.g. pad found-ation\/columns, column\/beams, stairs etc. Just as a single bar automatically locates itself within the shell, each component attempts to fit itself within the \u2018formwork\u2019 according to its pre-defined parametric properties. This has the potential to save huge amounts of time.\nNaturally, as with any automatic CAD process, it\u2019s still important to have complete control over how your reinforcement is placed and represented in the final drawings, and as a result Allplot provides you with a wide range of tools to do this. For example, you can modify shells, bar placement, bar parameters, bar marks, bar labels and hooks. Of course, whilst not a necessity by any means, your drawings can also be exported into another CAD program such as AutoCAD, to finalise your finished RC drawings, or add title blocks etc.\nIn conclusion\nI have to say I am incredibly impressed with Allplot\u2019s concrete detailing module. With the inherently complex nature of the discipline, working with a 3D model makes an incredible amount of sense, not only because you only need to work on one set of data to produce multiple drawings, but the system automatically creates reinforcement schedules as well. As a result any changes to the core model will automatically be updated everywhere else, which, of course, also helps reduce errors. Additionally, as with the core Architectural Allplan product, the use of the OpenGL-driven animation window makes it incredibly easy to visualise exactly where you are placing your reinforcement.\nWhile there is an obvious advantage of using Allplot alongside the architectural module of Allplan, this is by no means a necessity. There are many Allplot users working from imported 2D or 3D models from AutoCAD or other CAD applications, or indeed starting from scratch.\nAs with Allplan, the Allplot interface can be a little complex and bewildering at times, but once you get used to this aspect of the program, what lies beneath is a highly sophisticated solution that has the potential to dramatically increase productivity and accuracy throughout the RC detailing process, and this in itself could be worth its weight in gold.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/ai\/digital-twins-the-next-frontier\/","title":"Digital twins: the next frontier","date":1606003200000,"text":"Greg Corke caught up with Keith Bentley, CTO of Bentley Systems, to find out why he thinks machine learning is so important to digital twins and why we\u2019ve only just scratched the surface\nIn 2018 Bentley Systems acquired machine learning and IoT development company AIworx. At the time, CTO Keith Bentley described it asan \u2018acqui-hire\u2019, \u2018just for the talent\u2019. After all, the Canadian firm didn\u2019t actually have any products.\nTwo years later and it\u2019s starting to become clear exactly why Bentley brought in that talent. The team has been playing a key role in developing new technologies for Bentley\u2019s Digital Twin platform, as the company looks to establish itself as the leading force in this emerging sector.\nAccording to Keith Bentley, the AIworx team has been split into two groups: one is looking at ways to make smarter, better quality input to the digital twin, while the other is trying to analyse the dynamics within the digital twin.\nFeeding the twin\nIf all digital twin projects were created for brand new assets, then the development of a digital twin would be much simpler. The reality is, most digital twins are likely to be built for existing buildings or infrastructure, for which the quality and format of data can be extremely varied.\nWith this in mind, the AIworx team has been working out how to \u2018up-level\u2019 the content that goes into a digital twin, as Keith Bentley explains, \u201cSo we have paper, we have pictures, we have CAD files, and sometimes BIM models, and when you combine all that together, you find that there\u2019s a lot of inconsistency.\u201d\nKeith Bentley highlights one AIworx project that automatically recognises all the tags on a scanned Piping and Instrumentation Diagram (P&ID) drawing, then relates that into the 3D model. Another example is the automatic object classification of reality capture meshes through analysing the input of photographs.\nSmart thinking\nDigital twins are fundamentally about big data: collecting old, new, live, and future data, geo-referencing it, finding out what has changed, displaying it, and doing something meaningful with the data. It\u2019s in this last part that machine learning can be used to great effect, studying patterns in the data, to help AEC teams and infrastructure owners assess the impact of past decisions to help make more informed design, business and operational decisions moving forward.\nKeith Bentley shared some specific examples of what the team are exploring, including studying what makes a good design or what makes something fail frequently by analysing maintenance records.\n\u201cRecognising patterns and being able to analyse the contents of a digital domain or the digital twin ecosystem, combining the design, the reality data, the IoT data, and trying to put that all together to give better informed kind of business input. You know, should your needle look red, if you\u2019re headed for disaster or something like that. And there\u2019s no end of that\u2026\u201d\nKnowledge sharing\nOf course, the more data that\u2019s collected, the better the insight. A few years ago, Bentley floated the idea of engineering firms sharing analysis data from their projects so they could be compared against others. Could machine learning be used in this way for the greater good?\n\u201cWhen you get to things like trying to recognise P&ID documents, there\u2019s a lot of patterns that are the same across everybody\u2019s P&ID,\u201d explains Keith Bentley. \u201cSo, if you could study the Universe of all P&IDs and have that go into the model, the results would be way better.\n\u201cAsking people \u2018hey, would you be willing to donate the analysis of your data into the machine learning model that will then give back to you?\u2019 that hasn\u2019t happened yet, but I still hold out hope that people will be motivated by that, if we could show them value at the other end.\n\u201cBut I have to confess so far, it\u2019s just we do it on each user\u2019s data in isolation.\u201d\nThe future\nIt\u2019s still very early days for the use of machine learning in infrastructure digital twins, especially when it comes to recognising patterns. There have been several examples of the technology used for predictive maintenance, but in reality, we\u2019ve only just scratched the surface.\nKeith Bentley explains that one of the reasons it hasn\u2019t happened yet is that the data just doesn\u2019t exist or isn\u2019t available. But this will change over time.\n\u201cTen years from now, all the demos will be about how smart your digital twin is \u2013 smart, meaning your machine learning can recognise patterns in your data that you didn\u2019t know were there,\u201d he says.\n\u201cDigital Twins will be the input system to the machine learning system that I think can revolutionise the concept of designing, operating, building, and maintaining large scale assets in the decades to come.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-aconex-helps-drive-singapore-bim-initiative\/","title":"NEWS: Aconex helps drive Singapore BIM initiative","date":1420675200000,"text":"Aconex Connected BIM will support Real Estate Developers\u2019 Association of Singapore (REDAS) on pilot projects\nAconex has formed a major partnership with the Real Estate Developers\u2019 Association of Singapore (REDAS), who will deploy Aconex Connected BIM and other collaboration solutions for selected pilot projects of its member developers. The strategic partnership agreement is part of the REDAS Developer-Centric BIM Initiative to improve collaboration, workflow management and productivity across architects, engineers, quantity surveyors, and contractors in the Singapore real estate value chain.\nThe 10 member developers engaged in the initiative \u2014 Allgreen, Bukit Sembawang, CapitaLand, CDL, CEL, Far East Organization, Frasers Centrepoint, Guocoland, Keppel Land, and Wing Tai \u2014 will each nominate a specific live project to pioneer the Connected BIM approach. The objectives are to detect and avoid design clashes early, reduce rework, accelerate construction schedules, and deliver safer, higher-quality and more sustainable built assets.\nThe agreement between REDAS and Aconex covers the cloud-based Aconex platform, which connects multi-company teams to facilitate project-wide collaboration for managing all documents, drawings, BIM models, communications, and workflows.\n\u201cThis initiative will be a game-changing effort that will pave the way to transform current practice in the real estate and construction sector,\u201d said Chia Boon Kuah, president of REDAS. \u201cThrough the pilot process, REDAS will partner with Aconex to provide the 10 member developers with leading cloud-based construction collaboration technology. Once the pilots have been successful, the live project experience will help REDAS scale up the use of BIM for the entire industry.\u201d\nConnected BIM extends the Aconex platform to manage BIM data and processes for what is described as \u2018deep collaboration\u2019 between design and construction teams and handover to the owner.\nDesigners for specific disciplines can create and modify BIM models in their native authoring tools and use software plug-ins to publish them in the Aconex BIM Cloud. On the Aconex platform, all members of the project team \u2013 other designers, engineers, consultants, contractors, subcontractors, and owners \u2013 can view, distribute, mark up, and contribute to model data at the object level. This coordinated process is designed to support timely clash detection and optimise constructability.\nAt practical completion, Aconex says a comprehensive and accurate set of interrelated project information \u2013 including the model, all of the documentation associated with each of its objects, and an audit trail \u2013 can be handed over to the owner for operation.\n\u201cThe goal of the REDAS BIM Initiative is to create a more collaborative project environment,\u201d added Lee Suan Hiang, CEO of REDAS. \u201cCurrently, many real estate value chain partners use BIM to submit electronic drawings to the Building and Construction Authority (BCA) for approval. However, they may not be fully utilizing the functions and features of the technology or reaping its full benefits. The pilot framework will enable these value chain partners to securely manage and share BIM models with their entire project teams according to pre-defined workflows.\u201d\nIf you enjoyed this article, subscribe to AEC Magazine for FREE\nRelated articles:\nKPF and SimScale develop wind analysis tool\n14th Gen Intel Core processors launch\nNEWS: $10m helps Frame take cloud CAD platform into public beta\nCintoo streams laser scan mesh data into Unreal Engine\nEdward Williams Architects invests in cloud workstations\nNEWS: Chaos Group to host new event on future of rendering\nHarrow creates \u2018digital twin\u2019 with street imagery and LiDAR\nEnscape gears up for Envision 21 virtual event\nAdvertisement","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/new-structural-optimisation-plug-in-for-rhino-grasshopper\/","title":"New structural optimisation plug-in for Rhino Grasshopper","date":1578960000000,"text":"Peregrine software designed to find optimal designs in seconds, particularly during the conceptual phase\nPeregrine is a new structural optimisation plug-in for Rhino\u2019s algorithmic modelling tool Grasshopper that is designed to rapidly identify highly efficient (minimum volume) frame topologies for any given set of loads, supports and material properties.\nAccording to LimitState, the developer of the software, the key benefit is that it allows near-globally optimal solutions to often be obtained in seconds, with these readily transformable into more practical designs using a range of built-in tools. It\u2019s particularly suited to conceptual design.\nA technology preview is available free-of-charge until 30 June 2020. The software will then be available on subscription, although there will still be a free package available to academic users for teaching and research.\nPeregrine has been developed as part of the UK government-funded BUILD-OPT research project. It involved the Universities of Sheffield, Bath and Edinburgh, AECOM, Arup, BuroHappold, Expedition Engineering, IStructE, Ramboll and the Steel Construction Institute.\nAn enhanced version of Peregrine is currently being developed as part of a partnership between LimitState, the University of Sheffield and Arup. This will incorporate tools to enable more complex building design problems to be tackled.\n\u201cGiven the climate crisis there is an urgent need to reduce the material consumption associated with the construction sector,\u201d said BUILD-OPT project principal investigator and LimitState MD Prof. Matthew Gilbert. \u201cWe therefore think it is important for engineers to have access to tools such as Peregrine that are capable of rapidly identifying materially efficient designs.\u201d\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/autodesk-s-new-ceo\/","title":"Autodesk\u2019s new CEO","date":1502150400000,"text":"Martyn Day caught up with Andrew Anagnost to hear the new Autodesk CEO\u2019s vision for the company, including subscription, enterprise licensing, 3D printing and the future of Revit\nIn February, after being at Autodesk on and off for over 24 years, company CEO, Carl Bass, stepped down. Having run Autodesk since 2006, Bass changed the company culture to focus on product innovation, developing new code streams, addressing new design areas, with a focus on manufacturing. The company\u2019s business model also changed towards delivering services on the cloud with a move from perpetual licensing to subscription.\nWhile Bass was planning to leave his role after 11 years, he also fell foul to active investors, Sachem Head Capital and Eminence Capital, which bought enough stock to join the board of directors and demanded changes to the company\u2019s business to make it more profitable, quicker to reward shareholders \u2013 something which Autodesk used to be focussed on under previous CEO, Carol Bartz. Bass offered a refreshing change by investing more in product diversity and attempting to drive value in its tool suites. The active investors wanted more profit now and went on record saying they wanted to replace Bass as CEO.\nWith the transition to subscription creating a \u2018trough\u2019 as income changed from big perpetual acquisitions to long term payments, as investors love more regular subscription more than \u2018joining fees\u2019, eventually Autodesk share price rose by 70-80% on the prospect of higher income in the future with new subscription model. A deal was reached with Bass stepping down if the investors also left the board in February. This left Autodesk with two acting co-CEOs, long-time Autodeskers Andrew Anagnost who pretty much ran the move to the company\u2019s business model and Amar Hanspal, who was in charge of all product development. Meanwhile the board pondered internal and external candidates for the role.\nIn June, Autodesk announced that Anagnost had won the job and on that news Hanspal decided to immediately leave the company. In just a few weeks the company had lost two of its most experienced product development champions but was now free of the direct interference of the active investors. Anagnost is certainly a safe pair of hands, he has been responsible driving the new business model, and over the decades has held various roles within Autodesk, product managing in the manufacturing division, and he had been Chief Marketing Officer and SVP of Business Development. Anagnost was one of the original driving forces behind the development of Inventor in the company, Autodesk\u2019s first serious new code stream after AutoCAD, and prior to joining Autodesk, he worked at Lockheed Aeronautical Systems and NASA Ames Research Center.\nAnagnost inherits a company with a buoyant share price and gets to continue with his model of moving the $2.14 billion company to Subscription and the cloud, but he can now obviously control all aspects of the company, including product development direction. At Autodesk University in London, AEC Magazine had a Skype call with Anagnost to find out a little more of his vision for the company and to hear his views on some key areas.\nAEC Magazine: Historically you have come from the manufacturing side of the business, so which of Autodesk\u2019s divisions is your natural passion?\nAnagnost: I have a passion for solving really big problems. There\u2019s no doubt about my background being in manufacturing but there are two areas I\u2019m really excited about. One is that I am super excited about construction \u2014 and I guess I am cheating a little bit here, as it\u2019s industrialising and we have the opportunity to bring the BIM model into construction. Here I think it\u2019s exciting for our customers and for Autodesk, I think we have a really big value which we can add, as we have a lot of knowledge as how models progressing in manufacturing as models become more important in driving that process.\nAnd the other area that I am excited about is getting us to push button manufacture. I know it sounds whacky at the moment but five years from now we are going to be really close to automate the process of having a really complicated 3D model, pushing a button and getting a part come out the other side from a multi-purpose factory. I am super energised about that.\nSo I am super excited about the industrialisation of construction, that presents a lot of problems which need to be solved and we have these push button manufacturing issues that we are monitoring. So that\u2019s what I\u2019m looking at in the long-term.\nIn the short-term, I have more customer based objectives. I\u2019m excited about subscriptions and collections; we aren\u2019t there yet, and I\u2019m going to make sure that they get there.\nAEC Magazine: Autodesk has divested itself of its 3D printer (Ember) and seems to be getting out of hardware, although you still have Pier 9 manufacturing facility, and replicating that in Portland and Boston offices. What\u2019s the view on hardware now?\nAnagnost: You remember the rise of desktop publishing? The world changed when Adobe came out with PostScript because what you saw is what you got coming out of the printer. Our goal is to make sure what you see is what you make and it\u2019s much more important and valuable that we played the PostScript role in manufacturing, than play a hardware vendor role. So if we\u2019ve got a dollar to spend we\u2019re going to spend a dollar on the development of PostScript and the new types of automation from manufacturing rather than building hardware. Ember was a test bed of the technology so we can see how it worked and get experience with that. It was never going to be our mainline business.\nAEC Magazine: On Autodesk subscriptions, there is an issue. Products like Revit have not met the expectation of companies that have invested in them. Yes, Revit\u2019s a mature product so there\u2019s a velocity issue but customers are concerned as to the value they are getting. Having gone from yearly release to bi-annual the \u2018value\u2019 is also spread out and possibly less recognisable. But at the moment you are asking customers to move from perpetual licences to subscription to pay more for something they were already unhappy with the value of.\nAnagnost: There are a couple of things we can do. First off, there\u2019s still a long way to go for Revit in two constituencies. If you talk to architects there\u2019s still a long list of features that they would like to see and we have made progress with them but there\u2019s a long way to go. And secondly, in the pre-construction side, construction planning, 4D and 5D, there\u2019s still a lot of value to mature in Revit that we haven\u2019t given to the customers yet and we know what it is because they complain about it.\nWe also have to effectively blend new types of capabilities into the product and a lot of those are going to be cloud-based. Looking at Collaboration for Revit and Quantum which is on the same continuum, when we get some of that stuff working in a mainstream way, without it costing a small fortune \u2014 all of those things will be super valuable. I think when all that is stitched in customers will start to feel differently.\nWall Street kind of co-opted our subscription transition, but that\u2019s not how we started this. Our goal is to become a cloud company, which means our goal is to change the complete value chain with a different set of price points. I think in all the rough-and-tumble of people talking about the \u201cbad\u201d thing we were doing, people had forgotten the good thing we were doing. When Wall Street gets involved and people start paying attention, the maintenance base feels like we\u2019re raising prices but as a whole bunch of customers who [due to their purchasing experience of Autodesk products] think we\u2019ve lowered prices. It\u2019s the whole Yin Yang of all of this. The original goal was to become a cloud company, with cloud price points all the way through our value chain.\nAEC Magazine: We have yet to meet anyone who signed an ELA Agreement [Enterprise Licence Agreement \u2013 lasts for 3 years for biggest Autodesk customers] in the AEC space who is happy and we have talked to a lot. The price hikes have been so big that they can\u2019t quite believe it. Subscription is asking firms how much they want to pay for their tools. It\u2019s not business as usual.\nAnagnost: I agree with you. I don\u2019t think it\u2019s business as usual. I think some customers are struggling with the pricing, but here\u2019s the dichotomy I have put in front of me. Yes, the maintenance base is struggling with this, asking \u201cwhat\u2019s going on?\u201d and \u201cwhere is the value?\u201d I talk to them and I even call the one-man shops, as well as the big guys, and I say this consistently: I understand and I want you to give us a year to prove to you that we are delivering value, don\u2019t leave us. I have this conversation over and over again but I have to juxtapose this with the hundreds of thousands of new seats that are coming in from people who have never bought from us before.\nSo our existing customers are deeply suspicious about what\u2019s going on, but these new guys are emailing us saying you just put food on my family\u2019s table. You probably don\u2019t see both sides of this. I see the difficult side and recognise all the work Autodesk needs to do to solve this and build trust. And then there\u2019s the other side where we created a pool of trust\u2026 and if I can bring those two together then it\u2019s a home run for everybody.\nAEC Magazine: Will you be a hands-on CEO like Carl Bass was?\nAnagnost: I\u2019m pretty deep into everything that this company does, which is a curse and a blessing for people that work here. Carl went deep a lot in product, I\u2019m probably going to go deep in multiple things. Things I am passionate about \u2013 construction and push-button manufacturing but also with the sales force and customers. So yes, I\u2019m going to be pretty hands-on.\nAEC Magazine: What\u2019s your vision for route to market and resellers, changes to margins are reducing the number of VARs?\nAnagnost: There\u2019s no doubt, it\u2019s already happening. We will have a larger, smaller partner channel, there\u2019s no doubt about that. The number of channel partners we have has dramatically decreased over the last three years. When you look at our business three years out and then five years out, we are going to be doing at least 50% of our business through channel partners. So we are still a channel company five years out \u2013 and by the way that half our business is bigger than the size of business we have today. So that means there is a huge role for VARs in our business. We do not sell word processing software. We sell complicated stuff.\nThe future of our partners falls into three segments. They\u2019re going to be a collections channel, they\u2019re going to be a consumption channel (which is going to be an increasing part of our portfolio, pushing buttons for one time value add is going to be huge), and third they\u2019re going to become third-party developers all over again. They\u2019re going to get back to where they were 30 years ago, customising what we do, but that will be on the Forge platform, not on the desktop. There\u2019s really no future any more for a partner that just sells AutoCAD or LT. It\u2019s not where the business is going.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/bentley-ereport\/","title":"Bentley","date":1181174400000,"text":"This year\u00dds \u00d9Bentley Empowered\u00dd (BE) Conference was held in downtown Los Angeles.\nMartyn Day attended to both judge the company\u00dds Excellence awards and get the latest Bentley strategy.\nBentley\u00dds User conferences are always big affairs and were historically held in the company\u00dds home town of Philadelphia. But for a number of years now the event has toured the United States, landing at Baltimore, Atlantic City, Orlando and Charlotte. This year it was its first west coast destination, Los Angeles, in the Staples convention centre. I can\u00ddt help but think that being a Bentley customer is a great way to see the US and now also Europe. Last year saw BE come to Prague for a European leg and this year London is the host city. However, there\u00dds little time for sightseeing as the company always lays on almost a week\u00dds worth of keynotes, training and demonstrations \u2013 everything you wanted to know about MicroStation and more!\nBentley is a company that keeps a fairly low profile, so for those of you that don\u00ddt know much about the company, it is one of the largest privately owned software companies in the US, with revenues of almost $400 million, and the Bentley brothers, of which there are more than a few all take active roles. Greg Bentley is CEO, Keith Bentley is CTO and Ray and Barry Bentley are Executive VPs \u2013 the company also has 2,495 other employees.\nOver the years Bentley has developed into the CAD company for Infrastructure. Nearly all the US State Department of Transports (DOTs) use Bentley products, the company dominates the Civils market, is a key player in Process and Plant, supplies the US Army Corps of Engineers, is big in railways and water treatment and mapping and Geo. Then, of course, especially in the UK, there\u00dds AEC design especially in mid-sized firms and at signature architect level. By attending BE one really gets a great insight into the breadth of markets that Bentley competes in, and by taking part in the judging of the Bentley Excellence awards, I had additional exposure to the size and scale of projects that Bentley\u00dds customers undertake.\nGreg Bentley CEO\nAnd so having given you a little background, what did Bentley have to say to its customers? Greg Bentley\u00dds Keynote kicked off with an examination of the increased productivity of its users, tying this into Bentley\u00dds relatively new On Demand learning and Institute services, allowing users to train where and when they want.\nGreg Bentley announced ProjectWise V8 Passport, offering portable and global licenses allowing access to any ProjectWise client, desktop or web application. The aim is to reduce the costs of collaborating across multiple projects, when it has been based on a per-project model. ProjectWise V8 Navigator is a new review and analysis package for multiple file formats, in a ProjectWise Startpoint or ProjectWise Integration Server.\nWith the focus of the event clearly being Bentley\u00dds customers, Greg Bentley then literally went around the world, starting off in Los Angeles, highlighting a number of customers (many household names) and their projects, explaining how they managed to innovate and achieve better economic productivity. In his final round up, the CEO showed Bentley\u00dds acquisition momentum, as it buys up many of the specialist analysis firms. As Bentley predominantly operates a subscription model, licenses are the lifeblood of the company revenues, and again for the fifth consecutive year, subscriptions were up. A new and innovative company division, Bentley Direct, headed up by former Chief Marketing officer, Tony Flynn, has been established to sell to new and existing customers. Bentley has few dealers and tends to sell direct, this new division is a more volume approach using the latest web and electronic technologies.\n{mospagebreak}\nBhupinder Singh, Senior VP, Bentley Software\nNext on stage was Bhupinder Singh, who has taken over the strategy and direction of Bentley\u00dds software development. In his first statement, Singh recommitted to the Bentley vision of, one platform, a comprehensive portfolio and ongoing technology innovation. Bentley is the MicroStation company and nearly everything is based on that underlying architecture, or developed to extend or enhance these MicroStation-based applications. By having one platform Singh elaborated, it means that all development is standards-based, extensible desktop and server, through multi-connected workflows.\nComing from the ProjectWise and Collaboration development side of the business, it was unsurprising to hear Singh spend some time here. ProjectWise Navigator, for viewing and mark-up is actually designed on MicoStation V8 and is proof of the one platform promise.\nThe Bentley portfolio of products is very, very big \u00b1 Building, Plant, Civil and Geospatial with MicroStation and ProjectWise are the substrate for all. Again using customers as examples, Singh toured the verticals showing how Bentley was innovating within each market, addressing and refining workflows and providing efficiencies through application integration and interoperability.\nHowever, Singh did reiterate a promise that Bentley has tried to keep but obviously had some problems keeping. With so many products, making sure everything works together at the same time is a massive task. When MicroStation V8 was introduced, Bentley attempted to synchronise the releases of all its products. Obviously over time this has been difficult to maintain. The next release of MicroStation, codenamed Athens, will be another chance to pull all the Bentley development teams together. For now Bentley has introduced the \u00d9Compatibility zone\u00dd, where at a glance you can tell what versions of each application are certified to work together.\nThe Athens project was the big announcement of the event. This next release of MicroStation, slated for sometime next year, will feature conceptual design tools, dynamic 3D views, distributed project capability and built-in geo-co-ordination.\nConceptual tools in CAD is the new big thing. CAD has always been this regimented computerised way of accurately drawing things. Conceptual modelling requires less precise, freehand sketching and the capability to turn these sketches into something more useful for proper detailing. It\u00dds a tough computer science task but there have been some great innovations in this area. Autodesk\u00dds AliasStudio and Google\u00dds SketchUp are the kinds of products I\u00ddd advise you to look at to get a view as to what is possible today. Bentley\u00dds vision is to provide a tool that provides a rapid transition from idea to form, iterate quickly through design alternatives and provide a tool that answers more questions earlier on in the design process. In the package this will mean that MicroStation will be able to do interactive push\/pull modelling, manipulate integrated surface, solids and mesh modelling and there will be a Rhino import capability. The company\u00dds exciting Generative Components technology will also benefit from these additions. So expect some very cool stuff for architects in the Athens release.\nDynamic views offer a new way to author and visualise designs. It will be interactive and work off live content, so there will be no need to extract drawings, with section and elevation markers. These features will feed through into all the vertical applications, e.g. in Civil it will be possible to work in a cross section and exaggerated profile views.\nFor distributed teams, Bentley is planning Athens to improve on performance of uploading and downloading over high latency bandwidth connections. Together with improvement of sharing managed content.\nWith Singh\u00dds presentation there were a number of platform keynotes. Here each vertical got to explain the highlights of Athens for their customers. The one presentation and technology that stood out was Bentley VP Styli Camateros. Athens will have Geo-co-ordination built-in. Camateros pointed out that there are many co-ordinate systems for geo data and it would in fact be easier if the world was actually flat but it\u00dds not, it\u00dds an Ellipsoid. As a result we have to put up with multiple representations, which unfold the Earth and mimics \u00d9flatness\u00dd. These projections are all based on co-ordinate transforms and don\u00ddt easily map to traditional Cartesian geometry systems. Athens will now take that problem away by automatically coalescing content in different formats, from different sources both on the desktop and on the server.\nThe user simply assigns the spatial location and MicroStation will automatically re-project for attach display and search. As with all geo presentations, somehow we ended up in Google Earth.\nKeith Bentley, CTO\nWith much of the future tech now out of the bag, Keith Bentley mainly reinforced the company\u00dds vision of development, explaining the platform layers and the benefits of a single platform, albeit an evolving one. Bentley goes to great lengths to try and lessen software impact, having only changed the DGN format twice in the company\u00dds history.\nWhen it comes to hardware, Keith Bentley was offering the following advice; Moore\u00dds Law is running out of steam, so parallelism is the new source of power \u00b1 multiple processors on a single chip. Multi-core and the rise of something called a GPU (Graphics Processor Units). In the not too distant future, computers will have many processors which will be dynamically allocated to running applications. However with the move to 64-bit everything is going to take more memory, including caching and working with Tera-drives.\n{mospagebreak}\nKeith Bentley explained that today MicroStation XM will use all the available memory on a 64-bit computer but not the extra 32-bits of processing power. The 64-bit version is coming (Athens), so don\u00ddt buy anymore 32-bit computers and make sure you buy multi core 64-bit processors. In the graphics world Bentley is seeing big developments with Shader Model 3 and Direct3D 10.\nBuddy Cleveland, Senior VP, Applied Research\nAnother new innovation is Bentley Applied Research, under Bentley long-timer Buddy Cleveland. The purpose of the division is to expand the company\u00dds R&D activities to partner with customers and research institutions to select, fund and develop new product-focussed technologies. While Bentley has always spent a large amount on R&D, most of this effort has been internal. This will now expand to include input from other sources. Areas highlighted for research include Digital Fabrication, Law Curves, Pattern Editing, Remote Video, RFIDs and Digital Pen and Paper.\nMalcolm Walter, COO\nWalter wrapped up the main plenary session with a keynote on \u00d9Return on Innovation\u00dd. According to the National Institute of Standards and Technology (USA), the lack of interoperability among CAD engineering and design systems costs $15.8 billion. While the manufacturing market has seen gains in productivity, the productivity of architects over the same timeframe has become even more inefficient. Approximately 25% of an engineer\u00dds time is spent looking for data and an additional 20% is wasted in data conversion. Added to this it\u00dds estimated that by 2010, the USA will be 700,000 engineers and technicians short. So, we waste lots of time and money, challenges are escalating and there is a shortage of resources that\u00dds only going to get worse.\nBuilding\nBIM for Architecture: Rogers Stirk Harbour & Partners Ltd \u2014 The Leadenhall Building\nBIM for Building Engineering: M\/E Engineering, P.C. \u2014 BioMed Engineering and Optics Laboratory Building\nBIM for Multiple Disciplines: Arup \u2014 Review and Study of the Opera Theatre Interior and New Works, Sydney Opera House\nBIM for Visualisation and Simulation: Dvorak architekti s.r.o. \u2014 Vrnata House\nBIM for Sustainable Design: Fortis Bank \u2014 Kanselarij Cluster Restructuring Project\nBuilding Innovation: LJB, Inc. \u2014 Wells Fargo Home Mortgage\nBuilding Structural Analysis, Design, and Documentation: WSP Group plc \u2014 Manchester Hilton\nBuilding Distributed Enterprise: The Standard Bank of South Africa Ltd \u2014 Central Premises Information Repository Civil\nCivil\nCivil Innovation: HNTB \u2014 TrueViz OnTarget\nCivil Road Infrastructure: Scott Wilson Kirkpatrick (India) Pvt Ltd \u2014 Laning of Hyderabad-Vijayawada and Norra L?nken\nCivil Site Design: Vela VKE Consulting Engineering \u2014 Zimbali Civil Infrastructure\nCivil Visualisation and Simulation: Mott MacDonald Ltd \u2014 East London Line Project\nCivil Structural Analysis, Design, and Documentation: Arup \u2014 Marina Bayfront Pedestrian Bridge\nCivil Distributed Enterprise: The Banks Group \u2014 Centralized Data Storage Using StartPoint and Riverbed WAN Optimization\nGeospatial\nGeospatial Communications: Nacap Telecom BV \u2014 Fibre Optics \u2018Zuid-Limburg\u2019 -Isilinx\nGeospatial Government: Gemeente Amsterdam, Dienst Ruimtelijke Ordening \u2014 IJburg\nGeospatial Mapping and Cadastre: Petrobras SA \u2014 SGO Project \u2013 Petrobras Subsea Assets Tracking\nGeospatial 3D GIS: AAMHatch \u2014 True Orthophotography and 3D Model of the City of Melbourne\nGeospatial Water Resource Management: ISKI Genel Mudurlugu \u2014 Infrastructure Integration of Mega City \u2013 Istanbul\nGeospatial Utilities: Baltimore Gas & Electric Co. \u2014 The BGE Bentley Project\nGeospatial Innovation: Dutch Ministry of Finance \u2014 Geospatially Enabling the Dutch Ministry of Finance; Putting \u2018Where\u2019 into SAP\nGeospatial Distributed Enterprise: Sandia National Laboratories \u2014 Power Through Integration\nPlant\nPlant Multi-Discipline Design: BSPiR Energoprojekt-Katowice S.A. \u2014 The Coke Plant \u201cPrzyjazn\u201d\nPlant Lifecycle Information Management: North China Power Engineering (Beijing) Co. Ltd \u2014 Building Up NCPE Content Management Platform With ProjectWise as the Core\nPlant Structural Analysis, Design, and Documentation: Southern Co. Services \u2014 Plant Wansley Units 1-2 FGD Addition\nPlant Visualisation and Simulation: Atkins Water \u2014 Bolton Wastewater Treatment Works\nPlant Innovation: VECO Alaska Inc. \u2014 Conoco Phillips Drill Site Technologies\nPlant Rookie of the Year: Sanitation Districts of Los Angeles County \u2014 San Jose Creek Central Plant Modification\nPlant Distributed Enterprise: Bechtel \u2014 Jamnagar Export Refinery Project\nSpecial Awards\nBest Overall IT Strategy: VECO Alaska Inc. \u2014 Bentley Enterprise Implementation and Management\nWell-Trained Organisation: Halff Associates, Inc.\nWalter then looked at how a number of customers were using ProjectWise to collaborate and save time handling data in distributed projects, saving money. One interesting customer case study was the Dutch Ministry of Property which is the largest land owner in the Netherlands. The Dutch Cadastra and the Ministry of Finance were totally separate until they integrated their Bentley GIS products with SAP. With a $20 million return on investment, this works out at a 1,600% return on investment.\nAwards\nAs part of my invite to BE, I took part n the judging of the Civil component of the Excellence awards and it was a day long event with much point and counterpoint, working through the submissions in the various categories with my fellow independent jurors. In other rooms similar teams gathered to evaluate other vertical segments. It was a very inspiring experience, seeing and reading all the entries and it made me think how much we all take the infrastructure around us for granted.\nI didn\u00ddt take part in the Building judging but one UK project that really stuck out was Rogers Stirk Harbour Partners\u00dd Leadenhall Building, which came first in the BIM for Architecture category. The stunning rendering on the front cover shows how the Leadenhall Building will look next to Foster\u00dds \u00d9Gherkin\u00dd and is set to become yet another iconic piece of London architecture when it is completed in 2011.\nThis building will be a 48-storey tower located opposite Roger\u00dds Lloyd\u00dds of London, rising to a height of 224.5 metres (802 feet). Its tapering profile is prompted by a requirement to respect views of St Paul\u00dds Cathedral, in particular from Fleet Street. The office floors take the form of rectangular floor plates which progressively diminish in depth towards the apex. Instead of a traditional central core providing structural stability, the building employs a full perimeter braced tube which defines the perimeter of the office floor plates and creates stability under wind loads.\nAny project in the City of London will be faced with challenges of construction and the close proximity of other occupied buildings, access and tube lines combined mean every stage of the design needs to cater for the constructability of every component. Leadenhall Street is no exception, although these challenges are exacerbated by the scale and aesthetic requirements of the steelwork.\nThe steelwork is designed as an integrated structural\/architectural frame, comparable to the type of design favoured by luminaries such as Eiffel. The structural requirements inform the architectural appearance of the elements. Additionally, as a traditional cladding was not possible, the success depended on a truly integrated design approach within all the involved disciplines, With major nodes connecting the members standing at over 8m tall limited by the maximum 30 tonnes lifting capacity the challenge was to design and define a structure that was both elegant and constructable within the tight constraints of the site.\nConclusion\nLA was a great BE and the most customer-centric I have been to. Not only did all the execs reference what customers were doing with the technology but most of the presentations were built on what customers had achieved and I think to the audience the implications and benefits of the technology was really immediate.\nAthens looks to have some good new technology and the research announcement promises to come up with some real innovations. Bentley\u00dds core technologies are MicroStation and ProjectWise and the company is probably one of the few vendors still to remain true to so few code streams. Bentley does purchase many small technology companies and it has taken a while for it to integrate and digest these within the MicroStation framework. Still you can\u00ddt really fault Bentley for its progressive evolution of a toolset that is being used in such large and diverse projects.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-rob-charlton-space-group\/","title":"Video: NXT BLD London conference \u2013 Rob Charlton, Space Group","date":1502150400000,"text":"The positive impact of accelerating technologies \u2013 NXT BLD London, June 2017\nFrom Reality Modelling and drones to VR and AR, Rob gives a critical review of current digital construction technologies. He also gives predictions on emerging AI technologies and how they will transform current building methods and revolutionise project planning, management and delivery. It\u2019s not about telling the computer what to do, he says, it\u2019s about telling it what we want to achieve.\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Mike Leach, Lenovo\nEnhancing performance through the workflow\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Paul Nichols, Skanska\n\u25a0 Arthur Mamou-Mani, Mamou-Mani\nConstructing (and deconstructing) buildings with cable robots\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/visualisation\/preview-v-ray-next-for-3ds-max\/","title":"PREVIEW: V-Ray NEXT for 3ds Max","date":1526515200000,"text":"The latest version of the popular design viz renderer uses scene analysis to optimise rendering and gives GPU rendering the same standing as CPU rendering, writes Greg Corke\nV-Ray has become the industry-standard renderer for architectural design visualisation. The software is primarily used alongside 3ds Max, the tool of choice for arch viz artists, but in recent years it has spread its wings into 3D CAD and BIM with versions for Autodesk Revit, SketchUp and Rhino, as well as other 3D modelling tools.\nChaos Group has now launched a brand-new version, V-Ray NEXT, which will be made available first for 3ds max. It\u2019s a break from the traditional numerical naming conventions, but the company justifies this by saying that the new branding signifies a significant leap forward in terms of both technology and capability. And, as Chaos Group CCO Lon Grohs admits, \u201cV-Ray 4 for 3ds max didn\u2019t sound right.\u201d In other words, the number \u20184\u2019 followed by the word \u2018for\u2019 would have been a bit of a mouthful.\nThe new version is very much focused on performance. \u201cIt is about 25% faster just as a baseline. That\u2019s everything \u2013 all ray tracing calculations are going to speed up,\u201d says Grohs, adding that this is down to optimised code.\nOn top of that, the developers have been working on Scene Intelligence, a collective group of technologies that allow V-Ray to \u2018learn\u2019 from a scene to help cut render times.\nThen there\u2019s a completely new GPU rendering architecture called the multi-kernel, which is based on Nvidia CUDA. \u201cIt does parallel processing a lot faster \u2013 about two times faster than the previous gen,\u201d says Grohs. More importantly, he believes that its V-Ray GPU renderer (previously called V-Ray RT) is now on par with V-Ray itself in terms of quality. It can be considered properly production-ready.\nScene Intelligence\nScene Intelligence is not unlike machine learning, in that it analyses a scene and then decides the best way to optimise lighting calculations, sampling and so on.\nHowever, it\u2019s not considered to be machine learning in the true sense, as this generally means processing a huge pool of data on a powerful server or cluster in order to \u2018learn\u2019 and refine actions based on that learning. \u201cOurs is that on a micro level,\u201d explains Grohs. \u201cWe\u2019re learning directly from the scene we\u2019re working with. And we\u2019re using that person\u2019s own machine to do that learning.\u201d\nScene Intelligence is an umbrella term applied to several different technologies inside V-Ray. The first were introduced in V-Ray 3.X and included the Adaptive Lights algorithm, which learns about the scene by analysing the Light Cache data and determining which lights to sample and which to ignore.\nV-Ray NEXT introduces two new features powered by Scene Intelligence \u2014 Adaptive Dome Light and Automatic Camera Exposure & White Balance. Adaptive Dome Light is designed to deliver faster, cleaner and more accurate renders when using HDR environment images to light a scene. It can be used for exteriors or interiors, but it\u2019s especially good at interiors, says Grohs, adding that it can make image-based environment light renders between two and seven times faster.\n\u201cIn the previous generation, at each of the windows and openings where you wanted light to come into your interior space, you would add something called a skylight portal,\u201d he explains. \u201cThat skylight portal was basically a rectangular area of light that saw out to the environment and tried to optimise the light or bring more rays in through that opening.\n\u201cBut it was a little bit cumbersome and a little bit tedious and not quite exact and so, with the adaptive dome light, that all goes away. You don\u2019t have to set up skylight portals anymore \u2013 at all \u2013 and what\u2019s even better is the adaptive dome light does a much better job of bringing light in through those portals.\u201d\nChaos Group is also using Scene Intelligence to help make renders as close to \u2018point and shoot\u2019 as possible. With V-Ray\u2019s Physical Camera, you can imitate all the effects of an SLR, but historically, you\u2019ve had to properly understand all the camera settings \u2013 F-stop, shutter speed, depth of field, lens distortion and so on \u2013 as all adjustments were manual.\nWith the new Automatic Camera Exposure & White Balance controls, V-Ray looks at the entire scene to automatically determine the proper exposure and white balance, just as a modern camera would. However, it can also automatically adjust the ISO value without changing the F-stop or shutter speed, which lets you control depth of field and motion blur separately.\nGPU rendering\nFor some years now, V-Ray has supported two renderers: the traditional CPU renderer and V-Ray RT, a so-called \u2018real time\u2019 GPU renderer that can also run in hybrid mode on CPUs. For this release, V-Ray RT has been re-named V-Ray GPU to reflect its new standing within the software.\n\u201cSpeed is the new hallmark here, but also with support for a lot more features that people can use in production,\u201d says Grohs. \u201cWe\u2019re seeing some really great stuff done in architecture, big scenes, interior scenes, all done on the GPU and it\u2019s pretty awesome.\u201d\nVolumetric effects like smoke, fire and fog have been added to V-Ray GPU. There\u2019s also been a significant update to the UI, not least to remove confusion when rendering. \u201cWhen you are working in V-Ray GPU, you only have access to the features that are supported by V-Ray GPU.\u201d Previously you were able to select a feature in the V-Ray GPU UI that was only relevant to the CPU renderer and it simply wouldn\u2019t work.\nFor preview renders, the new release introduces an AI Denoiser, which uses Nvidia\u2019s OptiX technology to denoise the ray tracing. It\u2019s not a production solution; instead, it is designed to give a best guess, very quickly, of what the software thinks the final image might look like.\n\u201cIf I\u2019m working in my viewport and I\u2019m setting up lighting and shading and materials, instead of it looking like a grainy render, in real time it\u2019s making it look smooth,\u201d explains Grohs. \u201cNow, it does kind of fuzz up some of the details a little bit, but from a workflow standpoint, it does a tremendously good job.\u201d\nThis is machine learning in the true sense, insofar as it uses data that has been previously learned from thousands of rendered images and not just the rendering it is currently working on, as is the case with Scene Intelligence.\nChaos Group currently uses a set of data from Nvidia that has already been calculated. However, Grohs acknowledges that it could be re-trained with new sets of images. \u201cWe do have some ideas about the possibility of looking at our own set of data,\u201d he says.\nHe admits that, while V-Ray GPU now offers just as good quality as V-Ray, there are still some inherent limitations in GPU rendering in terms of scene size and memory. But these are being addressed, he insists.\n\u201cWe tried to combat that in the previous (3.x) version. We included automatic mip-mapping, which adjusts your texture resolution and allows your textures to be a lot more memory-efficient,\u201d he explains. \u201cTextures were the thing that took up most of the GPU memory, so that really opened up the floodgates to having a lot more on [the GPU].\n\u201cWe are working on a solution for \u2018out of core\u2019 rendering and that basically means that if you go beyond your GPU memory, we\u2019ll have a way to cycle the data in and out. That\u2019s not in [V-Ray] NEXT yet but it is something we are working on.\u201d\nMemory size on physical GPUs is also increasing. The new Nvidia Quadro GV100, for example, features 32GB of high-bandwidth memory. And two Quadro GV100 cards can be combined using Nvidia NVLink interconnect technology to double that to 64GB.\nIf memory size is still an issue on large scenes, particularly for users who can\u2019t afford two GV100s at \u00a315,000+ a pair, then it\u2019s still possible to fall back on the CPU, as Grohs explains. \u201cIf I set up my scene in V-Ray GPU and it turned out that my scene got too big, I can still render that with V-Ray GPU on the CPU, or I can take it all over to my V-Ray renderer and let that render on the CPU as well.\u201d\nLighting analysis\nIn previous versions of 3ds Max, when mental ray was included in the software, architects and interior designers were able to do lighting analysis using a specialist tool.\nNow, that mental ray has been removed from the software and Chaos Group has developed its own lighting analysis tools, which allow designers to both measure and analyse light levels in a scene.\nGrohs told AEC Magazine that the new tool is comparative to the mental ray-based technology in terms of accuracy, but maybe has a bit more functionality.\nExploring materials\nV-Ray NEXT for 3ds Max now includes a much easier way to explore materials as part of the design process. With the new V-Ray Switch Material, multiple materials can be applied to a single object and then selected for output at render time.\n\u201cThe way to tackle that before would be to set up all those materials and then you just dragged into that object and applied it when you were ready to render,\u201d says Grohs.\n\u201c[With Switch Material], I can now apply wood, granite, marble all to the same kitchen counter top, for example, then just check the box that I want when I render the scene.\u201d\nThis feature can also be used to add random variations or for creating extra passes for compositing.\nGetting more from the cloud\nOne of the challenges that 3ds max users have when it comes to cloud rendering is that the software is Windows-only. It\u2019s possible to render with Windows in the cloud, but as Grohs explains, it can be two to three times more expensive than doing so on Linux.\nV-Ray users can get around this by moving the scene into V-Ray standalone, which works with Linux. However, while many elements of the scene move across smoothly, others are incompatible, as Grohs explains: \u201cSome things are out of our control \u2013 it depends on what feature set you are using in max and, honestly, what other plug-ins you are using.\u201d\nTo improve this workflow, Chaos Group has developed a new \u2018cloud check\u2019 utility that offers advice on what users might need to change. Previously, if there were any incompatibilities, it just wouldn\u2019t work.\nConclusion\nWith V-Ray NEXT, Chaos Group is looking to help viz artists accelerate rendering in more intelligent ways, rather than simply throwing more processing power at a problem. Smart algorithms that analyse the current scene and then optimise calculations accordingly mean that hardware can work harder on the things that really matter.\nThe GPU has also been elevated in status and its highly scalable compute power is starting to be used to full effect. The AI denoiser, for example, has the potential to accelerate preview renders dramatically. And, if users do need more power, they can simply add additional GPUs to workstations, PCIe lanes permitting.\nWhile most of the developments in this new release are under the hood, there are also some great workflow enhancements. The V-Ray Switch Material, for example, looks to be a simple but effective way for architects to explore materials during the design process. We hope this technology is rolled out to Revit, Rhino and SketchUp, where it can be most effective.\nV-Ray NEXT for 3ds Max will be out soon. Grohs estimates that the new rendering engine will be available for Rhino, SketchUp, Revit and others by the end of the year.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/news-cyclone-9-0-boasts-host-of-new-features-for-point-cloud-processing\/","title":"NEWS: Cyclone 9.0 boasts host of new features for point cloud processing","date":1411516800000,"text":"Leica Geosystems is claiming a major leap forward in laser scanning project efficiency with Cyclone 9.0, the latest release of its point cloud solution for processing laser scan data.\nAdvances in the software are said to benefit both field and office thanks to \u2018significantly faster\u2019, easier scan registration, plus quicker deliverable creation thanks to better 2D and 3D drafting tools and steel modelling. Cyclone 9.0 is also said to scale easily for larger, more complex projects.\nWhen Leica Geosystems first introduced cloud-to-cloud registration, it enabled users to accurately execute laser scanning projects without having to physically place special targets around the scene, scan them, and model them in the office. With cloud-to-cloud registration software, users take advantage of overlaps among scans to register them together.\n\u201cThe new Automatic Scan Alignment and Visual Registration capabilities in Cyclone 9.0 represent the biggest advancement in cloud-to-cloud registration since we introduced it,\u201d explains Dr. Chris Thewalt, VP Laser Scanning Software. \u201cCyclone 9.0 lets users benefit from targetless scanning more often by performing the critical scan registration step far more efficiently in the office for many projects. As users increase the size and scope of their scanning projects, Cyclone 9.0 pays even bigger dividends. Any user who registers laser scan data will find great value in these capabilities.\u201d\nWith the push of a button, Cyclone 9.0 automatically processes scans and, if available, digital images to create groups of overlapping scans that are initially aligned to each other. Once scan alignment is completed, algorithmic registration is applied for final registration. This new workflow option can be used in conjunction with target registration methods as well.\nPower user Marta Wren, technical specialist at Plowman Craven Associates (PCA \u2013 leading UK chartered surveying firm) found that Cyclone 9.0\u2019s Visual Registration tools alone sped up registration processing of scans by up to four times faster than previous methods. PCA uses laser scanning for civil infrastructure, commercial property, forensics, entertainment and Building Information Modelling (BIM) applications.\nFor civil applications, new roadway alignment drafting tools let users import LandXML-based roadway alignments or use simple polylines imported or created in Cyclone. These tools are said to allow users to easily create cross section templates using feature codes, as well as copy them to the next station and visually adjust them to fit roadway conditions at the new location. A new vertical exaggeration tool in Cyclone 9.0 allows users to clearly see subtle changes in elevation; linework created between cross sections along the roadway can be used as breaklines for surface meshing or for 2D maps and drawings in other applications.\nFor 2D drafting of forensic scenes, building and BIM workflows, a new Quick Slice tool streamlines the process of creating a 2D sketch plane for drafting items, such as building footprints and sections, into just one step. A user only needs to pick one or two points on the face of a building to get started. This tool can also be used to quickly analyse the quality of registrations by visually checking where point clouds overlap.\nAlso included in Cyclone 9.0 are powerful, automatic point extraction features, first introduced in Cyclone II TOPO and Leica CloudWorx. These include efficient SmartPicks for automatically finding bottom, top, and tie point locations and Points-on-a-Grid for automatically placing up to a thousand scan survey points on a grid for ground surfaces or building faces.\nFor plant, civil, building and BIM applications, Cyclone 9.0 also introduces a patent-pending innovation for modelling steel from point cloud data more quickly and easily. Unlike time consuming methods that require either processing an entire available cloud to fit a steel shape or isolating a cloud section before fitting, this new tool is said to allow users to quickly and accurately model specific steel elements directly within congested point clouds. Users only need to make two picks along a steel member to model it. Shapes include wide flange, channel, angle, tee, and rectangular tube shapes.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/two-indian-boys-in-nasa-mars-mission\/","title":"Two Indian boys in NASA Mars mission","date":1066262400000,"text":"Satvik Aggarwal, a class X student of Amity International School in Saket in South Delhi, is among the two Indian students selected by National Aeronautical and Space Administration (NASA) for its ongoing project on Mars exploration. Aggarwal and Vignan Patmatta, another student from Hyderabad, will spend two weeks at the Jet Propulsion Laboratory, Pasadena, California, NASA\u2019s leading centre for robotics exploration of the solar system. They would work together with NASA scientists for two weeks as part of the science operation wing between January and February, 2004.","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/remote-sensing-satellites-help-research-of-mid-atlantic-ridge\/","title":"Remote sensing satellites help research of Mid-Atlantic Ridge","date":1131062400000,"text":"The deepest, darkest, most inhospitable place on Earth is the focus of a new \u00a32 million research project funded by the Natural Environment Research Council (NERC). The ECOMAR project will explore the Mid-Atlantic Ridge located deep beneath the Atlantic Ocean. The researchers will be aided in their quest by the use of advanced technology and equipment, including remote sensing from satellites, unmanned robotic vehicles and precise acoustic techniques. The research will be mainly concentrated around the Charlie Gibbs Fracture Zone, a giant canyon hundreds of miles long and about 20 miles wide that cuts through the mountain range and connects the two halves of the ocean.\nLed by Professor Monty Priede, Director of Oceanlab at the University of Aberdeen, the consortium of researchers aims to determine the local, regional and global ecological impact of the Mid-Atlantic Ridge as a physical structure. It will provide a comprehensive overview of how all forms of life interact and function in this environment. The researchers\u2019 findings will feed into a global Census of Marine Life project.","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/multimode-satellite-signal-processor-delivers-gps-performance-in-compact-and-low-cost-solution\/","title":"Multimode satellite signal processor delivers GPS performance in compact and low cost solution","date":1171324800000,"text":"San Jose, USA, 9 February 2007 \u2013 SiRF Technology Holdings, Inc. a provider of GPS-enabled silicon and premium software location platforms, has introduced the SiRFstarIII GSD3t, a Multimode Aided-GPS Satellite Signal Processor, in an extremely small-footprint, that brings industry leading SiRFstarIII GPS performance to price-conscious and space-constrained mobile devices such as cell phones.\nAvailable in an extremely thin package, less than 10 square millimeter in size, the GSD3t combines radio frequency, as well as analog and signal processing functionalities in a single 90 nm RFCMOS die. The GSD3t takes full advantage of SiRF\u2019s Adaptive Power Management and Trickle Power technology to deliver high performance while consuming very low power.\nMinimizing the loading but taking advantage of the host platform resources, the GSD3t allows manufacturers to add GPS functionality to their products at minimal cost in terms of components and circuit board area, achieving SiRFstarIII performance in the most economical way.\nThe GSD3t combines all baseband and radio-frequency functions of the SiRFstarIII architecture on a single die to create a host coupled GPS solution with market leading price\/performance. Available in both WLCSP and FBGA packages, the GSD3t is able to deliver -160 dBm signal acquisition in AGPS mode and better than -160 dBm signal tracking capability to provide location data under the harshest outdoor conditions, and even in many indoor locations.\nThe accompanying SiRFNav software, utilizing SiRF\u2019s market tested and proprietary navigation algorithms, runs on the host processor in parallel with other host applications. The SiRFNav software supports autonomous GPS navigation, Multimode Aided-GPS navigation (both user plane and control plane) and SiRF\u2019s exclusive SiRFInstantFix, a service that minimizes the start-up wait time for GPS systems.\nThe GSD3t handles all of the time critical aspects of obtaining measurements from the GPS satellites, and passes the pre-processed non real-time critical data to the SiRFNav software running on the host for further processing.\nThe SiRFNav host software integrates easily with host processors and operating systems using minimal resource overhead. The GSD3t with SiRFNav standard software provides SiRF autonomous mode GPS navigation, and sets new performance benchmarks. Optional SiRFInstantFix technology eliminates the need to decode GPS satellite data from the satellites themselves, to deliver a faster time-to-first-fix (TTFF), even in weak signal environments.\nThe GSD3t also supports SiRF\u2019s patented SiRFLoc technology, the world\u2019s first multimode Aided-GPS solution, for powering mobile phones optimized for location-based services. SiRFLoc technology improves GPS location capability in wireless system environments by utilizing various modes of wireless infrastructure assistance to improve weak signal reception. In Aided-GPS applications, the GSD3t can achieve TTFF in about 60 seconds at -160 dBm signal levels, typical of indoor reception environments.\nThe GSD3t\u2019s GPS performance is fully compliant with the industry-standard Third Generation Partnership Project (3GPP) TS25.171 and CDMA TIA916 requirements, with extremely fast assisted fix speeds that are often significantly faster than required by the standard.\nThe SiRFstarIII architecture used in the GSD3t can track more than 20 GPS satellites, employs the equivalent of more than 200,000 correlators and sophisticated navigation algorithms and makes real-time navigation practical in almost any environment.\n\u2013 About SiRF Technology Holdings, Inc.\nSiRF Technology Holdings, Inc. develops and markets semiconductor and software products that are designed to enable location-awareness utilizing GPS and other location technologies, enhanced by wireless connectivity capabilities such as Bluetooth, for high-volume mobile consumer devices and commercial applications. Founded in 1995, SiRF is headquartered in San Jose, California, and has sales offices, design centers and research facilities around the world. The company trades on the Nasdaq Stock Exchange under the symbol SIRF. Additional information about SiRF and its location technology solutions can be found at www.sirf.com.","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/geoanalytics-awarded-sas-outstanding-government-partner-of-the-year\/","title":"GeoAnalytics awarded SAS Outstanding Government Partner of the Year","date":1178064000000,"text":"Wisconsin, USA, 30 April 2007: GeoAnalytics announced that it received the SAS Outstanding Government Partner of the Year award.\nSAS is one of the leading software companies providing the premier solutions in data warehousing, analytics, business intelligence, and predictive analytics.The award was presented at SAS Global Forum in Orlando, Florida, GeoAnalytics CEO William Holland accepted the award representing the firm\u2019s leadership in the US-based government market.\n\u201cGeoAnalytics is honored to receive this award because we were selected by the SAS marketing and business development team. That kind of peer recognition is rewarding and demonstrates our value to our clients and to SAS,\u201d said GeoAnalytics CEO William Holland.\nGeoAnalytics is an IT consulting firm that specializes in the planning, design, and implementation of enterprise information systems in support of spatially enabled business and predictive intelligence, asset and resource management, and decision-making. For more information visit www.geoanalytics.com","source":"geospatialworld.net"}
{"url":"https:\/\/geospatialworld.net\/news\/clark-labs-to-develop-system-for-analyzing-time-series-of-remotely-sensed-images-to-infer-changes-in-biodiversity-and-ecosystem-function\/","title":"Clark Labs to develop system for analyzing time series of remotely sensed images to infer changes in biodiversity and ecosystem function","date":1173312000000,"text":"7 March 2007 \u2013 Clark Labs recently received a grant from the Gordon and Betty Moore Foundation to develop a system for analyzing time series of remotely sensed images to infer changes in biodiversity and ecosystem function.\nThe result will be a protocol and software system that will permit scientists and conservationists to monitor ecosystem responses to a wide range of anthropogenic disturbances such as land cover conversion and climate change.\n\u201cThis grant makes possible not only the development of an early warning system to predict changes in biodiversity and ecosystem function, but it also serves as a means to inform a number of important policy and management questions posed by scientists and decision makers. Clark Labs is known worldwide for their experience on time series analysis of remotely sensed imagery and has an established history of research in this area using unique algorithms for data analysis,\u201d said Luis A. Sol\u00f3rzano, PhD, Senior Science Program Officer for the Gordon and Betty Moore Foundation.\nResearchers at Clark Labs are exploring a range of image archives for the system, including the NOAA-AVHRR global images of Normalized Difference Vegetation Index (NDVI) and a variety of derivative products from the MODIS instrument on the Terra and Aqua satellites in NASA\u2019s EOS mission. Similarly they are conducting experiments with a range of time series and data mining tools in the process of developing a protocol.\nA particular challenge in the development of the system is the ability to separate ecosystem signals from noise (most particularly opaque and partially transparent clouds) and to be able to separate the short- and intermediate-term climate variations from longer-term trends.\n\u201cLoss of biodiversity and associated ecosystem services is one of the most challenging problems we face,\u201d emphasized Dr. Ron Eastman, principal investigator on the grant. \u201cDeveloping the technology to monitor these changes will be an important first step in addressing the problem.\u201d\nIn the final stages of the process, Clark Labs will develop an interactive software product that will allow ecologists and conservation biologists to analyze any area of interest for trends in ecosystem response, including changes in productivity and phenology.\n\u2013 About the Gordon and Betty Moore Foundation\nThe Gordon and Betty Moore Foundation, established in 2000, seeks to advance environmental conservation and cutting-edge scientific research around the world and improve the quality of life in the San Francisco Bay Area. For more information, visit www.moore.org.\n\u2013 About Clark Labs\nClark Labs (https:\/\/www.clarklabs.org) is based within the world-renowned Graduate School of Geography at Clark University and is the developer of the IDRISI GIS and Image Processing software.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/can-digital-twins-improve-bim-workflows\/","title":"Can digital twins improve BIM workflows?","date":1605830400000,"text":"As Bentley Systems continues to bang the digital twins drum, it may be missing an opportunity to engage with the wider AEC industry and help optimise their fragmented BIM workflows, writes Greg Corke\nAs regular readers of this magazine will know, Bentley Systems has bet the farm on digital twins. And for the last \u2013 couple of years, infrastructure digital twins have been at the heart of all of its messaging.\nWith its powerful iModelHub backbone, which focuses on change, and an open strategy which acknowledges that no single vendor could ever supply everything needed for a digital twin, Bentley is certainly making all the right noises.\nHowever, while the company\u2019s digital twin messaging may resonate with forward thinking infrastructure owners and the larger AEC firms, in its quest to establish itself as a digital twin technology leader, it has perhaps forgotten the importance of using language that will resonate more with the wider AEC industry \u2013 firms that simply continue to struggle with fragmented workflows.\nThe fact is, in the early stages of development, a Bentley \u2018digital twin\u2019, which it calls a \u2018project digital twin\u2019, is essentially a hub for the BIM process. It brings together data from a variety of cross disciplinary 3D tools \u2013 architecture, structural engineering, civil engineering and more \u2013 and other sources, into a federated model that resides in the cloud.\nAs the model evolves, new data can be streamed in, and every change that was made throughout every phase of the design process is tracked, giving insight into how it came to be.\nThis is not just from Bentley products, such as MicroStation, ContextCapture OpenRoads, and OpenBuildings Designer, but third-party products as well, including Autodesk Revit and AutoCAD Civil 3D.\nThe constantly evolving model can then be used to seamlessly feed other processes such as design review, planning and construction. In the most optimised workflows, manual file-based transfer is a thing of the past.\nFrom desktop to cloud\nTo get CAD, BIM or reality models into the Project Digital Twin, data can be streamed from desktop applications to the cloud, where everything is represented by a common data schema stored inside an iTwin (iTwin.js).\nMost of Bentley\u2019s desktop modelling and simulation tools are now \u2018iTwin Enabled\u2019, so models can be automatically synced to the cloud. Any subsequent changes made can be continuously \u2018pushed\u2019 to the iTwin to keep it up to date. And each time this happens, only the deltas are sent, not the entire model.\nIt\u2019s not all one way traffic. MicroStation and Bentley\u2019s open modelling applications can also consume and reference digital twin content, just as they can consume reality meshes produced by ContextCapture.\nAt the moment, syncing is largely a manual process, and even when it is automated, the end user will still have control. To keep the data flowing, users will be able to schedule when this happens or let the system choose, by detecting changes in the BIM model.\nOnce inside the iTwin, design changes are recorded in a timeline, providing an audit trail of who changed what, when, which is unique in the industry. Team members and project stakeholders can also view, validate, and analyse the data from any location using a web browser.\nFor team collaboration, Bentley offers iTwin Design Review, a browser-based 2D\/3D design\/review environment that provides tools for comments, clash detection and version comparison, as well as workflows to help resolve issues that can be assigned to individuals. For Bentley\u2019s engineering simulation applications like Staad, there are design\/review capabilities to measure quantities, areas, volume, geometry, and cost.\nTo explain how this might work in practice Raoul Karp, Bentley Systems VP of engineering simulation, gave an example of an engineer collaborating with an architect or a geotechnical engineer around an issue. \u201cFrom their desktop product they can create a design\/review session, create an iTwin, create a cloudbased collaboration session, invite the other attendees directly through inbox email notification, and then collaborate in the cloud directly.\n\u201cAnd what\u2019s great about it, at the end of those sessions (and those sessions live in perpetuity until closed) you can actually go back into your desktop product and you can see the session issues directly in the desktop experience.\n\u201cYou can start to see if there\u2019s action items that are decided in the review session, and they can kind of pull that session back up directly and then see what actions need to be taken.\u201d\nNot all Bentley desktop products have this level of integration yet and, at the moment, the focus is very much on geometry and collaboration between disciplines. However, as capabilities build in the future, we could see iTwin Design Review extended to handle other data types, such as structural performance data, including deflections and stresses.\nOf course, Bentley takes a pragmatic approach to collaboration and acknowledges that it\u2019s not only data from Bentley tools that need to contribute to the iTwin. With the iTwin Synchronizer, models can also be fed in from third party applications, including Autodesk Revit and AutoCAD Civil 3D.\nData propagation\nThe beauty of an iTwin, is once the data is in the cloud, it can be used to feed other workflows. A good example here is campus and city planning, enabled through Bentley OpenCities Planner, a web-based tool that combines GIS, reality, and BIM data to create 3D city-scale digital twins for collaboration and public consultation.\nTo bring in 3D CAD models, OpenCities Planner used to rely solely on file-based workflows. This was a manual process and did not include metadata. If metadata was then added in OpenCities Planner, but the design subsequently evolved and an updated 3D CAD model was reimported, then everything had to be redone. Now that the software is \u2018iTwin Enabled\u2019 data can stream in direct from an iTwin, enabling architects and engineers to seamlessly bring in their BIM models to give them a city context.\nA great example of this technology in use is the City of Dublin, which adopted the web-based tool to carry out public consultation on a project in the Docklands area. Due to Covid-19, the traditional Town Hall engagement model of gathering people together to view development plans was not possible, so Dublin went digital. Not only did it solve the problem, but it managed to get an incredible 160,000 views of the project, with feedback gathered digitally via polls and comments.\nFrom design to construction\niTwins have also enabled Bentley to dramatically improve the flow of data from design to construction. Synchro, Bentley\u2019s construction management software, was recently \u2018iTwin enabled\u2019 to not only allow \u2018as-designed\u2019 BIM models to be brought in more easily, but for 4D models and fully detailed schedule simulations with animation and sequencing to be viewed entirely in a web browser.\nLike OpenCities Planner, Synchro previously had to rely solely on file-based import, as Mark Hattersley, Bentley Systems senior director, construction operations, explains. \u201cWithin the Synchro Pro session, rather than saying, \u2018I\u2019m going to open from my [desktop Synchro] .SP file\u2019 I simply connect to my Synchro project, which is managed in the cloud, and I stream that BIM data in.\n\u201cOne of the great benefits here is, because it\u2019s in the cloud, we can now take it onto our Synchro Control Web app, so we have a web view of the of the 4D model, and we also can take it to our Synchro Field mobile app.\u201d\nIf issues are identified on site, these can be logged in the Synchro Field mobile app. Then, if the project team decides it needs to go back to design, any changes made inside the 3D design tool can be automatically fed back through into Synchro via the iTwin.\nThe beauty of this workflow is the software can give the project manager insight about what has changed, as Hattersley explains. \u201cThat\u2019s the magic of the iTwins database \u2013 [it] tracks every change. It\u2019s what they call a change set, so it\u2019s basically every difference that has been published, is recorded and so what we\u2019re actually seeing is a collection of differences. And that allows you to roll back and roll forward and see [and compare] what it used to be, to what it is.\u201d\n\u201cWe actually show a report that lists those changes, but also visually shows the changes,\u201d adds Rich Humphrey, Bentley Systems vice president construction. \u201cSo, you\u2019ll see the model and it\u2019ll highlight like a clash detection, that tells you \u2018hey the geometry changed\u2019. And that either impacts or doesn\u2019t impact what you\u2019re doing with the construction model,\u201d adding that with this information the project manager can then decide the most appropriate action.\n\u201cYou\u2019re no longer really importing and exporting anymore, you\u2019re just live streaming and live connected to the cloud. That\u2019s a default for us,\u201d concludes Humphrey.\nConclusion\nIt\u2019s hard to fault Bentley for going hard after digital twins. It wants to be to infrastructure digital twins what Autodesk is to BIM, and with its technology and open strategy it certainly has a good chance of achieving this. But one can\u2019t help but wonder if it\u2019s missing a trick by not hammering home the message that it can simply help AEC firms that are still struggling with fragmented workflows.\nIn many ways, what Bentley is offering AEC firms is an optimised BIM workflow, built around a sophisticated Common Data Environment (CDE) that looks to eliminate traditional file-based workflows, with live streaming of data.\nWhile Bentley customers might get it, there\u2019s a huge number of AEC firms that could potentially benefit from iTwins, including those looking to better connect design with construction.\nWhat\u2019s more, by solving current challenges around data interoperability and data flow, AEC firms could be creating a digital twin without even knowing it. So, in the future, if the decision was made to add more value to an asset once in operation, by connecting real-time IoT data or using AI-based analytics, then the foundations have already been laid. And with seamless integration with Microsoft Azure Digital Twins, Azure IoT Hub and others, this doesn\u2019t have to exist solely in the world of Bentley Systems.\nHowever, while openness offers countless possibilities for customisation, it also brings potential complexity and a need for consultancy. Autodesk Tandem is the complete opposite; it\u2019s a much simpler concept for architecture and construction firms, with the onus on them to seek out new business opportunities by repurposing their Revit BIM data to clients as a digital twin. The same can be said for TwinView.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/opinion\/video-nxt-bld-2019-federico-rossi-darlab-digital-architecture-robotic-lab\/","title":"Video: NXT BLD 2019 \u2013 Federico Rossi, DARLAB (Digital Architecture & Robotic Lab)","date":1565049600000,"text":"Advanced Robots for Advanced Architecture \u2013 NXT BLD London, June 2019\nThe newly renovated Digital Architecture Robotics Lab (DARLAB) at London South Bank University leverages state-of-the-art industrial technology to perform architectural fabrication research. It is one of few selected academic institutions around the world utilising robotic automation to perform both subtractive and additive manufacturing processes. Frederico Rossi will take us through the lab\u2019s capabilities and recent project work.\nView the other NXT BLD 2019 presentations\nNassim Saoud, Trimble Consulting\nApplications of Mixed Reality in design and construction\nMoritz Luck, Enscape\nFrom real-time to realism.\nSandeep Gupte, NVIDIA\nRe-imagine cities of the future with next gen visualisation.\nFlorian Frank, Herzog & De Meuron\nUser Defined Software.\nRichard Harpham, Katerra\nSilicon and Sawdust \u2013 Deconstructing Construction.\nTal Friedman, Foldstruct\nBetween the folds \u2013 Towards a material revolution.\nMelike Alt\u0131n\u0131\u015f\u0131k, Melike Alt\u0131n\u0131\u015f\u0131k Architects\nDialogue between architecture and robotic construction.\nAlexander Le Bell, Tridify\nThe impact of automated web VR workflows and streamlined collaboration.\nMarc Fornes, THEVERYMANY\nExploring forms through Computational Design to Digital Fabrication.\nSimeon Balabanov, Chaos Group\nGetting it real: AEC workflows real-time, real fast and ray traced.\nMichael Perry, Boston Dynamics\nWhat if human-like mobility could be added to automation on construction sites?\nMariana Popescu, Block Research Group\nBringing together advances in digital fabrication, computation, and structural design.\nMartyn Day, AEC Magazine & NXT BLD\nIntroducing NXT BLD and AEC Magazine.\nXavier De Kestelier, HASSELL\nExtra-Terrestrial Architecture.\nCobus Bothma, Kohn Pedersen Fox (KPF)\nAccelerating design decisions with rapid visualisation.\nHilmar Gunnarsson & Johan Hanegraaf, Arkio\nBringing architectural design into VR.\nKen Pimentel , Epic Games\nHow Fortnite is changing AEC.\nCarlos Cristerna , Neoscape\nHarnessing the power of real-time ray tracing.\nMike Leach , Lenovo\nNavigating challenges surrounding AR and VR hardware.\nMikolaj Bazaczek , VR+ARCH: workflows in past, present and future\nVR+ARCH: workflows in past, present and future.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD 2020 will take place at the Queen Elizabeth II Centre, London on 9 June, in association with Lenovo.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/new-research-into-future-automation-of-building-code-checks\/","title":"New research into future automation of building code checks","date":1537747200000,"text":"The Digital Compliance Network (D-COM), funded by the Centre for Digital Built Britain, is seeking participants for a study that explores how automation could assist in the auditing, checking and compliance of regulations, standards and requirements that govern the entire lifecycle of the built environment.\nThe checking of compliance against these requirements is a complex task, that is currently performed on a manual basis, thus is highly resource intensive. Thus, the digitalisation of requirements\/regulations and the development of (semi) automated compliance checking systems is seen as an important area of study.\nBy gathering industry views and requirements through a short online survey, researchers at D-COM hope to understand the present capabilities, short falls and efficacy of compliance systems along with the current attitudes and industry readiness for the digitisation of regulations and compliance checking. Additionally, the research will attempt to elicit the opportunities and blockers in the development of these technologies.\nThe findings of the study will be published in a report and there will be opportunity for those who contributed to the study to participate in the D-COM consultation event later in the year.\nThe Centre for Digital Built Britain is a partnership with the Department of Business, Energy & Industrial Strategy and the University of Cambridge that aims to deliver a smart digital economy for infrastructure and construction for the future and transform the UK construction industry\u2019s approach to the way we plan, build, maintain and use our social and economic infrastructure.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/clear-vision\/","title":"Clear vision","date":1181174400000,"text":"Fluid Engineering Limited produces highly developed structural designs by applying advanced 3D analysis software and a great deal of experience. The way software is integrated into the practice is key to the company\u00dds award winning performance. By Nick Lerner\nFluid Structures, which was granted an Institute of Structural Engineers Award for the Glass Pavilion, London, has been commissioned to produce designs across a range of large and small projects for domestic, corporate, academic and institutional clients. Of Fluid\u00dds Glass Pavilion the judges commented \u00fd exceptional engineering is immune to scale\u00af and a current project is proving an example that nicely illustrates the point.\nHaving cut his structural engineers\u00dd teeth on radical new towers in Dubai, Manhal Ibrahim is among 20 Structural Engineers at Fluid. Several of the team are working on the small but fiendish problem of how to preserve the top two floors of a historic St. Johns Wood Villa, install a giant basement and underground swimming pool while removing most of the ground floor internal walls.\nManhal takes up the story, \u00fdThere are many elements to this project which are complex because of their interdependency. For example a 300 sq. m concrete floor pad, to be supported by contiguous 4m deep concrete supports, when fully installed, must have temporary support, while the basement is dug out, to retain structural integrity.\u00af He cites another example. \u00fdSteelwork flexion must be very tightly controlled to preserve the substantial brick structure above, meanwhile, budgetary and aesthetic constraints require minimising the steelwork.\u00af\nRemoving the temporary pad supports presented more complication while the discovery of one of North London\u00dds lost rivers just beyond the swimming had also to be factored in. The enviably big swimming pool, at 4m deep presents its own problems of wall thickness and possibilities of heave come into play when it is empty.\nThe Finite Element analysis for the work on this house is handled in-house. The company uses Robot Millennium software, which the practice has been using since before the Glass Pavilion, London project.\nSee through solutions\nFluid Structure\u00dds Founding partner, David Crookes, explains how the software fits with the firm\u00dds work. \u00fdThe structures that we devise are concurrently designed and analysed using Robot. The software allows FE (Finite Element analysis) results to be factored into designs as soon they are solved. That means we are able to consider several alternative, but always structurally compliant, design options.\u00af \u00fdThe St. Johns Wood Villa, presents in a microcosm, many of the challenges that we encounter on other projects,\u00af continues Crookes. \u00fdOur ability to solve the structural design needs for one job lead us to better solutions on others. Robot is used as a repository for much of this work and from our databases we can reference a growing library of project solutions and outcomes.\n{mospagebreak}\n\u00fdUsing Robot Millennium as a design and analysis tool helps not only with the actual design, but also with communicating it to clients, contractors, planners and other stake holders. They can see the design in 3D and assess it from their own perspective and needs. This helps to reduce everybody\u00dds risk and means that non-technical people can get involved too.\u00af\nUplifting light\nDesigning in glass has become a speciality at Fluid. An ability to design and analyse, and thereby to optimise, glass designs, has become a matter of course at the practice. Crookes explains. \u00fdGlass is a fantastic material to work with. Large sheets up to 6 x 3 sq. m are readily available and becoming more common in domestic refurbishments as people see the possibilities that are on offer. Glass, when used in sympathetic combination with massing creates uplifting environments that people enjoy. Glass is developing from a secondary building material into a primary one.\n\u00fdAdding large sheets of glass to the construction palette produces a range of new challenges. Using Robot\u00dds software tools we can use thinner glass over bigger spaces and compare the performance of several sheet types. We can also use glass structurally and analyse other novel designs with great accuracy.\u00af One significant example of the use of software to solve problems of this type is found in Fluid\u00dds iconic, post-tensioned glass staircase. In this piece the glass column and treads were subject to FE analysis leading to several design iterations before the optimum balance between aesthetics and functionality was discovered.\nCautious approach\nManhal stresses that you need an engineer\u00dds mind for this work because, advanced as the software is, the rule of GIGO (garbage in garbage out) does apply. \u00fdThe software presents solutions that lead to thinner material, lower mass, smaller fixings and very often the need for less structural support. These results all need to be checked, and checked again,\u00af he advises. \u00fdBecause, if the numbers that are entered into the equation have been erroneously produced the software will solve them anyway. If the results look too \u00d9optimistic\u00dd you may have to consider that the survey was flawed and go back to check where the error came from.\u00af\nWind load, thermal impact and natural frequency measurements along with soil structure interaction data are calculated by Robot. These results are made available to others within and beyond the design team. With this powerful resource to hand, Fluid is often called on to provide the benefits of its work to fabricators and others in the construction supply chain.\nProblem solved\nDavid Crookes concludes. \u00fdAmassing knowledge of materials and their properties has always been an aim for the building industry. The technology now exits to build with lower material mass in more attractive and appealing ways. Clients want this, and architects have been pursuing for it for years.\nThe availability of new materials, such as plastics, and new ways to use existing materials, like glass joists, produce desirable buildings. These developments bring new challenges too but in calculating the limits of physical integrity, Fluid Structures is confident that it can produce award-winning designs based on a combination of Robot Millennium software, many years\u00dd experience as structural design leaders, and a good eye for a winning design.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/bentley-systems-embraces-the-cloud\/","title":"Bentley Systems embraces the cloud","date":1385856000000,"text":"Bentley is taking a measured approach to cloud services, allowing its users to set their own pace as they embrace data mobility, simulation and flexible licensing\nAs a consequence of an industry-wide move to Building Information Modelling (BIM), it is without doubt that collaboration has become harder. File formats have given way to relational spatial databases; lines, circles and arcs have become parametric models of real-word components; drawings are increasingly seen as a byproduct, or legal requirement, not an end goal.\nTo this backdrop, one of Bentley CEO, Greg Bentley\u2019s favourite topics is that of collaboration, what he terms \u2018Information Mobility\u2019. At Bentley\u2019s recent Year in Infrastructure event in London Mr Bentley reinforced the problems faced by the federated infrastructure industry. Project information must not be locked inside corporate databases, accessible to only the privileged few, but made available in a controlled fashion to all project participants.\nBIM presents new challenges as well as new opportunities to revisit the whole \u2018connectedness\u2019 of project information and its flow. Bentley\u2019s ecosystem of tools, servers, mobile applications and i-models (containers for the open exchange of infrastructure information), is aiming to ease this flow of data between industry standard applications.\nBentley recently sponsored a SmartMarket report on Information Mobility, along with Bluebeam, BIMForum and the Building Smart Alliance. The free 48-page document makes for interesting reading, highlighting that only 20% of companies track project information to or from other firms and 41% track internal flows (tinyurl.com\/IM-smart-report).\nImproved collaboration helps every aspect of the design and build process, from scheduling to versioning, with new mobile devices and applications enabling onsite information to be fed back into the system. Everyone knows that reducing errors, decreases costs and providing access to accurate information can be achieved just by using existing tools.\nWhen asked what formats firms use to exchange project information, as probably expected, PDF and CAD files, paper drawings and handwritten notes took the lion\u2019s share of responses (72-92% for each). However in two years\u2019 time, the majority of firms predict that \u2018central\u2019 cloud storage will host their data, with the majority (89%) expecting a major reduction in the use of paper and handwritten notes.\nThe combined benefits of \u2018digitalisation\u2019 of project information, the centralisation of management and the expanded reach of mobile devices such as smart phones and tablets, means that projects can be better connected end-to-end.\nMr Bentley highlighted how increasing mobile bandwidth such as 4G (LTE) certainly helps in cities but other technologies such as satellite data is also being employed to enable WiFi onsite. By utilising these cloud-enabling services, design and construction teams can be connected, getting the latest design information to those in the field and the latest reality on site back to base. Mobile apps like Bentley\u2019s Field Supervisor for the iPad and iPhone are helping shape this vision.\nTo highlight the importance of collaboration to the entire BIM world, Mr Bentley has taken to calling BIM, B\/IM, where IM stands both for Information Modelling and Information Mobility. We are unsure how many other CAD vendors will take up the convention but can see how Bentley\u2019s strength in management and distribution technology would make it want to give it equal billing in the move to the industry adopting new complex processes.\nBentley Systems today\nBentley Systems has come a long way from being known as the developer of MicroStation 2D drafting software. It now has annual revenues of over $500 million and remains a privately held firm, with over 3,000 employees in 45 countries and is a major player in all the AEC, plant and infrastructure markets. With MicroStation still the founding platform technology, the company has developed a plethora of industry-specific solutions and applications to create, manage, and distribute complex design data.\nThe company specialises in supporting major projects through its ecosystem of desktop, mobile and server-based design and analysis tools together with its management \/ collaboration platform, ProjectWise, which lies at the heart of many major multi-billion pound projects, such as the UK\u2019s Crossrail. Every year it compiles a list of the top 500 Infrastructure owners, in terms of billions and trillions worth of assets owned. The kinds of firms and government bodies listed are an indication of the typical Bentley customer.\nAs the company\u2019s products have developed it\u2019s clear to see that Bentley is not just engaged in adding \u2018features\u2019 to products but is just as focused on attempting to solve the many process issues that face us in creating complex models and sharing this rich 3D design information from client to construction worker, from concept through lifecycle to demolition.\nBentley leverages the cloud\nWith the focus on connectivity and the benefit of cloud within the design ecosystem, last year Bentley announced \u2018Bentley Connect\u2019, a cloud-based storage and sharing service, which provided each user with full audit trails, versioning and data file transfer.\nNow Bentley has fleshed out some soon to be available cloud-based services, some of which could radically impact the way core applications are located and accessed.\nBentley has been experimenting with Microsoft\u2019s Azure cloud service for a number of years now and it seems that next year the company will have enough confidence in the technology to provide rental of portfolio licenses as well as hosted ProjectWise services.\nSelect Open Access is a quarterly-based subscription based service that provides access to any Bentley information modelling application at any time, by any user. These subscriptions come with the Bentley LEARNservices training to speed up proficiency.\nNo pricing was available at the event but it is expected to be \u2018competitive\u2019 to other rental products from companies such as Autodesk. This is good news for customers that have perpetual licenses and need \u2018overflow\u2019 access when times get busy. It also opens up access to Bentley products for those that don\u2019t want the expenditure of purchasing perpetual licenses and paying an annual Bentley Select fee.\nBentley has an established Enterprise License Subscription (ELS), which is an all \u2018you can eat\u2019 proposition for large firms. With the new Select Open Access, individuals and smaller firms can also gain access to the whole breadth of Bentley tools at the right time for an incremental time-licensed price.\nBentley MANAGEservices provides cloud deployable access to ProjectWise and AssetWise. This enables rapid deployment, and operation of Bentley\u2019s collaboration and asset tracking platforms. The service is not only available as a pure cloud deliverable but provides hybrid\/ cloud and desktop usage for those that prefer to operate an internal private solution. Through Bentley\u2019s MANAGEservices, firms can lower their internal IT infrastructure spend by opting for a SaaS (Software as a Service) model or mid-size firms can now deploy ProjectWise or AssetWise without some of the investment overhead. The company recently released templates for projects of up to 25 users and for design collaboration and work-share for up to 1,000 users.\nThe technology behind all these capabilities is particularly interesting, with Bentley being very aware that it has many customers that don\u2019t want their data transacting to a third-party server hosted remotely. Bentley is adopting a hybrid approach where services can be deployed on internal corporate clouds, as well as on public networks.\nProducts such as AECOsim will be available for quarterly rental and delivered across the cloud to customers\u2019 desktops using Citrix technology to deliver the screen pixels. Products such as ProjectWise may be delivered with technologies from Numescent, which use some very clever \u2018cloudpaging\u2019 technology to stream the actual application code to desktops on-demand, without having to install the entire application.\nThis move by Bentley is very much in keeping with the way that the industry is moving and follows news that Autodesk will offer its applications via rental, as well as through \u2018the browser\u2019. Bentley, however, has respected that many of its larger customers prefer to manage data on their side of the firewall. By opening the access to its tools without the \u2018joining fee\u2019 of buying a perpetual license, Bentley could finally be able to address the needs of SMEs (small to medium enterprises) and compete against the likes of Revit, ArchiCAD and Vectorworks for small team BIM collaboration in the volume market.\ni-model management\nOne of the cornerstones to Greg Bentley\u2019s vision for \u2018Information Mobility\u2019 are Bentley i-models. These act as a container for exchanging all types of infrastructure information and can be used in a variety of applications, desktop and mobile.\nThis year Bentley introduced the i-model composition server, which it describes as the publishing hub for Information Mobility. The technology can be used to combine a variety of types of information, including DGN, DWG, raster, PDF, Microsoft Office, and IPS files into intelligent i-models, PDFs, and raster files.\nThe i-model composition server is all about automation. Files can be generated according to rules or project standards \u2014 published to a schedule or by triggers, such as when a model is approved, edited, or viewed. The technology can use ProjectWise or AssetWise as the information source and, because it\u2019s server-based, is scalable across an enterprise to reach all parts of a company.\nWith Bentley\u2019s commitment to pushing data out to mobile devices this looks to be an exceedingly important technology. The challenge of course is ensuring users have the latest revision and this is where the new i-model validation services come into play.\nWhen in a managed environment like ProjectWise, users know the content is up to date because it\u2019s a live system. However, when interacting with content out in the field, or that\u2019s been checked out or emailed, it\u2019s hard to tell.\ni-model validation services allows users to check that the information they have is up to date. The service isn\u2019t restricted to i-models per se; it can also be used to check printed drawings. Simply scan the QR code using any smart phone QR reader and this links to a website that states if the drawing is the latest revision. If not it can also redirect the user to the most current version. This looks to be a great tool, not least because of its simplicity.\nAnalysis and \u2018optioneering\u2019\nAt the event we had time to catch up with Santanu Das, senior vice president, design & simulation to hear how Bentley plans to use the cloud to not only cut analysis times, but encourage the use of simulation to get early stage feedback on different design options.\nAfter years of aggressive acquisition, Bentley owns many of the key structural analysis tools including the popular STAAD and RAM product suites. The company has now rewritten all of its analysis engines so they can be used in the cloud, making use of clusters to crunch big datasets in double-quick time. This is enabled through Bentley Simulation Services.\nBut cloud-based simulation isn\u2019t just about cutting solve times; it\u2019s about using scalable resources to test out multiple design variants at the same time, then choosing the best one.\nBentley refers to this as optioneering, enabled through the new Bentley Connect Scenario Services. The technology isn\u2019t just restricted to structural analysis or use by specialists though \u2014 far from it.\nBentley plans to boil down the capabilities of the various engines it owns in civils, structural, and energy simulation and make them available to designers so they can get early feedback on the performance of their designs. Third party software developers can also plug-in their engines so there\u2019s a huge scope for this technology.\nDesigners will be able to get baseline feedback on different design options taking into account anything from heat gain and occupancy to right to light and structural systems. Constraints can be set for cut and fill, material availability in the supply chain, even the cost of lettable floor space.\nThis may sound like an absolute nightmare to manage, not least because of all the different studies that need to be set up, but Mr Das says Bentley is committed to keeping things simple. The technology can use Generative Components to automatically come up with a number of different options.\nOnce all the different engines have done their analysis everything is aggregated and users can compare designs against each other. Results are presented in pareto charts and users can give weight to different criteria in terms of their importance to the project.\nBentley Connect Scenario Services sounds like a very exciting technology, which should steer projects to more optimal outcomes by enabling informed decisions early on. It\u2019s not an entirely new concept \u2013 Autodesk talked about using the cloud for optioneering as long as four years ago \u2014 but Bentley\u2019s novel take is that Scenario Services can be deployed across multiple disciplines. Importantly, the service will also be able to be used from within non-Bentley applications including SketchUp, Rhino and Revit. Indeed, with Bentley\u2019s close ties to Trimble, access to the service will be available in the shipping version of SketchUp without the need for a plug in. Pricing hasn\u2019t been set yet but is likely to be very affordable, either on a pay per use basis of just a few dollars per run or a set fee per month.\nAECOsim Building Designer\nWith the growing importance of \u2018BIM-ready\u2019 manufacturer content AECOsim Building Designer\u2019s new ability to interpret Revit\u2019s RFA format is big news.\nRFA content is becoming widely available from building equipment manufacturers so this gives Bentley customers access to a whole new world of parametric components.\nBentley software has long been able to bring in Revit models as geometry with information but the parametrics were less predictable. Now RFA content can be brought into AECOsim Building Designer and interpreted as intelligent parametric objects, retaining rules, constraints, connection points, and other data. Users can change materials, configurations and use these objects intelligently.\nAECOsim also has \u2018dramatically enhanced\u2019 capabilities in terms of how it supports IFC and COBie data through the use of Bentley\u2019s iModel technology. \u201cFundamentally there is nothing that supports either of these formats better than AECOsim Building Designer does,\u201d said Huw Roberts, Bentley\u2019s VP, core marketing.\nBentley also introduced new rendering capabilities for its MicroStation-based products. A new Effects Manager enables users to get real-time visual feedback on lighting and other rendering adjustments before committing to a time consuming production render. The technology uses sliders, and a preview pane to allow users to change the settings dynamically and see the impact in real time. Images can be adjusted for brightness or users can swap out new materials and it only renders the new material without having to re-calculate the whole scene.\nThe future: Augmented reality\nBentley rarely shies away from showing future technology and research director, Stephane C\u00f4t\u00e9 gave a glimpse of what Bentley is up to in the field of augmented reality \u2013 putting data in the context of the physical world.\nThere are countless applications. For maintenance \u2013 seeing through buildings or under roads, to query physical assets such as structure, services or underground pipes.\nFor construction using virtual sticky notes to flag up differences between a 3D model and the on-site reality. Builders could also display drawings in context, superimpose animations to help understand how components should be assembled on site.\nAccording to C\u00f4t\u00e9, the big challenge is accuracy, tracking the exact position and orientation of your mobile device as you move on site and making sure it matches the 3D CAD model. GPS and in-built motion sensors can only do so much, but Bentley has found a workaround in the form of panoramic images.\nRather than taking a live feed from the backfacing camera on a tablet, C\u00f4t\u00e9 is exploring the use of geo-referenced panoramic images that match the virtual CAD model exactly. It\u2019s not augmented reality in the traditional sense, but the results look impressive.\nConstruction\nLast year Bentley bought SpecWave, a developer of software for the creation of structured text content, including engineering specifications, codes and standards. Now the technology has been integrated into Bentley\u2019s software and connected with ProjectWise.\nSpecWave Composer allows users to author, compose and consume specifications as structured documents instead of dumb text. So, instead of having a Word doc or reams of printed output, each spec can be managed as an individual object.\nThe big benefit is that if the design changes, the spec can be updated automatically reducing the risk of using wrong or outdated information. Specifications can also be filtered. e.g. by discipline, phase of work, or work package.\nSpecWave Composer works with another new Bentley technology \u2013 ProjectWise Construction Work Packaging Server, which is due for release next year. The software is designed to manage the lifecycle of construction work packages, improving the flow of information between engineering, procurement, construction, commissioning, and handover.\nA key part of its capabilities is providing clear visibility on progress. For this it features a dashboard where users can see, for example, which packages have been issued, how the different contractors are performing, what\u2019s completed and what\u2019s on schedule. It also links to ConstructSim, which is enabled by Bentley Navigator, for dynamic project review and analysis. Here, thematic representation helps users visualise all manner of status information such as what\u2019s on site, what\u2019s ready to install, what\u2019s delayed, who\u2019s carrying out which tasks, and which supplier is delivering to site.\nStructural\nBentley has a new release of Structural Synchronizer v8i (SELECTseries 5), which creates a shared repository of common structural model data so it can be synchronised between a range of applications, including STAAD, RAM, SACS, AECOsim Building Designer, Revit, Tekla Structures and others.\nThe big news for the new release is that it now fully integrates IFC, as well as allowing users to manage sub\u2013structures, so models can be broken down for different teams to work on.\nStructural Synchronizer was originally developed to help bring together the smorgasbord of structural applications Bentley has acquired and developed over the years. And while it\u2019s well regarded in industry, Raoul Karp, VP structural and BrIM, Bentley Systems, acknowledges that there are still times where direct, closer integration is wanted between Bentley\u2019s products\nMr Karp shared details of a forthcoming standalone structural modelling technology currently in development called \u2018shared modelling components\u2019.\nHe explained how the technology will find its way into Bentley\u2019s vertical products that require structural modelling \u2013 products like OpenPlant and Pro\/Steel.\nWith a set of shared component set of modelling tools a structure created in OpenPlant will immediately be able to be opened in Pro\/Steel without transformation, he said. We should hear more about this interesting technology next year.\nIn other structural news the SELECTseries 6 releases of ProConcrete and ProSteel V8i will now run standalone on the Power Platform and do not need AutoCAD or MicroStation.\nProConcrete has a new capability called Template driven Solids, where users can create 3D linear objects from 2D templates. According to Mr Roberts, this can give a more accurate quantity take off for concrete as the volume excludes rebar.\nThere are also new parametric modelling tools for foundations and spiral columns and B-spline structural shapes \u2014 complex concrete structural shapes to match the geometries of today\u2019s building, says Mr Roberts.\nProSteel also has some new intelligent parametric modelling tools for stairs, handrails and anchors. According to Mr Roberts detail goes right down to plate connections, stiffners and the threading of holes, so the model can go straight into fabrication.\nCivils\nThere was a big focus on rail and transit at the event, a key takeaway being that there were huge benefits from combining point-cloud and CAD data. Also, by using mobile applications to access ProjectWise, companies could save a large amount of money in reduced site visits.\nBentley\u2019s acquisition of UK point cloud software developer Pointools was an investment in the core MicroStation technology, which the company sees as being a key future format in infrastructure projects.\nWith HS2 being mentioned throughout the event, an excellent rail example was shown, which combined a CAD model of an animated train travelling down a section of rail. This was combined with a point cloud captured model of the trees, bridges and environment. As the train sped along the track the system clash detected the CAD geometry against the point cloud foliage and bridge to check for problem areas.\nConclusion\nBentley Systems is a very technically proficient company that doesn\u2019t rush into the \u2018new\u2019 thing until its portfolio of products can benefit. Its commitment to using MicroStation as a platform and building extensive vertical capabilities upon it provides a great sense of commonality between its solutions. However, with the software development market moving to the cloud as the platform of choice, this could have meant a huge amount of re-architecting of all its solutions, which many of its competitors have had to do.\nBentley is a devoted single platform Microsoft-based company so it\u2019s perhaps no surprise that it has opted to use the Microsoft Azure cloud platform. With this addition to its desktop-based Windows applications, the new capabilities of license rental and cloud delivery will mean Bentley solutions will finally be available on the Mac, or any other operating system.\nIf Bentley gets the rental price right and continues to grow the number of its applications that can be accessed in this way, especially with ProjectWise, there is a potential for the company to finally appeal to those outside of its \u2018corporate\u2019, \u2018big project\u2019 traditional client-base. Small architectural practices could also bid and take part in DGN-based workflows without the overhead of buying perpetual licenses with SELECT.\nWithin the next year the CAD market will have radically changed the options open to customers to pay for and access design tools. The old model of highly priced perpetual licenses is set to fade, with individual term-rental or subscription pricing becoming the preferred payment options by vendors.\nWith more capability being available immediately on-line the market is certainly turning on its head. While this could be seen as users being even more in the vendors\u2019 pockets, the advantage of being able to drop seats or quickly swap design technology providers should keep the software developers as keen as ever to provide value for money.","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/dublin-stadium-raises-the-design-game\/","title":"Dublin stadium raises the design game","date":1279584000000,"text":"Parametric modelling facilitates form-first collaboration to construct curvilinear design.\nThe stadium with the world\u2019s oldest rugby pitch, Lansdowne Road Stadium in Dublin, Ireland, is being retired and replaced by the \u20ac400 million Aviva Stadium scheduled for completion in 2010. The new stadium will be home to both the Irish rugby and football international teams and will play host to the Europa League final in 2011. While this will be the first time a Europa League final has ever been held in Ireland, the old stadium hosted many European Championship (UEFA) and World Cup (FIFA) matches over the last 25 years.\nDesigned by Populous, a leading sports architecture firm formerly known as HOK Sport Venue Event, together with local architects Scott Tallon Walker, Aviva Stadium has 50,000 seats in four tiers, all with excellent views. The seating is sheltered by a sinuous form that combines roof and fa\u00e7ade in one shimmering, organic shell.\n\u201cOur main challenge was to ensure that the unified form of the building\u2019s concept was maintained from the design development through to construction phases of the project,\u201d said Populous senior architect David Hines.\nWith such emphasis placed on maintaining the purity of the original concept, functional considerations were made to serve the building\u2019s form. This meant that the skin\u2019s basic geometry had to be coded into design processes from the beginning of the project.\nStructural engineering was provided by Buro Happold, which collaborated on critically important geometric issues of the structure and architecture whose solutions were part of the fundamental design goal. GenerativeComponents emerged as an important facilitating tool.\n\u201cCertain variables and base principles were established within the GenerativeComponents model, allowing Populous to maintain control over the final form of the model,\u201d Mr Hines said. \u201cThis allowed the model to be parametric \u2014 having internally defined variables \u2014 and also constrained the geometry to certain grids and base extremities. For Populous, this was the most critical single aspect of the parametric design, as the finished construction derived directly from the shape of the parametric skin of the building.\u201d\nAs the stadium\u2019s structural underpinnings and functional details were finalised, the mesh underlying the roof and fa\u00e7ade could be reorganised as needed, without changing the shape that the mesh assumed. Eventually, information from this model was exported to multiple Excel spreadsheets that listed the size, shape, structural member index, and placement details of each translucent panel used in the exterior cladding. These panels could then be manufactured off site and shipped to contractors as needed.\n\u201cThere is no doubt that the whole process could have been done in a non parametric model and developed through extensive redrawing and remodelling of forms and geometry,\u201d Mr Hines said. \u201cBut given the complexity of the design in hand and the numerous elements that were co-ordinated across different offices, following a non-parametric approach to this design would have allowed room for numeric errors, which are dealt with through computing, and also taken a humanly unfeasible length of time.\u2019\u2019 Mr Hines admitted that developing the initial parametric model takes quite a long time compared to traditional methods but says the effort pays off.\n\u201cUsing parametric design allows designers to quickly assess schemes and layouts over complex forms in shorter amounts of time in order to get to the desired solution faster while eliminating basic human error,\u201d he said. \u201cThe model can compute numerous calculations over a vast number of elements.\u201d\nIn addition to serving as spec sheets for panel manufacturers and others, Excel spreadsheets based on GenerativeComponents proved to be an excellent way to communicate with Buro Happold on structural issues. Teams from both firms worked together to establish the principles governing the relationship between the skin and structural roof members.\n\u201cWe needed to develop a framework by which the information between both forms could be translated, with Populous driving the form and cladding of the building and Buro Happold driving the sizing and positioning of the structural members,\u201d said Mr Hines. The Populous team created a simplified GenerativeComponents script that would produce the stadium skin based on setout figures from an Excel spreadsheet.\n\u201cThat way, Populous and Buro Happold could work simultaneously on the model at different offices, with Buro Happold further developing the structural members on top of the same file, and Populous further developing the cladding layout on top of the original GenerativeComponents file,\u201d said Mr Hines. With the underlying frames to both firms\u2019 models being dependent on the base figures within the Excel document, ultimately Populous could amend and refine the skin\u2019s forms by altering the established variable figures within Excel. Co-ordination between the two firms relied on the transfer of a single Excel file.\nAviva Stadium is owned by the Irish Rugby Football Union and the Football Association of Ireland, and both organisations are trying to be good neighbours on Lansdowne Road. At the north end of the stadium, for example, there is just one tier of seats, not four, in order to lessen the impact on nearby homes. Close attention was paid to acoustic properties, so noise from events will be largely confined to stadium grounds.\nSimilarly, local roadways and the Lansdowne Road train station were renovated to smoothly accommodate traffic without unnecessary congestion. Several visualisation programs were used to render the MicroStation model into 3D displays, which helped the development team and community organisers communicate their concerns about accessibility.\nAviva Stadium also features several green initiatives. The remarkable roof and fa\u00e7ade shell minimise environmental impact by collecting rainwater, which is stored and used later for watering the pitch. The new stadium is on the site of the previous stadium, which keeps residential communities intact, and encourages the continued use of mass transit facilities \u2014 reducing fuel use, noise, and pollution. Other green design elements include low environmental impact concrete, intelligent controls for escalators, and low-energy lighting.\nThe bar has been raised for large-scale sport venues. Communities now demand attractive, high-performance buildings that have minimal impact on the environment. The Aviva Stadium design by Populous delivers an innovative architectural form that is in harmony with the surrounding cityscape. The union of form and function demonstrates that GenerativeComponents can be used for rapid prototyping of sophisticated designs to deliver modern, sustainable structures.\n\u201cThe Aviva Stadium project would not have been possible without GenerativeComponents,\u201d Mr Hines concluded. \u201cThe program has given the architects and engineers the ability to deliver the complexity and beauty of the stadium within the given time frame and budget.\u201d","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/video-nxt-bld-2018-london-conference-dipa-joshi-director-of-assael-architecture\/","title":"Video: NXT BLD 2018 London conference - Dipa Joshi, Director of Assael Architecture","date":1530144000000,"text":"Smart cities & emerging technologies: Cutting through the noise \u2013 NXT BLD London, June 2018\nDipa Joshi talked about how city spaces and their uses were changing with working practices and generational changes and how architects needed to address this in design thinking. It isn\u2019t just the building design but also the space in-between buildings that needs to be considered.\nJoshi looked at how the use IoT data collected from \u2018smart cities\u2019 helps better inform urban designers. For instance in China, AI has been used to better understand its inhabitant\u2019s preferences building a \u2018City Brain\u2019 and used to reduce traffic, accidents and crime.\nView the other NXT BLD 2018 presentations\nMike Leach, Lenovo\nEnhancing performance.\nRebecca De Cicco, Digital Node\nHow Smart Cities, BIM and Digital Construction will alter future skill requirements.\nMarc Petit, Unreal Enterprise\nThe journey to real time.\nHedwig Heinsman, DUS architects \/ Aectual\nAectual construction \u2013 sustainable, customizable, 3D printed.\nDr Abel Maciel, Bartlett School of Architecture\nDesign Thinking, Teams and Disruptive Technologies.\nDr Max Mallia Parfitt, Fulcro Group\nVR and AR visualisation of BIM data: Changes in tech over the last 10 years.\nEleni Papadonikolaki, UCL Bartlett School & Construction Blockchain Consortium\nBeyond crypto: Digital translformation in construction through blockchain technologies.\nMarianna Kopsida, Trimble\nMixed Reality Solutions for AEC.\nBruce Bell, Facit Homes\nPre-fabrication has had its day \u2013 Digital Construction is the future.\nAndrew Watts, Newtecnic\nFuture Technologies for Architecture, Engineering and Construction (AEC).\nAndrei Jipa, ETH Zurich\nSmart Concrete.\nStefana Parascho, Gramazio Kohler Research\nCooperative robotics in architecture.\nDaniel Schmitter, Mirrakoi SA\nXirus: 3D CAD \u2013 From Biomedicine to AEC.\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 13 June at Congress Centre, London in association with Lenovo. The conference covered innovations in digital fabrication, Virtual and Mixed Reality, design visualisation, AI, Blockchain and lots more.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/matching-site-data-to-mastermap\/","title":"Matching site data to MasterMap","date":1156291200000,"text":"The recent launch of OS Net brings real time kinematic (RTK) GPS to the field and site surveyor and centimetre level positioning. But what does this mean to those in the construction industry?\nPrevious articles have focused on exploiting and integrating existing detailed digital mapping in your GIS and CAD applications. Most CAD and GIS users work with and around or actually also carry out site surveys. Many are thus aware of the potential errors in such data and what needs to be done to eliminate or remove the different types of error.\nThe advent of GPS technology in the 1980s, combined with the ever shrinking size of the equipment and corresponding increase in performance and processing power, brought centimetre accuracy to the desktop and, for those prepared to invest, to the top end of the profession. The recent launch of OS Net as an adjunct to all Ordnance Survey\u00dds other data capture and distribution capabilities now brings real time kinematic (RTK) GPS to the field and site surveyor where previously only differential GPS (dGPS) existed. This article explores what this might mean to the readership.\nWhat is OS Net?\nGPS technology has changed and continues to inform the way we understand our position and the way we do things. It\u00dds a technology now enjoyed by almost everyone in Great Britain (in some way or another) but GPS as it currently stands only delivers positional accuracy to around 10m or more. OS Net takes raw GPS data and through building complex error models can provide real-time centimetre level positioning to those with the appropriate equipment.\nOS Net is a national GPS infrastructure and service developed by Ordnance Survey to provide up to 1cm accuracy real-time positioning to the map base (and thus to OS MasterMap). OS Net is not directly available in the marketplace (see later) but should be seen as the enabling technology that serves application markets via the services that Ordnance Survey partners build on top of OS Net.\nAs a nationally consistent source of position referencing, OS Net implicitly supports local and national applications that require a positioning element enabling quicker and easier positioning and the use of a common coordinate system.\nThe network is made up of more than 90 permanent GPS base stations at a density that enables RTK positioning services across most of the country. From the observations made by these stations errors can be calculated in the GPS system to provide a range of correction services that seek to rectify the majority of errors caused by the satellite orbits, clocks and the atmosphere.\nFor the technically minded it is worth noting that RTK initialisation time is less than a minute, with RTK rmse (root mean squares error, the standard mechanism of error measurement) horizontal accuracy being 1-2cm and vertical accuracy 3cm with a precision of less than 1cm. Pre-launch tests revealed that 95% of the 2D error vectors are <= 35mm and 95% of the height errors are <= 65mm. Also, the accuracy of test points on the outside of the network of base stations or in sparse regions was seen to be negligible. OS Net also includes dGPS services whose positioning accuracy is sub-metre (down to about 20cm). Figure 1 illustrates what this means\u00cd\nWhat does OS Net enable?\nUnsurprisingly perhaps, the base station network has been providing centimetric level positioning service for Ordnance Survey surveyors; this should mean that new data reaches Ordnance Survey databases faster, delivering efficiency and flexibility.\nOS Net as a service is not available commercially, however, the reason this is now of relevance to users is that Ordnance Survey has commodotised the service to allow partners to utilise this network data to develop and provide a basket of real-time and post-process GPS correction services from 20mm upwards to customers within a wide range of markets.\nOrdnance Survey will maintain the accuracy and quality of geodetic calculation within the network, while partners will in the same way as Ordnance Survey does, use advanced software tools to create models of GPS errors. Once these errors are known, corrections can be calculated and then services transmitted to customers or their contractors in the field to improve their positional accuracy.\nOrdnance Survey has already reached agreement with at least two major survey equipment manufacturers to enable them to license OS Net. At the moment this is done on a per device basis so that users request the appropriate software post-processing module. This is a revenue opportunity for Ordnance Survey and its partners and one that brings real productivity benefits to users:\n\u2022 single person operation\n\u2022 quicker, easier correction available almost anywhere\n\u2022 reduced hardware costs (only one receiver required, no base station)\n\u2022 faster set up\n\u2022 adoption of a common coordinate system (the official name of this is ETRS89)\nl direct linkage through the licensed service to GB National Grid and the Ordnance Survey height datum via OSTN02 and OSGM02.\nLicensing of RTK GPS correction services changes the way surveying is carried out, increasing the value of the surveyor and offering up a more flexible and efficient data capture environment.\nCertain users, for example those developing new applications, the Met Office and academics are able to register for access to these services. And for others Ordnance Survey continues to offer a free service for post-process applications maintained (using readings every 30 seconds for correction to the national coordinate reference frame) from www.ordnancesurvey.co.uk\/gps .\nWhat doesn\u00ddt it change?\nLeisure and some other users of GPS labour under the misapprehension that GPS is always precise and accurate (and many don\u00ddt know the difference but that\u00dds another story!). In fact GPS signals deteriorate quite easily in certain environments, notably urban canyons and under vegetative cover (and under water without an antenna!).\nFigure 2 illustrates the problem and users should be aware that while OS Net can improve the position you get from your GPS, but it doesn\u00ddt help you get a position \/ get more satellites! A similar situation occurs in forested and wooded areas, one that can in part be resolved by fixing an antenna to a telescopic device.\nPotential Applications\nYou won\u00ddt need this article to spell out the obvious ones in the construction industry but a few tangential ones are worth mentioning:\n\u2022 Underground assets\/utilities \u00b1 where OS Net has been used, utility networks will be very accurately positioned in three dimensions and thus should come more cheaply and easily constructed and maintained (lower works costs, interoperability with other networks owing to use of ETRS89)\n\u2022 Port activities \u00b1 OS Net can help in near-shore pilotage, vehicle tracking and container location\n\u2022 Machine automation \u00b1 adoption of OS Net for vehicle tracking in agriculture, earth works, terra-forming, coastal defence creation will decrease direct (fuel\/maintenance) costs, reduce resource waste (e.g. over spraying), reduce carbon footprint and other environmental impacts\n\u2022 Event and asset records \u00b1 street furniture, flora\/fauna observations, track and roadside assets, accidents, incidents, emergencies, evidence gathering (from police checks to street fouling)\n\u2022 VISTA \u2013 The DTI funded Vista Project (visualising integrated information on buried assets to reduce street works) looks to develop methods to bring together utility asset records and link them to new surveys. Technologies like GPS will play a key role in locating and relocating these assets.\nAnd the Future\u00cd\nThe future of satellite navigation could mean that there are 60 to 80 usable positioning satellites in orbit by 2012, compared to the current crop of about 24 GPS satellites. This will mean much better availability in built-up areas as well as quicker and more accurate positioning. These extra satellites come from two sources:\n\u2022 Galileo is the new European satellite system, which is due to launch up to 30 satellites from 2008 to 2010.\n\u2022 GLONASS is the Russian equivalent of GPS. There are currently nine working GLONASS satellites in orbit; new funding could mean that this is increased to 24.\nOnce dual GPS\/Galileo receivers are fully available and Galileo satellites are starting to be launched, Ordnance Survey will plan to replace the OS Net base station infrastructure with GPS\/Galileo receivers. These receivers may also be able to receive GLONASS signals. This is planned to happen in late 2006-07 and will mean that OS Net is able to generate corrections for whatever type of receiver a user has. A second re-equipping of the GNSS (Global Navigation Satellite System, generic term for GPS, Galileo et al) infrastructure will probably occur in about 2013, but it is seen that there will be little change to the functionality of the receivers and the end-products that they could offer.\nFor those of you wondering why Ordnance Survey bothered with OS Net when Galileo and EGNOS (an advanced differential GPS service using geostationary satellites already built into many receivers) were slowly creeping over the horizon then the reality is that the infrastructure being marketed as OS Net was already a key enabler for Ordnance Survey themselves and that in bringing it to market the best interests of users and of joined up government can begin to be served immediately.\nThis article was written by James Cutler, CEO at eMapSite, a platinum partner of Ordnance Survey and online mapping service to professional users","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/features\/openbuildings-design-and-beyond\/","title":"OpenBuildings: design and beyond","date":1542931200000,"text":"Bentley Systems\u2019 new OpenBuildings platform is more than just AECOsim Building Designer rebranded. Greg Corke explores how Bentley intends to play to its strengths with focused, vertical design tools, cross disciplinary workflows and open data\nWith the arrival of OpenRoads and OpenRail for civils and rail design, and the much-needed simplification of the Bentley Systems product portfolio, an \u2018OpenBuildings\u2019 was inevitable. But the new Bentley Systems brand, announced in October, isn\u2019t just about giving AECOsim Building Designer a new name; it\u2019s about transforming the building design tool into an open platform that can be tightly integrated with tools for other disciplines; not just the road \/ rail design and simulation (structural, geotechnical, energy and pedestrian) applications from the Bentley stable but those from third parties as well.\nBentley is also looking to use OpenBuildings as a platform on which to build niche vertical design tools, focused on specific workflows. AECOsim Building Designer simply isn\u2019t getting a look in on many bread and butter building projects because of the dominance of Autodesk Revit.\n\u201cWe realised that half of the [awards] submissions at YII [Bentley\u2019s annual Year in Infrastructure conference] use [AECOsim] Building Designer; they don\u2019t use it for vertical buildings, they use it for non-traditional buildings \u2014 stadiums, airports, metro stations, water treatment plants,\u201d explains Santanu Das, Bentley\u2019s SVP of design modelling. He adds that firms working on those types of specialist projects don\u2019t want a \u2018Swiss Army knife\u2019 type product like Revit or ArchiCAD that they then have to try to customise extensively.\n\u201cThe deliverables, the integration with specific analyses, the sharing [collaboration], it\u2019s different whether you\u2019re doing an airport or a train station, versus a hospital \u2013 even the construction sequencing,\u201d he adds.\nAll aboard\nAECOsim Building Designer has always been relatively strong in rail and metro station design because of Bentley\u2019s dominance in the civil and rail markets, so it makes sense that the first asset-specific design tool that Bentley has built on top of its new platform is OpenBuildings Station Designer.\nDas uses the example of HS2, the highspeed rail link that will connect London to Manchester in the North of England, to explain the need for a bespoke station design tool that works closely with other disciplines. \u201cThey have their Mott MacDonalds doing the rail \/ civil side of it, then they\u2019ve got people doing the train station, the architectural stuff, in Revit or something like that,\u201d he says. \u201cThe first thing they come to us and say is, can you fix this interoperability, because when we\u2019re changing the track alignment or changing the tunnel, everything [in the station design] has got to be changed manually.\u201d\nOpenBuildings Station Designer includes special templates for creating tunnel segments (by utilising Bentley\u2019s computational design tool GC), which are based on linear alignments from OpenRail. So, if the track changes, the tunnel will automatically change as well, and the station will also align.\nAs one would expect, the software also includes parametric digital components such as gates, turnstiles, signs, lifts and escalators that are very specific to an asset type like a station.\nDas emphasises the importance of being able to link seamlessly to Bentley\u2019s simulation tools, highlighting the company\u2019s \u2018unique\u2019 geotechnical engineering capability through the recently acquired Plaxis.\nHowever, simulation isn\u2019t limited to stresses, strains, deformation and stability; it also encompasses human behaviour. Bentley\u2019s newly acquired pedestrian simulation technology, Legion, which uses machine learning to predict how people will move through a building, will sit on top of OpenBuildings Station Designer. The software can be used for emergency evacuation, circulation and operational efficiency, such as working out how to get passengers off trains faster, down into the metro or into the retail stores.\n\u201cWe can actually identify where the congestion is happening at any time, and therefore, as an owner, as a designer, do something about it,\u201d explains Das. \u201cMaybe move my lift over 15 feet, maybe put an extra turnstile in there, maybe switch the barrier around at 5 O\u2019clock so there are more people going in one direction than going in another direction.\u201d\nPedestrian simulation is not just applicable to station design and will almost certainly play an important role in other vertical OpenBuildings design tools, which Bentley has said it intends to launch in the future. Stadium and airport design seem the most likely candidates although there is a risk of becoming too niche.\nCustomisation and collaboration\nThere is a long history of Bentley customers developing bespoke solutions to work with MicroStation and MicroStationbased product like AECOsim Building Designer. Das says that this has happened a lot in China, where workflows and deliverables are often different. \u201c[Chinese infrastructure services company] ECIDI, for example, has been taking our product, MicroStation, for years and bolting on their own workflows and calling it their own,\u201d he says. Customisation and collaboration are key differentiators between AECOsim Building Designer and OpenBuildings. They currently share the same traditional software development kit (SDK) but what makes OpenBuildings different is that it can take full advantage of Bentley\u2019s new integrated product stack. Firms can use the new cloud-based services and also connect to third-party applications through the open source iModel.js library and the iModelHub, which offer web-based APIs. \u201cBecause the technologies are open source, everybody has access to them and it actually makes it really, really easy \u2014 much easier than the standard SDKs that came with AECOsim Building Designer,\u201d says Das who goes on to explain that because the schema is open, firms can also dive into extract additional data from OpenBuildings, such as meta data that may not have come across. Returning to the example of ECIDI, Das explains that the Chinese firm is already looking to build its next generation of applications on top of OpenBuildings, rather than on top of MicroStation. He says this is because it gives them a better granular starting point and with iModels they get web visualisation tied into the iModelHub, and they can also work with their cost estimation partner.\nConclusion\nFor some years now, Bentley has been losing the battle with Autodesk and other firms when it comes to mainstream building design software. Now with a focus on verticals like station design it is starting to play to its strengths, offering a building design tool that can take advantage of its \u2018open\u2019 applications in areas such as rail and road design, pedestrian simulation, structural analysis and geotechnical engineering.\nWhile we expect to see Bentley develop other niche building design tools for large scale assets such as airports and stadiums, perhaps the biggest opportunity for the company is not as a traditional provider of design software. Through iModel.js and iModelHub it\u2019s all about what you can do with the project data that comes from Bentley tools and other applications, which is something we explore in more detail here.\nIf you enjoyed this article, subscribe to our email newsletter or print \/ PDF magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/sponsored-content-autodesk-structural-precast-extension-for-revit-2018\/","title":"Sponsored: Autodesk Structural Precast Extension for Revit 2018","date":1506384000000,"text":"A notable step toward the future of automatically making structural things\nSPONSORED CONTENT\nAutodesk brings into play a new automated workflow for the Precast Concrete industry. The Autodesk\u00ae Structural Precast Extension for Revit\u00ae 2018 is a BIM-centric offering for modeling and detailing precast elements that promotes productivity and precision for engineers, detailers and fabricators working on typical building projects in the precast industry. The app provides Revit users access to powerful tools for automatic rule-based segmentation, reinforcement, shop drawings and CAM files generation of precast planar concrete elements. The app is available on the Autodesk App Store here and via the Autodesk Desktop App.\nLeveraging the concept of Parts, which support the construction modeling process by letting you divide certain elements from the design intent model into discrete parts, the app provides the opportunity of having one single source of truth for various personas that need to work with the model. This way, Designers\u2019 and Fabricators\u2019 perspectives are respected and various Levels of Development can be displayed \u2013 two key benefits of embracing a Revit-based workflow for Precast projects.\nBased on predefined parameters in the \u201cConfiguration\u201d dialogue box, you can specify the rules that will help drive the automated workflow downstream from Design to Fabrication. First up, the elements are automatically segmented and fitted with connectors, lifters and bushings. All of these are actually Revit families, so customization is easy. Reinforcement is also done automatically and you can define multiple patterns, based on fabric sheets or rebar sets. There is even a tool that creates Custom Fabric Sheets, where each wire can have its own diameter, length and distance with respect to the adjacent wires. This is useful both for optimizing the rebar consumption based on structural analysis and for minimizing clashes with various MEP equipment that might be embedded or going through the panels.\nSpeaking of embeds, the app comes with another tool that automatically adds to the corresponding precast assemblies all the electrical sockets, cable ducts, extra rebars or any other kind of component that is in the model. This way, the precast walls and slabs will contain the logic that is required for fabrication, minimizing much of the hassle in the factory or on site.\nOne particularly powerful tool that Structural Precast for Revit offers is Automatic Shop Drawings. Once a company\u2019s standards related to drawing style and content are embedded in the drawing templates, for each precast element the drawing is created, with all relevant views and bills of materials. If required, multiple shop drawings for each assembly can be generated; for instance, one showing the reinforcement and one highlighting the position of the embeds. It\u2019s also worth mentioning that this Automation tool can be used with company standards for all elements at once, all elements per submittal or per element.\nAnd because we are working in Revit, coordination of the precast model with Architecture and MEP comes as a natural benefit. In the highly likely event that changes need to be performed to the precast elements (we all know change is a daily routine in the construction industry) you don\u2019t have to worry: the precast elements, shop drawings and bills of materials are automatically updated\u2014helping to keep information up-to-date.\nWhen Fabrication is ready to start, with just one click, the creation of CAM files is done. Both Unitechnik (versions 5.2 and 6.0) and PXML (version 1.3) are supported. The various file naming options and output settings offer flexibility to generate these deliverables simultaneously in a swift and tailored fashion.\nThe product is mostly suitable for typical building projects, made up of slabs and walls produced in factories with a high level of automation. Currently, three types of elements are supported by the new app: Solid Walls, Solid Slabs and Hollow Core Slabs.\nOf course, we need to remind ourselves that it\u2019s not only about design and detailing, but also about construction coordination, planning and execution. And that\u2019s when we recommend you export the Revit model to Navisworks Manage and BIM 360 Team. Or, if you are in the position to meet with your customer and walk him or her through the details of their future building, why not do it in Revit\u00ae Live, so she\/he can view, better understand, feel and experience it before it is being built?\nWith Structural Precast for Revit, Autodesk makes a notable step ahead for the future of automatically making structural things.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/aecmag.com\/news\/nxt-bld-video-mike-leach-lenovo\/","title":"Video: NXT BLD London conference \u2013 Mike Leach, Lenovo","date":1502150400000,"text":"Enhancing performance through the workflow \u2013 NXT BLD London, June 2017\nMike presents the key technology developments driving the workstation marketplace today and, more importantly, how these will impact your AEC workflows, from CAD and BIM to rendering and VR. Learn what to look out for, where to invest your hard-earned budgets and how to best configure your next workstation investment.\nView the other NXT BLD presentations\n\u25a0 Tom Greaves, DotProduct\nReality modelling with phones and tablets\n\u25a0 Tim Geurtjens, MX3D\nTo print a steel bridge in Amsterdam\n\u25a0 Faraz Ravi, Bentley Systems\nVirtualised environments in infrastructure\n\u25a0 Martin McDonnel, Soluis \/ Sublime\nVR, MR, real time viz and the Augmented Worker\n\u25a0 Dan Harper, Cityscape\nVirtual Reality (VR) beyond the hype\n\u25a0 Paul Nichols, Skanska\n\u25a0 Rob Charlton, Space Group\nThe positive impact of accelerating technologies\n\u25a0 Arthur Mamou-Mani, Mamou-Mani\nConstructing (and deconstructing) buildings with cable robots\n\u25a0 Philippe Par\u00e9 and Akshay Sethi, Gensler\nSeeing is believing: using game-changing tools to discover the soul of design\n\u25a0 Johan Hanegraaf, Mecanoo Architecten\nCommunicating the certainty of conceptual ideas through immersive means\nNXT BLD is organised by AEC Magazine and brings next generation architecture, engineering and construction technologies to life in an exclusive conference and exhibition. These emerging technologies facilitate new ways of designing, enhancing the use of 3D models, applying Artificial Intelligence (AI) and offering new possibilities in digital fabrication and construction.\nNXT BLD London took place on 28 June at The British Museum, London in association with Lenovo. The conference covered innovations in Virtual and Augmented Reality, design visualisation, digital fabrication and AI.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/geospatialworld.net\/news\/globexplorer-to-provide-high-quality-aerial-and-satellite-imagery\/","title":"GlobeXplorer to provide high quality Aerial And Satellite Imagery","date":1015286400000,"text":"GlobeXplorer, a company that distributes interactive aerial and satellite photos via the Internet, has announced that Internet users can now order high-quality prints of their favorite destinations online. Users can find their home, favorite vacation spot or sports venue and purchase high quality prints or postcards of exactly what they see. EZPrints, an online digital photo and image fulfillment company, has signed an agreement with GlobeXplorer to provide the online ordering, print processing and delivery for a variety of print products featuring aerial and satellite imagery.\nEZPrints will provide a fast and easy online ordering process for users who view GlobeXplorer\u2019s extensive earth imagery library on a variety of websites including major mapping portals, real estate sites and travel portals. Web customers can now purchase aerial images easily and inexpensively online. The interactive aerial viewer developed by GlobeXplorer allows users to pan and zoom to pinpoint the exact location they would like to print.","source":"geospatialworld.net"}
{"url":"https:\/\/aecmag.com\/features\/aligning-real-and-virtual-construction\/","title":"Aligning real and virtual construction","date":1473984000000,"text":"From drone-gathered site data to augmented reality (AR), advanced technologies are bringing new perspectives on real-world environments and works in progress, writes Randall S Newton\nThe construction industry in Japan has a problem: the shrinking birth rate is creating a shortage of skilled construction workers. So heavy equipment manufacturer Komatsu has created a Smart Construction division, building automated excavators and dozers. The robotic equipment pushes dirt without a human driver, following excavation plans with great accuracy. The automated heavy equipment is so fast and precise that existing methods of ground-based data gathering (surveying, ground-based scanning) cannot keep up.\nTo tackle this issue, Komatsu Smart Construction worked with San Franciscobased start-up Skycatch to improve data collection and processing. Skycatch specialises in industrial applications for Unmanned Aerial Vehicles (UAVs), commonly known as drones.\nNow, whenever Komatsu Smart Construction vehicles are on site, they are guided by one or more drones gathering terrain data. Komatsu refers to the resulting workflow as the world\u2019s first machine-to-machine automated construction system.\nSkycatch\u2019s drones photograph job sites, capturing imagery and automatically generating 3D site data. The data can be quickly laid over site drawings or models to automatically calculate area and volume of earth to be moved.\nThe results are transmitted as instructions to Smart Construction machinery for fully autonomous work on the site. Think of it as drone-to-phone construction management.\n\u201cOnce we have efficiently planned the work amount, [clients] can use the Komatsu machinery. Smart Construction can successfully plan the efficient system from the beginning to the end so clients save significant amount of workload, time and costs,\u201d says Fujitoshi Takamura, CTO of Komatsu.\nEyes inside the project\n\u201cWe are a data company that happens to use drones to collect data,\u201d says Patrick Stuart, director of products at Skycatch. The company\u2019s combination of drones, mobile apps and cloud processing turns raw data from the drones into usable information for autonomous equipment or for use by engineers in existing CAD or BIM workflows.\nFor example, a utilities engineer can monitor how utilities are being installed in near real time. By tracking progress so closely, the engineer can see if and when a design change must be made, then take measurements and decide what to do. What took days or weeks with manual data gathering and oversight can be now done in minutes.\n\u201cWe give people in construction something they never had before \u2013 eyes into an entire construction project,\u201d says Stuart. Traditionally, construction managers gathered information from a variety of sources and methods, from discussions on-site to marked-up plans, to email and phone exchanges.\n\u201cIt is like a phone tree game,\u201d says Stuart. \u201cUsually the data is late and not very accurate.\u201d\nThe Skycatch technology stack comprises three layers: the drone hardware and control technology (Skycatch sells drones but also works with other brands); apps for data review and editing the gathered imagery; and a cloud intelligence platform, for turning imagery and scans into actionable data.\nOne example of the data processing is in creating topographic charts. There can be no trees, shrubs and so on in the final data. Removing all the flora by hand, using a computer workstation, can take hours or weeks, depending on the scale of the project. The Skycatch cloud platform does it in minutes, scaling to use as many processors as available to complete the task. The software can export to common AEC data formats for CAD and BIM.\nRunning robotic heavy equipment and monitoring construction progress are only two of the most obvious uses of UAVs. One Skycatch client started using the technology after ground had been broken and foundation work had begun on a campus-sized project.\nManagement started by using Skycatch technology to gather current site data, comparing it to the project design. On the first day, they found a $400,000 slab of concrete was eight inches misaligned. Work stopped; an investigation revealed flaws in the original survey that echoed through the design phase and into construction. If not found so quickly, a $400,000 concrete mistake could have ballooned, costing several times more than that sum, depending on when traditional construction oversight methods spotted it.\n\u201cI use the Skycatch data on the site daily if not hourly,\u201d says Victoria Julian, a superintendent for DPR Construction. \u201cWe can take the Skycatch plan, colour-code it, mark it up, label it and use it for logistics planning, daily, weekly, monthly \u2013 whatever we need.\u201d\n\u201cEverybody who sees this is amazed by it. Loves it. And is using it. It\u2019s really an incredible tool that I don\u2019t want to have to do without on the next big project.\u201d\nRecurring imagery as sequential data\nSkycatch is one of several vendors working to unite the real and the virtual in construction. The new element is recurring imagery. A \u2018single source of truth\u2019 has always been the holy grail, but the construction process has made it elusive. The ability to add recurring highresolution actionable imagery data to BIM models or CAD drawings is a gamechanger. BIM no longer has to be only the design\/engineering model; it can be the constantly updated record of construction.\nThe use of high-res recurring imagery can mitigate risk by providing evidence of deviation from the design, as in the case above. Instant aerial 2D linear and 3D volumetric measurements replace manual estimates or hands-on retrieval, with significant time savings and improved safety. Using imagery for job site flow analysis can identify high traffic areas immediately, allowing managers to create workarounds. Terrestrial 3D laser scanners have been in use on large projects for years, but the addition of drones adds both the new dimension of fly-over viewing and the ability to turn site analysis into a new form of sequential data.\nA construction site is constantly changing; research shows the constant state of flux is one key reason construction has not made efficiency gains in recent years compared to other industries. Specifically, the issues of on-site coordination, planning and communication have not reaped significant automation gains, with the obvious exception of making data accessible on mobile devices. The creation of data and editing of data has remained a desktop activity. Using drones to gather recurring imagery means site maps can become daily reports. Planning logistics and managing assets is simplified when exact coordinates replace estimates. Communication with quality control, safety and other departments becomes faster and more accurate.\nFlying Superintendent\nTurner Construction is working with researchers from the University of Illinois (UI) to create a real-time construction monitoring system using drone imagery. Flying Superintendent uses both still images and video to guide quality control, safety compliance and logistics. \u201cThe analytics we conduct on these survey-grade 3D visual production models offer construction managers a transparent view into what\u2019s happening on site each day, empowering them to improve reliability in short-term plans and eliminate problems before they happen,\u201d says Mani Golparvar-Fard, lead principal investigator and an associate professor of civil engineering at UI. The research team is currently testing and refining Flying Superintendent on the construction of a new professional sports complex in Sacramento, California. The goal is to create a predictive computer vision system for construction site data analysis that runs with minimal human interaction and can navigate using construction site data. By not relying on GPS data as its only navigation method, drones can be used for indoor work where the GPS satellite signal is weak or non-existent. The research is being commercialised by a university spin-off, Reconstruct Inc. Flying Superintendent is also currently being used on a high-rise project in Arizona and by a leading Japanese construction company on multiple projects.\nState of the art\nThe technology for aligning the real world of construction data with the virtual world of design and BIM is very much a work in progress. Start-ups like Skycatch and Reconstruct and others are working from the drone-and-imagery side, while experienced AEC technology vendors like Autodesk, Bentley and Trimble are working from the design data side. An overview of the leading vendors and their work reveals a variety of approaches to the challenge of aligning the real and the virtual in construction. Major players and technologies include:\n3D Robotics\n3D Robotics (3DR) was an early player in the commercialisation of drones and recently dropped out of the consumer market to focus on enterprise applications. It has received $99 million in venture capital funding and is working in close cooperation with Autodesk on SiteScan, an aerial analytics platform designed to integrate the drone-and-camera hardware with various Autodesk products including ReCap and A360. Earlier this year a SiteScanenabled 3DR drone created 3D imagery of the famed Red Rocks Amphitheatre near Denver as part of a renovation project. Autodesk helped scan Red Rocks a few years ago using terrestrial 3D laser scanners, but this update significantly improved upon the existing data because of the aerial dimension. Following a borrow- not-build ethic, 3DR is also collaborating with Sony (for mobile computing) and GoPro (for photography) as it builds out the capabilities of SiteScan.\nAutodesk\nYears of research and a variety of acquisitions has given Autodesk a rich treasure trove of relevant technologies for aligning real and virtual construction data. Until recently most of it was scattered in various divisions. Last year, Autodesk launched the Forge Initiative, designed to consolidate its various technologies and services related to mixing the use of realworld data (\u201creality capture\u201d in Autodesk\u2019s words) with design automation. Some of the technology is designed for product development and 3D printing, but there are also tools for AEC. ReCap 360 provides automatic stitching and registration of scan data to CAD\/BIM files and can be used to measure items depicted in scans. A360 is a cloud collaboration tool that works in conjunction with ReCap, simplifying access to data for all stakeholders. The resulting data can be used in all Autodesk AEC and civil engineering applications including AutoCAD, Revit, Navisworks, Infraworks and others.\nBentley Systems\nBentley brings its enterprise approach to AEC and geospatial engineering with a line of software products for what it calls \u201creality modelling\u201d. ContextCapture turns photos into detailed photo-textured 3D mesh models that integrate with MicroStation and other Bentley design and engineering products as well as CAD or GIS software from other vendors; the models are precise enough to measure from or use for comparing as-built with design. With ContextCapture, a set of photos up to 100 gigapixels in size can be converted into a single detailed model. The models are inherently georeferenced through both device positioning data and precise control points. Other modules in the ContextCapture line can create videos or interactive web models.\nTrimble\nIt seems each of the major AEC vendors has a different phrase to describe using 3D site data in the construction workflow; for Trimble, it is \u201cmixed reality\u201d, to describe the blending of realworld objects with digital content from design in real time. Trimble\u2019s roots are in construction, not design; in recent years it has been building a BIM portfolio from a construction tech point of view, and also working closely with other vendors where interests align. Trimble has interoperability agreements with both Autodesk and Bentley, and is using software from both companies in their mixed reality R&D. Trimble sees mixed reality as a form of spatial interaction, in which design teams and construction teams both review, interact and share using 3D models in the context of the physical environment instead of the virtual design space. \u201cIn the context of the building industry, this is the phase in which digital and real content co-exist,\u201d says Aviad Almagor, Trimble\u2019s director of Mixed Reality. \u201cArchitectural design collides with reality and construction teams transform digital content into physical objects.\u201d\nTrimble is working closely with Microsoft to bring Hololens technology to AEC. Hololens is a wearable, self-contained holographic display computer. Users can interact with 3D models that blend into the view of real objects. The Hololens display can be adjusted on a continuum from \u2018real objects only\u2019 to \u2018digital model only\u2019 \u2013 or anywhere in between.\nTrimble is working from a deviceagnostic perspective. In addition to Microsoft Hololens, it\u2019s also exploring the use of the Google Tango platform to develop augmented reality applications and Facebook Oculus Rift for virtual reality.\nTwo on-going pilot projects with very large companies highlight what Trimble is developing.\nWith engineering giant AECOM, Trimble is developing a system for collaborative review of engineering design from a construction perspective. Engineering data and site data collected by drones are overlaid for viewing inside a Microsoft Hololens display system. Design alternatives can be explored in a collaborative context and team members can immediately see how the changes will interact with the existing site.\nIn June 2016, Trimble announced a mixed reality pilot program with CSCEC Group 1, one of the largest construction firms in China. Similar to the AECOM pilot, drone data will be blended with design, but for construction operational overview, not engineering review. Given the need for on-site collaboration in the Group 1 pilot, the Google Tango tabletbased augmented reality technology may play the lead role from the display side.\nUnlike the products from Autodesk and Bentley, the Trimble mixed reality technology is still in the proof-of-concept and pilot programme stage.\nThe Big Data future\nVirtual reality (VR) and augmented reality (AR) are attracting a lot of attention in AEC at the moment. Project visualisation (whether for sales or architectural design review) is an obvious use for VR. Construction and operations are looking closely at AR as a new way to deliver assembly, operations and repair data to the job site. These are interesting applications, each worthy of their own articles. But there is something unique to realtime site information in the BIM model that transcends either specific display technology.\nVirtual reality and augmented reality freeze data at a point in time, yet construction sites change constantly. Superimposing drone-gathered site data onto the BIM model in real time makes new technologies possible; Komatsu\u2019s autonomous earth movers are just a first step. As all construction processes become accessible as data, technologies now used in so-called big data applications such as analytics and artificial intelligence can be applied. It should not take too long for autonomous robotic welders and riveters to assemble highrises and autonomous cement trucks to deliver just-in-time loads where they are needed on the job site without a specific advanced order or schedule. Adding other data sources such as weather and traffic could improve logistics and oversight. Add financial data to the mix and, over time, data analysis could more accurately predict costs for specific design styles and construction methods.\nRandall S. Newton is Principal Analyst at Consilia Vektor. He has been writing about AEC since 1987.\nIf you enjoyed this article, subscribe to AEC Magazine for FREE","source":"aecmag.com"}
{"url":"https:\/\/www.globalconstructionreview.com\/charting-the-path-to-2026-what-construction-needs-to-know\/","title":"Charting the path to 2026: What construction needs to know - Global Construction Review","date":1766102400000,"text":"In our final episode of the year, we take stock of the biggest issues that are defining construction and look ahead to what will be on the industry\u2019s agenda in 2026.\nFrom reforms of the skills system and strengthened safety oversight, to the government\u2019s response to its late payments consultation, we unpack the developments that matter most to professionals across construction.\nTechnology is another major focus for this discussion as we explore the rise of artificial intelligence in design, planning and site operations. The episode also examines the role of modern methods of construction and why questions remain over whether 2026 will be a turning point for wider adoption of repeatable designs.\nWe also hear from Pete McKinley, commercial director for Seddon Property Services, and Ryan O\u2019Loughlin, regional director at Henry Boot Construction, about the opportunities and trends construction teams should be preparing for in the New Year.\nWhether you are looking back at the challenges and shifts of the past 12 months or planning for the year ahead, this episode brings together the insights that will shape construction\u2019s journey into 2026.\n- Subscribe here to get stories about construction around the world in your inbox three times a week","source":"globalconstructionreview.com"}
{"url":"https:\/\/www.wired.com\/story\/apple-app-making-course-michigan-state-university\/","title":"Apple\u2019s App Course Runs $20,000 a Student. Is It Really Worth It?","date":1766534400000,"text":"Two years ago, Lizmary Fernandez took a detour from studying to be an immigration attorney to join a free Apple course for making iPhone apps. The Apple Developer Academy in Detroit launched as part of the company\u2019s $200 million response to the Black Lives Matter protests and aims to expand opportunities for people of color in the country\u2019s poorest big city.\nBut Fernandez found the program\u2019s cost-of-living stipend lacking\u2014\u201cA lot of us got on food stamps,\u201d she says\u2014and the coursework insufficient for landing a coding job. \u201cI didn\u2019t have the experience or portfolio,\u201d says the 25-year-old, who is now a flight attendant and preparing to apply to law school. \u201cCoding is not something I got back to.\u201d\nSince 2021, the academy has welcomed over 1,700 students, a racially diverse mix with varying levels of tech literacy and financial flexibility. About 600 students, including Fernandez, have completed its 10-month course of half-days at Michigan State University, which cosponsors the Apple-branded and Apple-focused program.\nWIRED reviewed contracts and budgets and spoke with officials and graduates for the first in-depth examination of the nearly $30 million invested in the academy over the past four years\u2014almost 30 percent of which came from Michigan taxpayers and the university\u2019s regular students. As tech giants begin pouring billions of dollars into AI-related job training courses across the country, the Apple academy offers lessons on the challenges of uplifting diverse communities.\nMeasuring Success\nSeven graduates who spoke with WIRED said they had good experiences at the academy, citing benefits such as receiving mentorship from past students. Fernandez says she was impressed by a focus on developing inclusive apps and a series of speakers from Apple who were genuinely willing to help and share frank lessons. \u201cTheir heart was in the right place,\u201d she says.\nThe program does expose people of color to new possibilities. \u201cIt changed my life,\u201d says Min Thu Khine, who\u2019s now mentoring coding students and working at an Apple Store Genius Bar. \u201cMy dream is to be a software engineer at Apple.\u201d\nThe academy also draws positive grades from some researchers who study tech education, such as Quinn Burke. He says its fully subsidized in-person instruction surpasses the quality of many coding boot camps, which proliferated over the past decade and sometimes left students in debt and with narrow skills.\nBut the academy being open to all can complicate instruction and how to measure success. One entire family attended together, and at least two mothers have come with their daughters. Students on average are in their thirties, ranging from 18-year-olds to, for example, a grandfather in his seventies who wanted to develop a photo app for his grandchild, according to Sarah Gretter, the academy leader for Michigan State.\nOn the other end are students such as Joey Brinker, a 20-year-old also taking a full course load at University of Michigan\u2013Dearborn. \u201cI found my grades improved,\u201d he says, crediting the academy.\nThe program gives out iPhones and MacBooks and spends an estimated $20,000 per student, nearly twice as much as state and local governments budget for community colleges. Even so, Fernandez says, she leaned on waitressing shifts and government food aid to supplement the provided stipend and make it through the tuition-free academy.\nGordon Shukwit, a senior director at Apple overseeing the program in Detroit and 17 other Apple academies abroad that have been opened since 2013, says increasing student aid is a constant focus. The university declined to comment on cost-per-student estimates and comparisons to community college spending, saying that the academy offers an unparalleled experience.\nAbout 70 percent of students graduate, which Gretter describes as higher than typical for adult education. She says the goal is for them to take \u201ca next step,\u201d whether a job or more courses.\nRoughly a third of participants are under 25, and virtually all of them pursue further schooling. \u201cI went in with zero coding experience or knowledge,\u201d says Dayan Abdulla, 22, who graduated from the main program in June and works in quality assurance at a software startup while studying computer science at Henry Ford College. \u201cI\u2019m on a good trajectory,\u201d he says.\nAbout 71 percent of graduates from the past two years went on to full-time jobs across a variety of industries, according to academy officials. Amy J. Ko, a University of Washington computer scientist who researches computing education, calls under 80 percent typical for the coding schools she has studied but notes that one of her department\u2019s own undergraduate programs has a 95 percent job placement rate.\nThe academy also takes credit for spawning 62 apps and 13 businesses, such as an animation workshop for kids and a captioning tool for conferences. But Apple, the university, and another major funder\u2014the Gilbert Family Foundation\u2014declined to disclose the graduate-by-graduate employment data that the foundation requires the program to collect and share.\nIn response to a public records request, the university told WIRED in July that no such reports exist. On Tuesday, the university provided reports for some years but did not allow for a comprehensive analysis.\nSusan Prescott, Apple\u2019s vice president of worldwide developer relations, says most graduates take on roles that involve coding, design, project management, and marketing skills honed at the academy. In the company\u2019s view, alumni success isn\u2019t fully captured in statistics. \u201cWe\u2019re excited to continue our partnership with Michigan State University and partners across the state to build on this success,\u201d Prescott says.\nShukwit adds that the academy\u2019s primary goal is to teach teamwork, research, and technology skills relevant to any career students choose to pursue in the future. But the academy still carries risks for students. The emphasis is on learning Apple operating systems like iOS, and two graduates said their limited proficiency in competing platforms such as Android hurt their ability to find jobs.\nThe broader reality facing students is the global mobile app economy is growing more slowly than in the past, and generative AI coding tools are threatening to wipe out some entry-level software engineering jobs. Underwriting a program built around iOS development may no longer be sensible in the years to come.\n\u201cThe job market for junior developers and graduating computer science majors is certainly the worst it's been in some time,\u201d says Ashley Rea Maharaj, an assistant professor in technical communication at University of North Texas.\nBusinesses have long funded and shaped training programs in fields like nursing and construction in the hopes of creating a workforce that benefits them. Last year, industry accounted for about $7.6 billion of voluntary support for US higher education, a small percentage of overall college and university budgets, according to the nonprofit Council for Advancement and Support of Education.\nBut now, tech companies are spending big on education, betting that the rise of AI will muddy traditional career paths. This year, Google committed to spending $1 billion over the next three years on AI education and job training programs in the US. Microsoft plans to outlay $4 billion in the next five years worldwide. A few months ago, Apple and Michigan State began putting millions of dollars a year into a separate program, the Apple Manufacturing Academy, to train small businesses on using AI in their factories.\nThe question looming over the developer academy and the other initiatives is whether they can keep pace with technologies that are evolving every few months and equip students with the resilience to navigate a rapidly shifting job market.\nState Subsidies\nThe Detroit academy, Apple\u2019s first and only in the US, was founded after CEO Tim Cook said the company must play a role in advancing racial equity. Michigan State was a natural partner, because Apple had already agreed to support a separate undergraduate app design program at the university.\nPublic records show Apple signed a three-year \u201cscientific and technological cooperation agreement\u201d with Michigan State in May 2021\u2014several months after the initial announcement of the academy.\nThat announcement said Apple expected to help about 1,000 students each year \u201ccultivate the skills necessary for jobs in the rapidly growing iOS app economy.\u201d Though thousands now apply annually, the academy averages just a few hundred attendees each year across a four-week introductory program and a monthslong intensive course.\nUnder the agreement, Apple provides the curriculum, devices, and some funding, while Michigan State handles instructors, classrooms, and recruitment. Use of any non-Apple software requires Apple\u2019s approval, the agreement states. In practice, officials say students are free to use and explore competing systems.\nThe contract allows for up to 200 students annually in the longer program, in which teams of students develop apps for Apple\u2019s App Store. About 50 graduates are invited for a second year, during which teams learn about client relations by developing apps for local organizations, such as the Detroit Historical Museum and the Michigan Black Business Alliance.\nFunding is largely split between the university, Apple, and the Gilbert Family Foundation, the philanthropic organization of Michigan billionaire Dan Gilbert of Quicken Loans and Rocket Mortgage. The foundation committed to $11 million over five years.\nDarnell Adams, a foundation vice president, says the Gilberts wanted to build ties to the Detroit community that Apple or the university\u2014based 90 miles away in Lansing\u2014could not as easily forge. \u201cWe needed to make sure Detroiters were equipped with the right skills to compete for tech jobs,\u201d he says. The university\u2019s Gretter adds the funding arrangement has worked well.\nPete Lasher, a principal at management consultancy Huron who advises higher education institutions on fundraising, says he\u2019s encouraged by the university\u2019s \u201cskin\u201d in the deal because it typically leads to healthy alignment.\nPreviously unreported funding records for the academy\u2019s first four years show Apple contributed about $11.6 million. Gifts from the foundation and the university\u2019s credit union accounted for over $9.4 million. Nearly $2.6 million came from the state and non-academy students\u2019 tuition.\nAn additional $6 million from the state, effectively from taxpayers, helped cover the cost of living checks. The agreement between Apple and Michigan State requires the university to offer stipends \u201cto ensure equal opportunities.\u201d\nSeveral graduates say they received about $800 to $1,500 a month, and while Fernandez found it insufficient, some viewed the money as transformative. \u201cIt helped me get out of debt, get my first car, and move out of my parents\u2019,\u201d says Tyson Walker II, who acknowledges that making coding \u201chis main thing\u201d remains a distant dream.\nWork in Progress\nSome current students are receiving less financial support than they expected, which Shukwit attributed to an unspecified new Michigan rule that he claims restricted access to state funding.\nA current second-year student in their twenties, who declined to be named out of fear of retaliation, says the stipend for second-year students was cut to $800 from $1,500, amounting to about $9 an hour and forcing them to hold three side jobs. In addition, they say free parking passes, sick days, remote working, and collaboration spaces were also limited. \u201cIt makes it hard to just focus on school,\u201d they say.\nBut they allege the biggest challenge is that many current second-year projects are for small businesses that are unclear about their goals for an app and seem unlikely to hire the students when they finish. \u201cI was feeling hopeful about moving into the tech world, and it just feels like that isn\u2019t really going to be possible because I am not getting the experience,\u201d the student says. \u201cIt creates a big uncertainty for us.\u201d\nBy the time Fernandez graduated, she had been inspired: Coding was no longer \u201crocket science\u201d to her. The politics undergrad was eager to break into software development, but she recognized that she wasn\u2019t the best coder.\nHer team\u2019s final project, which was a drawing app, wasn\u2019t functional and never made it onto the App Store. Upon leaving, Fernandez wanted to develop a diet tracking app. She also hoped to take on freelance projects. None of it materialized. She never felt equipped and secure enough to fully abandon her dream of going to law school.\nFernandez wishes everyone could attend for two years, which is the case at some academies. \u201cIt was only halfway through that I started understanding concepts,\u201d she says.\nApple\u2019s Shukwit says he loves that students want more, and adjustments are possible. The academy reworks its curriculum every few weeks. For example, when students wanted to develop apps for Apple Vision Pro headsets and Apple TV boxes, the academy added relevant workshops.\nMore recently, students have been using AI to auto-generate code, but they must be able to explain it all. AI can\u2019t be a shortcut to avoid learning, Shukwit says. Alumni can also access virtual lessons on generative AI.\nFour years in, the academy has made a small dent in a big problem. The tech industry still has a massive lack of diversity and, in some cases, it has become less transparent about it. Apple\u2019s own data shows that its US tech workforce went from 6 percent Black before the Detroit academy opened to about 3 percent this year.\nFor some alums, their experiences at the academy and their unfulfilled dreams are constantly at their fingertips. \u201cI use the MacBook all the time,\u201d Fernandez says.","source":"wired.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/26\/the-9-top-cybersecurity-startups-from-disrupt-startup-battlefield\/","title":"The 9 top cybersecurity startups from Disrupt Startup Battlefield | TechCrunch","date":1766707200000,"text":"Every year, TechCrunch\u2019s Startup Battlefield pitch contest draws thousands of applicants. We whittle those applications down to the top 200 contenders, and of them, the top 20 compete on the big stage to become the winner, taking home the Startup Battlefield Cup and a cash prize of $100,000. But the remaining 180 startups all blew us away as well in their respective categories and compete in their own pitch competition.\nHere is the full list of the cybersecurity Startup Battlefield 200 selectees, along with a note on why they landed in the competition.\nAIM Intelligence\nWhat it does: AIM offers enterprise cybersecurity products that both protect against new AI-enabled attacks and use AI in that protection.\nWhy it\u2019s noteworthy: AIM uses AI to conduct penetration tests of AI-optimized attacks and to protect corporate AI systems with customized guardrails, and it offers an AI safety planning tool.\nCorgea\nWhat it does: Corgea is an AI-driven enterprise security product that can scan code for flaws as well as find broken code intended to implement security measures such as user authentication.\nWhy it\u2019s noteworthy: The product allows the creation of AI agents that can secure code and works with, it says, any popular language and their libraries.\nCyDeploy\nWhat it does: CyDeploy offers a security product that automates asset discovery and mapping of all the apps and devices on a network.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nWhy it\u2019s noteworthy: Once the assets are mapped, the product creates digital twins to sandbox testing and allows security orgs to use AI to automate other security processes as well.\nCyntegra\nWhat it does: Cyntegra offers a hardware-plus-software solution that prevents ransomware attacks.\nWhy it\u2019s noteworthy: By locking away a secure backup of the system, ransomware doesn\u2019t win. It can restore the operating system, apps, data, and credentials in the minutes after an attack.\nHACKERverse\nWhat it does: HACKERverse\u2019s product deploys autonomous AI agents to implement known hacker attacks against a company\u2019s defenses in \u201cisolated battlefield.\u201d\nWhy it\u2019s noteworthy: The tool tests and verifies that vendor security tools actually work as advertised.\nMill Pond Research\nWhat it does: Mill Pond detects and secures unmanaged AI.\nWhy it\u2019s noteworthy: As employees adopt AI to assist them in their jobs, this tool can detect AI tools that are accessing sensitive data or otherwise creating potential security issues in the organization.\nPolygraf AI\nWhat it does: Polygraf AI offers small language models tuned for cybersecurity purposes.\nWhy it\u2019s noteworthy: Enterprises use the Polygraf models to enforce compliance, protect data, detect unauthorized AI usage, and spot deepfakes, among other examples.\nTruSources\nWhat it does: TruSources can detect AI deepfakes, be they audio, video, images.\nWhy it\u2019s noteworthy: This tech can work in real time for areas like identity authentication, age verification, and identity fraud prevention.\nZEST Security\nWhat it does: AI-powered enterprise security platform that helps infosec teams detect and solve cloud security issues.\nWhy it\u2019s noteworthy: Zest helps teams speedily keep up with and mitigate known but unpatched security vulnerabilities and unifies vulnerability management across clouds and apps.","source":"techcrunch.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/26\/whats-ahead-for-startups-and-vcs-in-2026-investors-weigh-in\/","title":"What's ahead for startups and VCs in 2026? Investors weigh in | TechCrunch","date":1766707200000,"text":"Each year, we ask some top investors what they think the next year will bring. Last year, some investors thought the IPO market would be back up and running by now (which didn\u2019t quite happen), while others thought the momentum behind AI was poised to accelerate (and they were right). This year, TechCrunch did the same thing, talking to five investors from various markets about what they are preparing for in 2026.\nHere is what they said.\nWhat will it take for a founder to raise next year, compared to last year?\nJames Norman, Managing Partner, Black Ops VC\nRaising in 2025 requires a shift from \u201cvisionary\u201d to \u201cbattle-tested.\u201d In previous years, capital has been a primary moat; now investors are wary of \u201cpilot purgatory,\u201d where enterprises test AI solutions without an urgent need to buy. In 2026, the bar is rising. Founders must prove to VCs they have more than just traction; they need a distribution advantage. Investors are digging deeper into repeatable sales engines, proprietary workflow\/processes, and deep subject matter expertise that holds up against the \u201ccapital arms race.\u201d VCs no longer care about who\u2019s first to market with a flashy demo. They want to know who\u2019s building something that can last, earn trust, and scale long-term.\nMorgan Blumberg, Principal, M13\nWe believe the funding markets will always be available for the best founders, but the bar will rise. At the earliest stages, especially in AI application software, I do expect fewer mega seed rounds given intense competition and capital already deployed across many categories. Founders will need to stand out with unique distribution channels or perspectives, not just by relying on a large market opportunity and strong backgrounds. Capital moats have already formed around crowded sectors. At the Series A and B stages, top-quartile rounds will require clear evidence of explosive momentum. The market has now adjusted to these expectations with increased scrutiny on the sustainability of revenue.\nAllen Taylor, Managing Partner, Endeavor Catalyst\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nBigger, faster, better: bigger total addressable market, faster growth, better unit economics. We made 50 investments last year across 25 countries, and we expect to do more this year, so we\u2019re seeing founders at very different stages and in very different markets. The strongest founders aren\u2019t just showing what they\u2019ve built so far \u2014 they\u2019re helping investors understand where the business is going next. Real revenue and real customers still matter, but they\u2019re not sufficient on their own. As an investor, I\u2019m always asking: Where is this company today, and where could it realistically be in the next 12, 18, or 24 months? The founders who raise are the ones who can answer that question clearly and credibly.\nDorothy Chang, Partner, Flybridge Capital\nA lot of founders are finding it easy to build new things because generative AI coding tools are so advanced today. But in truth, those tools are leveling the playing field for everyone, and competition is more fierce than ever. So founders building for venture scale need to make sure that they are (1) truly tackling a big idea, not just something that\u2019s easy to vibe code; (2) building in a problem area that they are uniquely positioned to win; and (3) bringing something proprietary that can\u2019t easily be replicated. This could be a contrarian approach with unique insights, proprietary access to data, deep networks\/relationships, a technological advantage, etc. These aren\u2019t new concepts, but the stakes and expectations are higher than ever.\nShamillah Bankiya, Partner, Dawn Capital\nFor founders selling to enterprises, I think the entire world has gotten smarter on the value that AI can deliver, and as such, proving \u2014 showing line of sight to ROI \u2014 will be more important than ever to investors. Founders who can prove that their products offer much higher value have the best shot at raising capital.\nWhat areas are you looking to invest in and why?\nNorman\nAs a fund, we remain industry-agnostic generalists, but we are always sharpening our lens. Today we\u2019re looking for \u201chigh-context founders.\u201d In a world where AI has commoditized the ability to write code, the winning edge is now lived experience. We want to invest in the founder who has spent years in the trenches of a complex industry and possesses the bespoke expertise that can be 10x\u2019d by AI. For us, the ideal investment is a marriage of deep subject matter expertise and a \u201cday zero\u201d distribution advantage, meaning founders don\u2019t just know what to build but already know exactly who is going to buy it.\nBlumberg\nWe are particularly interested in sleepy or legacy industries that sit outside core tech founder appetite, where AI can offer step-change ROI that drives adoption. These markets have lower competition and moats driven by complexity that often come with less obvious sectors. We also believe 2026 will be a great year for infrastructure supporting foundational model development, as well as frontier research categories like embodied AI and world models. Healthcare remains a major focus given clear signs of buyer demand; we focus on systems of record and platforms rather than point solutions.\nTaylor\nOutside the United States! The best risk-adjusted venture returns are not in Silicon Valley anymore. They are in markets like Poland, Turkey, and Greece.\nWhen you invest across 25 countries in a single year, you stop thinking of venture as something that happens in one place and then spreads outward. Twenty years ago, roughly 90% of venture dollars went to the United States. That flipped in 2018. Today, more than half of venture investment \u2014 and more than half of the world\u2019s unicorns \u2014 are outside the U.S.\nWe see this every day. Founders in Latin America, Africa, the Middle East, and South Asia are building venture-scale companies \u2014 often serving massive markets from the start. In our pipeline, it\u2019s normal to see founders from Venezuela building in Iraq, or from Sudan building global businesses.\nChang\nI\u2019m most interested in founders who are tackling massive problems and leveraging technology for forward progress. I\u2019m rather unmoved by the plethora of startups focused on agentically automating workflows for specific verticals. I\u2019m much more interested in the larger platform shifts that will define this era of technological and societal progress.\nBankiya\nWe\u2019ve seen tremendous impact on software from AI. I think the next frontier is at the intersection of software and hardware. Most of the world\u2019s GDP is locked up in physical industries, and software-only solutions aren\u2019t enough to fully unlock the world\u2019s growth potential.\nDo you think the IPO market will thaw? Why or why not?\nNorman\nYes, the IPO market is likely to thaw, not because conditions are suddenly ideal, but because the system is running out of viable alternatives. We\u2019re approaching a tipping point where the private market\u2019s ability to sustain multibillion-dollar valuations, often disconnected from profitability or liquidity, is wearing thin. Years of \u201cpaper markups\u201d have postponed reality, but they haven\u2019t eliminated it. Companies, boards, and late-stage investors increasingly need a mechanism to reset expectations, generate real liquidity, and re-establish price discovery.\nPrivate credit has acted as a stopgap, extending runways without forcing valuation discipline. But that bridge is starting to look more like a pressure cooker. Debt can delay decisions, not solve structural capital needs, especially for companies built for equity-style growth. At some point, fresh capital becomes necessary, and public markets remain the only place capable of providing it at scale. Their growth narratives and strategic importance can provide the \u201cair cover\u201d needed to reopen the IPO window. Once investors re-engage around category-defining leaders, it creates permission for the broader high-growth software sector to follow.\nBlumberg\nI think we will see a reopening of the IPO markets driven by the backlog of companies planning to list. Many large tech IPOs are anticipated, including darlings like Anthropic and OpenAI, and I believe one of these mega IPOs will drive considerable momentum for others.\nTaylor\nYes \u2014 2026 will be a big year for IPOs in New York as dozens of the top companies simply decide \u201cit\u2019s time.\u201d It will also be a banner year for tech IPOs in places folks are not used to seeing them \u2014 like the stock market in Saudi Arabia.\nI think people underestimate how global the thaw will be. We\u2019ve had nearly four years of muted IPO activity, which has created a backlog of high-quality companies that are ready to be public. When the window opens, it won\u2019t just be U.S. companies stepping through it. There\u2019s already a cohort of major U.S.-listed technology companies from Latin America, including MercadoLibre and Nubank, and there\u2019s another wave right behind them that public-market investors haven\u2019t fully priced in yet. I don\u2019t think all of those companies list in 2026, but several will.\nWhat\u2019s even more unexpected is what happens locally. You\u2019re going to see meaningful technology IPOs in places like Saudi Arabia, listed on the Saudi Stock Exchange (Tadawul). When companies like Tabby [a buy now, pay later outfit] go public locally, it will challenge assumptions about where global tech outcomes happen.\nChang\nWe\u2019re looking to make slightly fewer, more concentrated bets. There is a ton of startup activity, so when we meet founders who really stand out, we want to be able to back up our high conviction with a higher check size and higher ownership percentage.\nBankyia\nI think a hard catalyst would be required to reset the IPO markets \u2014 something akin to mega AI players facing unprecedented cost increases or sharp revenue declines. Think, for example, of energy prices sharply rising, such that it\u2019s unaffordable to offer compute for AI training and inference.\nHow are you looking at the venture market for next year as a fund manager?\nNorman\nWe\u2019re entering what I\u2019d describe as a clearing event for the venture market, and next year will separate durable platforms from transient ones. The fallout will hit Fund I managers who haven\u2019t found their footing, and active Fund II managers struggling with a [distributions-paid-in-capital, or DPI] drought from 2021 vintages. Traditional institutional anchors, particularly university endowments, are effectively in repair mode. After being squeezed by the absence of liquidity in 2021 and 2022, many are leaning on secondaries, pacing adjustments, and portfolio-smoothing strategies just to maintain existing commitments.\nThat means fewer new relationships and far less tolerance for emerging or undifferentiated managers. Stepping into their place are family offices that have moved from passive LP roles to active market forces. They aren\u2019t just filling the \u201cLP oxygen\u201d left by retreating institutions; they are scoping direct mandates and using [registered investment advisors] to hunt for unique, high-conviction strategies.\nIn 2026, there is no viable middle ground. You need to have a clinical, defensible track record and\/or truly unfair access to differentiated deal flow. Lightly grounded generalist positioning, soft networks, and \u201cgood enough\u201d performance won\u2019t survive this cycle.\nBlumberg\nWe believe we are in the early innings of AI transformation, so we expect next year to be a strong vintage. Capital continues to concentrate in a select number of winners so we focus on being selective and operationally supporting our companies to earn our right to concentrate. We are advising our portfolio companies to strengthen their balance sheets in case of a downturn in 2026 while focusing on building for the long term rather than optimizing for fast funding.\nTaylor\nAmazing time to back the boldest founders building for the next 10+ years! From a fund manager\u2019s perspective, 2026 looks strong on both deployment and liquidity. Last year we had 12 liquidity events \u2014 all through M&A and secondaries. That matters because venture has grown dramatically over the last two decades, while paths to liquidity didn\u2019t keep pace. What\u2019s changing now is that venture is building a more complete liquidity toolkit \u2014 M&A, secondaries, and IPOs working together.\nThat\u2019s critical when founders are committing 10, 15, even 20 years to building companies. At the same time, we\u2019re seeing real structural shifts in core sectors. Financial technology, especially stablecoins, moved from experimentation to mainstream adoption in 2025, particularly in markets like Latin America and Africa. In those places, this isn\u2019t speculative technology; it\u2019s infrastructure. That combination is why 2026 looks like a strong year to be deploying capital.\nBankyia\nWe\u2019re still searching for phenomenal European founders building groundbreaking companies. Great companies are formed in all cycles.\nWhat will happen to all the investor and startup interest in AI next year?\nNorman\nIn 2026, the \u201cAI curiosity\u201d that fueled the last two years is being replaced by a demand for application and scale. We are moving from the era of building models to the era of building businesses. The fastest, most innovative companies aren\u2019t the ones with the largest LLMs; they are the ones using AI to solve high-value, domain-specific problems that were previously too complex or too manual to scale. Investors aren\u2019t looking for \u201cAI startups\u201d anymore; we\u2019re looking for exceptional tech founders who have found a way to use this intelligence to 10x the efficiency of a massive, traditional market.\nBlumberg\nWe expect investor and startup interest to continue at all-time highs. However, I do think we will start to see tuck-in acquisitions, acquihires, and wind-downs in highly concentrated sectors such as coding automation, sales automation, marketing, and advertising as market share starts to concentrate in select assets.\nTaylor\nIt will continue. But by the end of 2026, I predict AI will stop being a separate category, as it will just be a part of all new technology companies being built.\nThere\u2019s a lot of breathless talk about AI right now \u2014 and that\u2019s understandable. We\u2019re still very early in understanding what this technology will actually change. In moments like this, excitement tends to run ahead of clarity. Some companies will be transformational, many won\u2019t, and pricing will take time to adjust as real use cases emerge. The opportunity isn\u2019t in labeling everything as \u201cAI.\u201d It\u2019s in understanding where AI meaningfully changes cost structures, speed, or decision-making inside real businesses. That\u2019s where durable value gets created.\nThis is one of those moments when the fog is thick, and that\u2019s when outcomes diverge the most.\nChang\nI don\u2019t see it slowing down anytime soon. We\u2019ve seen a lot of dollars go into infrastructure and theory; this year we\u2019ll see a lot more of that investment more clearly turn into enterprise value at the application level.\nBankyia\nAI will remain a hot topic, barring negative hard catalysts that dramatically change conditions, like an energy crisis or a rise in default rates.\nWhat is something unexpected that could happen in 2026 in the world of venture and startups?\nNorman\nOne of the most unexpected shifts of 2026 will be the quiet end of the \u201cChatGPT-first\u201d era in startups. Not because generative AI loses importance, but because no single model remains the default starting point. GPT is no longer consistently best-in-class for search, image generation, or video, which fundamentally changes how tech companies architect their products. The savvy founders in 2026 have already graduated to a multi-model world, and instead their focus has shifted to specialization.\nFor example, Anthropic has effectively captured the developer\u2019s mindshare because Claude Code is better at building with you, and Google has finally activated its structural advantages. With Gemini 3, it\u2019s pairing top-tier image and video generation with deep multimodal capability and native access to Google\u2019s search and data ecosystem. That combination is proving hard to compete with. Model choice becomes an infrastructure decision, not a moat. The winners in 2026 won\u2019t be the companies that \u201cuse GPT,\u201d but the ones that orchestrate multiple models seamlessly, abstract complexity away from users, and build proprietary workflows on top.\nBlumberg\nWe expect to see many successful startups built with only one or two rounds of capital. AI tooling (especially coding automation) enables many early-stage companies to accomplish profitability without excessive burn. From a technology perspective, while LLMs are expected to be everywhere, companies will start to scale back usage in favor of more controlled use as enterprises prioritize explainability, cost, and reliability. This could drive heavier use of small models, deterministic and probabilistic hybrid models, world models, or simulation modeling.\nTaylor\nThe end of the Russia-Ukraine war will bring about a renaissance of investing in Ukrainian founders, who are some of the best in the world! Two additional things will genuinely surprise people. First, international companies \u2014 especially from Latin America \u2014 going public in New York at scale. Second, major technology IPOs coming out of the Middle East, listed locally. When companies like Tabby go public on the Saudi Stock Exchange (Tadawul), it will reset expectations about where global tech leadership lives.","source":"techcrunch.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/26\/treat-yourself-the-best-smart-glasses-to-buy-with-your-holiday-gift-money\/","title":"Treat yourself: The best smart glasses to buy with your holiday gift money | TechCrunch","date":1766707200000,"text":"Although smart glasses have been around for a while, the technology is getting more advanced each year, so much so that Meta CEO Mark Zuckerberg claims the wearables will replace smartphones in the next decade (though many people disagree, of course).\nSmart glasses are no longer just a futuristic gadget \u2014 they\u2019ve become practical tools that people use to communicate, navigate, track fitness, watch movies, enjoy immersive gaming, and more.\nWhether you\u2019re looking to get a pair for yourself, buy one as a gift, or simply explore what\u2019s available on the market today, we\u2019ve compiled a list of the most compelling options. This list features smart glasses with and without displays, designed for everyday wear, sports, work, and gaming.\nWe\u2019ve also highlighted upcoming launches to watch as the smart glasses market heats up.\nRay-Ban Meta (Gen 2)\nThe nice thing about the Ray-Ban Meta glasses is that they look like regular glasses, which is great for people who don\u2019t want to compromise on aesthetics. The smart glasses come in many styles, so you can pick a pair based on your personal style.\nThe Ray-Ban Meta Gen 2 glasses have a 12-megapixel camera, open-ear speakers, and five microphones. They offer twice the battery life of their predecessor, lasting up to eight hours with typical use. They can charge to 50% in just 20 minutes, and you can purchase a case that provides an additional 48 hours of battery life.\nThe glasses feature 3K Ultra HD video capture, which is two times more pixels than the previous generation. They come with 32GB of storage and an IPX4 water-resistance rating, which means they\u2019re protected in light rain.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nThey also have a slew of AI features, including the ability to say \u201cHey Meta\u201d to do things like get information and set reminders, along with real-time translation, hands-free photo and video capture, the option to ask about what you\u2019re seeing, and more.\nThe glasses cost $379.\nViture Luma Pro\nThe Viture Luma Pro glasses offer one of the best displays on smart glasses available right now. They feature Sony\u2019s micro-OLED panels to deliver a crisp 1200p image on a 152-inch virtual screen, with a 120 Hz refresh rate, a wide 52-degree field of view, and up to 1,000 nits of perceived brightness. (Nits measure screen brightness \u2014 higher numbers mean you can see the display more clearly in bright environments.)\nYou can plug the Luma Pro XR glasses into many phones, tablets, handhelds, and laptops with USB-C to mirror their display. This makes them a great option for people who want to game on a large, immersive screen, whether they\u2019re on the go or the TV is in use. They\u2019re also useful for frequent travelers and people who want a virtual multi-screen setup for work.\nThey come with built-in support for mild myopia, allowing users with up to -4.0 diopters of nearsightedness to experience a clear virtual display without needing prescription lenses.\nIn terms of aesthetics, they\u2019re a bit chunky, but not overly bulky.\nThey retail for $499 (currently on sale for $449).\nXreal One Pro\nThe Xreal One Pro smart glasses are one of the most premium models available on the market right now, with a $649 price tag. They\u2019re a good choice for people who are already familiar with smart glasses and want an upgrade.\nThe glasses\u2019 flat-prism design allows for a sleek and comfortable fit. They feature a virtual display of up to 171 inches, with 1080p resolution, a refresh rate of up to 120 Hz, and a 57-degree field of view.\nThe Xreal One Pro smart glasses come with built-in Bose speakers and 700 nits of brightness, allowing for use in brighter environments.\nThey are built on Xreal\u2019s custom X1 chip, which enables 3DoF (Three Degrees of Freedom). This means the glasses track your head\u2019s rotational movements, keeping virtual content pinned in space relative to your head. In short, if you turn your head, the virtual screen stays in place rather than moving with you.\nThey feature a built-in button array that lets you quickly change your screen size, distance, color temperature, and more.\nOakley Meta Vanguard\nThe Oakley Meta Vanguard smart glasses are the best option for outdoorsy people and athletes, as they were specifically designed with them in mind. The glasses can capture video in up to 3K resolution and feature a 12-megapixel camera with a 122-degree wide-angle lens.\nThe Oakley Meta Vanguard smart glasses feature a programmable button that can trigger a custom AI prompt, which you can set up using the Meta AI app. All the buttons on the glasses are located underneath to allow athletes to wear helmets comfortably while using them.\nThey offer up to nine hours of battery life, or up to six hours of continuous music playback.\nThe glasses come with a charging case that can provide an additional 36 hours of charge on the go. Plus, you can charge the glasses to 50% in 20 minutes via the charging case.\nThey also feature a five-microphone array optimized to reduce wind noise while on calls, messaging, or using Meta AI with your voice. The glasses have an IP67 dust and water-resistance rating for use during intense workouts (this rating means they can withstand being submerged in up to 1 meter of water for 30 minutes).\nThe Oakley Meta Vanguard smart glasses cost $499.\nRayNeo Air 3s\nThe RayNeo Air 3s are a great budget-friendly choice for anyone just getting started with smart glasses. They offer impressive visuals and features for the price and are great for light gaming or watching content on long journeys.\nThey feature micro\u2011OLED displays that create a virtual 201\u2011inch screen at 1080p resolution, with a 120 Hz refresh rate, a 46-degree field of view, and about 650 nits of perceived brightness.\nWhile the field of view is narrower than some premium models and they don\u2019t have head tracking, they\u2019re a good option for people who simply want an external display on the go without breaking the bank.\nThe glasses use a micro\u2011OLED \u201cHueView\u201d display with eye protection features like 3840 Hz hybrid dimming and low blue\u2011light certification to reduce eye strain during extended use.\nThe RayNeo Air 3s cost $269 (currently on sale for $219).\nUpcoming launches to keep your eye on\nProject Aura: Google and Xreal are collaborating on a pair of Android XR-powered glasses that are equipped with a 70-degree field of view and optical see-through technology. The glasses can function as an extended workspace or entertainment device, allowing the user to use Google\u2019s suite of products or stream video as they would on a more advanced headset. The glasses are launching next year.\nSnap Specs: Snap is set to release a lightweight consumer version of its AR glasses in 2026, bringing many of the same augmented reality and AI features found in the developer-focused Spectacles 5. The new glasses are expected to be smaller and lighter, making them more discreet and comfortable to wear in public compared to their bulky predecessors.\nApple AI smart glasses: Apple is reportedly sidelining plans to overhaul its Vision Pro headset to focus on developing AI smart glasses that can compete with products from Meta. They will have many of the same features available on the social media giant\u2019s current smart glasses. The glasses are expected to debut in 2026.","source":"techcrunch.com"}
{"url":"https:\/\/www.wired.com\/story\/uncanny-valley-podcast-new-chapter-for-uncanny-valley\/","title":"Introducing a New Chapter for \u2018Uncanny Valley\u2019","date":1766102400000,"text":"As the year draws to an end, hosts Michael Calore and Lauren Goode have some news to share with listeners. But first, they head over to the pool to unwind and reflect on what they\u2019ll be watching for in the tech space this upcoming year, and what they think should stay behind.\nArticles mentioned in this episode:\n- OpenAI's Big Bet That Jony Ive Can Make AI Hardware Work\n- Tech CEOs Praise Donald Trump at White House Dinner\nYou can follow Michael Calore on Bluesky at @snackfight, Lauren Goode on Bluesky at @laurengoode, Brian Barrett on Bluesky at @brbarrett, and Zo\u00eb Schiffer on Bluesky at @zoeschiffer. Write to us at uncannyvalley@wired.com.\nHow to Listen\nYou can always listen to this week's podcast through the audio player on this page, but if you want to subscribe for free to get every episode, here's how:\nIf you're on an iPhone or iPad, open the app called Podcasts, or just tap this link. You can also download an app like Overcast or Pocket Casts and search for \u201cuncanny valley.\u201d We\u2019re on Spotify too.\nTranscript\nNote: This is an automated transcript, which may contain errors.\nMichael Calore: Hey Lauren, how's it going?\nLauren Goode: It's going pretty good. How are you?\nMichael Calore: Good. The pool is very nice.\nLauren Goode: It is so nice. Well, the pool itself is really cold, but this hot tub?\nMichael Calore: Even better.\nLauren Goode: Fantastic. Yeah. Not a bad workday.\nMichael Calore: Not a bad workday. It's a nice way to end the year.\nLauren Goode: Is it the end of the year already?\nMichael Calore: It really is.\nLauren Goode: How did we get here? Good lord.\nMichael Calore: I know, I know. But I think it's a good opportunity for us to float and soak and think about the year that was and look forward to next year.\nLauren Goode: We actually have some big news that we're going to be sharing with people today.\nMichael Calore: We do. And the birds have some big news too. The birds are chirping.\nLauren Goode: The birds are in on the pod. What do you call a flock of birds chirping on a podcast?\nMichael Calore: I don't know. The comment section? Should we get going?\nLauren Goode: Yeah, let's do it.\nMichael Calore: All right.\nThis is WIRED's Uncanny Valley, a show about the people, power, and influence of Silicon Valley. Today, Lauren and I are bringing you our last episode of the show as you know it. But don't fret, because the show will not be going away. We're going to continue bringing you all of the best of WIRED's reporting, analysis, and informed takes of what's happening in Silicon Valley. But this upcoming year, the show will be entering a new era with our colleagues, Brian Barrett, Zo\u00eb Schiffer, and Leah Feiger as the hosts. We're going to get to hear from them a little bit later. But for now, Lauren and I want to share with you a special round of our WIRED and TIRED picks to take you into the holiday season. I'm Michael Calore, director of consumer tech and culture.\nLauren Goode: And I'm Lauren Goode, senior correspondent.\nMichael Calore: OK. So we're here in the pool, and we are going to reflect on 2025 and talk about what's happening now and what's happening in 2026. And we're going to do it as WIRED and TIRED. And if you know the show, or if you know WIRED as a brand, you know that it's like the rubric that we use, yes, I said rubric, this is a tech podcast, that we use to sort of talk about the trends that are on the way out and the trends that are on the way in and offer some commentary.\nLauren Goode: I have to say, last night, Mike, in preparation for this pool pod, I went on to one of the social media sites and looked up WIRED and TIRED to see if people were still using this rubric as a meme. And it's still a thing.\nMichael Calore: Oh, totally.\nLauren Goode: People still use WIRED and TIRED, all the time and it really warmed my heart.\nMichael Calore: Totally.\nLauren Goode: Yeah. So let's do it.\nMichael Calore: OK.\nLauren Goode: I have to wipe off my glasses from the steam of the pool so I can see the future. OK. All right. I'm ready. Why don't you go first? What is your first WIRED and TIRED for this year?\nMichael Calore: OK. I would say TIRED is voice-activated AI assistance in hardware. So talking to your watch, talking to your pendant, talking to your smart ring, talking to your glasses. And the thing that is WIRED going into the future is silent AI assistance.\nLauren Goode: Wait, OK. Let's unpack this a little bit, because to hear any pundit or person who's on the inside of AI talk about it, like multimodal and voice agents are the thing, are the future. And you're saying, \"Nope, that's TIRED.\"\nMichael Calore: Yes.\nLauren Goode: And instead it's\u2014explain this.\nMichael Calore: OK. So I think the big dream at the end of this tunnel of despair that we're in with AI right now is that it's just going to work ambiently. And people have been trying to do this for a long time. The term ambient computing has been around for a decade at this point, at least. The idea that you walk into your home and your home just reacts to your presence and does all of the things that you want it to do right when you walk in without you having to press a button or say a wake word or even tap the side of your glasses and start talking to Meta AI or whatever it is. There's no action on your part. It just knows that you're there, and it does it. And you do this through a combination of sensors and through location and just ambient awareness so your devices can do the thing that they are guessing that you want done.\nAnd I think that we have been on the cusp of that for a long time. And I think we're going to see devices in the new year that are going to allow you to do that. There's the big OpenAI mystery project coming that they're codeveloping with Jony Ive and a bunch of other designers. Meta just poached the two top design people, two of the top design people from Apple to work on new hardware. So I really think that this is the direction that hardware's going. It's not going to be stuff that's just for your body. It's going to be stuff that is for the home and maybe also your body. And I really do think it's going to be ambient.\nLauren Goode: What do you think the potential downside is to that?\nMichael Calore: Privacy.\nLauren Goode: Of course. I teed you up nicely for that.\nMichael Calore: Yeah, privacy. That's going to be a tricky one. OK. So that's my first one. What's your first one?\nLauren Goode: My first WIRED and TIRED. WIRED, IPOs 2026.\nMichael Calore: OK.\nLauren Goode: IPOs. And TIRED, tech CEOs are no longer our saviors, and they never should have been, really. Yeah. So for IPOs, there are some highly anticipated or rumored initial public offerings that we're kind of expecting to see in 2026. Let's see. We've got, Stripe is one. OpenAI, big one. Anthropic, Notion, Databricks.\nMichael Calore: I've never heard of any of these companies.\nLauren Goode: SpaceX. You've definitely heard of SpaceX.\nMichael Calore: They're all going to IPO?\nLauren Goode: This is just rumors. This is possible IPOs in 2026, or companies that have signaled intent before to go public and they obviously haven't done it by the end of this year. So this would be like a major mega cap liquidity event for tech. It would be huge. I mean, we already know what it's like living in San Francisco amidst the tech millionaires and billionaires. There are going to be so many more newly minted millionaires running around. And then it'll be interesting to see how people sort of reinvest into the tech economy from that. But these are all companies that have raised so much money in venture capital funding and other types of funding over the past decade or so, and so they have these massive valuations, and it's going to be really interesting to see if that is actually sustainable for them once they go public and every quarter they're reporting their financials and they better have some truly aggressive revenue strategies in place, I think.\nMichael Calore: Wow.\nLauren Goode: And we'll get a closer look as journalists at their financials once they are required to share them four times a year.\nMichael Calore: Yeah, that'll be a big day.\nLauren Goode: So yeah, I think that's going to be really interesting to see and then also see some of the trickle-down effects of that. And then, I mean, tech CEOs really\u2014at the end of the day, they're about selling their products. They're about generating more and more and more users as much as humanly possible. They're greasing the palms of officials in the administration to get whatever deals done they need to get done. I think for a long time now, we, as WIRED journalists, have known the way this world works and that even back in the days of Steve Jobs, we shouldn't necessarily have been putting CEOs up in a pedestal, but I think it's very obvious that we should not be looking to them for any kind of moral guidance at this point. And we just continue to follow the industry as it is and report on it as it is and not how we would like it to be.\nMichael Calore: I'm snapping my fingers underwater.\nLauren Goode: Let's do it.\nMichael Calore: Let's take a quick break while we get out of the pool and then we're going to come right back.\nOK. We're back in the studio of our San Francisco offices after a fabulous time floating in the pool.\nLauren Goode: It was a lot colder than I expected, but I don't know why I expected it to be much warmer. It is December.\nMichael Calore: Yeah. You're like, let's book a pool in December.\nLauren Goode: Well, you know what? We said we were going to do it and we had to do it. And there was a hot tub.\nMichael Calore: There was.\nLauren Goode: So we can't complain too much.\nMichael Calore: It all worked out.\nLauren Goode: It all worked out. And right after this, I'm going right back there. I'm retired now.\nMichael Calore: Well, now I think we're ready to introduce the new hosts of Uncanny Valley for the upcoming year, Brian Barrett, Zo\u00eb Schiffer, and Leah Feiger. Leah, our fantastic senior editor of politics is out this week, but Brian and Zo\u00eb are here to say hi. So hi, Brian and Zo\u00eb.\nBrian Barrett: Hello.\nZo\u00eb Schiffer: Hello. I'm trying not to panic that my best tech culture and many other things reporter just said she was retiring, but I'm here. I'm paying attention. I'm locked in.\nLauren Goode: Thank you. Retirement here in the news is for just about five minutes and then we're back into it. So don't worry about that.\nZo\u00eb Schiffer: OK, OK. OK, I support that. You've earned it. I just want to talk about the retirement plan, if you will.\nBrian Barrett: I missed that because I was thinking about the hot tub and how I can get one.\nZo\u00eb Schiffer: Brian is also retired. We're going to be fine, you guys.\nBrian Barrett: As soon as I find that hot tub, I'm out.\nMichael Calore: So Zo\u00eb, I think a lot of our listeners are already familiar with you from our news episodes. You are our director of business and industry here at WIRED, and you always have your finger on the pulse of all the comings and goings around Silicon Valley. So something that listeners may not know, however, is that you wrote a book about Silicon Valley about Elon Musk and his takeover of Twitter. It's called Extremely Hardcore. Tell us the thing that most surprised you about the book while you were reporting it and writing it.\nZo\u00eb Schiffer: Honestly, that was the easiest reporting I've ever done in my life in a lot of ways because Elon Musk does so little to get people to remain loyal to him, but he's constantly creating new sources all the time. And people in that story in particular were really willing to talk. So I think I kind of went into the book. I wrote the book and thought, I guess the rest of my career is like this. I've just unlocked some incredible quote. And then in fact, no, it's much harder now.\nLauren Goode: Did Elon agree to talk to you for the book?\nZo\u00eb Schiffer: He responded once to one of my messages with a crying laughing emoji. So I would say close friends, comrades.\nMichael Calore: I would say so. I mean, you didn't get the poop emoji.\nLauren Goode: Right. That's an upgrade.\nZo\u00eb Schiffer: I felt like he probably meant to type the poop emoji and then did crying laughing. But I've heard that it's his most, the crying laughing emoji is his most commonly used emoji.\nLauren Goode: Oh, and haven't the kids said that that one's out now? We're not supposed to be using the crying laughing emoji.\nZo\u00eb Schiffer: Yeah. I use it all the time too, so it's embarrassing.\nLauren Goode: No, Zo\u00eb, I remember earlier this year when we first kicked off the Uncanny Valley podcast and we were talking about DOGE's takeover within the government, and this was just a few weeks before, so we hadn't actually seen what was going to happen yet. You correctly predicted that based on Elon Musk's very chaotic takeover of Twitter, that you thought what would happen within DOGE would look similar. How did that turn out?\nZo\u00eb Schiffer: I mean, I do think it turned out similar in a lot of ways. And there's one key difference that I've talked about before, but I'll say it again here. I mean, in terms of the way that he communicated with people, I think there was a lot of similarity, if not subject lines of emails that appeared to be directly lifted from the Twitter takeover, the zero-based budgeting, the kind of idea that you come in, you slash a budget down to zero and you force the people who work for you to justify every single expense. Those things are kind of textbook Elon Musk. But I think that with Twitter and with Elon's other companies, the feedback loop for Elon is pretty small. And that's been beneficial to him because he's a risk-taker. He's willing to make decisions on the fly and then he'll get feedback on those decisions and he'll pivot again.\nAnd you see that work pretty well for him at his companies, at least historically speaking. In government, that feedback loop is a lot longer. And in fact, I think by the time we're starting to see the true ramifications of a lot of his decisions on DOGE, he's already ostensibly taken a step back from that enterprise. And so I think that you're in a really different position if you're making those same, what appear to be rash decisions, but you're not really hearing from people in a timely manner on what happens as a result of that.\nLauren Goode: So does that mean we can expect your next book to be extremely hardcore inside Elon Musk's DOGE?\nZo\u00eb Schiffer: Thank you so much for the question. Absolutely not. No.\nMichael Calore: Well, everybody should read your last book either way.\nZo\u00eb Schiffer: Thank you. I so appreciate that. I agree.\nMichael Calore: OK. So Zo\u00eb we all know. The person that we may not know as much about is Brian Barrett.\nBrian Barrett: What?\nMichael Calore: He is our executive editor here at WIRED. He's been on the show before, but now he's going to be sitting in the host chair.\nZo\u00eb Schiffer: Operates in the shadows.\nBrian Barrett: I'm here every time you talk about Peloton, and that's what this podcast will be from now on, right?\nLauren Goode: I'm looking to sell mine, Brian. Are you interested?\nBrian Barrett: No. No, but I hate that I'm the last true believer.\nMichael Calore: So Brian, you oversee all of our coverage here on WIRED.com and you have oversight into everything that we have cooking every day, but you have a long career in journalism. You were previously the editor-in-chief of Gizmodo, and something that many people don't know is that you spent a couple years as a business reporter for a Japanese newspaper. Please tell us about this.\nBrian Barrett: That's right, the Yomiuri Shimbun. I was a\u2014don't laugh.\nZo\u00eb Schiffer: No, it's serious. I don't know why. You just said it in such a delightful way.\nBrian Barrett: It's a delightful paper. At the time, it was the biggest newspaper in the world with a circulation of 14 million. Preemptively, I will say, I don't speak Japanese and I've never been to Japan. I was in the New York Bureau. I know. I was in the New York Bureau of this office and it was very much a reporting job, which I differentiate from a writing job. I did interviews, I did research. I helped my Japanese counterparts prepare for the stories that they were doing. I would write a few stories for the English language edition, but it was really my first job in journalism and a crash course into how to actually stand in front of someone and ask them hard questions and write it down.\nMichael Calore: My favorite detail about your years at the Yomiuri Shimbun is their baseball reporting. Can you please share this with our audience?\nBrian Barrett: I can. So there were maybe, I think there were five Japanese journalists in the New York office. They were each on a three-year rotation. I think those numbers are roughly right. So there's a couple of business reporters, a politics reporter, and then two sports reporters. One of the sports reporters, their only job was to cover Hideki Matsui, a slugger who came over from Japan to play for New York Yankees. The other sports reporter, their job was everything else. This is how they broke down. So it's great. And I think in fairness, I think there was another, there was also a reporter following around each row at the same time because they were playing at the same time, but that was how they divided up coverage.\nLauren Goode: And our listeners may not know, because Brian, you operate in the shadows as we've established, but you are one of-\nZo\u00eb Schiffer: The shadows of Slack.\nBrian Barrett: I write a lot of stories. I've been on this podcast a bunch of times.\nLauren Goode: Well, that's the thing about your story-writing. You are so fast and so whip smart. I swear just last week when OpenAI announced its deal with Disney, I had woken up on the West Coast and was getting through the news release still about it. And then I went to our website and I was like, \"Oh, Brian already has this story up on the website that he's written and is one of the smartest takes on this whole thing.\" And that's standard. That's just what you do. And so I am delighted to see how this translates into a regular podcast format.\nBrian Barrett: Well, thank you, Lauren. I don't know about smartest, but I try to go fast. I think that is from, I was at Gizmodo, which is part of Gawker Media, which is what it was called back then for five or six years. And those were the days of writing seven or eight stories a day. You always had to be able to find an angle on anything to try to make it interesting. We don't do that anymore, which is good and healthy, but I do at least\u2014the ability to type fast is something that I have taken from that. And then you hopefully pair it with some reporting and some other experience.\nMichael Calore: And we still love a blog here at WIRED.\nLauren Goode: We do.\nBrian Barrett: Yeah.\nLauren Goode: We do still love a blog.\nMichael Calore: Let's talk about some of the other things that we loved by going through our recommendations. Over the years, Lauren and I have been sharing our recommendations with listeners, talking about the things that we are enjoying this week. So let's go around the room and talk about some of the things that us and our new hosts are enjoying. Let's start with Zo\u00eb. What's your recommendation?\nZo\u00eb Schiffer: OK. So I read this photo essay. I was catching up on New York Time magazines over the weekend because I have kids, so there's like stacks and stacks of magazines everywhere that I don't actually have time to read when they come out. But I had a little time on Saturday morning and I looked at this photo essay that the New York Times magazine published back in November about people who have fallen in love with AI chatbots. And I really thought it was the most empathetic and honest look at this phenomenon that I'd read so far. I feel like a lot of coverage of this issue wants to be empathetic, but it ends up feeling like you're laughing at people or othering them in some way because I think to a lot of us, it does sound rather foreign. And this one really didn't feel like that.\nI thought that the mom who's in the story in particular looked like someone I would be friends with. I totally got her. There were two men in the story who really understood their points of view and why these relationships felt really profound and real to them. So yeah, that's my recommendation. I loved it. OK. Brian, what about you?\nBrian Barrett: Here's my recommendation this week, everybody. Are you ready for it?\nZo\u00eb Schiffer: I'm ready.\nBrian Barrett: It's an up-and-coming band. Yeah, yeah. They're known as Twin Dimensions. They are a little bit psychedelic, a little bit post-punk. Michael, would you say that's how\u2014would that be a good way to describe Twin Dimensions?\nMichael Calore: Sure. Yes.\nBrian Barrett: Yeah. Yeah. So a little bit psychedelic, a little bit post-punk. Their latest EP came out in October. It's worth a listen. You should run out your record needle. Am I saying that right? We're on it. Michael, I'm going to check again. And a member of this band is your very own Michael Calore, an-up and-coming musician in the Bay Area.\nZo\u00eb Schiffer: Oh my God.\nLauren Goode: Snaps fingers.\nZo\u00eb Schiffer: Now I understand.\nBrian Barrett: Yeah, they're great. It's great music from a great person, and I assume the rest of the band, also great, but I don't know. I'm not going to vouch for them. I will vouch for Michael Calore. Go check out Twin Dimensions. Go listen to Trans Lunar. Do yourself a favor. Gift it this holiday season by burning a CD of it and giving it to a friend.\nMichael Calore: You can download it on Bandcamp.\nLauren Goode: Cool.\nMichael Calore: Or you can stream it everywhere. It's streaming everywhere.\nLauren Goode: I love that. Is it on Spotify?\nMichael Calore: Yeah, we're everywhere.\nLauren Goode: All right.\nMichael Calore: Yeah. We're Trans Lunar on this one.\nLauren Goode: That's amazing.\nMichael Calore: Thanks, Brian.\nBrian Barrett: Yeah.\nMichael Calore: What do you like about it? What's your favorite song? Name three songs.\nLauren Goode: Oh no, he didn't.\nBrian Barrett: Just off the new album? I would have to say I enjoy Unwound Inside, Paradise and Klik Klak, but for me, it's Careening, the lead track off the album that really distills what Twin Dimensions is all about.\nLauren Goode: Now, Brian did say that he types very fast. So I think-\nZo\u00eb Schiffer: That man came in ready to go.\nBrian Barrett: I think you've evolved a lot since Stray Stars, the 2021 EP in ways that I think real Twin Dimensions heads will appreciate. For all the joking, it is genuinely good music, so please go listen to it.\nLauren Goode: It really is.\nMichael Calore: All right.\nLauren Goode: It's delightful.\nMichael Calore: Brian and Zo\u00eb, thank you so much for being here this week and we look forward to hearing you in the chairs in January.\nZo\u00eb Schiffer: Thank you guys so much for having me.\nBrian Barrett: Thank you guys so much. This is great.\nLauren Goode: Well, this must be a Michael Calore fan club day because part of my recommendation, actually both of my recommendations are inspired by you.\nMichael Calore: Oh no.\nLauren Goode: Well, the first is that you have these cool new boots on. And they're these Chelsea boots.\nMichael Calore: Yep.\nLauren Goode: They look great. I'm looking at them right now.\nMichael Calore: Thank you.\nLauren Goode: And you got them at the flea market.\nMichael Calore: I did, yeah, for 10 bucks. They're Thursdays.\nLauren Goode: They're incredible because they would normally be 20 times that. Does that make sense from a math perspective?\nMichael Calore: Yeah, they're $200.\nLauren Goode: Yeah. OK.\nMichael Calore: I got them for 10 bucks.\nLauren Goode: I'm like, we can do math, even though we're journalists. And so my recommendation would be for the new year, maybe try to be a little more part of the circular economy, like buy secondhand stuff, go to your neighbor's groups, right? Share freely with neighbors' groups, buy-nothing groups, all of that. Just there's so much stuff out there. We don't need all of it.\nMichael Calore: Do you know about the app Lucky Sweater?\nLauren Goode: I do not, but I do now.\nMichael Calore: It's like you can put things that you don't want to wear anymore into the app and trade them with other people who are interested in them.\nLauren Goode: Love it.\nMichael Calore: Yeah.\nLauren Goode: I'm into it. So that would be my first recommendation. And my next recommendation is something that I think is something that maybe not everyone's going to be able to accomplish, but I would say make a podcast with one of your best friends.\nMichael Calore: Oh, that's great.\nLauren Goode: Because we have had this podcast now for\u2014for me, it's been nearly eight years doing this with you. You've been doing it for a decade.\nMichael Calore: Yeah.\nLauren Goode: We talked about this a little bit earlier.\nMichael Calore: Yeah.\nLauren Goode: And it has been so fun, even on the weeks when\u2014sometimes it's not fun to make a thing every single week. Sometimes you have to troubleshoot or something falls through or you're responding and reacting to some really disturbing news in our job quite often. And just being here with you in studio or remotely during a pandemic has just brightened my days. And I'm so appreciative.\nMichael Calore: Well, thank you, Lauren.\nLauren Goode: So thank you for everything Snackfight.\nMichael Calore: Thank you. That's very sweet. I appreciate it.\nLauren Goode: What's your recommendation?\nMichael Calore: Oh, God. Now I just have to say something I just totally wrote.\nLauren Goode: No, you don't. You don't have to reciprocate. It's fine.\nMichael Calore: Oh my goodness. I was going to recommend the new American Giant zipper hoodie.\nLauren Goode: Nice.\nMichael Calore: Yeah. Because going back to the first year of this podcast was the first year that American Giant put out its hoodie, its famous hoodie that our friend and colleague, Farhad Manjoo over at Slate, wrote a story about it and said, \"This is the greatest hoodie ever made.\" And it was like this big hyperbolic declaration, but it actually was a really good hoodie and the company sold scads of them. In fact, they've sold a million of them to date. And now they've completely redesigned it. Well, not completely redesigned it, but it just feels much different.\nIt's softer. It fits a little bit looser and it has all of the same durability appointments that you have grown to love. If you like this hoodie, if you're one of their fans, it has the double-lined hood, it has the elbow patches and it has the big chunky zipper, but it's just softer and nicer and easier to wear. So that's my recommendation. If you've tried the American Giant Zipper hoodie and you have worn it out, or if you just weren't satisfied with it, maybe try the new one because it might be a good fit.\nLauren Goode: Good call. Now we know what your retirement sweatshirt is.\nMichael Calore: Yeah, I guess so.\nLauren Goode: This is also just very stereotypical, I think of any work wife, work husband, relationship. I'm like, \"Oh, I'm going to miss you so much.\" And you're like, \"Here's a sweatshirt.\"\nMichael Calore: And I'm like, \"Buy the hoodie.\" Yeah. Go spend $168 on this hoodie.\nLauren Goode: Right on.\nMichael Calore: I mean, I wasn't prepared. I'm sorry. I prepared for just regular recommendation. I didn't prepare for your heartfelt, lovely recounting of the last-\nLauren Goode: That's OK. I literally came up with it last second because I needed a recommendation and I saw your boots.\nMichael Calore: Well, thanks, Lauren.\nLauren Goode: Anytime.\nMichael Calore: It really has been a really wonderful run and it would not have been so wonderful had you not been here, so thank you.\nLauren Goode: Onto new things in the new year.\nMichael Calore: Yeah, for sure.\nLauren Goode: And thank you to everyone who has listened to us over the years. You can still find us on wired.com and online generally. We'll be there.\nMichael Calore: We will be.\nThank you for listening to Uncanny Valley. If you'd like what you heard today, make sure to follow our show and rate it on your podcast app of choice. If you'd like to get in touch with us with any questions, comments, or show suggestions, you can write to us at uncannyvalley@WIRED.com.\nToday's show was produced by Adriana Tapia. Amar Lal at Macro Sound mixed this episode. Kate Osborn is our executive producer, and Katie Drummond is WIRED's global editorial director.","source":"wired.com"}
{"url":"https:\/\/www.wired.com\/story\/expired-tired-wired-tesla\/","title":"How Elon Musk Won His No Good, Very Bad Year","date":1766361600000,"text":"What a weird time to be Elon Musk.\nThis year opened with the businessman turned political operator throwing what appeared, to Nazis at least, to be a Sieg heil.\nThis spring, activists frequently congregated outside the showrooms of his automaker, Tesla, to protest his foray into the US federal government and cozy relationship with President Trump. They argued that his so-called Department of Government Efficiency, a cost-slashing operation named after a more than decade-old internet meme, didn\u2019t slash much beyond foreign aid. They cheered as Tesla missed Wall Street\u2019s financial expectations.\nIn May, Musk blew up his relationship with the most powerful man on Earth with a few vicious posts on X; the most powerful man on Earth called him a \u201cTRAIN WRECK.\u201d Tesla kept leaking money. DOGE fell victim to Washington infighting; it is now reportedly decentralized and a shadow of the Musk personal fiefdom it once was.\nAnd yet: It seems Elon Musk is getting out of 2025 OK? Or, to put it succinctly: Elon Musk may not be the king of the world, but Elon Musk is still very, very powerful.\nMoney isn\u2019t everything, but when it comes to the richest human in the known universe, it\u2019s worth looking at the numbers. Tesla did a bit of flailing this year (more on that in a sec), and historically, much of Musk\u2019s wealth has been derived from the world\u2019s most valuable automaker. That should be bad news for Musk. But it\u2019s become increasingly clear that the Tesla CEO\u2019s other, private companies\u2014SpaceX, xAI, Neuralink\u2014have kept his portfolio diversified, shielding the aspiring trillionaire from some of the whipsaws of the public market.\nMusk is worth some $462 billion according to Bloomberg, and his stake in Tesla is valued at $140 billion, according to financial filings\u2014less than half of his fortune. Overall, Musk\u2019s wealth is up $29 billion since last year.\nSo how did a guy who began the year as the president's \u201cFirst Buddy\u201d and ended it as a political pariah pull it off? It was, mostly, a tale of two companies: Tesla hitting some white water while SpaceX shot satellites into orbit and snapped up government contracts. Musk, meanwhile, cheered on from X, a website that he appears to have massively overpaid for in 2022. But the investment worked; the platform set the pace for political and social commentary amongst the global right wing for much of 2025.\nBy all accounts, Tesla had a less-than-stellar year. The US electric vehicle market foundered as Trump\u2019s government cut tax subsidies and support for battery and vehicle manufacturing plants. It levied, then pulled back, and then levied again global tariffs that have strained the entire US auto industry. Musk's company wasn\u2019t immune from the effects.\nTesla is reportedly trying but struggling to rid its cars of Chinese components. The Wall Street Journal reported that the US\u2019s rapid shifts in trade policy, especially toward China, have \u201cmade it difficult for the carmaker to formulate a coherent pricing strategy.\u201d\nThe policy changes have created potential issues for Tesla\u2019s bottom line in a very specific way. Various government entities\u2014the US Department of Transportation, the Environmental Protection Agency, the state of California\u2014have historically penalized automakers for failing to meet fleetwide environmental goals. Those manufacturers that still make gas-powered cars have been able to avoid some of these financial dings by buying credits from those that make plenty of emission-free vehicles\u2014electric car makers, for example. About a third of Tesla\u2019s profits since 2014 are estimated to have come from these compliance credits.\nNow the Trump administration has nixed penalties for missing fuel economy standards, and the administration has made clear it wants California\u2019s influential zero-emission vehicle program gone too. Big changes in regulations could take years to come to fruition, but the shifts\u2014brought about by the administration that Musk ushered into the White House\u2014could do lasting damage to Tesla\u2019s revenue stream.\nPlus, Tesla\u2019s cars just didn\u2019t sell well this year. Deliveries are down, especially in Europe, where customers objected to Musk\u2019s spin rightward just as cheaper and well-made Chinese EVs provided more attractive alternatives. In China itself, Tesla sales are down 8 percent through this October.\nMeanwhile, the automaker's current lineup and product strategy feels both tired and overambitious. The Model Y is still the most popular electric vehicle in the world, but a refresh earlier this year doesn\u2019t seem to have boosted sales in the way some investors had hoped. The controversial Cybertruck looks like a flop, with an estimated 16,000 sold through this fall\u2014well short of the 250,000 annual goal Musk set years ago.\nMusk has tried to focus analysts\u2019 attention on Tesla\u2019s robotics and autonomous vehicle efforts, but neither are moving as quickly as he had promised. Musk said in July that about half of the American population would have access to robotaxi rides by the end of the year. But at the end of November, he posted that the number of vehicles operating in Austin, Texas, would \u201croughly\u201d double, which suggests that Tesla\u2019s global total of self-driving taxis is about 60. Half the US population feels far off.\nSpaceX\u2019s fortunes are harder to read, because the company is private and doesn\u2019t share its financial hits and misses. But it sure looked busy from the outside, and it seems to have spent the year shoring up its technology and making clear it has long-term plans to make money.\nSpaceX equals rockets. But the star of 2025\u2019s show was Starlink, the company\u2019s satellite internet service. It has made SpaceX look less like a rocket company with an internet service provider side hustle and more like an ISP with a rocket habit. Elements of Starlink began to report a positive net income within the past year, and SpaceX said in November that it connects 8 million customers to the internet. The company is pushing aggressively into contracts with not only governments and airlines, but smartphone makers as well. SpaceX launched its 10,000th Starlink satellite in October, and it has used an annual record number of missions from its Falcon 9 rockets to launch new satellites, replace old ones, and expand into new markets.\nThat moneymaking is good news for SpaceX\u2019s larger ambitions, including its heavier Starship rockets, which it tested for the 11th time this fall. Starship has taken and will continue to take billions to develop; the upside, though, is the rockets should be able to carry heavier payloads into space. That includes a human moon-landing mission with NASA that could happen in the next several years, possibly in 2028.\nSpeaking of government partnerships, Musk\u2019s company also spent this year locking in lucrative contracts with Washington. In April, SpaceX won $5.9 billion in contracts to support the US Space Force through 2029. It has also previously secured a $102 million Air Force contract to study the role its rockets could play in cargo delivery.\nDespite all this, SpaceX isn\u2019t necessarily destined for success. The company\u2019s ambitions are expensive, and will require it to ace very complex technical challenges before its piggy bank starts to squeal. The company needs to keep cutting through regulatory issues, market by market, as it might have to beat back an increasingly empowered set of competitors, including Amazon\u2019s Leo. Relying on government contracts to bring in money, meanwhile, feels tricky when certain global politicians seem willing to flip their allegiances on a dime\u2014or post, as it were.\nThe effects of a wild year won\u2019t be clear when the calendar flips. Has Musk\u2019s extended foray into politics done lasting damage to his brand? Will his hot-cold relationship with Trump prove to be a boon or a burden? Have the polarizing steps he\u2019s taken this year, and his continued addiction to shitposting, precluded future success?\nThis fall, Musk once again made global headlines when Tesla shareholders voted to pay him up to an unprecedented $1 trillion in the next decade. That gigantic figure carries a heavy asterisk: Musk will have to hit a series of big-time benchmarks to win the whole pot, including leading Tesla to an $8.5 trillion market cap and making it into an undisrupted leader in autonomous vehicles and humanoid robots. This is easier said than done. Musk will still receive billions if he comes up short of these goals, depending on how close he gets. It will be interesting to see whether Musk can actually cash the checks his mouth\u2014or X account\u2014has written.\nKeen observers, meanwhile, spotted a familiar face at a White House state dinner for Saudi Crown Prince Mohammed bin Salman in November: Elon Musk. For now, at least, it seems impossible to keep the richest man in the world down for long.","source":"wired.com"}
{"url":"https:\/\/www.constructiondive.com\/news\/top-construction-legal-issues-2025\/808533\/","title":"Top construction legal news of 2025","date":1766361600000,"text":"This year, contractors had reason to keep their lawyers on speed dial. A new presidential administration brought a flurry of executive orders, tariffs and a record-long government shutdown. All of that meant builders had a lot to track this year.\nFueled by the insights of some of the country\u2019s top construction lawyers, Construction Dive\u2019s Dotted Line column broke down the industry\u2019s biggest concerns to help readers understand how these topics would affect their businesses. Read on for tips on how to conduct a DEI audit, what\u2019s important in data center contracts and why artificial intelligence (still) can\u2019t steal your attorney\u2019s job. Consider yourself served for construction\u2019s top legal questions of 2025.","source":"constructiondive.com"}
{"url":"https:\/\/www.constructiondive.com\/news\/jacobs-louisiana-data-center-hut8\/808553\/","title":"Jacobs starts $10B Louisiana data center","date":1766361600000,"text":"An energy infrastructure provider is moving forward with a multibillion-dollar data center project in West Feliciana Parish, Louisiana, marking another megaproject win for the Bayou State.\nHut 8, a Miami-based company, reported the first phase of construction is underway on its River Bend data center campus, valued at approximately $10 billion, according to a news release. The energy infrastructure company said it expects the facility to begin operations in the second quarter of 2027.\nDallas-based Jacobs will serve as the engineering, procurement and construction management partner for the project, in collaboration with Vertiv, a Westerville, Ohio-based critical digital infrastructure provider, according to a separate Hut 8 news release. J.P. Morgan and Goldman Sachs will serve as loan underwriters on project-level financing expected to cover up to 85% of total costs.\nThe win adds to Jacobs\u2019 growing data center portfolio, which CEO Bob Pragada said has jumped fivefold during the company\u2019s latest fiscal fourth quarter earnings call.\n\u201cJacobs brings decades of global expertise in delivering complex infrastructure for advanced facilities to some of the most discerning clients in the world,\u201d Pragada said in the Hut 8 release. \u201cOur collaboration with Hut 8 reflects the shared discipline and ambition needed to deliver a project we believe will become the benchmark for AI infrastructure.\u201d\nHut 8 has secured an initial 330 megawatts of utility capacity for the campus from Entergy Louisiana, with the potential to scale by up to an additional 1,000 megawatts of utility capacity. At that magnitude, the project would rank among the largest data center campuses globally and among the largest private capital projects in Louisiana history, according to Hut 8.\nThe Hut 8 project adds to a growing pipeline of hyperscale data center construction activity in Louisiana. For example, around this time last year, Meta tapped New York City-based Turner Construction, Redwood City, California-based DPR and Minneapolis-based Mortenson to build a $10 billion data center in Richland Parish near Monroe, Louisiana.\n\u201cLouisiana continues to win,\u201d Gov. Jeff Landry said in the release. \u201cHut 8\u2019s investment in River Bend builds on our track record of attracting global-scale projects in the industries of the future.\u201d\nAt peak construction activity, Hut 8 expects about 1,000 construction workers onsite, according to the release.","source":"constructiondive.com"}
{"url":"https:\/\/www.wired.com\/story\/why-spacex-is-finally-gearing-up-to-go-public\/","title":"Why SpaceX Is Finally Gearing Up to Go Public","date":1765497600000,"text":"SpaceX is planning to raise tens of billions of dollars through an initial public offering next year, multiple outlets have reported, and Ars can confirm. This represents a major change in thinking from the world\u2019s leading space company and its founder, Elon Musk.\nThe Wall Street Journal and The Information first reported about a possible IPO last Friday, and Bloomberg followed that up on Tuesday evening with a report suggesting the company would target a $1.5 trillion valuation. This would allow SpaceX to raise in excess of $30 billion.\nThis is an enormous amount of funding. The largest IPO in history occurred in 2019, when the state-owned Saudi Arabian oil company began public trading as Aramco and raised $29 billion. In terms of revenue, Aramco is a top-five company in the world.\nNow SpaceX is poised to potentially match or exceed this value. That SpaceX would be attractive to public investors is not a surprise\u2014it\u2019s the world\u2019s dominant space company in launch, space-based communications, and much more. For investors seeking unlimited growth, space is the final frontier.\nBut why would Musk take SpaceX public now, at a time when the company\u2019s revenues are surging thanks to the growth of the Starlink Internet constellation? The decision is surprising because Musk has, for so long, resisted going public with SpaceX. He has not enjoyed the public scrutiny of Tesla, and feared that shareholder desires for financial return were not consistent with his ultimate goal of settling Mars.\nData Centers\nArs spoke with multiple people familiar with Musk and his thinking to understand why he would want to take SpaceX public.\nA significant shift in recent years has been the rise of artificial intelligence, which Musk has been involved in since 2015, when he cofounded OpenAI. He later had a falling out with his cofounders and started his own company, xAI, in 2023. At Tesla, he has been pushing smart-driving technology forward and more recently focused on robotics. Musk sees a convergence of these technologies in the near future, which he believes will profoundly change civilization.\nRaising large amounts of money in the next 18 months would allow Musk to have significant capital to deploy at SpaceX as he influences and partakes in this convergence of technology.\nHow can SpaceX play in this space? In the near term, the company plans to develop a modified version of the Starlink satellite to serve as a foundation for building data centers in space. Musk said as much on the social media network he owns, X, in late October: \u201cSpaceX will be doing this.\u201d\nBut using a next-generation Starlink satellite manufactured on Earth is just the beginning of his vision. \u201cThe level beyond that is constructing satellite factories on the Moon and using a mass driver (electromagnetic railgun) to accelerate AI satellites to lunar escape velocity without the need for rockets,\u201d Musk said this weekend on X. \u201cThat scales to >100TW\/year of AI and enables non-trivial progress towards becoming a Kardashev II civilization.\u201d\nBased on some projected analyses, SpaceX is expected to have in the neighborhood of $22 to $24 billion in revenue next year. That is a lot of money\u2014it\u2019s on par with NASA\u2019s annual budget, for example, and SpaceX can deploy its capital far, far more efficiently than the government can. So the company will be able to accomplish a lot. But with a large infusion of cash, SpaceX will be able to go much faster. And it will take a lot of cash to design and build the satellites and launch the rockets to deploy data centers in space.\nAbhi Tripathi, a long-time SpaceX employee who is now director of mission operations at the UC Berkeley Space Sciences Laboratory, believes that once Musk realized Starlink satellites could be architected into a distributed network of data centers, the writing was on the wall.\n\u201cThat is the moment an IPO suddenly came into play after being unlikely for so long,\u201d Tripathi told Ars. \u201cIf you have followed Elon\u2019s tactics, you know that once he commits to something, he leans fully into it. Much of the AI race comes down to amassing and deploying assets that work quicker than your competition. A large war chest resulting from an IPO will greatly help his cause and disadvantage all others.\u201d\nForemost among Musk\u2019s goals right now is to \u201cwin\u201d the battle for artificial intelligence. He is already attacking the problem at xAI and Tesla, and he now seeks to throw SpaceX into the fray as well. Taking SpaceX public and using it to marshal an incredible amount of resources shows he is playing to win.\nWhat About Mars?\nMusk founded SpaceX in 2002 with the goal of one day settling Mars. He has never wavered from that goal, and indeed, the company has made considerable progress in more than two decades. SpaceX now launches more than 90 percent of the world\u2019s mass to orbit, has nearly 90 percent of the satellites in orbit, and backstops a large portion of the US government\u2019s civil and military activities in space. Moreover, with Starship, SpaceX is building the first vehicle that could realistically send humans and a lot of the stuff humans need to survive to Mars one day.\nBut if Musk\u2019s rationale for keeping SpaceX private was to protect the Mars dream, is he abandoning this long-standing aim?\nNot necessarily. It\u2019s likely that Musk sees artificial intelligence as a key part of the Mars vision. Whether one believes the Optimus robot will become a viable product or not, Musk does. And he\u2019s spoken about sending the robots to Mars to make the way smoother for the first human settlers.\nMusk also believes that a larger and more financially robust SpaceX is necessary to undertake the settling of Mars. He understands that NASA will not pay for this, as the civil space agency is in the business of exploration and not settlement. For several years now, he has expressed that it will require about 1 million tons of supplies to be shipped to Mars to make a self-sustaining settlement. This is roughly 1,000 ships, and including refueling, at least 10,000 Starship launches. At $100 million per launch, that\u2019s $1 trillion in launch costs alone.\nMusk has frequently expressed a concern that there may be a limited window for settling Mars. Perhaps financial markets collapse. Perhaps there\u2019s a worse pandemic. Perhaps a large asteroid hits the planet. Taking SpaceX public now is a bet that he can marshal the resources now, during his lifetime, to make Mars City One a reality. He is 54 years old.\nThe plan is not without risks, of course. If AI is something of a bubble, 10 years from now, SpaceX may be sitting on hundreds of billions of dollars\u2019 worth of satellites in space for which there is limited use. Maybe shareholders would rather SpaceX make them multimillionaires than make humans multiplanetary.\nBut Musk has never shied away from risks. So doubling down on his most successful asset in this moment is precisely what one would expect him to do.\nThis story originally appeared on Ars Technica.","source":"wired.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/24\/the-14-top-agtech-food-tech-startups-from-disrupt-startup-battlefield\/","title":"The 14 top agtech, food tech startups from Disrupt Startup Battlefield | TechCrunch","date":1766534400000,"text":"Every year, TechCrunch\u2019s Startup Battlefield pitch contest draws thousands of applicants. We whittle those applications down to the top 200 contenders, and from them, the top 20 compete on the big stage for the Startup Battlefield Cup and a cash prize of $100,000. But the remaining 180 startups blew us away, too, in their respective categories and in their own pitch competition.\nHere is the full list of the agtech and food tech Startup Battlefield 200 selectees, along with a note on why they landed in the competition.\n\u00c4IO\nWhat it does: \u00c4io has developed a method to produce edible fat from agricultural waste.\nWhy it\u2019s noteworthy: \u00c4io has developed a strain of yeast that turns abundant agricultural waste like sawdust into a fat suitable for food and cosmetics.\nAquawise\nWhat it does: Aquawise provides AI-powered water-quality monitoring for shrimp and fish farms using satellite imagery.\nWhy it\u2019s noteworthy: The startup eliminates the need for expensive sensors while offering real-time insights and predictive analytics.\nClave\nWhat it does: Clave offers AI agents that help fast-food restaurant franchises better interact with their data.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nWhy it\u2019s noteworthy: Clave analyzes historical and real-time store data to help franchise restaurants quickly develop promotions that increase sales.\nCredoSense\nWhat it does: CredoSense offers an AI-powered portable plant diagnostic system that measures crop health.\nWhy it\u2019s noteworthy: Crop-health diagnostics are trapped in silos but CredoSense handles a broad spectrum of crop diagnosis tech and data in one small, low-power device.\nForte Biotech\nWhat it does: Forte Biotech has created a patented technology to test for illnesses among prawns in fish farms.\nWhy it\u2019s noteworthy: Developed in partnership with the National University of Singapore (NUS), this tech helps shrimp farmers quickly diagnose common diseases without the need to hire expert help.\nGenesis\nWhat it does: Genesis offers a business intelligence platform for soil data that helps agricultural businesses make better, regenerative decisions about their land assets and crops.\nWhy it\u2019s noteworthy: Genesis says it has collected one of the most comprehensive databases on raw materials that augments soil analysis to increase yields through regenerative practices.\nGreeny Solutions\nWhat it does: Greeny Solutions offers AI-powered software and IoT tools for indoor commercial farming.\nWhy it\u2019s noteworthy: Greeny\u2019s tech promises to automate nutrient dosing, climate control, and disease monitoring to increase yields.\nInstacrops\nWhat it does: Instacrops uses AI, IoT sensors, and satellite imagery to monitor and optimize farming fields.\nWhy it\u2019s noteworthy: Y Combinator grad Instacrops uses hardware sensors and AI agents to help farms respond to crop health indications \u2014 irrigation, fertilization, etc. \u2014 in real time, boosting yields and reducing water usage.\nKadeya\nWhat it does: Kadeya operates beverage vending stations for offices that use reusable bottles, which can be returned and are then cleaned and reused.\nWhy it\u2019s noteworthy: This startup eliminates single-use plastic bottles (or cans) in the workplace, while also providing and cleaning the bottle, thereby eliminating the need for companies to buy beverages in plastic bottles to begin with.\nMUI-Robotics\nWhat it does: MUI-Robotics develops AI scent detection for robots.\nWhy it\u2019s noteworthy: MUI-Robotics is digitizing smell, which not only paves the way for multisensory robotics but also has commercial scent\/odor-detection applications in food, chemical, medical, and environmental applications.\nShin Starr Robotics\nWhat it does: Shin Starr Robotics builds robotics that automate food preparation for meal delivery.\nWhy it\u2019s noteworthy: The autonomous kitchens cook meals in a truck, driven by a human, while en route to a delivery destination. The idea is to deliver restaurant-quality Korean BBQ timed to arrival.\nTensorfield Agriculture\nWhat it does: Tensorfield uses AI-powered robotics to identify and kill weeds without pesticides in densely packed crop beds like carrots, spinach, and lettuce without disturbing the crop or its soil.\nWhy it\u2019s noteworthy: It can identify weeds when they just sprout and injects them with superheated vegetable oil instead of herbicides.\nUnibaio\nWhat it does: Unibaio develops biodegradable polymers that deliver agrochemicals more efficiently.\nWhy it\u2019s noteworthy: The microparticles are a natural polymer derived from shrimp waste and are suitable for over 35 crops.\nVerley\nWhat it does: Verley manufactures bioidentical dairy proteins using a precision fermentation technology.\nWhy it\u2019s noteworthy: Verley helps maintain the supply of dairy protein products while minimizing the environmental impact of dairy farming.","source":"techcrunch.com"}
{"url":"https:\/\/www.wired.com\/story\/pinterst-ai-slop-content\/","title":"Pinterest Users Are Tired of All the AI Slop","date":1766534400000,"text":"For five years, Caitlyn Jones has used Pinterest on a weekly basis to find recipes for her son. In September, Jones spotted a creamy chicken and broccoli slow-cooker recipe, sprinkled with golden cheddar and a pop of parsley. She quickly looked at the ingredients and added them to her grocery list. But just as she was about to start cooking, having already bought everything, one thing stood out: The recipe told her to start by \u201clogging\u201d the chicken into the slow cooker.\nConfused, she clicked on the recipe blog\u2019s About page. An uncannily perfect-looking woman beamed back at her, golden light bouncing off her apron and tousled hair. Jones realized instantly what appeared to be going on: The woman was AI-generated.\n\u201cHi there, I\u2019m Souzan Thorne!\u201d the page read. \u201cI grew up in a home where the kitchen was the heart of everything.\u201d The accompanying images were flawless but odd, the biography vague and generic.\n\u201cIt seems dumb I didn\u2019t catch this sooner, but being in my normal grocery shop rush, I didn\u2019t even think this would be an issue,\u201d says Jones, who lives in California. Backed into a culinary corner, she made the dubious dish, and it wasn\u2019t good: The watery, bland chicken left a bad taste in her mouth.\nNeeding to vent, she turned to the subreddit r\/Pinterest, which has become a town square for disgruntled users. \u201cPinterest is losing everything people loved, which was authentic Pins and authentic people,\u201d she wrote. She says that she has since sworn off the app entirely.\n\u201cAI slop\u201d is a term for low-quality, mass-produced, AI-generated content clogging up the internet, from videos to books to posts on Medium. And Pinterest users say the site is rife with it.\nIt\u2019s an \u201cunappetizing gruel being forcefully fed to us,\u201d wrote Alexios Mantzarlis, director of the Security, Trust, and Safety Initiative at Cornell Tech, in his recently published taxonomy of AI slop. And \u201cSouzan\u201d\u2014for whom a Google search doesn\u2019t turn up a single result\u2014is only the tip of the iceberg.\n\u201cAll platforms have decided this is part of the new normal,\u201d Mantzarlis tells WIRED. \u201cIt is a huge part of the content being produced across the board.\u201d\n\u201cEnshittification\u201d\nPinterest launched in 2010 and marketed itself as a \u201cvisual discovery engine for finding ideas.\u201d The site remained ad-free for years, building a loyal community of creatives. It has since grown to over half a billion active users. But, according to some unhappy users, their feeds have begun to reflect a very different world recently.\nPinterest\u2019s feed is mostly images, which means it\u2019s more susceptible to AI slop than video-led sites, says Mantzarlis, as realistic images are typically easier for models to generate than videos. The platform also funnels users toward outside sites, and those outbound clicks are easier for content farms to monetize than onsite followers.\nAn influx of ads may also be partly to blame. Pinterest has rebranded itself as an \u201cAI-powered shopping assistant.\u201d To do this, it began showering feeds with more targeted ads in late 2022, which can be \u201cgreat content\u201d for users, CEO Bill Ready told investors at the time. When WIRED searched for \u201cballet pumps\u201d on a new Pinterest account using a browser in incognito mode, over 40 percent of the first 73 Pins shown were ads.\nLast year, Pinterest also launched a generative AI tool for advertisers. Synthetic content enhances users\u2019 ability \u201cto discover and act on their inspiration,\u201d the company wrote in an April blog.\nAI slop has proliferated on every social media site in recent years. But Pinterest users say this content betrays the site\u2019s function as a marketplace for trading real-world inspiration.\n\u201cIt is the antithesis of the platform it once was, unabashedly prioritizing consumerism, ad revenue, and non-human slop over the content that carries the entire premise of the site on its shoulders,\u201d says college student Sophia Swatling. Growing up in rural upstate New York, she struggled to find like-minded creatives who shared her hobbies. Pinterest was a lifeline.\n\u201cThe greed and exploitation has become steadily more obtrusive and has now reached a point where the user experience is entirely marred,\u201d says Swatling.\nThe issues Pinterest users raise would fall into a category that Cory Doctorow, the Canadian activist, journalist, and sci-fi author, calls \u201censhittification,\u201d which refers to the gradual decay of internet platforms people rely on due to relentless profit-seeking at the expense of user experience.\nWhile Pinterest\u2019s user count may be growing, that doesn\u2019t mean they like the slop, Doctorow says. New arrivals may feel there\u2019s no alternative, while old ones may hate slop less than they love the Pins and boards they\u2019ve shared and saved over the years, he explains.\nCompanies know that people's digital trails are a \u201cpowerful force,\u201d Doctorow tells WIRED, allowing them to act without penalty. \u201cTo me, that's where enshittification lies, right?\u201d\nGhost Stores\nIf Pinterest hoped that leaning into AI would be enough to accelerate its fortunes, it hasn\u2019t worked out that way. The company's shares tanked 20 percent in November after its third-quarter earnings and revenue outlook fell short of analysts\u2019 expectations.\nClicking on Pins containing what appeared to be AI-generated images on Pinterest took WIRED to blogs featuring generically worded listicles offering vague advice, paired with pictures that have the eerily polished hallmarks of AI. They were also littered with banner ads and pop-ups.\n\"It's like endless window shopping but there is no store, no door, no sign. It's just really nice-looking windows,\u201d says Janet Katz, 60, a long-term Pinterest user from Austin, Texas. When redesigning her living room this year, she kept noticing images where the furniture dimensions didn\u2019t add up\u2013chairs defying physics, coffee tables balanced precariously on two legs.\n\u201cIt\u2019s the decor equivalent of the uncanny valley,\u201d Katz says. \u201cIt looks close to real, but there\u2019s something not quite right.\u201d\nWIRED tried clicking on 25 ads for the search term \u201cballet pumps\u201d on Pinterest, which led to ecommerce sites that followed a pattern: steeply discounted apparel, no physical address, and often featuring a glossy, synthetic-seeming picture of the boutique\u2019s owner paired with an origin story. \u201cI grew up in a family full of love for art, craftsmanship, and tradition,\u201d one such site declares. On two near-identical sites, retired couples announce they\u2019re closing their doors after \u201c26 unforgettable years\u201d in New York City.\nThe boutiques have several hallmarks of a phenomenon known as \u201cghost stores,\u201d an online scam whereby fake websites are created, claiming to sell high-quality products at significant discounts due to closing down.\n\u201cThe whole means of production around these sorts of campaigns has radically changed,\u201d Henry Ajder, a generative AI expert and cofounder of the University of Cambridge\u2019s AI in Business Program, tells WIRED. \u201cIt\u2019s more realistic, it\u2019s less expensive, and it\u2019s more accessible. That all comes together to make a compelling package for saturating platforms with synthetic spam,\u201d he says.\nThe websites did not respond to WIRED\u2019s request for comment. When WIRED shared these sites with Pinterest, they deactivated 15 of them for violating policies that prohibit Pins that link to deceptive, untrustworthy, or unoriginal websites.\n\u201cWhile many people enjoy GenAI content on Pinterest, we know some want to see less of it,\u201d a Pinterest spokesperson told WIRED, referencing tools for users to limit AI-generated content. They added that Pinterest prohibits \u201charmful ads and content, including spam\u2014whether it\u2019s GenAI or not.\u201d\nSearching for Solutions\nThe influx of AI-generated content has made some users paranoid that content from humans is being lost amid the rising tide.\nA common complaint on r\/Pinterest is from users who say their impressions have rapidly dropped for reasons unbeknownst to them, but they suspect that AI is drowning them out. Software engineer Moreno Dizdarevic, who also runs a YouTube channel investigating ecommerce scams, has worked with small businesses who share those complaints.\nOne of his clients, a stay-at-home mom and jewelry maker, no longer receives comments or likes on her Pins, and garners less than 5,000 pageviews each month. She\u2019s found much more success when posting on Instagram or TikTok, says Dizdarevic, because there's \u201cstill a bit more of a human connection,\u201d which offers her an edge.\nIn April, citing complaints from users, Pinterest introduced \u201cGen AI Labels\u201d that disclose when content is \u201cAI modified.\u201d Then, in October, it rolled out tools allowing users to customize how much AI-generated content they see.\nBut the labels only appear once a user clicks on a Pin, not in the feed itself, and they aren\u2019t applied to ads. WIRED found several AI-generated Pins that weren\u2019t labeled as such.\nThe sea of AI-generated user content and ads has created a paradox for tech firms, Ajder says: \u201cHow on earth do you prove that the eyeballs you\u2019re selling are actually eyeballs?\u201d he asks.\nCompanies may shift toward tools that verify human-made content, says Ajder. The French music-streaming service Deezer, for example, pledged to remove fully AI-generated tracks from its algorithmic recommendations, after disclosing in September that such uploads now make up 28 percent of daily submissions, equivalent to 30,000 songs per day.\nFor Jones, though, the transformation on Pinterest already feels complete. What was once a place of authentic inspiration has become, in her words, \u201cdepressing.\u201d","source":"wired.com"}
{"url":"https:\/\/www.wired.com\/story\/scammers-in-china-are-using-ai-generated-images-to-get-refunds\/","title":"Scammers in China Are Using AI-Generated Images to Get Refunds","date":1766102400000,"text":"I don\u2019t want to admit it, but I did spend a lot of money online this holiday shopping season. And unsurprisingly, some of those purchases didn\u2019t meet my expectations. A photobook I bought was damaged in transit, so I snapped a few pictures, emailed them to the merchant, and got a refund. Online shopping platforms have long depended on photos submitted by customers to confirm that refund requests are legitimate. But generative AI is now starting to break that system.\nA Pinch Too Suspicious\nOn the Chinese social media app RedNote, WIRED found at least a dozen posts from ecommerce sellers and customer service representatives complaining about allegedly AI-generated refund claims they\u2019ve received. In one case, a customer complained that the bed sheet they purchased was torn to pieces, but the Chinese characters on the shipping label looked like gibberish. In another, the buyer sent a picture of a coffee mug with cracks that looked like paper tears. \u201cThis is a ceramic cup, not a cardboard cup. Who could tear apart a ceramic cup into layers like this?\u201d the seller wrote.\nThe merchants reported that there are a few product categories where AI-generated damage photos are being abused the most: fresh groceries, low-cost beauty products, and fragile items like ceramic cups. Sellers often don\u2019t ask customers to return these goods before issuing a refund, making them more prone to return scams.\nIn November, a merchant who sells live crabs on Douyin, the Chinese version of TikTok, received a photo from a customer that made it look like most of the crabs she bought arrived already dead, while two others had escaped. The buyer even sent videos showing the dead crabs being poked by a human finger. But something was off.\n\u201cMy family has farmed crabs for over 30 years. We\u2019ve never seen a dead crab whose legs are pointing up,\u201d Gao Jing, the seller, said in a video she later posted on Douyin. But what ultimately gave away the con was the sexes of the crabs. There were two males and four females in the first video, while the second clip had three males and three females. One of them also had nine instead of eight legs.\nGao later reported the fraud to the police, who determined the videos were indeed fabricated and detained the buyer for eight days, according to a police notice Gao shared online. The case drew widespread attention on Chinese social media, in part because it was the first known AI refund scam of its kind to trigger a regulatory response.\nLowering Barriers\nThis problem isn\u2019t unique to China. Forter, a New York-based fraud detection company, estimates that AI-doctored images used in refund claims have increased by more than 15 percent since the start of the year, and are continuing to rise globally.\n\u201cThis trend started in mid-2024, but has accelerated over the past year as image-generation tools have become widely accessible and incredibly easy to use.\u201d says Michael Reitblat, CEO and cofounder of Forter. He adds that the AI doesn\u2019t have to get everything right, as frontline retail workers and refund review teams may not have the time to closely scrutinize each picture.\nReitblat says organized crime groups are using the same tactics as individuals to orchestrate refund fraud at scale. In one case, he says, scammers submitted over a million dollars worth of refund claims using AI-altered images that showed cracks or dents in various home goods. The requests were submitted in a tight time window, seemingly to overwhelm the system, and the fraudsters also used rotating IP addresses to conceal their identity.\nSome sellers are using AI to fight back against AI. A Chinese toy seller demonstrated to WIRED how they feed refund requests to an AI chatbot to analyze if the photos are doctored. But these tools are far from perfect right now. Plus, even with supposed confirmation from a chatbot, ecommerce platforms won\u2019t necessarily always side with the seller. Reitblat warns that retailers might eventually respond by tightening their return policies, but that would hurt the shopping experience of customers acting in good faith.\nThis story echoes an earlier backlash that happened on Chinese digital marketplaces, when sellers were the ones being criticized for using AI-generated product photos. Shoppers complained that buying online had become like gambling, and you never knew if the product that arrived would actually look like the pictures.\nBut really, these trends are two sides of the same problem: Ecommerce relies heavily on trust, and widespread availability of AI is making it increasingly difficult to operate under the assumption that the majority of people are honest actors. Existing guardrails, like AI watermarks, are often too easy to remove. If shopping platforms want systems built for humans to keep working, they\u2019ll need to figure out how to respond, whether with new verification rules, revised refund policies, or better accountability mechanisms for AI-enabled scams.\nThis is an edition of Zeyi Yang and Louise Matsakis\u2019 Made in China newsletter. Read previous newsletters here.","source":"wired.com"}
{"url":"https:\/\/www.wired.com\/story\/openai-child-safety-reports-ncmec\/","title":"OpenAI\u2019s Child Exploitation Reports Increased Sharply This Year","date":1766361600000,"text":"OpenAI sent 80 times as many child exploitation incident reports to the National Center for Missing & Exploited Children during the first half of 2025 as it did during a similar time period in 2024, according to a recent update from the company. The NCMEC\u2019s CyberTipline is a Congressionally authorized clearinghouse for reporting child sexual abuse material (CSAM) and other forms of child exploitation.\nCompanies are required by law to report apparent child exploitation to the CyberTipline. When a company sends a report, NCMEC reviews it and then forwards it to the appropriate law enforcement agency for investigation.\nStatistics related to NCMEC reports can be nuanced. Increased reports can sometimes indicate changes in a platform\u2019s automated moderation, or the criteria it uses to decide whether a report is necessary, rather than necessarily indicating an increase in nefarious activity.\nAdditionally, the same piece of content can be the subject of multiple reports, and a single report can be about multiple pieces of content. Some platforms, including OpenAI, disclose the number of both the reports and the total pieces of content they were about for a more complete picture.\nOpenAI spokesperson Gaby Raila said in a statement that the company made investments toward the end of 2024 \u201cto increase [its] capacity to review and action reports in order to keep pace with current and future user growth.\u201d Raila also said that the time frame corresponds to \u201cthe introduction of more product surfaces that allowed image uploads and the growing popularity of our products, which contributed to the increase in reports.\u201d In August, Nick Turley, vice president and head of ChatGPT, announced that the app had four times the amount of weekly active users than it did the year before.\nDuring the first half of 2025, the number of CyberTipline reports OpenAI sent was roughly the same as the amount of content OpenAI sent the reports about\u201475,027 compared to 74,559. In the first half of 2024, it sent 947 CyberTipline reports about 3,252 pieces of content. Both the number of reports and pieces of content the reports saw a marked increase between the two time periods.\nContent, in this context, could mean multiple things. OpenAI has said that it reports all instances of CSAM, including uploads and requests, to NCMEC. Besides its ChatGPT app, which allows users to upload files\u2014including images\u2014and can generate text and images in response, OpenAI also offers access to its models via API access. The most recent NCMEC count wouldn\u2019t include any reports related to video-generation app Sora, as its September release was after the time frame covered by the update.\nThe spike in reports follows a similar pattern to what NCMEC has observed at the CyberTipline more broadly with the rise of generative AI. The center\u2019s analysis of all CyberTipline data found that reports involving generative AI saw a 1,325 percent increase between 2023 and 2024. NCMEC has not yet released 2025 data, and while other large AI labs like Google publish statistics about the NCMEC reports they\u2019ve made, they don\u2019t specify what percentage of those reports are AI-related.\nOpenAI\u2019s update comes at the end of a year where the company and its competitors have faced increased scrutiny over child safety issues beyond just CSAM. Over the summer, 44 state attorneys general sent a joint letter to multiple AI companies including OpenAI, Meta, Character.AI, and Google, warning that they would \u201cuse every facet of our authority to protect children from exploitation by predatory artificial intelligence products.\u201d Both OpenAI and Character.AI have faced multiple lawsuits from families or on behalf of individuals who allege that the chatbots contributed to their children\u2019s deaths. In the fall, the US Senate Committee on the Judiciary held a hearing on the harms of AI chatbots, and the US Federal Trade Commission launched a market study on AI companion bots that included questions about how companies are mitigating negative impacts, particularly to children. (I was previously employed by the FTC and was assigned to work on the market study prior to leaving the agency.)\nIn recent months, OpenAI has rolled out new safety-focused tools more broadly. In September, OpenAI rolled out several new features for ChatGPT, including parental controls, as part of its work \u201cto give families tools to support their teens\u2019 use of AI.\u201d Parents and their teens can link their accounts, and parents can change their teen\u2019s settings, including by turning off voice mode and memory, removing the ability for ChatGPT to generate images, and opting their kid out of model training. OpenAI said it could also notify parents if their teen\u2019s conversations showed signs of self-harm, and potentially also notify law enforcement if it detected an imminent threat to life and wasn\u2019t able to get in touch with a parent.\nIn late October, to cap off negotiations with the California Department of Justice over its proposed recapitalizations plan, OpenAI agreed to \u201ccontinue to undertake measures to mitigate risks to teens and others in connection with the development and deployment of AI and of AGI.\u201d The following month, OpenAI released its Teen Safety Blueprint, in which it said it was constantly improving its ability to detect child sexual abuse and exploitation material, and reporting confirmed CSAM to relevant authorities, including NCMEC.","source":"wired.com"}
{"url":"https:\/\/www.wired.com\/story\/apple-manufacturing-academy-michigan\/","title":"Apple Engineers Are Inspecting Bacon Packaging to Help Level Up US Manufacturers","date":1765929600000,"text":"About 10 Apple employees spent some of their valuable hours over recent months on a project that might seem unusual for the tech giant: customizing an open source AI tool for ImageTek, a small manufacturer in Springfield, Vermont, whose lines of business include printing millions of labels for food packaging.\nThe Apple engineers developed a computer vision system to automatically identify color errors, and on one run it picked up bacon labels with a far-too-pinkish beige before they got shipped, according to Marji Smith, ImageTek\u2019s president. She says the timely catch helped ImageTek from losing a crucial customer. While not revolutionary, the technology requires finagling that comes easy with experience in AI\u2014something that ImageTek lacks.\n\u201cWe\u2019re not a gigantic company, and we don\u2019t have any AI or software team,\u201d Smith says of the 31-year-old, 54-employee business. \u201cWhat Apple is doing is positively impactful for us.\u201d\nImageTek isn\u2019t an Apple supplier. Instead, the engineering assistance it\u2019s receiving is a previously unreported portion of the $600 billion investment in US manufacturing through 2028 that Apple announced this year. The iPhone maker committed to opening up a server factory in Houston, which it did recently. It also pledged to increase spending with domestic suppliers and educate \u201cthe next generation of US manufacturers.\u201d\nFor a company with 166,000 employees and $112 billion in annual profit last fiscal year, the investment in education is small. In August, the company launched a training program, known as the Apple Manufacturing Academy. It\u2019s run in partnership with Michigan State University, which is receiving $2.5 million from Apple to partially reimburse for classrooms, marketing, and instructors as part of the first 12 months of a three-year deal, according to a contract obtained by WIRED through a public records request.\nThe academy has held free monthly workshops in Detroit to share lessons with and provide networking opportunities for over 100 small-time manufacturers from around the country. What\u2019s significant is that ImageTek and two other participants revealed to WIRED that they are receiving an unexpected bonus in the form of site visits and deep technical support from Apple employees. \u201cI haven\u2019t found any strings attached,\u201d Smith says.\nJamie Herrera, a director of product operations at Apple who oversees the academy, says its goal is to make an impact. \u201cIt takes a little bit more than just what you can get out of a training session,\u201d he says. \u201cWe're able to pair them up with engineers, experts \u2026 and go deeper into, how do we take that learning and start to turn it into application?\u201d\nApple has just one factory, which assembles iMacs in Ireland, and is generally secretive about its manufacturing processes. But its staff have decades of knowledge from collaborating with partners such as Foxconn\u2014mostly outside the US\u2014that make parts or put them into iPhones and other Apple products. Academy participants believe they have been treated to unique candor, including about how Apple recovered from its 2014 Bendgate scandal, in which some iPhone 6 models warped in tight pockets.\nThe company has run a training program for manufacturers in South Korea for several years. By opening up in the US, Apple could show the Trump administration, which is focused on increasing domestic manufacturing, that the company is rolling up its sleeves. That could help it win favors on tariffs and other potentially costly policies. \u201cIt\u2019s goodwill,\u201d says Harry Moser, founder of the industry-supported Reshoring Initiative, which tracks and encourages US manufacturing investments. \u201cIt\u2019s great they are doing it, and there\u2019s very few companies that have the money to do it.\u201d\n| Got a Tip? |\n|---|\n| Are you a current or former Apple employee who wants to talk about what's happening? We'd like to hear from you. Using a nonwork phone or computer, contact the reporter securely on Signal at 415-565-1302. |\nTo a small extent, working with upstarts could provide Apple employees fun opportunities for experimentation that may even inform its own manufacturing.\nHerrera says Apple is not seeking any direct benefit from what he described as the significant investment of labor. \u201cWhat we're looking at is that rising sea for all ships,\u201d he says. \u201cThe fact that we're able to help US manufacturers in any way we can to elevate and accelerate their progress, it's only going to be better for everyone.\u201d\nJobs in US manufacturing have barely budged over the past decade, but trillions of dollars in projects are in the works by Moser\u2019s estimate. Ultimately, a more robust US manufacturing sector could help companies like Apple potentially lower their costs.\n\u201cQuite Transparent\u201d\nSmith, a longtime manufacturing executive, joined ImageTek last year and applied for Apple\u2019s academy at the first chance. \u201cWe see what\u2019s happening with the return of tech manufacturing to the US, and we want to be a part of that,\u201d she says. \u201cWe\u2019re investing and growing a lot right now, and we\u2019re hungry for support.\u201d\nThis year, three people from Apple traveled to the machinery manufacturing region in Vermont colloquially known as Precision Valley to visit ImageTek, which also assembles products including circuit boards, and as of recently, agricultural drones. Smith laid bare to the visitors ImageTek\u2019s challenges. When she mentioned how humidity, worker errors, and machine failures were affecting color quality in label printing, the Apple team suggested setting up a camera and an automated tool to compare an ideal sample to fresh prints.\n\u201cPeople on the internet have been known to argue about the color of stripes on a dress,\u201d Smith says. \u201cHaving a data-based approach was something we needed.\u201d\nBeginning in September, a larger group from Apple has joined 30-minute calls nearly every week to coach ImageTek through the process and hand off code. Smith described the employees as mostly having over a decade of experience at Apple in manufacturing operations and quality.\nThe color comparison is currently done off to the side of the factory. The goal is to integrate the setup directly into the press and expand it to catch errors in other products. Smith says the Apple team has been eager to keep helping. No one has mentioned who owns the code and whether a bill will ever come due.\n\u201cWe haven\u2019t talked about licensing or rights,\u201d Smith says. But with customers such as the bacon maker renewing their contracts with ImageTek because of quality improvements, the company is in its best financial position ever and striving to grow enough to supply Apple someday.\nAmtech Electrocircuits, a family-owned electronics manufacturer based in suburban Detroit, also applied to the manufacturing academy as soon as it could. CEO Jay Patel says he came to the realization that he needed outside expertise to compete with overseas manufacturers and grow the company his father had started. \u201cI will not camp outside an Apple store to get an iPhone,\u201d he says. \u201cBut I will camp outside the manufacturing academy to make sure we get in.\u201d\nPatel did get into the first workshop in August. Ever since, he and his team have been meeting over video with a couple of Apple process engineers for an hour every one or two weeks. They are helping Amtech introduce sensors and analytics tools to reduce downtime in the production of electronics used in agriculture, medicine, and other fields. \u201cWe need to mitigate the waste so we can be more competitive,\u201d Patel says.\n\u201cPilot Purgatory\u201d\nOverall, about 15 companies have received extensive consulting, Apple\u2019s Herrera says. An additional beneficiary has been Walkerton, Indiana-based Polygon, which began making fishing rods about 75 years ago before specializing in industrial products such as tubes that are snaked through the body to remove tumors. Apple isn\u2019t a customer.\nBen Fouch, chief financial officer for Polygon, says older machines give the company troubles; for example, a Haas mill that occasionally punches poorly-located holes in tubes and a centerless grinder that can\u2019t track its output. Workers have to manually inspect thousands of parts a day, limiting production, and holding Polygon back from its goal of doubling annual sales to north of $100 million.\nFouch knew automated sensors could help by, for example, identifying the environmental culprits of the hole-punching issues, but with so many potential options to try he didn\u2019t know where to start. \u201cThe worst thing you can do, in a smaller business especially, is muddle through pilot purgatory, hoping to find a viable product,\u201d he says. \u201cWhen someone else has done it before, they know the viable path, and they can save you the time and the expense.\u201d\nThat\u2019s just what three directors and managers from Apple\u2019s engineering and operations teams offered when Fouch and Quinn Shanahan, who oversees Polygon\u2019s medical device production and special products, visited the manufacturing academy in October and November, respectively. Over what Fouch estimates was five hours, the Apple employees evaluated Polygon\u2019s challenges and applied the industrial engineering equation of Little\u2019s Law\u2014which can identify capacity bottlenecks\u2014to devise solutions.\nThe result was a detailed strategy mapping out sensors and software that could affordably track production and alert about anomalies. Polygon can now count the number of passes the tube makes through the grinder, and it will soon be able to understand whether an overheated motor or other factors could explain the botched hole punching, Shanahan says.\nIf all goes as planned, Polygon will have implemented a working system to address its most significant bottlenecks for no more than $50,000, compared to the $500,000 that an automation consultancy may have charged, according to Fouch. The Apple team is working on visiting Polygon to talk through other upgrades. \u201cThey have walked these paths before,\u201d Fouch says. \u201cWithout their help, it's going to take us much longer.\u201d\nApple\u2019s Herrera says giving small manufacturers a sense of the benefits of automation and other technologies could eventually lead them to work with consultants and invest in more expensive systems.\nTwo other academy participants tell WIRED that they have not received extensive assistance from Apple\u2014Herrera says it comes down to which companies have prepared a \u201cproblem statement\u201d that Apple can help with\u2014but they are working to bring what they learned to their factories. Jack Kosloski, a project engineer at Blue Lake, a plastic-free packaging startup, says it was eye-opening for him to hear about the depth of Apple\u2019s product testing.\nIn one academy session, Apple employees described a robot that wears jeans and simulates bending over as part of stress testing to try to prevent a repeat of Bendgate, Kosloski recalls. \u201cThey go down into the specs of jeans, the materials, to make it the most accurate and reliable data they can have,\u201d Kosloski says. \u201cI had never seen something like that.\u201d\nSeth Greenberg, a senior account manager at Focus Integration\u2014which is developing robots to load pallets\u2014says technical drills and thought exercises led by Apple experts during his visit to the academy in September energized him so much that he happily returned this month.\nLast week, Apple released online courses covering topics including quality control and computer vision to open the academy to a broader audience. Virtual participants are also offered extensive consulting from Apple.\nAs President Trump\u2019s tariffs force some companies to find US manufacturing partners, the recipients of Apple\u2019s help believe they are now better positioned to win some of the business.\nImageTek\u2019s Smith wants to invite Apple engineers back to Vermont to celebrate as its factory expands and off-color bacon labels become a relic. \u201cWe have been very glad they took an interest in us,\u201d she says.","source":"wired.com"}
{"url":"https:\/\/www.globalconstructionreview.com\/holcim-buys-majority-stake-in-peruvian-cement-maker\/","title":"Holcim buys majority stake in Peruvian cement maker - Global Construction Review","date":1766102400000,"text":"Switzerland\u2019s Holcim will take a 50.01% stake in Peruvian materials manufacturer Cementos Pacasmayo, in a deal worth approximately $1.5bn.\nCementos Pacasmayo has a $630m net sales estimate for 2025 and a 28% EBITDA margin.\nThe $1.5bn price tag is based upon an 8.8x multiplier on the 2025 market consensus EBITDA, or a 7.1x multiplier after run-rate synergies, estimated at $40m in the third year.\nCementos Pacasmayo was founded in 1958 and currently has 2,000 employees. It runs three cement plants with a combined capacity of around 5 million tons per year, plus 28 ready-mix and precast concrete plants. Cementos Pacasmayo operates 300 retail outlets that will complement Holcim\u2019s Disensa franchise.\nMiljan Gutovic, Holcim\u2019s chief executive, said: \u201cThe synergistic acquisition of Cementos Pacasmayo is fully in line with our NextGen Growth 2030 strategy to accelerate growth in the attractive Latin America region.\n\u201cThis is an opportunity to continue Cementos Pacasmayo\u2019s exceptional legacy, built on its strong performance culture, its deep commitment to its people and its well regarded brand in Peru. The company is highly cash generative with a complementary portfolio in building materials as well as building solutions.\u201d\nHolcim and Cementos Pacasmayo\u2019s deal is due to be completed in the first half of 2026 and is subject to regulatory approval.\nHolcim recently entered Peru with the acquisition of manufacturers Comacsa, Mixercon and Compa\u00f1\u00eda Minera Luren.\n- Subscribe here to get stories about construction around the world in your inbox three times a week\nFurther Reading:","source":"globalconstructionreview.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/24\/the-9-top-biotech-startups-from-disrupt-startup-battlefield\/","title":"The 9 top biotech startups from Disrupt Startup Battlefield | TechCrunch","date":1766534400000,"text":"Every year, TechCrunch\u2019s Startup Battlefield pitch contest draws thousands of applicants. We whittle those applications down to the top 200 contenders, and of them, the top 20 compete on the big stage to become the winner, taking home the Startup Battlefield Cup and a cash prize of $100,000. But the remaining 180 startups all blew us away as well in their respective categories and compete in their own pitch competition.\nHere is the full list of the biotech and pharma Startup Battlefield 200 selectees, along with a note on why they landed in the competition.\nCasNx\nWhat it does: CasNx has invented a new kind of antivirus treatment for organs from organ donors.\nWhy it\u2019s noteworthy: The startup has invented a gene-editing CRISPR kit that eliminates viruses and installs \u201cuniversal donor\u201d markers while the organ is being preserved outside the body.\nChipiron\nWhat it does: Chipiron is building a light and inexpensive, open full-body MRI machine intended to make MRI cancer diagnostics more widely available.\nWhy it\u2019s noteworthy: The medical MRI machine is being built using a superconducting quantum interference device (SQUID), a highly sensitive magnetometer that can measure extremely weak magnetic fields, more commonly used in array antennas.\nExactics\nWhat it does: Exactics is building a platform that creates rapid diagnostic tests.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nWhy it\u2019s noteworthy: Exactics is attempting to make consumer diagnostic kits more widely available, beginning with at-home screening of Lyme disease, with kits for other illnesses on the roadmap.\nLumos Strategies O\u00dc\nWhat it does: Lumos has created a consumer high-frequency electromagnetic device named Avara, targeted at red blood cells.\nWhy it\u2019s noteworthy: Avara has been designed to provide gentle, non-contact \u201cinductive therapy\u201d to improve sleep, relaxation, and exercise recovery.\nMiraqules\nWhat it does: Miraqules developed a nanotechnology in powder form that mimics blood-clotting proteins.\nWhy it\u2019s noteworthy: This technology provides instant blood clotting and is a unique, potentially lifesaving alternative to traditional wound treatments, particularly when treating patients on the scene of the injury.\nNephrogen\nWhat it does: Nephrogen is creating gene therapy solutions for kidney illnesses.\nWhy it\u2019s noteworthy: Nephrogen is solving the hardest part of the problem when it comes to gene-editing medicines. Its tech uses AI to accurately target gene-editing to the exact cells in the kidney that are causing the illness.\nPraxisPro\nWhat it does: PraxisPro is an AI-powered training system for sales and marketing roles in life science industries.\nWhy it\u2019s noteworthy: The system provides compliance-approved content, complete with simulations and real-time analytics to ensure those who represent life science companies are properly prepared to do so.\nReme-D\nWhat it does: Reme-D is developing reliable and affordable diagnostic tests specifically geared toward underserved communities.\nWhy it\u2019s noteworthy: Reme-D is developing rapid diagnostic tests that are not only particularly affordable but also stable in hot and humid climates.\nSurgicure Technologies\nWhat it does: Surgicure has created a patented solution that more safely and reliably secures endotracheal tubes (ET).\nWhy it\u2019s noteworthy: This device makes ET tubes, the flexible tubes inserted through the mouth or nose during surgeries or other treatments, safer and more comfortable for patients.","source":"techcrunch.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/24\/the-year-data-centers-went-from-backend-to-center-stage\/","title":"The year data centers went from backend to center stage | TechCrunch","date":1766534400000,"text":"There was a time when most Americans had little to no knowledge about their local data center. Long the invisible but critical backbone of the internet, server farms have rarely been a point of interest for folks outside of the tech industry, let alone an issue of particularly captivating political resonance.\nWell, as of 2025, it would appear those days are officially over.\nOver the past 12 months, data centers have inspired protests in dozens of states, as regional activists have sought to combat America\u2019s ever-increasing compute buildup. Data Center Watch, an organization tracking anti-data center activism, writes that there are currently 142 different activist groups across 24 states that are organizing against data center developments.\nActivists have a variety of concerns: the environmental and potential health impacts of these projects, the controversial ways in which AI is being used, and, most importantly, the fact that so many new additions to America\u2019s power grid may be driving up local electricity bills.\nSuch a sudden populist uprising appears to be a natural response to an industry that has grown so quickly that it\u2019s now showing up in people\u2019s backyards. Indeed, as the AI industry has swelled to dizzying heights, so, too, has the cloud computing business. Recent U.S. Census Bureau data shows that, since 2021, construction spending on data centers has skyrocketed a stunning 331%. Spending on these projects totals in the hundreds of billions of dollars. So many new data centers have been proposed in recent months that many experts believe that a majority of them will not \u2014 and, indeed, could not possibly \u2014 be built.\nThis buildout shows no signs of slowing down in the meantime. Major tech giants \u2014 including Google, Meta, Microsoft, and Amazon \u2014 have all announced significant capital expenditure projections for the new year, a majority of which will likely go toward such projects.\nNew AI infrastructure isn\u2019t just being pushed by Silicon Valley but by Washington, D.C., where the Trump administration has made artificial intelligence a central plank of its agenda. The Stargate Project, announced in January, set the stage for 2025\u2019s massive AI infrastructure buildout by heralding a supposed \u201cre-industrialization of the United States.\u201d\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nIn the process of scaling itself exponentially, an industry that once had little public exposure has suddenly been thrust into the limelight \u2014 and is now suffering backlash. Danny Cendejas, an activist with the nonprofit MediaJustice, has been personally involved in a number of actions against data centers, including a protest that took place in Memphis, Tennessee, earlier this year, where locals came out to decry the expansion of Colossus, a project from Elon Musk\u2019s startup, xAI.\nCendejas told TechCrunch that he meets new people every week who express interest in organizing against a data center in their community. \u201cI don\u2019t think this is going to stop anytime soon,\u201d he said. \u201cI think it\u2019s going to keep building, and we\u2019re going to see more wins \u2014 more projects are going to be stopped.\u201d\nEvidence in support of Cendejas\u2019 assessment is everywhere you look. Across the country, communities have reacted to newly announced server farms in much the same way the average person might react to the presence of a highly contagious plague. In Michigan, for instance, where developers are currently eyeing 16 different locations for potential data center construction, protesters recently descended upon the state\u2019s capitol, saying things like: \u201cMichiganders do not want data centers in our yards, in our communities.\u201d Meanwhile, in Wisconsin \u2014 another development hot spot \u2014 angry locals appear to have recently dissuaded Microsoft from using their town as a headquarters for a new 244-acre data center. In Southern California, the tiny city of Imperial Valley recently filed a lawsuit to overturn its county\u2019s approval of a data center project, expressing environmental concerns as the rationale.\nThe discontent surrounding these projects has gotten so intense that politicians believe it could make or break particular candidates at the ballot box. In November, it was reported that rising electricity costs \u2014 which many believe are being driven by the AI boom \u2014 could become a critical issue that determines the 2026 midterm elections.\n\u201cThe whole connection to everybody\u2019s energy bills going up \u2014 I think that\u2019s what\u2019s really made this an issue that is so stark for people,\u201d Cendejas told TechCrunch. \u201cSo many of us are struggling month to month. Meanwhile, there\u2019s this huge expansion of data centers\u2026[People are wondering] Where is all that money coming from? How are our local governments giving away subsidies and public funds to incentivize these projects, when there\u2019s so much need in our communities?\u201d\nIn some cases, protests appear to be working and even halting (if only temporarily) planned developments. Data Center Watch claims that some $64 billion worth of developments have been blocked or delayed as the result of grassroots opposition. Cendejas is certainly a believer in the idea that organized action can halt companies in their tracks. \u201cAll this public pressure is working,\u201d he said, noting that he could sense a \u201cvery palpable anger\u201d around the issue.\nUnsurprisingly, the tech industry is fighting back. Earlier this month, Politico reported that a relatively new trade group, the National Artificial Intelligence Association (NAIA), has been \u201cdistributing talking points to members of Congress and organizing local data center field trips to better pitch voters on their value.\u201d Tech companies, including Meta, have been taking out ad campaigns to sell voters on the economic benefits of data centers, the outlet wrote. In short: The tech industry\u2019s AI hopes are pegged to a compute buildout of epic proportions, so for now it\u2019s safe to say that in 2026 the server surge will continue, as will the backlash and polarization that surround it.","source":"techcrunch.com"}
{"url":"https:\/\/www.globalconstructionreview.com\/bouygues-partnership-to-bring-more-robotics-to-site\/","title":"Bouygues partnership to bring more robotics to site - Global Construction Review","date":1766016000000,"text":"Bouygues Construction has signed a partnership with French company Innodura to bring more robotics to construction sites.\nA team from the two businesses will begin developing robots and a dedicated construction lab able to test technology in real-world conditions.\nThe goal is to reduce the physical strain on builders, increase safety and make the industry more attractive by improving working conditions.\nThe companies have been working together for four years, deploying an inspection robot able to operate in restricted spaces and a drilling robot capable of installing thermal insulation.\nInnodura has previously built a robot arm with a 3D camera that lets it perceive its surroundings while AI algorithms give it a robotic \u201cbrain\u201d.\nMaxime Robin, Innodura\u2019s chief executive, said: \u201cToday, we are combining our skills to pursue a shared ambition: developing, testing and deploying intelligent robotic solutions for construction sites in a short timeframe.\u201d\n- Subscribe here to get stories about construction around the world in your inbox three times a week\nFurther Reading:","source":"globalconstructionreview.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/24\/how-mill-closed-the-deal-with-amazon-and-whole-foods\/","title":"Exclusive: How Mill closed the deal with Amazon and Whole Foods | TechCrunch","date":1766534400000,"text":"Mill may have started with households, but co-founder and CEO Matt Rogers says the food waste startup has long aspired to expand to commercial customers.\n\u201cThis has been part of our plan since our Series A deck,\u201d Rogers told TechCrunch.\nNow, with an official deal locked in with Amazon and Whole Foods the company\u2019s plan to profit from handling other people\u2019s food waste is a bit more public.\nWhole Foods will deploy a commercial-scale version of Mill\u2019s food waste bin in each of its grocery stores beginning in 2027. The bins will grind and dehydrate waste from the produce department, reducing costly landfill fees while also providing feed for the company\u2019s egg producers. Both trim the company\u2019s overhead and lower its ecological footprint.\nAt the same time, Mill\u2019s bins will collect data to help Whole Foods understand what gets wasted and why, helping the grocer further control costs. \u201cUltimately, our goal is not just to make their waste operations more efficient, but also to move upstream so they actually waste less food,\u201d Rogers said.\nThe company started selling food waste bins to households a few years ago. As can be expected from a team that made the Nest thermostat, the devices are well designed and \u2014 to lean on a Silicon Valley clich\u00e9 \u2014 they can be a delight to use. My kids got a kick out of the bins while testing the first and second generations.\n\u201cStarting in consumer was very intentional because you build the proof points, you build the data, the brand, loyalty,\u201d Rogers said. Many members of the Whole Foods team were already familiar with Mill when the two companies started talking.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\u201cIt\u2019s actually kind of our enterprise sales strategy,\u201d Rogers continued. \u201cWe have conversations with senior leadership at our various ideal customers, and if they haven\u2019t had Mill at home yet, we say, \u2018Hey, try Mill at home, see what your family thinks.\u2019 It is a surefire way of getting folks excited.\u201d\nThe startup began having conversations with Whole Foods about a year ago, Rogers said. In the ensuing months, Mill demonstrated the consumer version in some of the chain\u2019s grocery stores.\nMill has also developed an AI that uses a range of sensors to determine whether food that enters the bin should still be on the shelf. Minimizing \u201cshrink\u201d \u2014 the industry\u2019s term for sales lost through waste or theft \u2014 can give grocers an edge in a cutthroat market.\nAdvances in large language models have been key, Rogers said. When he and Mill co-founder Harry Tannenbaum were at Nest, it took dozens of engineers and a \u201cGoogle budget\u201d more than a year to train Nest Cameras to recognize people and packages. With new LLMs, Mill only needed a handful of engineers and far less time to deliver superior results, according to Rogers, who said \u201cAI is a huge enabler.\u201d\nThe use of AI allowed Mill to deliver a commercial version faster, diversifying its customer base and source of revenue.\n\u201cIf you are a single channel, single customer business, you\u2019re fragile,\u201d Rogers said. \u201cI grew up at Apple during the iPod era,\u201d he said. \u201cApple at the time was a single leg business. iPod was like 70% of company revenue. This was why we did the iPhone. Steve [Jobs] pushed us really hard on the iPhone because he was worried that folks like Motorola \u2014 who were working on smartphones at the time \u2014 would start to eat our lunch on the iPod business and that that would crush us. We needed to build another leg of the stool.\u201d\nAnd it seems that Mill isn\u2019t finished adding legs to its figurative stool. Rogers said it\u2019s working on building out a municipal business as well.\n\u201cWe\u2019re continuing to add more legs to the stool and adding more diversity to the business,\u201d he said.","source":"techcrunch.com"}
{"url":"https:\/\/www.constructiondive.com\/news\/house-passes-bill-expedite-data-center-project-permits\/808478\/","title":"House passes bill that could fast-track AI infrastructure projects","date":1766361600000,"text":"Dive Brief:\n- The U.S. House of Representatives on Thursday passed a bipartisan bill to modernize the National Environmental Policy Act and fast-track federal approval of U.S. infrastructure projects, according to a press release from Rep. Jared Golden (D-Maine), one of the bill\u2019s co-sponsors.\n- The Standardizing Permitting and Expediting Economic Development (SPEED) Act seeks to limit the number of federal actions that can trigger NEPA project reviews in an attempt to accelerate the permitting process, according to the bill\u2019s summary. The SPEED Act passed the House in a 221-196 vote.\n- \u201cThe SPEED Act will help launch America into a future where we can effectively innovate and implement to revitalize our infrastructure, meet skyrocketing energy demands, lead the world in the AI race and work in harmony with our natural environment,\u201d Rep. Bruce Westerman (R-Ark.) said in a press release introducing the bill in July.\nDive Insight:\nThe permitting reform bill\u2019s advancement comes as tech giants join the Trump administration in a massive push to expand the country's AI capabilities and infrastructure \u2014 demand that hinges on enterprise success with AI projects.\nPresident Donald Trump launched the Genesis Mission in November, a coordinated national effort to build an integrated AI platform called the American Science and Security Platform. The U.S. Department of Energy on Thursday announced collaboration agreements with 24 tech companies, including Google, AWS, Microsoft, OpenAI and Nvidia to advance the Genesis Mission.\nThe Genesis Mission aims to converge AI and high-performance computing, enabling technology such as AI agents to evaluate experimental outcomes and automate workflows, David Appel, VP of global government, national security and defense at AWS, wrote in a blog post. AWS in November announced an investment of up to $50 billion in AI infrastructure for government agencies.\n\u201cWhat we\u2019re building provides exactly these capabilities to meet the mission\u2019s aggressive 270-day timeline and demonstrates current operating capability,\u201d Appel wrote.\nAs tech companies invest aggressively in AI infrastructure, the SPEED Act\u2019s next steps might not come too soon as it heads to the Senate for approval. CIO concerns about compute capacity amid rising cloud costs are partly fueling the rush toward infrastructure buildouts.\nMicrosoft spent $11.1 billion on long-term assets such as expanding data center sites in the quarter ending Sept. 30. But the company reported struggling with capacity shortages while facing growing demand from AI workloads, an issue felt among other tech companies as well. OpenAI entered into a $38 billion partnership with AWS to expand its cloud infrastructure access to support AI workloads last month.\nData centers will require $6.7 trillion worth of investment to keep up with demand for compute power by 2030, according to management consulting firm McKinsey & Company. Data centers equipped to process AI workloads are projected to need $5.2 trillion in capital expenditures, McKinsey said.\nHyperscalers are attempting to project AI needs based on uncertain factors, such as how enterprises translate AI into business value, according to McKinsey. Enterprises are already struggling to demonstrate ROI from their AI investments.\n\u201cIf companies fail to create meaningful value from AI, demand for compute power could fall short of expectations,\u201d the McKinsey report said. \u201cConversely, transformative AI applications could fuel even greater demand than current projections suggest.\u201d","source":"constructiondive.com"}
{"url":"https:\/\/www.wired.com\/story\/backchannel-2026-predictions-tech-robots-ai\/","title":"6 Scary Predictions for AI in 2026","date":1766102400000,"text":"When OpenAI declared a \u201ccode red\u201d this month to refocus its teams on competing with Google, I couldn\u2019t help but think back to December three years ago when the companies\u2019 roles were reversed. Google was the one blasting the sirens to catch up to OpenAI. What followed the next month, in January 2023, were the first sweeping layoffs in Google\u2019s history. \u201cA difficult decision to set us up for the future,\u201d as the company described it at the time.\nI wonder whether the ChatGPT developer could make similar workforce cuts early next year. This speculation inspired me to come up with a whole set of predictions about what might come in the year ahead. Here\u2019s a look at six of the ideas, fine-tuned with the real intelligence of WIRED colleagues.\nData Center Disinformation\nCommunities across the world are fighting the construction of data centers. In the US, many activists are organizing on social media using tools such as Facebook Groups. The Chinese and Russian governments continue to exploit social media to disseminate disinformation masquerading as real news and authentic opinion. Slowing data center development in the US would be a boon for China and Russia, which are both seeking to surpass the US in industrial and military AI capabilities.\nAustin Wang, a researcher at the nonprofit think tank RAND who has studied China-controlled propaganda farms, says there\u2019s no signs of concerning activity right now. \u201cMany newly established anti-data-center pages seem controlled by real US citizens so far,\u201d Wang says.\nBut as the anti-data-center fervor picks up, China and Russia could try to pile on to the grassroots organizing. And the work has gotten even easier thanks to AI that can quickly generate images and videos to rile up people on social media.\nRobot Demos Everywhere\nIn 2026, tech conferences from the Consumer Electronics Show to Amazon\u2019s hardware event will likely be buzzing about AI-powered robots. Google and other big tech companies have spent years trying to train robots to handle household tasks through repeated practice. But now there\u2019s a fresh round of hype. The type of AI models used in services such as ChatGPT and Gemini are being integrated into robots in hopes that they will handle chores, like folding clothes, with less training and greater accuracy.\nThis past September, Google released a video of a robot sorting trash, compost, and recycling in response to a user\u2019s voice commands. When Google executives take to the stage at the company\u2019s next I\/O conference, I expect them to prompt a robot to take on tasks such as, for example, sliding a pizza into a type of oven it\u2019s never encountered before and, while it cooks, retrieving a half-full Diet Coke from the back of a crowded fridge.\nBarak Turovsky, the recently departed chief AI officer at General Motors and a former leader in Google\u2019s AI division, says advancements in robots\u2019 capabilities are possible because large language models can understand a dishwasher manual, learn how to operate a dishwasher from watching a video, and comprehend how to grab a specific part by deciphering a drawing. \u201cThe next frontier for large language models is the physical world,\u201d he says.\nTo be clear, the showcases of next year will be demonstrations. Selling technology that could physically wreck people\u2019s homes if they err will first require additional testing.\nThe Bubble Deflates\n2025 began with China\u2019s DeepSeek showing the world that you don\u2019t need a ton of cutting-edge GPUs to make a solid AI system. This prompted a fleeting stock market selloff over fears that chip sales would tank. The fears didn\u2019t materialize. But next year may bring a bigger, once-again ephemeral dip. Leading AI companies may need a reset to doubledown on successful investments and trim struggling ventures after a period of torrid growth, and their moves could end up being cast by tech pundits as a sign of overspending on AI data centers and researchers.\nOpenAI has pentupled during the past two years to about 4,500 employees, according to company data. It is fighting many battles\u2014not just against Google\u2014and expanding into many new facets, like designing its own chips alongside Broadcom, so the personnel growth could be warranted. But does it still have the best people in the best roles? Newly onboarded management may see things differently, and that\u2019s why the 10-year-old organization\u2019s first major layoffs may be coming next year. If that happens, other AI labs could follow OpenAI\u2019s lead with their own restructuring.\nOpenAI spokesperson Jason Deutrom says, \"ChatGPT may be everywhere, but we\u2019re still a relatively small team\u201d and that it is \u201cexcited to keep hiring and building more stuff people love in 2026.\"\nSome tech companies will try to launch initial public offerings to cash in on peak valuations before the AI-fueled stock market sours. Analysts who study IPO prospects expect a brimming pipeline to gusher in 2026, with chat service Discord, payments processor Stripe, and cloud platform Databricks among perennial rumored names. That said, preparing for an IPO is a challenging task, and nailing the timing to capitalize on a bullish mood on Wall Street is even more difficult. Companies that miss the coveted \u201cwindow\u201d could add to the wave of workforce cuts.\nTraining Work Agents\nFor years, companies have installed \"bossware\" on workers' computers to monitor for potentially inappropriate behavior. I am not looking forward to a possible reality of the coming year: Surveillance software aimed at recording employees\u2019 work to train AI agents to automate some of the tasks.\nAgentic AI that can reduce manual labor by responding to, say, customer service queries is already spreading. These tools are often trained using data that is synthetically generated by computers or gathered from monitoring the activity of click workers who are paid to simulate work.\nBut as businesses look to automate complex jobs and tasks that are simple but take a lot of steps, they will need training data more specific to their working environments. Cue the software to slurp up user click, scroll, and typing activity. \u201cThe capabilities are there and emerging where one can see this happening,\u201d says Wilneida Negr\u00f3n, a workers\u2019 rights activist who has studied employment tech.\nThe situation could add to worker concerns about job loss. And there\u2019s also the fear that one of these tools inadvertently captures more than it should, including personal information, and accidentally makes it accessible to colleagues.\nAlways On, Always Danger\nAI gadgets like necklaces with always-on microphones that are capable of tracking a user\u2019s every word turned out to be duds in 2025. But AI software that listens to video calls and other audio interactions on the computer were a surprise hit.\nOne option, Granola, uses AI to generate meeting notes without storing a permanent audio recording. \u201cTheir output is a relevant, well-organized and truly useful outline of what took place in a long complex call with lots of participants,\u201d says Javier Soltero, the former head of Google Workspace software. The rub is that Granola can operate without other call participants knowing, though the company advises seeking consent. The company\u2019s \u201cargument seems to be \u2018Well, you could be taking notes and not feel compelled to tell people about it,\u2019\u201d Soltero says.\nThe proliferation of these services is prompting new questions about digital etiquette, accessibility, and the law. I am betting these issues will come to the forefront through at least one major data breach or privacy lawsuit in 2026. \u201cThe question of how AI systems affect third parties, other than the user who\u2019s actually engaging with the system, is important\u2014and agentic AI is likely to make this even more pressing,\u201d says Alicia Solow-Niederman, associate professor of law at George Washington University.\nSome in Silicon Valley are adapting. \u201cI\u2019ve taken the view personally that there\u2019s a lot of instances in which I'm not aware things are being recorded, and that\u2019s scary,\u201d says Talia Goldberg, an investor at venture capital firm Bessemer. Steve Jang, founder and managing partner of Kindred Ventures, says, \u201cAll of AI sits in this gray uncertain area in terms of protocol and usage.\u201d\nAlways-recording wearables and apps seem here to stay\u2014and it\u2019s not all bad. They\u2019re a helpful tool for people who are not only in endless meetings but deaf and reliant on assistive technology. Companies just may need to put in some more guardrails next year.\nRobotaxi Takeover, Without Incident\nUS robotaxi services are poised for a big expansion in 2026. Chiefly, Waymo expects to provide more than 1 million rides per week by the end of next year, up from hundreds of thousands. The Google sibling company, which is reportedly fundraising for $15 billion, will potentially increase service to up to about 25 cities from five, including going abroad to London and Tokyo. Tesla and Amazon-owned Zoox also have announced plans to increase their offerings.\nA popular prediction would be that this buildup in driverless ride-hailing services in the US will lead to the industry\u2019s first deadly accident for which the computer is at fault. Self-driving cars are in dozens of accidents per month, according to estimates from federal authorities. But federal, state, and industry data show robotaxis are rarely the cause, and only a small percentage of the incidents have resulted in deaths to living beings.\nThe number of robotaxis on the road remains limited. While some now serve highways, speeds are generally slow. More likely, accidents caused by human drivers will pile up. As will accidents caused by pseudo-autopilot systems that people overly rely on to maneuver their cars. Robotaxis, though, will be incentivized to play it safe and stay horror-free in 2026.","source":"wired.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/24\/waymo-is-testing-gemini-as-an-in-car-ai-assistant-in-its-robotaxis\/","title":"Waymo is testing Gemini as an in-car AI assistant in its robotaxis | TechCrunch","date":1766534400000,"text":"Waymo appears to be testing adding Google\u2019s Gemini AI chatbot to its robotaxis in an effort to integrate an AI assistant that would accompany riders and answer their queries, according to findings by researcher Jane Manchun Wong.\n\u201cWhile digging through Waymo\u2019s mobile app code, I discovered the complete system prompt for its unreleased Gemini integration,\u201d Wong wrote in a blog. \u201cThe document, internally titled \u2018Waymo Ride Assistant Meta-Prompt,\u2019 is a 1,200+ line specification that defines exactly how the AI assistant is expected to behave inside a Waymo vehicle.\u201d\nThe feature hasn\u2019t shipped in public builds, but Wong says the system prompt makes it clear that this is \u201cmore than a simple chatbot.\u201d The assistant is said to have the ability to answer questions, manage certain in-cabin functions like climate control, and, if required, reassure riders.\n\u201cWhile we have no details to share today, our team is always tinkering with features to make riding with Waymo delightful, seamless, and useful,\u201d Julia Ilina, a spokesperson for Waymo, told TechCrunch. \u201cSome of these may or may not come to our rider experience.\u201d\nThis wouldn\u2019t be the first time Gemini has been integrated into the Alphabet-owned self-driving company\u2019s stack. Waymo says it has used Gemini\u2019s \u201cworld knowledge\u201d to train its autonomous vehicles to navigate complex, rare, and high-stakes scenarios.\nWong writes the assistant is instructed to possess a clear identity and purpose: \u201ca friendly and helpful AI companion integrated into a Waymo autonomous vehicle\u201d whose primary goal is \u201cto enhance the rider\u2019s experience by providing useful information and assistance in a safe, reassuring, and unobtrusive manner.\u201d The bot is directed to use clear, simple language and avoid technical jargon, and is instructed to keep its responses succinct to one to three sentences.\nAccording to the system prompts, when a rider activates the assistant via the in-car screen, Gemini can choose from a set of pre-approved greetings personalized with the rider\u2019s first name. The system can also access contextual data about the rider, like how many Waymo trips they\u2019ve been on.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nThe prompts currently let Gemini access and control in-car features, like the temperature, lighting and music. Notably absent from the function list are volume control, route changes, seat adjustment, and window control, Wong pointed out. If a rider asks for a feature that Gemini can\u2019t control, the bot is to reply with \u201caspirational phrases,\u201d like, \u201cIt\u2019s not something I can do yet.\u201d\nInterestingly, the assistant is directed to maintain a clear distinction between its identity as Gemini the AI bot and the autonomous driving technology (the Waymo Driver). So when replying to a question such as, \u201cHow do you see the road?\u201d Gemini shouldn\u2019t say \u201cI use a combination of sensors,\u201d and instead should reply, \u201cThe Waymo Driver uses a combination of sensors\u2026\u201d\nThe system prompts include a range of compelling tidbits, such as how the bot is meant to handle being asked questions about competitors like Tesla or the now-defunct Cruise, or which trigger keywords will get it to stop talking.\nThe assistant is also directed to avoiding speculating on, explaining, confirming, denying, or commenting on real-time driving actions or specific driving events. So if a passenger asks about a video they saw of a Waymo hitting something, the bot is instructed to not answer directly and deflect.\n\u201cYour role is not to be a spokesperson for the driving system\u2019s performance, and you must not adopt a defensive or apologetic tone,\u201d the prompt reads.\nThe in-car assistant is allowed to answer general knowledge questions like about the weather, the height of the Eiffel Tower, what time the local Trader Joe\u2019s closes, and who won the last World Series. It is not allowed to take real-world actions like ordering food, making reservations, or handling emergencies.\nWaymo isn\u2019t the only company integrating AI assistants into driverless vehicles. Tesla is doing something similar with xAI\u2019s Grok. The two different car assistants serve different functions, however. Gemini appears to be programmed to be more pragmatic and ride-focused, while Grok is pitched more as an in-car buddy that can handle long conversations and remember context from previous questions.","source":"techcrunch.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/24\/italy-tells-meta-to-suspend-its-policy-that-bans-rival-ai-chatbots-from-whatsapp\/","title":"Italy tells Meta to suspend its policy that bans rival AI chatbots from WhatsApp | TechCrunch","date":1766534400000,"text":"Italy has ordered Meta to suspend its policy that bans companies from using WhatsApp\u2019s business tools to offer their own AI chatbots on the popular chat app.\nThe Italian Competition Authority (AGCM) on Wednesday said it had found enough cause in its ongoing investigation into whether Meta was abusing its dominant position in the market to offer its Meta AI chatbot within WhatsApp to order the suspension of the policy.\n\u201cMeta\u2019s conduct appears to constitute an abuse, since it may limit production, market access, or technical developments in the AI Chatbot services market, to the detriment of consumers,\u201d the Authority wrote. \u201cMoreover, while the investigation is ongoing, Meta\u2019s conduct may cause serious and irreparable harm to competition in the affected market, undermining contestability.\u201d\nThe AGCM in November had broadened the scope of an existing investigation into Meta, after the company changed its business API policy in October to ban general-purpose chatbots from being offered on the chat app via the API.\nMeta has argued that its API isn\u2019t designed to be a platform for the distribution of chatbots and that people have more avenues beyond WhatsApp to use AI bots from other companies. The policy change, which goes into effect in January, would affect the availability of AI chatbots from the likes of OpenAI, Perplexity, and Poke on the app.\nThe policy doesn\u2019t affect businesses that are using AI to serve customers on WhatsApp. For instance, a retailer running an AI-powered customer service bot won\u2019t be barred from using the API. Only AI chatbots like ChatGPT or Claude are prohibited from being distributed via the API.\nThe European Commission this month also launched an investigation into the new policy, raising concerns that it may \u201cprevent third-party AI providers from offering their services through WhatsApp in the European Economic Area (\u2018EEA\u2019).\u201d\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nCalling the Authority\u2019s decision \u201cfundamentally flawed,\u201d Meta said WhatsApp\u2019s business API isn\u2019t a route to the market for AI companies.\n\u201cThe emergence of AI chatbots on our Business API put a strain on our systems that they were not designed to support. The Italian authority assumes WhatsApp is somehow a defacto app store. The route to market for AI companies are the app stores themselves, their websites and industry partnerships; not the WhatsApp Business Platform. We will appeal,\u201d Meta said in an emailed statement.\nNote: This story was updated to add Meta\u2019s response to the decision.","source":"techcrunch.com"}
{"url":"https:\/\/www.wired.com\/story\/uncanny-valley-podcast-wired-roundup-tech-politics-trends-2025\/","title":"WIRED Roundup: The 5 Tech and Politics Trends That Shaped 2025","date":1766102400000,"text":"For better or for worse, this year had it all\u2014from the AI industry shaping the global economy and our lives, to the so-called Department of Government Efficiency taking over US federal agencies under Elon Musk\u2019s leadership. In today\u2019s episode, host Zo\u00eb Schiffer and executive editor Brian Barrett get together to reflect on some of this year\u2019s key moments\u2014and how they give us important clues as to what we can expect this upcoming year.\nArticles mentioned in this episode:\n- The AI Data Center Boom Is Warping the US Economy\n- The US Needs an Open Source AI Intervention to Beat China\n- DOGE Is the Deep State\n- ICE Wants to Build Out a 24\/7 Social Media Surveillance Team\n- The FBI's Jeffrey Epstein Prison Video Had Nearly 3 Minutes Cut Out\nYou can follow Zo\u00eb Schiffer on Bluesky at @zoeschiffer and Brian Barrett on Bluesky at @brbarrett. Write to us at uncannyvalley@wired.com.\nHow to Listen\nYou can always listen to this week's podcast through the audio player on this page, but if you want to subscribe for free to get every episode, here's how:\nIf you're on an iPhone or iPad, open the app called Podcasts, or just tap this link. You can also download an app like Overcast or Pocket Casts and search for \u201cUncanny Valley.\u201d We\u2019re on Spotify too.\nTranscript\nNote: This is an automated transcript, which may contain errors.\nZo\u00eb Schiffer: Welcome to WIRED\u2019s Uncanny Valley. I'm Zo\u00eb Schiffer, WIRED's director of business and industry. Today on the show, we're wrapping up our news episode series by reflecting on the trends and stories that shaped 2025. And who better to do that with than Brian Barrett, our executive editor who works tirelessly in the shadows?\nBrian Barrett: Zo\u00eb, thank you. Thank you for having me. Happy to emerge from my shadowy lair.\nZo\u00eb Schiffer: From the dark, dark cave. Thank you.\nBrian Barrett: What a year it's been, and I'm so excited for it to be almost over.\nZo\u00eb Schiffer: Oh, my gosh. Me too. OK. Because it's been quite a year news-wise, safe to say, especially in tech and politics. Honestly, it was a little bit tricky to pick which trends we should discuss today, but we settled on five stories that kind of encapsulate this year pretty well, and I think give us clues as to what is going to be unfolding in 2026. The first one that I want to talk about is dear to my heart, and it's about AI data centers. So we all know that the investment, the amount of money being spent on data centers is absolutely staggering, with companies like Meta, Google, and Microsoft tripling down on AI infrastructure spending this year. But it's not just about the money that's being spent. It's also about how that money is being invested and the domino effect that it's already having on the rest of the tech industry and honestly our entire economy.\nBrian Barrett: I'm glad we're talking about data centers because they sound boring, they should be boring. In a just world they would be, but they're at the center of so much that is going on right now. They are sort of single-handedly propping up the economy in a lot of ways. They are responsible for so much environmental upsetment. They are driving up energy prices for people, and they are powering AI experiences that are sometimes really great and sometimes really not, depending on who's using them and how. If you had said five years ago that the biggest story in tech would be data centers, I don't think anybody would've believed it.\nZo\u00eb Schiffer: I mean, if you had told me that we were going to have multiple reporters whose beat was directly related to data centers, I would've said absolutely boring. Skip. Next. But we're always looking for points of agreement with Peter Thiel, and I think this is one of them. Peter Thiel says that the US really doesn't have another big moonshot project. We don't have a Manhattan Project right now. All we have is artificial intelligence. And I think when he's talking about this, he's saying it as a bad thing. We should have other big initiatives that we're doing. But I think even more specifically within AI, it's like it's AI data centers. And we're hearing a lot of people in this industry, Sam Altman, other executives talk about the fact that it really looks like there's an AI bubble that is forming and expanding possibly.\nAnd the data centers are right at the center of that. I think that a lot of these financial deals are being set up through special purpose vehicles. So the money that is being spent isn't directly on these companies' balance sheets. It's also true that 60 percent of the cost of building a data center is basically just on the GPUs and you need to replace those GPUs every three years. So I think a lot of people are looking at this and getting pretty worried saying, \u201cThe math doesn't look like it's going to work out.\u201d\nBrian Barrett: Yeah. So Michael Burry, who made a name for himself around the housing crisis, right? He was the center of The Big Short. He has made a decently big bet exactly that, that the accounting here is kind of funky, that it is a bubble, as you said, and it's going to burst. Now, he's made bets before that have not panned out.\nZo\u00eb Schiffer: I was going to say.\nBrian Barrett: He is not infallible, but his argument is indicative of the argument that you see, and he is someone who has caught a bubble before. Is 2026, Zo\u00eb, the year that this kind of comes to a head? Even Sam Altman says it's probably a bubble, there's going to be winners and losers. He's obviously betting that he's going to be to the AI bubble, what Google was to the internet bubble. It made it through, it survived, it grew, it dominated the market for years. Do you think that's where we're heading in \u201926 or do we have a little bit more time than that?\nZo\u00eb Schiffer: I actually think we have another good year, good depending on who you are in this space. But I would guess, and this is based mostly on vibes, so take it with a giant grain of salt, but that we have until 2027 until this really starts to come to a head. And one thing that I'm going to be looking at specifically is, I've visited some of these data centers. I've talked to politicians who are really supportive of having data centers in their city and right now it looks like there's not a lot of cost politically speaking in a lot of places. Obviously there is a lot of local pushback, but in a lot of places, these data centers are being built in very red states. There's not a lot of job opportunity. And so they're being heralded as kind of like, \u201cLook, this economic boon that's coming, it's going to be really great for the local economy. There's going to be all these jobs.\u201d\nBut the truth is that, well, one, you need thousands of people to build a data center, but you need far fewer to maintain a data center. So once it's already up and running, those jobs, a lot of them will evaporate. Two, they are very, very resource-intensive. It takes a lot of water to cool a data center. It obviously uses up an absolute ton of energy. And so I think we're going to see a switch. I mean, this is just a prediction, but where it's going to be politically untenable to support a data center and we're going to have an offshoring push in the next three years.\nBrian Barrett: And even those thousands of jobs of people who build the data center aren't always local because a lot of these are specialized people coming in. I live in a red state and there is pushback. They're trying to build a data center near me. It's not enough to actually do anything. The thing's going to get built. But yeah, I do think as people start to put the pieces together of, \u201cOh, my energy bill's higher now. Oh, the jobs aren't here that I though we're going to be.\u201d I agree. I think we're going to see pushback. And that's when we're going to build data centers in space.\nZo\u00eb Schiffer: I was literally just going to say on Mars. We don't have a data center in Santa Barbara where I currently live, but we do live very close to a SpaceX rocket launch site. And for a really politically unengaged community, wow, do people rally against those rocket launches, because it freaks out their dogs and horses and such. And so I would just love to see what happens if they try and build a data center here.\nBrian Barrett: Amazing.\nZo\u00eb Schiffer: OK. So I feel like we would be remiss not to talk about what these data centers are being built for, because it's chatbots, it's ChatGPT, they're being built for inference, meaning not just to train the models, but actually to support millions and millions of people asking chatbots questions and wanting to get a result. One trend that I feel like we really saw pretty acutely this year was the rise of chatbot companions and AI relationships. I'm curious how you feel about that now and how has that feeling, I guess, changed since the start of the year?\nBrian Barrett: It's interesting. I think on the one hand, you've got so many individual incidents where things have gone really wrong with chatbot companions. You've got interactions allegedly leading to suicide or contributing to suicide. You've got some seemingly pretty unhealthy relationships, but at the same time, you also have cases where people seem genuinely happier. They seem like they are filling a void. I don't know. I, personally, it's one of those things where I have been sort of reflexively like, \"Oh no, let's not do that.\" But I recognize that I need to come around like, \"Well, it's not for me, but that doesn't mean it's not a place.\" I think really, most of all, it's still so early in all of this. And I think there needs to be so much more work done to figure out what these relationships are doing to people for good or bad or neither.\nAnd I wish that some of that work had been done on the front end by AI companies themselves before just sort of saying, \"Go have fun. Go have fun with your new AI boyfriend or girlfriend,\" without really understanding what the consequences are, because no one really knows yet. It's too early. So I'm glad to see there are more safeguards in place on a lot of levels from a lot of these places, but it does seem too much too fast. Let's figure out what's going on.\nZo\u00eb Schiffer: So this is something I've been thinking about with mania or what's being called AI psychosis because I think a lot of the time, at least in my experience as a tech reporter, it's like the issue at hand, whether it was misinformation or what have you, there was a technical cause to that, but it felt like it needed a multi-pronged solution, and we were really just looking at tech companies to fix this entire issue that was impacting our democracy. And with AI psychosis or AI mania, I don't know, that really does feel like a tech issue. If the chatbot is telling you that you've discovered some new frontier of physics or whatever, or if it's validating you again and again, and you believe it, that seems like a problem with the chatbot. With AI relationships, I wonder if the issue, when it is an issue, is more complex.\nIf there's something else going on at a societal level and chatbot relationships are a symptom of that rather than a cause, I do think we need more safeguards in place. Absolutely. I think also a lot of other things need to be done at the societal level so that we have more connections with people. It's easier to make those connections. There are more things pushing us to be in community with one another.\nBrian Barrett: Zo\u00eb, we're going to bring back bowling leagues. We're going to bring back bowling leagues. What could be better?\nZo\u00eb Schiffer: So another trend that we followed really closely this year at WIRED is the global competition in making frontier AI models. So we will get into stories and trends that are not centered on AI, I promise, but it would be disingenuous to pretend that AI hasn't been the defining story of our industry this year. And one of the moments in which that became really clear happened pretty early on. If you remember back in January when the Chinese AI research lab, DeepSeek released the R1 open model, and it felt like all hell broke loose.\nBrian Barrett: Yeah. It really came out of, I want to say it came out of nowhere, but it didn't. It came out of China and we shouldn't be surprised that China's doing really great work like this.\nZo\u00eb Schiffer: And there was an actual market impact, right? Investors got kind of freaked out.\nBrian Barrett: Yeah, especially, I mean, Nvidia is really the bellwether for the AI industry at this point. After DeepSeek came out, it lost nearly $600 billion in market cap on January 27. It is the largest single-day loss for a single stock in history.\nZo\u00eb Schiffer: And that was the end of Nvidia.\nBrian Barrett: Yeah. And then no one ever heard from them again.\nZo\u00eb Schiffer: No, they were covered just fine.\nBrian Barrett: They're doing great. No, but that tells you how\u2014I don't want to say inflated, but how much room there is for these sky-high stocks to vacillate. Also, the impact of just a model release. There are dozens of models out there, and the fact that one from China can have that big of an impact really tells you how big a deal everyone thought this was. And they're right, it is. The big thing about DeepSeek R1 to me was the openness of it. It's an open-weight model, anyone can use it. Really in the US, only Meta had really been pursuing that strategy on a pretty big scale.\nNow all of a sudden you've got a model that I think was competitive with Llama. Llama also kind of fell by the wayside. So now you're in a world where China is really leading on these open weight models that anyone can use, which is going to be a pretty big deal, because if you have a choice between paying for something or it being free, a lot of people are going to go for the free version, and these models are going to really inform a lot of how people are using AI in the next two, three, five, 10 years. And I think we're going to see a lot of influence from Chinese models doing that.\nZo\u00eb Schiffer: Yeah. And we should say before we get into this more that open-weight means that the weights of the model are published. So anyone can download the model on a personal device and they can modify it. You can't really do that with ChatGPT, but you could do it with DeepSeek. You could get an understanding of how it works and you could tailor it to your liking. The reason that this is attractive to AI firms is that instead of just having your, let's say, 300 researchers and developers working on the model, you release an open-weight model and all of a sudden everyone in the world who's tinkering with that model could be making improvements that you can then take, co-opt, and improve the model yourself. So you get access theoretically to a research community that's much, much bigger than the one that you have. And this is a huge advantage for China because they're really going hard on open source AI, open-weight AI, and it's allowing them to advance really, really rapidly.\nWhereas in the US, we've really become more closed-source. Even Meta, like you said, one of the first firms to build advanced open source AI, has signaled that their next series of models will likely be proprietary. And so among other things, people feel like it's a strategic disadvantage. Also, we're repeating the same training runs, like we're using all of this energy and resources and research and compute to essentially do the exact same thing. Each lab is having to repeat the exact same process versus building on the insights and innovations of another lab.\nBrian Barrett: Zo\u00eb, what do you think about DeepSeek being built in China? Sometimes Chinese models are subject to certain censorship situations. I think DeepSeek has run into that as have other Chinese models. Does that limit its potential upside, or are people just kind of not going to care so much about that? I suspect the latter, but\u2014\nZo\u00eb Schiffer: I would be curious what Will Knight, one of our really amazing AI reporters at WIRED would say about this. But so far in my conversations with him, the sense I get is that people don't care. And even US firms that have been kind of championing the US race against China, behind closed doors appear to be using DeepSeek. If for no other reason than it's obviously just a lot cheaper to do, and it's really advanced and its capabilities are good. I also think there's this interesting dynamic where this debate has been playing out in politics around how to handle export controls. Do we cut off China's access to advanced GPUs and chips and so try and slow their progress, or do we give them access to these GPUs and then hopefully make them dependent on US chips? And I think when DeepSeek came out, it felt like almost a signal that cutting them off was not a good move because look, it was spurring them to advance in all of these other ways, because DeepSeek was trained in a really cheap and efficient way.\nNow we've seen the Trump administration say, \"OK, wait, they can get access to certain cutting-edge chips.\" And actually China is coming in and saying, \"Well, if you're a company operating in this country, we don't want you to be using those American chips.\" They're trying to tie Chinese AI more closely to Chinese hardware.\nMoving on from AI. So the next trend that really defined the magazine this year was the creation and the workings of the so called Department of Government Efficiency or DOGE. Where to even begin, Brian?\nBrian Barrett: Yeah.\nZo\u00eb Schiffer: This is the story that kept on giving, and for good reason. We recently learned that members of the group are still working, largely we believe unsupervised, across the federal government. So I feel like it's as good a time as any to take stock as to why DOGE remained so important this year.\nBrian Barrett: I just this week was looking back at some of our earliest DOGE reporting and was reminded of what a crazy couple of months that was. So just as a reminder for folks who were slumbering through the first half of the year: I'm jealous. I respect that.\nZo\u00eb Schiffer: I was just going to say, good for you.\nBrian Barrett: The Department of Government Efficiency came about when Elon Musk and Donald Trump got together, and basically Trump gave Elon Musk kind of free rein to do whatever he wanted, and I'm not really exaggerating here, within the federal government. So Musk allies took over various government agencies, including the Office of Personnel Management, which is sort of the human resources for the whole government, the General Services Administration, which is its tech IT department, basically. And from there, kind of fanned out across agencies and were responsible for a lot of the chaos that we saw in this early administration, massive job cuts, massive cuts to USAID, regulations being slashed, not always for good, having everyone in the federal government having to write an email with five points of what they did that week and sending it to never be read. DOGE didn't end up doing what it set out to do.\nThe idea was to cut a trillion dollars from the budget, which you literally can't do unless you cut into entitlement programs, which (A) DOGE had no control over, and (B) would be politically untenable. So Zo\u00eb, what did they do?\nZo\u00eb Schiffer: Yeah, I think the goal ostensibly was to root out fraud and waste, to root out inefficiency.\nBrian Barrett: And abuse\u2014fraud, waste and abuse, I think.\nZo\u00eb Schiffer: Fraud, waste, and abuse. I was going to say it and I was like, I don't even know exactly what that means,\nBrian Barrett: But they didn't either. It's fine.\nZo\u00eb Schiffer: It sounded good on the surface, but it also, I think, very quickly felt like a political project for Elon Musk. And I think he said as much. It was interesting. I remember this moment when the executive order codifying and naming DOGE first came out and the way that they had been talking about it previously, it felt like it was going to be this big enormous thing. And then the language in the EEO was kind of like improve the government IT and modernize it. I don't even remember. It was very like, OK. But then when Elon talked about it subsequently, and when DOGE affiliates talked about it subsequently, the language they used was something akin to like, \"It is our job to enforce the will of the president.\" And when that's how you conceive of your job, it really does feel like there's nothing you can't do.\nI think the impact hit federal workers first, and we documented that pretty meticulously. Now I think we're kind of seeing the trickle-down effects, like a quarter of the CDC is gone at this point. I think around 300,000 federal workers are no longer in the government. USAID shutting down has led to an estimated hundreds of thousands of deaths, reportedly. So I think we're going to keep seeing the ripple effect. It does bring me to a slightly lighter note, which was Elon Musk talking about DOGE on Katie Miller's podcast recently. Stephen Miller, who's quite high up in the Trump administration, obviously, Katie Miller has worked for Elon and his companies as well as in government. And she kind of operated as the DOGE comms person when she was working in government. She now has a podcast. She sat down with Elon recently and she asked him, \u201cWould you do it all again? And do you think it was successful?\u201d\nAnd he kind of hems and haws, but ultimately what I got from it was Elon saying, \u201cI would've worked at my companies. In some ways, I should have just focused on my companies.\u201d It felt like a tacit admission that DOGE was not successful, which I think feels fairly obvious, but I also think it's not as simple as that. Of course, they didn't cut anywhere close to what they wanted to, but I do think they changed the federal government pretty profoundly.\nBrian Barrett: Yeah. Well, and not only that, I mean, Elon Musk's personal brand I think took a pretty big hit. If you sell EVs whose market is presumably people who lean a little bit more left and care more about the environment, and you become this sort of demagogical agent of destruction to the federal government, there's a distance there. Tesla sales have plummeted in Europe. They're holding out OK in the US, but he's even had to switch over to robotaxis and humanoid robots as the future. And I think to your point, those changed the government and the effects are, I think today, most acutely being felt in another unfortunately big theme of 2025, which is immigration and the immigration crackdown. A big DOGE project that has been going on since at least the summer is to combine different pockets of data from throughout the US government, whether that's Social Security data, tax data, Homeland Security data, cross reference it, pool it\u2014when it has always historically been kept separate, intentionally\u2014and use all of that combined information to surveil and track down immigrants.\nExplicitly, that's the purpose and it has really transformed, I think, how everyone's data is held in the US government, because everybody's data is now intermingled in ways that it was never supposed to be. And it has really given ICE an incredible wealth of data that again, they should not have, historically would not have had, and has given them tools to really fuel their mission here. So I think that is a consequence that we're going to be feeling indefinitely. Once you comingle all that, you can't separate it out again. So that's the world we're in now. Thank you, DOGE.\nZo\u00eb Schiffer: Yeah, that was another executive order that came out. I think it was literally titled something around eliminating data silos. Another thing that on its surface sounds good: data silos, inefficient, sounds bad, but there's a really good reason to keep a lot of this information separate. You don't necessarily want the federal government to be able to knit all of that together to track you across any platform to know your financial data, your health data, your whereabouts, all of that.\nBrian Barrett: Yeah. I mean, the Trump administration now wants to vet, what is it, five years of social media if you're coming into the country just to see what you've been saying. And also they're looking to strip more foreign-born Americans of citizenship. They're moving towards denaturalization. The predictions for 2026 and immigration aren't great. It's that it's going to get worse and more. Or let's say if you were an engineer born in a different country and you had an option of going to the US or Canada or China or somewhere else in Europe, it's a lot harder question to answer now than it was before all this happened, I think.\nZo\u00eb Schiffer: Yeah, I think that's exactly right. And we're going to have, I think, some really good reporting on that in 2026. So we're nearing the end of our 2025 recap, and we would be remiss not to mention the Jeffrey Epstein saga. So this is a story that has had so many lives across many, many years, but it felt like it really came to a boiling point this year, politically speaking. So Brian, I guess, can you just refresh our memory on what even happened this year?\nBrian Barrett: I get the easy job. Without going into the whole Jeffrey Epstein everything for so many reasons. Obviously he's a disgraced financier, convicted sex offender, also the locus of so many conspiracy theories from QAnon up to the White House and its supporters. What we saw this year was a real movement towards releasing the so-called Epstein files, which Donald Trump basically ran on. He said, \u201cLook, if I get in that office, I'm going to release the Epstein files. You're going to have them right as soon as I get there.\u201d And then we got to that point and he got there and he said, \u201cYou know what? Never mind.\u201d And there was people saying there are no Epstein files. Actually, the Epstein files are a hoax. Actually, no, they are here. It has been a mess.\nThe upshot of all this, maybe by the time this podcast airs, the Epstein files will be released or maybe sometime after\u2014there's a deadline for Friday, December 19th, that has been set by Congress. So we'll see what happens, what's actually in there. Personally, I think it's just going to add more fuel to various conspiratorial fires. I think the people who are fans of Donald Trump are going to find ways to decide that, oh no, he was actually an FBI informant the whole time. I think people who are not are going to have a more rational and sane view of whatever it is. I understand why people want to know what's in there. I'm also excited for everyone to get it and then move on.\nZo\u00eb Schiffer: I love that you think we're going to move on. I don't think that's going to happen.\nBrian Barrett: Oh, I just said I'd love to. I didn't say we would. I said it's a beautiful dream.\nZo\u00eb Schiffer: Yes, we would love to. I know you don't like to speculate, Brian. Obviously you're a reporter at heart, but I am genuinely curious, what do you think was going on here with Trump? Do you think that he was saying what he needed to say on the campaign trial and just wasn't thinking too far ahead in terms of what happens if I win and then people still want these?\nBrian Barrett: So are you suggesting that our president did not think through consequences?\nZo\u00eb Schiffer: I would never say that. I mean, explicitly outright. I'm just genuinely curious what was happening. What did you think was going to happen?\nBrian Barrett: I think that he says what he needs to say in the moment, and he knows that that was a thing that got his base stirred up, and then the check came due and it's actually caused real problems, which is rare for him, because usually he's able to sort of work around it. But it appears to have basically fractured his relationship with Marjorie Taylor Green, who was one of his biggest supporters, but this was a real dividing line for her. Again, people will justify what Donald Trump says and does all day long, but I do think there are a significant number of MAGA adherents for whom this is a real wedge issue. Depending on what happens when the files get released and the aftermath of that could actually impact the midterms next year.\nZo\u00eb Schiffer: OK. We have to talk about the video. We can't not.\nBrian Barrett: Yes.\nZo\u00eb Schiffer: Pam Bondi, Trump, they had talked about releasing the files and obviously then they win the election, they're in office. It appears they do not want to release the files, but they do release a video of Jeffrey Epstein's final moments in jail. And then, I mean, I hate to pat ourselves on the back, but we become pretty central to the story, Brian. No?\nBrian Barrett: We did. We broke the story. So the DOJ published a video. And the weirdest part about this was they said, \u201cLook, here's the unedited video.\u201d They went out of their way to say that this is an unedited video. And so we took a look at it and it took us about 30 seconds to see that, oh no, actually it was edited. It appears to have been actually two different videos that were pieced together to form one video. There's about two and a half minutes that are missing. I don't want to say they're unaccounted for, necessarily. What's weird about this is a lot of things could potentially explain it. I don't want to get too conspiratorial about it because it could be the kind of thing where, oh, the system changes over tapes at midnight and that's around when this happens. So we took the first tape and then we took the tape when it changed over and there's some overlap. There's a lot of technical explanations that could make this make sense, but at the end of the day, no one has given us those explanations.\nThey went out of their way to misrepresent what they had released and there's two and a half minutes that they didn't include. Again, I'm not saying there's a conspiracy here. I am saying that they are presenting conspiracy vibes. They are doing things that one would do if you were in the middle of a big conspiracy. I think there's probably a rational explanation for it. It's more the way that they have dealt with it is only fueling this fire.\nZo\u00eb Schiffer: Right. To quote Elon Musk, sunlight is the best disinfectant.\nBrian Barrett: And so it's again, why I work in the shadows.\nZo\u00eb Schiffer: Exactly. Some of the files were already approved to be released by Congress last month. So I think that was roughly 20,000 documents, and they're already kind of being dismissed by the MAGA base.\nBrian Barrett: Yeah. There's a lot of people who are just ignoring them or justifying them. A lot of Democrats in Congress have been releasing images of Jeffrey Epstein's island, a bunch of email exchanges and text message exchanges. Some of them mentioned Donald Trump explicitly. Larry Summers is in there, Bill Gates is in there a lot, but nothing has stuck. And I think Jeffrey Epstein, horrible guy, is seemingly smart enough not to literally write down, \u201cHey, do you want to do a sex crime with me?\u201d\nSo what you have is a lot of connections and innuendo and insinuations. I think where we get to an interesting point is in this new batch, if there are more concrete financial relationships, because the numbers don't really lie. If there is more concrete video, photographic evidence, what is in there that would actually get people to be on board with this? I don't know if there's anything. I think that for MAGA die-hards, I think that you get to a point where it's like, \u201cWell, that was just forged.\u201d That's the deep state. That's AI, to continue our year of AI. So I don't think anything is going to really turn the tide on it.\nZo\u00eb Schiffer: Yeah, I think that's right. I feel like we don't have a shared understanding of the truth anymore. And so this is really catering to people who for the most part already have some level of belief or don't like Donald Trump or don't like the people who are being named in these files. And then there are other people who it really doesn't feel like there's anything. I will say, he's not literally explicitly talking about crimes in the emails we've seen so far, but he named his plane, The Lolita. I'm like, \"This man wasn't hiding.\u201d\nBrian Barrett: No, absolutely. And the photos are creepy, if nothing else, right? Appropriate for 2025. I feel like it's the right note to go out on for this year.\nZo\u00eb Schiffer: I know. I was going to say, I wish we could end on a different note, but I feel like this was this year. This year had a lot in it, and boy, was it tough? We are ready to say goodbye. So Brian, thank you so much for joining me today.\nBrian Barrett: Zo\u00eb, thank you for having me.\nZo\u00eb Schiffer: That's our show for today. We'll link to all the stories we spoke about in the show notes. Make sure to check out Thursday's episode of Uncanny Valley, where you'll get to hear some news from Mike and Lauren about the show. I'll leave you with a hint that you'll be hearing a lot more from me, Brian, and our colleague Leah Feiger next year.\nBrian Barrett: Which feels like more than a hint, but\u2014\nZo\u00eb Schiffer: A little spoiler.\nBrian Barrett: Little spoiler.\nZo\u00eb Schiffer: A little tease.\nBrian Barrett: Tiny smile.\nZo\u00eb Schiffer: Adriana Tapia and Mark Leyda produced this episode. Amar Lal at Macro Sound mixed this episode. Kate Osborn is our executive producer, and Katie Drummond is WIRED's global editorial director.","source":"wired.com"}
{"url":"https:\/\/techcrunch.com\/2025\/12\/24\/waymo-explains-why-its-robotaxis-got-stuck-during-the-sf-blackout\/","title":"Waymo explains why its robotaxis got stuck during the SF blackout | TechCrunch","date":1766534400000,"text":"Waymo is shipping a software update to help its robotaxis navigate disabled traffic lights during power outages \u201cmore decisively,\u201d the company said Tuesday in a blog post that explains why its self-driving vehicles got stuck at intersections during a blackout in San Francisco this past weekend.\nWaymo said the self-driving system in its robotaxis treats dead stop lights as four-way stops, just like humans are supposed to. That should have allowed the robotaxis to operate normally in spite of the massive outage.\nInstead, many of the vehicles requested a \u201cconfirmation check\u201d from Waymo\u2019s fleet response team to make sure what they were doing was correct. All Waymo robotaxis have the ability to make these confirmation checks. With such a widespread outage on Saturday, there was a \u201cconcentrated spike\u201d in these confirmation requests, Waymo said, which helped create all the congestion caught on video.\nWaymo said it built this confirmation request system \u201cout of an abundance of caution during our early deployment\u201d but that it is now refining it to \u201cmatch our current scale.\u201d\n\u201cWhile this strategy was effective during smaller outages, we are now implementing fleet-wide updates that provide the [self-driving software] with specific power outage context, allowing it to navigate more decisively,\u201d the company wrote.\nThe software update will add \u201ceven more context about regional outages\u201d to the company\u2019s self-driving software. Waymo also said it will improve its emergency response protocols by \u201cincorporating lessons from this event.\u201d\nWhile a lot of focus has been placed on the instances where Waymo\u2019s robotaxis got stuck during the power outage, the company shared that its vehicles \u201csuccessfully traversed more than 7,000 dark signals on Saturday.\u201d\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\nJoin the Disrupt 2026 Waitlist\nAdd yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages \u2014 part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n\u201cNavigating an event of this magnitude presented a unique challenge for autonomous technology,\u201d the company wrote.\nSaturday\u2019s mess is the latest example of how Waymo is still uncovering unforeseen issues with its software and its approach to designing a reliable fleet of self-driving vehicles. The company already had to ship multiple software updates to make its robotaxis wait for stopped school buses, which prompted a National Highway Traffic Safety Administration investigation and led to a recall.","source":"techcrunch.com"}
